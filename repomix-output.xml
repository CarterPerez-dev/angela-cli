This file is a merged representation of a subset of the codebase, containing files not matching ignore patterns, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
4. Repository files, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching these patterns are excluded: **/__pycache__/**, **/.git/**, **/node_modules/**, **/.venv/**, **/target/**, **/dist/**, **/build/**, **/tests/**
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

<additional_info>

</additional_info>

</file_summary>

<directory_structure>
angela/
  ai/
    analyzer.py
    client.py
    confidence.py
    content_analyzer_extensions.py
    content_analyzer.py
    file_integration.py
    intent_analyzer.py
    parser.py
    prompts_update.py
    prompts.py
  cli/
    __init__.py
    files_extensions.py
    files.py
    generation.py
    main.py
    workflows.py
  context/
    __init__.py
    enhancer.py
    file_activity.py
    file_detector.py
    file_resolver.py
    history.py
    manager.py
    preferences.py
    project_inference.py
    session.py
  execution/
    adaptive_engine.py
    engine.py
    error_recovery.py
    filesystem.py
    hooks.py
    rollback.py
  generation/
    architecture.py
    documentation.py
    engine.py
    frameworks.py
    planner.py
    validators.py
  integrations/
    integrations5.py
    integrations6.py
  intent/
    advanced_planner.py
    models.py
    planner.py
  monitoring/
    __init__.py
    background.py
    network_monitor.py
  review/
    diff_manager.py
    feedback.py
  safety/
    __init__.py
    adaptive_confirmation.py
    classifier.py
    confirmation.py
    preview.py
    validator.py
  shell/
    angela.bash
    angela.zsh
    formatter.py
  toolchain/
    ci_cd.py
    git.py
    package_managers.py
  utils/
    __init__.py
    logging.py
  workflows/
    __init__.py
    manager.py
    sharing.py
  __init__.py
  __main__.py
  cli.py
  config.py
  constants.py
  orchestrator.py
MD/
  context.md
  Info.md
  NextSteps.md
  Phase1.md
  Phase2.md
  Phase3.md
  Phase4.md
  Phase5.md
  Phase6.md
  Phase7.md
  tree.md
scripts/
  install.sh
  uninstall.sh
.env.example
.gitignore
Makefile
pyproject.toml
pytest.ini
README.md
requirements.txt
setup.py
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="angela/ai/analyzer.py">
# angela/ai/analyzer.py

import re
import os
import sys
import shlex
from pathlib import Path
from typing import Dict, Any, Optional, List, Tuple, Union

from angela.context.history import history_manager
from angela.utils.logging import get_logger

logger = get_logger(__name__)

class ErrorAnalyzer:
    """Analyzer for command errors with fix suggestions."""
    
    # Common error patterns and their explanations/fixes
    ERROR_PATTERNS = [
        # File not found
        (r'No such file or directory', 'The specified file or directory does not exist', [
            'Check if the path is correct',
            'Use ls to view available files',
            'Use find to search for the file'
        ]),
        # Permission denied
        (r'Permission denied', 'You don\'t have sufficient permissions', [
            'Check file permissions with ls -l',
            'Use sudo for operations requiring elevated privileges',
            'Change permissions with chmod'
        ]),
        # Command not found
        (r'command not found', 'The command is not installed or not in PATH', [
            'Check if the command is installed',
            'Install the package containing the command',
            'Check your PATH environment variable'
        ]),
        # Syntax errors
        (r'syntax error', 'There\'s a syntax error in the command', [
            'Check for missing quotes or brackets',
            'Check the command documentation for correct syntax',
            'Simplify the command and try again'
        ]),
        # Network errors
        (r'(Connection refused|Network is unreachable)', 'Network connection issue', [
            'Check if the host is reachable',
            'Verify network connectivity',
            'Check if the service is running on the target host'
        ])
    ]
    
    def __init__(self):
        """Initialize the error analyzer."""
        self._logger = logger
    
    def analyze_error(self, command: str, error: str) -> Dict[str, Any]:
        """
        Analyze a command error and provide fix suggestions.
        
        Args:
            command: The failed command
            error: The error output
            
        Returns:
            Dictionary with analysis and suggestions
        """
        self._logger.debug(f"Analyzing error for command: {command}")
        
        # Extract the important parts of the error
        error_short = self._extract_key_error(error)
        
        # Check for known error patterns
        pattern_match = self._match_error_pattern(error)
        
        # Check command history for similar errors and their fixes
        historical_fixes = history_manager.find_error_patterns(error_short)
        
        # Analyze command structure for potential issues
        command_issues = self._analyze_command_structure(command)
        
        # Check for missing files or directories
        file_issues = self._check_file_references(command, error)
        
        # Build the response
        result = {
            "error_summary": error_short,
            "possible_cause": pattern_match[1] if pattern_match else "Unknown error",
            "fix_suggestions": pattern_match[2] if pattern_match else [],
            "historical_fixes": [fix for _, fix in historical_fixes],
            "command_issues": command_issues,
            "file_issues": file_issues
        }
        
        return result
    
    def _extract_key_error(self, error: str) -> str:
        """
        Extract the key part of an error message.
        
        Args:
            error: The full error output
            
        Returns:
            A shorter, more focused error message
        """
        # Split by lines and remove empty ones
        lines = [line.strip() for line in error.splitlines() if line.strip()]
        
        if not lines:
            return "Unknown error"
        
        # Check for common error patterns in the first few lines
        for line in lines[:3]:
            # Look for lines with "error" or "ERROR"
            if "error" in line.lower():
                return line
            
            # Look for lines with common error indicators
            for pattern, _, _ in self.ERROR_PATTERNS:
                if re.search(pattern, line, re.IGNORECASE):
                    return line
        
        # If no clear error pattern is found, return the first line
        return lines[0]
    
    def _match_error_pattern(self, error: str) -> Optional[Tuple[str, str, List[str]]]:
        """
        Match an error against known patterns.
        
        Args:
            error: The error output
            
        Returns:
            Tuple of (pattern, explanation, fixes) or None if no match
        """
        for pattern, explanation, fixes in self.ERROR_PATTERNS:
            if re.search(pattern, error, re.IGNORECASE):
                return (pattern, explanation, fixes)
        
        return None
    
    def _analyze_command_structure(self, command: str) -> List[str]:
        """
        Analyze command structure for potential issues.
        
        Args:
            command: The command string
            
        Returns:
            List of potential issues
        """
        issues = []
        
        # Check if the command is empty
        if not command.strip():
            issues.append("Command is empty")
            return issues
        
        try:
            # Try to parse the command with shlex
            tokens = shlex.split(command)
            
            # Check for basic command structure
            if not tokens:
                issues.append("Command parsing failed")
                return issues
            
            base_cmd = tokens[0]
            
            # Check for redirect without command
            if base_cmd in ['>', '>>', '<']:
                issues.append("Redirect symbol used as command")
            
            # Check for pipe without command
            if base_cmd == '|':
                issues.append("Pipe symbol used as command")
            
            # Check for unbalanced quotes (shlex would have raised an error)
            
            # Check for missing arguments in common commands
            if len(tokens) == 1:
                if base_cmd in ['cp', 'mv', 'ln']:
                    issues.append(f"{base_cmd} requires source and destination arguments")
                elif base_cmd in ['grep', 'sed', 'awk']:
                    issues.append(f"{base_cmd} requires a pattern and input")
            
            # Check for potentially incorrect flag formats
            for token in tokens[1:]:
                if token.startswith('-') and len(token) > 2 and not token.startswith('--'):
                    # Might be combining single-letter flags incorrectly
                    if any(not c.isalpha() for c in token[1:]):
                        issues.append(f"Potentially malformed flag: {token}")
            
        except ValueError as e:
            # This typically happens with unbalanced quotes
            issues.append(f"Command parsing error: {str(e)}")
        
        return issues
    
    def _check_file_references(self, command: str, error: str) -> List[Dict[str, Any]]:
        """
        Check for file references in the command that might be causing issues.
        
        Args:
            command: The command string
            error: The error output
            
        Returns:
            List of file issues
        """
        issues = []
        
        # Extract potential file paths from the command
        try:
            tokens = shlex.split(command)
            
            # Skip the command name
            potential_paths = []
            for token in tokens[1:]:
                # Skip options
                if token.startswith('-'):
                    continue
                
                # Skip operators
                if token in ['|', '>', '>>', '<', '&&', '||', ';']:
                    continue
                
                # Consider as potential path
                potential_paths.append(token)
            
            # Check if the paths exist
            for path_str in potential_paths:
                path = Path(path_str)
                
                # Only check if it looks like a path
                if '/' in path_str or '.' in path_str:
                    issue = {"path": path_str}
                    
                    if not path.exists():
                        issue["exists"] = False
                        issue["suggestion"] = f"File/directory does not exist: {path_str}"
                        
                        # Check for common typos
                        parent = path.parent
                        if parent.exists():
                            # Check for similar files in the same directory
                            similar_files = []
                            try:
                                for p in parent.iterdir():
                                    # Simple similarity: Levenshtein distance approximation
                                    if p.name.startswith(path.name[:2]) or p.name.endswith(path.name[-2:]):
                                        similar_files.append(p.name)
                            except (PermissionError, OSError):
                                pass
                            
                            if similar_files:
                                issue["similar_files"] = similar_files[:3]  # Limit to top 3
                        
                        issues.append(issue)
                    else:
                        # Check for permission issues
                        issue["exists"] = True
                        if "Permission denied" in error and not os.access(path, os.R_OK):
                            issue["permission"] = False
                            issue["suggestion"] = f"Permission denied for: {path_str}"
                            issues.append(issue)
        
        except Exception as e:
            logger.exception(f"Error checking file references: {str(e)}")
        
        return issues
    
    def generate_fix_suggestions(self, command: str, error: str) -> List[str]:
        """
        Generate fix suggestions for a failed command.
        
        Args:
            command: The failed command
            error: The error output
            
        Returns:
            List of suggested fixes
        """
        # Analyze the error
        analysis = self.analyze_error(command, error)
        
        # Combine all suggestions
        suggestions = []
        
        # Add suggestions from pattern matching
        suggestions.extend(analysis["fix_suggestions"])
        
        # Add suggestions from historical fixes
        if analysis["historical_fixes"]:
            for i, fix in enumerate(analysis["historical_fixes"], 1):
                suggestions.append(f"Previous fix: {fix}")
        
        # Add suggestions from command issues
        for issue in analysis["command_issues"]:
            if issue == "Command parsing failed":
                suggestions.append("Check for unbalanced quotes or special characters")
            elif "requires" in issue:
                suggestions.append(issue)  # Already a suggestion
            elif "flag" in issue:
                suggestions.append("Check command flags format")
        
        # Add suggestions from file issues
        for issue in analysis["file_issues"]:
            if "suggestion" in issue:
                suggestions.append(issue["suggestion"])
            
            if "similar_files" in issue:
                similar = ", ".join(issue["similar_files"])
                suggestions.append(f"Did you mean one of these: {similar}?")
        
        # Deduplicate suggestions
        unique_suggestions = []
        seen = set()
        for suggestion in suggestions:
            suggestion_key = suggestion.lower()
            if suggestion_key not in seen:
                seen.add(suggestion_key)
                unique_suggestions.append(suggestion)
        
        return unique_suggestions

# Global error analyzer instance
error_analyzer = ErrorAnalyzer()
</file>

<file path="angela/ai/confidence.py">
# angela/ai/confidence.py

import re
from typing import Dict, Any, List, Tuple, Optional

from angela.context.history import history_manager
from angela.utils.logging import get_logger

logger = get_logger(__name__)

class ConfidenceScorer:
    """
    System for scoring confidence in natural language understanding
    and command suggestions.
    """
    
    def __init__(self):
        """Initialize the confidence scorer."""
        self._logger = logger
    
    def score_command_confidence(
        self, 
        request: str, 
        command: str, 
        context: Dict[str, Any]
    ) -> float:
        """
        Score confidence in a command suggestion.
        
        Args:
            request: The original request
            command: The suggested command
            context: Context information
            
        Returns:
            Confidence score (0.0-1.0)
        """
        # Base confidence starts at 0.7 (moderate default)
        confidence = 0.7
        
        # 1. Check if similar commands have been used before
        historical_confidence = self._check_history(command)
        
        # 2. Check command complexity vs. request complexity
        complexity_confidence = self._check_complexity(request, command)
        
        # 3. Check for entity matches
        entity_confidence = self._check_entities(request, command, context)
        
        # 4. Check for flags/options that seem out of place
        flags_confidence = self._check_command_flags(command)
        
        # Combine all factors (with weights)
        confidence = (
            0.3 * historical_confidence + 
            0.3 * complexity_confidence + 
            0.3 * entity_confidence + 
            0.1 * flags_confidence
        )
        
        # Ensure we stay in valid range
        confidence = min(1.0, max(0.0, confidence))
        
        self._logger.debug(f"Command confidence: {confidence:.2f} (hist: {historical_confidence:.2f}, " 
                          f"comp: {complexity_confidence:.2f}, ent: {entity_confidence:.2f}, " 
                          f"flags: {flags_confidence:.2f})")
        
        return confidence
    
    def _check_history(self, command: str) -> float:
        """
        Check if similar commands have been used before.
        
        Args:
            command: The suggested command
            
        Returns:
            Confidence score component (0.0-1.0)
        """
        # Extract the base command (first word)
        base_command = command.split()[0] if command else ""
        
        # Get frequency of this base command
        frequency = history_manager.get_command_frequency(base_command)
        
        # Get success rate
        success_rate = history_manager.get_command_success_rate(base_command)
        
        # Calculate confidence based on frequency and success rate
        if frequency == 0:
            return 0.5  # Neutral for new commands
            
        # Scale based on frequency (up to 10 uses)
        frequency_factor = min(frequency / 10.0, 1.0)
        
        # Combine with success rate
        return 0.5 + (0.5 * frequency_factor * success_rate)
    
    def _check_complexity(self, request: str, command: str) -> float:
        """
        Check if command complexity matches request complexity.
        
        Args:
            request: The original request
            command: The suggested command
            
        Returns:
            Confidence score component (0.0-1.0)
        """
        # Simple heuristic: count tokens in request and command
        request_tokens = len(request.split())
        command_tokens = len(command.split())
        
        # Very simple requests should lead to simple commands
        if request_tokens <= 3 and command_tokens > 10:
            return 0.4  # Low confidence for complex command from simple request
            
        # Complex requests might lead to complex commands
        if request_tokens >= 10 and command_tokens <= 3:
            return 0.6  # Moderate confidence for simple command from complex request
            
        # Ideal ratio is roughly 1:1 to 1:2
        ratio = command_tokens / max(1, request_tokens)
        if 0.5 <= ratio <= 2.0:
            return 0.9  # High confidence when complexity matches
        elif 0.25 <= ratio <= 4.0:
            return 0.7  # Moderate confidence for reasonable mismatch
        else:
            return 0.5  # Low confidence for significant mismatch
    
    def _check_entities(
        self, 
        request: str, 
        command: str, 
        context: Dict[str, Any]
    ) -> float:
        """
        Check if entities in the request match those in the command.
        
        Args:
            request: The original request
            command: The suggested command
            context: Context information
            
        Returns:
            Confidence score component (0.0-1.0)
        """
        # Extract potential entities from request (simple approach)
        request_words = set(request.lower().split())
        
        # Check for important entities
        file_mentions = any(word in request_words for word in ["file", "files", "document", "text"])
        dir_mentions = any(word in request_words for word in ["directory", "folder", "dir"])
        
        # Check if command matches the entity types mentioned
        if file_mentions and not any(ext in command for ext in [".txt", ".md", ".py", ".js", ".html"]):
            return 0.5  # Request mentions files but command doesn't seem to deal with files
            
        if dir_mentions and not any(cmd in command for cmd in ["cd", "mkdir", "rmdir", "ls"]):
            return 0.6  # Request mentions directories but command doesn't seem to deal with directories
        
        # Check for specific paths or filenames
        # This is a simplified approach - real implementation would use regex
        path_pattern = r'[\w/\.-]+'
        request_paths = re.findall(path_pattern, request)
        command_paths = re.findall(path_pattern, command)
        
        if request_paths and not any(rp in command for rp in request_paths):
            return 0.7  # Paths mentioned in request don't appear in command
        
        # Default - reasonable confidence
        return 0.8
    
    def _check_command_flags(self, command: str) -> float:
        """
        Check for unusual flag combinations or invalid options.
        
        Args:
            command: The suggested command
            
        Returns:
            Confidence score component (0.0-1.0)
        """
        # This would ideally have a database of valid flags for common commands
        # For now, just do some basic checks
        
        # Check for potentially conflicting flags
        if "-r" in command and "--no-recursive" in command:
            return 0.3  # Conflicting flags
            
        if "-f" in command and "--interactive" in command:
            return 0.4  # Potentially conflicting (force vs. interactive)
        
        # Check for unusual combinations
        if "rm" in command and "-p" in command:
            return 0.5  # Unusual flag for rm
            
        if "cp" in command and "-l" in command:
            return 0.6  # Unusual flag for cp
        
        # Default - high confidence
        return 0.9

# Global confidence scorer instance
confidence_scorer = ConfidenceScorer()
</file>

<file path="angela/ai/content_analyzer_extensions.py">
# angela/ai/content_analyzer_extensions.py

from typing import Dict, Any, Union, Optional
from pathlib import Path
import re
import json

from angela.ai.content_analyzer import ContentAnalyzer, content_analyzer
from angela.utils.logging import get_logger

logger = get_logger(__name__)

class EnhancedContentAnalyzer(ContentAnalyzer):
    """Extended content analyzer with support for additional file types and languages."""
    
    # Language-specific analysis handlers 
    LANGUAGE_HANDLERS = {
        # Existing handlers
        "Python": "_analyze_python",
        "JavaScript": "_analyze_javascript",
        "HTML": "_analyze_html",
        "CSS": "_analyze_css",
        
        # New handlers
        "TypeScript": "_analyze_typescript",
        "Java": "_analyze_java",
        "Rust": "_analyze_rust",
        "Go": "_analyze_go",
        "Ruby": "_analyze_ruby",
        "PHP": "_analyze_php",
        "C": "_analyze_c",
        "CPP": "_analyze_cpp",
        "CSharp": "_analyze_csharp",
        "Swift": "_analyze_swift",
        "Kotlin": "_analyze_kotlin",
        
        # Data formats
        "JSON": "_analyze_json",
        "YAML": "_analyze_yaml",
        "XML": "_analyze_xml",
        "CSV": "_analyze_csv",
        
        # Config files
        "Dockerfile": "_analyze_dockerfile",
        "Makefile": "_analyze_makefile",
    }
    
    async def analyze_content(
        self, 
        file_path: Union[str, Path], 
        request: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Enhanced analyze_content that routes to language-specific handlers.
        """
        path_obj = Path(file_path)
        
        # Check if file exists
        if not path_obj.exists():
            return {"error": f"File not found: {path_obj}"}
        
        # Get file info for language-specific handling
        file_info = self._get_file_info(path_obj)
        language = file_info.get("language")
        
        # Read file content
        try:
            with open(path_obj, 'r', encoding='utf-8', errors='replace') as f:
                content = f.read()
        except Exception as e:
            logger.error(f"Error reading file: {str(e)}")
            return {"error": f"Error reading file: {str(e)}"}
        
        # Use language-specific handler if available
        handler_name = self.LANGUAGE_HANDLERS.get(language)
        if handler_name and hasattr(self, handler_name):
            handler = getattr(self, handler_name)
            return await handler(path_obj, content, file_info, request)
        
        # Fall back to generic analysis
        return await super().analyze_content(file_path, request)
    
    # Language-specific analysis methods
    
    async def _analyze_typescript(
        self, 
        path: Path, 
        content: str, 
        file_info: Dict[str, Any], 
        request: Optional[str]
    ) -> Dict[str, Any]:
        """TypeScript-specific analysis."""
        logger.debug(f"Analyzing TypeScript file: {path}")
        
        # TypeScript-specific analysis prompt
        prompt = f"""
Analyze the following TypeScript code with a focus on:
- Types and interfaces defined
- Function signatures and return types
- React component structure (if present)
- Type safety issues
- Potential type improvements

```typescript
{content[:50000]}
Analysis:
"""
# Call AI for analysis
response = await self._get_ai_analysis(prompt)
    # Extract important types and interfaces
    types_and_interfaces = self._extract_typescript_types(content)
    
    # Structure the analysis results
    result = {
        "path": str(path),
        "type": file_info.get("type", "unknown"),
        "language": "TypeScript",
        "analysis": response,
        "types_and_interfaces": types_and_interfaces,
        "request": request
    }
    
    return result

def _extract_typescript_types(self, content: str) -> List[Dict[str, Any]]:
    """Extract TypeScript types and interfaces from content."""
    types_and_interfaces = []
    
    # Simple regex to find interface and type definitions
    interface_pattern = r'interface\s+(\w+)(?:<[\w\s,]+>)?\s*{([^}]*)}'
    type_pattern = r'type\s+(\w+)(?:<[\w\s,]+>)?\s*=\s*([^;]*);'
    
    # Find interfaces
    for match in re.finditer(interface_pattern, content):
        name = match.group(1)
        body = match.group(2).strip()
        types_and_interfaces.append({
            "kind": "interface",
            "name": name,
            "definition": body
        })
    
    # Find types
    for match in re.finditer(type_pattern, content):
        name = match.group(1)
        definition = match.group(2).strip()
        types_and_interfaces.append({
            "kind": "type",
            "name": name,
            "definition": definition
        })
    
    return types_and_interfaces

# Add more language-specific methods as needed... IMPORTANT

async def _analyze_json(
    self, 
    path: Path, 
    content: str, 
    file_info: Dict[str, Any], 
    request: Optional[str]
) -> Dict[str, Any]:
    """JSON-specific analysis."""
    logger.debug(f"Analyzing JSON file: {path}")
    
    # Validate JSON and extract schema information
    try:
        json_data = json.loads(content)
        # Infer schema structure
        schema = self._infer_json_schema(json_data)
        
        # Generate human-readable analysis
        prompt = f"""
Analyze the following JSON data structure:
{content[:5000]}
Focus on:

The overall structure and purpose
Key fields and their meaning
Potential issues or inconsistencies
Schema validation suggestions

Analysis:
"""
response = await self._get_ai_analysis(prompt)
        return {
            "path": str(path),
            "type": "JSON",
            "language": "JSON",
            "valid": True,
            "schema": schema,
            "analysis": response,
            "request": request
        }
        
    except json.JSONDecodeError as e:
        # Invalid JSON
        return {
            "path": str(path),
            "type": "JSON",
            "language": "JSON",
            "valid": False,
            "error": f"Invalid JSON: {str(e)}",
            "error_position": {"line": e.lineno, "column": e.colno},
            "request": request
        }

def _infer_json_schema(self, data: Any, depth: int = 0) -> Dict[str, Any]:
    """Infer a basic JSON schema from data."""
    if depth > 5:  # Prevent infinite recursion
        return {"type": "unknown"}
        
    if data is None:
        return {"type": "null"}
    elif isinstance(data, bool):
        return {"type": "boolean"}
    elif isinstance(data, int):
        return {"type": "integer"}
    elif isinstance(data, float):
        return {"type": "number"}
    elif isinstance(data, str):
        return {"type": "string"}
    elif isinstance(data, list):
        if not data:
            return {"type": "array", "items": {}}
        # Sample the first few items to infer element type
        sample_items = data[:min(5, len(data))]
        item_schemas = [self._infer_json_schema(item, depth + 1) for item in sample_items]
        return {"type": "array", "items": item_schemas[0]}  # Simplification: use first item's schema
    elif isinstance(data, dict):
        properties = {}
        for key, value in data.items():
            properties[key] = self._infer_json_schema(value, depth + 1)
        return {"type": "object", "properties": properties}
    else:
        return {"type": "unknown"}

# Helper methods for AI analysis

async def _get_ai_analysis(self, prompt: str) -> str:
    """Get AI analysis using the Gemini API."""
    from angela.ai.client import gemini_client, GeminiRequest
    
    # Call AI service
    api_request = GeminiRequest(
        prompt=prompt,
        max_tokens=4000
    )
    
    response = await gemini_client.generate_text(api_request)
    return response.text
</file>

<file path="angela/ai/content_analyzer.py">
"""
File content analysis and manipulation for Angela CLI.

This module provides AI-powered capabilities for understanding and
manipulating file contents based on natural language requests.
"""
import os
import re
import difflib
from pathlib import Path
from typing import Dict, Any, List, Tuple, Optional, Union

from angela.ai.client import gemini_client, GeminiRequest
from angela.context import context_manager
from angela.context.file_detector import detect_file_type
from angela.utils.logging import get_logger

logger = get_logger(__name__)

class ContentAnalyzer:
    """
    Analyzer for file content with AI-powered understanding and manipulation.
    
    This class provides:
    1. Content understanding - extract meaning from code or text
    2. Content summarization - generate concise summaries
    3. Content manipulation - make targeted changes based on natural language requests
    4. Content search - find relevant sections or patterns
    """
    
    def __init__(self):
        """Initialize the content analyzer."""
        self._logger = logger
    
    async def analyze_content(
        self, 
        file_path: Union[str, Path], 
        request: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Analyze file content to extract meaningful information.
        
        Args:
            file_path: Path to the file to analyze
            request: Optional specific analysis request
            
        Returns:
            Dictionary with analysis results
        """
        path_obj = Path(file_path)
        
        # Check if file exists
        if not path_obj.exists():
            return {"error": f"File not found: {path_obj}"}
        
        # Get file info to determine type and appropriate analysis
        file_info = detect_file_type(path_obj)
        
        # Read file content
        try:
            with open(path_obj, 'r', encoding='utf-8', errors='replace') as f:
                content = f.read()
        except Exception as e:
            self._logger.error(f"Error reading file: {str(e)}")
            return {"error": f"Error reading file: {str(e)}"}
        
        # Generate analysis prompt based on file type and request
        prompt = self._build_analysis_prompt(content, file_info, request)
        
        # Call AI service
        api_request = GeminiRequest(
            prompt=prompt,
            max_tokens=4000
        )
        
        response = await gemini_client.generate_text(api_request)
        
        # Structure the analysis results
        result = {
            "path": str(path_obj),
            "type": file_info.get("type", "unknown"),
            "language": file_info.get("language"),
            "analysis": response.text,
            "request": request
        }
        
        return result
    
    async def summarize_content(
        self, 
        file_path: Union[str, Path], 
        max_length: int = 500
    ) -> Dict[str, Any]:
        """
        Generate a concise summary of file content.
        
        Args:
            file_path: Path to the file to summarize
            max_length: Maximum length of the summary in characters
            
        Returns:
            Dictionary with summary results
        """
        path_obj = Path(file_path)
        
        # Check if file exists
        if not path_obj.exists():
            return {"error": f"File not found: {path_obj}"}
        
        # Get file info
        file_info = detect_file_type(path_obj)
        
        # Read file content
        try:
            with open(path_obj, 'r', encoding='utf-8', errors='replace') as f:
                content = f.read()
        except Exception as e:
            self._logger.error(f"Error reading file: {str(e)}")
            return {"error": f"Error reading file: {str(e)}"}
        
        # Generate summarization prompt
        prompt = f"""
Provide a concise summary of the following {file_info.get('language', 'text')} file. 
Focus on the main purpose, structure, and key components.
Keep the summary under {max_length} characters.

```
{content[:20000]}  # Limit to first 20K chars for very large files
```

Summary:
"""
        
        # Call AI service
        api_request = GeminiRequest(
            prompt=prompt,
            max_tokens=1000
        )
        
        response = await gemini_client.generate_text(api_request)
        
        # Return the summary
        return {
            "path": str(path_obj),
            "type": file_info.get("type", "unknown"),
            "language": file_info.get("language"),
            "summary": response.text,
            "content_length": len(content)
        }
    
    async def manipulate_content(
        self, 
        file_path: Union[str, Path], 
        instruction: str
    ) -> Dict[str, Any]:
        """
        Manipulate file content based on a natural language instruction.
        
        Args:
            file_path: Path to the file to manipulate
            instruction: Natural language instruction for the manipulation
            
        Returns:
            Dictionary with manipulation results including old and new content
        """
        path_obj = Path(file_path)
        
        # Check if file exists
        if not path_obj.exists():
            return {"error": f"File not found: {path_obj}"}
        
        # Get file info
        file_info = detect_file_type(path_obj)
        
        # Check if this is a text file that can be manipulated
        if file_info.get("binary", False):
            return {"error": f"Cannot manipulate binary file: {path_obj}"}
        
        # Read current content
        try:
            with open(path_obj, 'r', encoding='utf-8', errors='replace') as f:
                original_content = f.read()
        except Exception as e:
            self._logger.error(f"Error reading file: {str(e)}")
            return {"error": f"Error reading file: {str(e)}"}
        
        # Generate manipulation prompt
        prompt = self._build_manipulation_prompt(original_content, file_info, instruction)
        
        # Call AI service
        api_request = GeminiRequest(
            prompt=prompt,
            max_tokens=20000  # Large token limit for returning the full modified content
        )
        
        response = await gemini_client.generate_text(api_request)
        
        # Extract the modified content from the response
        modified_content = self._extract_modified_content(response.text, original_content)
        
        # Generate diff
        diff = self._generate_diff(original_content, modified_content)
        
        # Return the results
        return {
            "path": str(path_obj),
            "type": file_info.get("type", "unknown"),
            "language": file_info.get("language"),
            "instruction": instruction,
            "original_content": original_content,
            "modified_content": modified_content,
            "diff": diff,
            "has_changes": original_content != modified_content
        }
    
    async def search_content(
        self,
        file_path: Union[str, Path],
        query: str,
        context_lines: int = 2
    ) -> Dict[str, Any]:
        """
        Search for relevant sections in a file based on a query.
        
        Args:
            file_path: Path to the file to search
            query: Natural language search query
            context_lines: Number of context lines to include around matches
            
        Returns:
            Dictionary with search results
        """
        path_obj = Path(file_path)
        
        # Check if file exists
        if not path_obj.exists():
            return {"error": f"File not found: {path_obj}"}
        
        # Get file info
        file_info = detect_file_type(path_obj)
        
        # Check if this is a text file that can be searched
        if file_info.get("binary", False):
            return {"error": f"Cannot search binary file: {path_obj}"}
        
        # Read content
        try:
            with open(path_obj, 'r', encoding='utf-8', errors='replace') as f:
                content = f.read()
        except Exception as e:
            self._logger.error(f"Error reading file: {str(e)}")
            return {"error": f"Error reading file: {str(e)}"}
        
        # Generate search prompt
        prompt = f"""
Search the following {file_info.get('language', 'text')} file for sections that match this query: "{query}"

For each matching section, provide:
1. Line numbers (approximate)
2. The relevant code/text section
3. A brief explanation of why it matches the query

```
{content[:50000]}  # Limit to first 50K chars for very large files
```

Search results:
"""
        
        # Call AI service
        api_request = GeminiRequest(
            prompt=prompt,
            max_tokens=4000
        )
        
        response = await gemini_client.generate_text(api_request)
        
        # Parse the search results to extract matches
        matches = self._parse_search_results(response.text, content, context_lines)
        
        # Return the results
        return {
            "path": str(path_obj),
            "type": file_info.get("type", "unknown"),
            "language": file_info.get("language"),
            "query": query,
            "matches": matches,
            "match_count": len(matches)
        }
    
    def _build_analysis_prompt(
        self, 
        content: str, 
        file_info: Dict[str, Any], 
        request: Optional[str]
    ) -> str:
        """
        Build a prompt for content analysis.
        
        Args:
            content: The file content
            file_info: Information about the file
            request: Specific analysis request
            
        Returns:
            A prompt string for the AI service
        """
        file_type = file_info.get("type", "unknown")
        language = file_info.get("language", "unknown")
        
        # Adjust analysis based on file type and language
        analysis_focus = ""
        if language == "Python":
            analysis_focus = """
- Identify main functions and classes
- Describe the overall code structure
- Note any imports and dependencies
- Identify potential bugs or code issues
- Suggest improvements or best practices
"""
        elif language == "JavaScript" or language == "TypeScript":
            analysis_focus = """
- Identify key functions and modules
- Note any imports, frameworks, or libraries used
- Analyze the code structure and patterns
- Identify potential bugs or code issues
- Suggest improvements or best practices
"""
        elif language == "HTML" or language == "CSS":
            analysis_focus = """
- Describe the document structure
- Identify key components or sections
- Note any external resources or dependencies
- Analyze accessibility and best practices
- Suggest improvements
"""
        elif file_type == "document" or language == "Markdown":
            analysis_focus = """
- Summarize the main topics and sections
- Identify key points and arguments
- Note the document structure and organization
- Analyze clarity and coherence
- Suggest improvements
"""
        elif "config" in file_type.lower() or file_type in ["JSON", "YAML", "TOML"]:
            analysis_focus = """
- Identify key configuration settings
- Explain the purpose of important parameters
- Note any environment-specific settings
- Identify potential issues or missing values
- Suggest improvements or best practices
"""
        
        # Create prompt based on request and file type
        if request:
            prompt = f"""
Analyze the following {language} file with this specific request: "{request}"

```
{content[:50000]}  # Limit to first 50K chars for very large files
```

Analysis:
"""
        else:
            prompt = f"""
Analyze the following {language} file.

{analysis_focus}

```
{content[:50000]}  # Limit to first 50K chars for very large files
```

Analysis:
"""
        
        return prompt
    
    def _build_manipulation_prompt(
        self, 
        content: str, 
        file_info: Dict[str, Any], 
        instruction: str
    ) -> str:
        """
        Build a prompt for content manipulation.
        
        Args:
            content: The original file content
            file_info: Information about the file
            instruction: Manipulation instruction
            
        Returns:
            A prompt string for the AI service
        """
        language = file_info.get("language", "unknown")
        
        prompt = f"""
You are given a {language} file and a request to modify it.

File content:
```
{content}
```

Request: {instruction}

Your task is to modify the file according to the request, maintaining the integrity, style, and purpose of the original file.
Return the ENTIRE modified content, not just the changed parts.
Only make changes that directly address the request.

Modified file content:
```
"""
        
        return prompt
    
    def _extract_modified_content(self, response: str, original_content: str) -> str:
        """
        Extract the modified content from the AI response.
        
        Args:
            response: The AI service response
            original_content: The original file content
            
        Returns:
            The modified content
        """
        # Try to extract content between ```
        match = re.search(r'```(?:.*?)\n(.*?)```', response, re.DOTALL)
        if match:
            return match.group(1)
        
        # If no code block, look for specific patterns indicating the start of content
        patterns = [
            r'Modified file content:\n(.*)',
            r'MODIFIED CONTENT:\n(.*)',
            r'Here\'s the modified content:\n(.*)',
        ]
        
        for pattern in patterns:
            match = re.search(pattern, response, re.DOTALL)
            if match:
                return match.group(1).strip()
        
        # If no clear content found, return the full response
        # (or the original content if response clearly isn't just content)
        if len(response.splitlines()) < 5 or len(response) > len(original_content) * 1.5:
            self._logger.warning("Could not clearly identify modified content, using original")
            return original_content
        
        return response.strip()
    
    def _generate_diff(self, original: str, modified: str) -> str:
        """
        Generate a diff between original and modified content.
        
        Args:
            original: The original content
            modified: The modified content
            
        Returns:
            A string with the unified diff
        """
        original_lines = original.splitlines(keepends=True)
        modified_lines = modified.splitlines(keepends=True)
        
        diff_lines = difflib.unified_diff(
            original_lines, 
            modified_lines,
            fromfile='original',
            tofile='modified',
            n=3  # Context lines
        )
        
        return ''.join(diff_lines)
    
    def _parse_search_results(
        self, 
        response: str, 
        content: str, 
        context_lines: int
    ) -> List[Dict[str, Any]]:
        """
        Parse search results from the AI response.
        
        Args:
            response: The AI service response
            content: The original file content
            context_lines: Number of context lines to include
            
        Returns:
            A list of match dictionaries
        """
        # Split content into lines for context
        content_lines = content.splitlines()
        
        # Look for patterns like "Lines 10-15" or "Line 20"
        line_patterns = [
            r'Lines? (\d+)(?:-(\d+))?',  # Standard "Line X" or "Lines X-Y"
            r'L(\d+)(?:-L(\d+))?',       # Shortened "LX" or "LX-LY"
            r'(\d+)(?:-(\d+))?\s*:',     # Line numbers with colon "X:" or "X-Y:"
        ]
        
        # Matches to return
        matches = []
        
        # Process multi-line response sections separately
        sections = re.split(r'\n\s*\n', response)
        
        for section in sections:
            # Skip empty sections
            if not section.strip():
                continue
                
            # Look for line number patterns
            line_start = None
            line_end = None
            
            for pattern in line_patterns:
                line_match = re.search(pattern, section)
                if line_match:
                    line_start = int(line_match.group(1))
                    if line_match.group(2):
                        line_end = int(line_match.group(2))
                    else:
                        line_end = line_start
                    break
            
            # If no line numbers found, continue to next section
            if line_start is None:
                continue
            
            # Extract code block if present
            code_block = None
            code_match = re.search(r'```(?:.*?)\n(.*?)```', section, re.DOTALL)
            if code_match:
                code_block = code_match.group(1)
            
            # Extract explanation
            explanation = section
            if code_block:
                explanation = re.sub(r'```(?:.*?)\n.*?```', '', explanation, flags=re.DOTALL)
            explanation = re.sub(r'Lines? \d+(?:-\d+)?:?', '', explanation, flags=re.DOTALL)
            explanation = explanation.strip()
            
            # Get context lines
            context_start = max(0, line_start - 1 - context_lines)
            context_end = min(len(content_lines) - 1, line_end - 1 + context_lines)
            
            # Get the actual content with context
            match_content = '\n'.join(content_lines[context_start:context_end + 1])
            
            # Create match entry
            match = {
                "line_start": line_start,
                "line_end": line_end,
                "context_start": context_start + 1,  # 1-indexed for display
                "context_end": context_end + 1,      # 1-indexed for display
                "content": match_content,
                "explanation": explanation
            }
            
            matches.append(match)
        
        return matches

# Global content analyzer instance
content_analyzer = ContentAnalyzer()
</file>

<file path="angela/ai/file_integration.py">
"""
Integration module for AI-powered file operations.

This module bridges the AI suggestions with actual file operations,
extracting file operations from commands and executing them safely.
"""
import re
import shlex
from pathlib import Path
from typing import Dict, Any, List, Tuple, Optional

from angela.execution.filesystem import (
    create_directory, delete_directory, create_file, read_file,
    write_file, delete_file, copy_file, move_file, FileSystemError
)
from angela.utils.logging import get_logger

logger = get_logger(__name__)

# Patterns for file operation commands
FILE_OPERATION_PATTERNS = [
    # mkdir patterns
    (r"^mkdir\s+(-p\s+)?(.+)$", "create_directory"),
    
    # rmdir and rm -r patterns
    (r"^rmdir\s+(.+)$", "delete_directory"),
    (r"^rm\s+(-r|-rf|--recursive)\s+(.+)$", "delete_directory"),
    
    # touch patterns
    (r"^touch\s+(.+)$", "create_file"),
    
    # cat/less/more patterns (read)
    (r"^(cat|less|more|head|tail)\s+(.+)$", "read_file"),
    
    # echo > patterns (write)
    (r"^echo\s+(.+)\s+>\s+(.+)$", "write_file"),
    (r"^echo\s+(.+)\s+>>\s+(.+)$", "append_file"),
    
    # rm patterns
    (r"^rm\s+(?!-r|--recursive)(.+)$", "delete_file"),
    
    # cp patterns
    (r"^cp\s+(?!-r|--recursive)(.+?)\s+(.+)$", "copy_file"),
    
    # mv patterns
    (r"^mv\s+(.+?)\s+(.+)$", "move_file"),
]

# Specific operation extractors
async def extract_mkdir_operation(command: str) -> Tuple[str, Dict[str, Any]]:
    """Extract mkdir operation parameters from a command."""
    tokens = shlex.split(command)
    
    # Check for -p/--parents flag
    parents = "-p" in tokens or "--parents" in tokens
    
    # Get directory paths
    paths = []
    for arg in tokens[1:]:
        if not arg.startswith("-"):
            paths.append(arg)
    
    return "create_directory", {"path": paths[0] if paths else ".", "parents": parents}


async def extract_rmdir_operation(command: str) -> Tuple[str, Dict[str, Any]]:
    """Extract rmdir operation parameters from a command."""
    tokens = shlex.split(command)
    
    # Check for recursive flag in rm commands
    recursive = any(flag in tokens for flag in ["-r", "-rf", "--recursive", "-R"])
    force = any(flag in tokens for flag in ["-f", "-rf", "--force"])
    
    # Get directory paths
    paths = []
    for arg in tokens[1:]:
        if not arg.startswith("-"):
            paths.append(arg)
    
    return "delete_directory", {
        "path": paths[0] if paths else ".",
        "recursive": recursive,
        "force": force
    }


async def extract_touch_operation(command: str) -> Tuple[str, Dict[str, Any]]:
    """Extract touch operation parameters from a command."""
    tokens = shlex.split(command)
    
    # Get file paths
    paths = []
    for arg in tokens[1:]:
        if not arg.startswith("-"):
            paths.append(arg)
    
    return "create_file", {"path": paths[0] if paths else ".", "content": None}


async def extract_cat_operation(command: str) -> Tuple[str, Dict[str, Any]]:
    """Extract cat operation parameters from a command."""
    tokens = shlex.split(command)
    
    # Get file paths
    paths = []
    for arg in tokens[1:]:
        if not arg.startswith("-"):
            paths.append(arg)
    
    # Check for binary flag
    binary = "-b" in tokens or "--binary" in tokens
    
    return "read_file", {"path": paths[0] if paths else ".", "binary": binary}


async def extract_echo_write_operation(command: str) -> Tuple[str, Dict[str, Any]]:
    """Extract echo write operation parameters from a command."""
    # Determine if this is append (>>) or overwrite (>)
    append = ">>" in command
    
    # Split by redirection operator
    parts = command.split(">>" if append else ">", 1)
    
    # Extract the echo part and the file path
    echo_part = parts[0].strip()[5:]  # Remove 'echo ' prefix
    file_path = parts[1].strip()
    
    # Handle quoted content
    if echo_part.startswith('"') and echo_part.endswith('"'):
        content = echo_part[1:-1]
    elif echo_part.startswith("'") and echo_part.endswith("'"):
        content = echo_part[1:-1]
    else:
        content = echo_part
    
    return "write_file", {
        "path": file_path,
        "content": content,
        "append": append
    }


async def extract_rm_operation(command: str) -> Tuple[str, Dict[str, Any]]:
    """Extract rm operation parameters from a command."""
    tokens = shlex.split(command)
    
    # Check for force flag
    force = "-f" in tokens or "--force" in tokens
    
    # Get file paths
    paths = []
    for arg in tokens[1:]:
        if not arg.startswith("-"):
            paths.append(arg)
    
    return "delete_file", {"path": paths[0] if paths else ".", "force": force}


async def extract_cp_operation(command: str) -> Tuple[str, Dict[str, Any]]:
    """Extract cp operation parameters from a command."""
    tokens = shlex.split(command)
    
    # Check for force/overwrite flag
    overwrite = "-f" in tokens or "--force" in tokens
    
    # Get source and destination
    args = [arg for arg in tokens[1:] if not arg.startswith("-")]
    
    if len(args) >= 2:
        source = args[0]
        destination = args[-1]  # Last argument is always the destination
    else:
        # Not enough arguments
        raise ValueError("cp command requires source and destination")
    
    return "copy_file", {
        "source": source,
        "destination": destination,
        "overwrite": overwrite
    }


async def extract_mv_operation(command: str) -> Tuple[str, Dict[str, Any]]:
    """Extract mv operation parameters from a command."""
    tokens = shlex.split(command)
    
    # Check for force/overwrite flag
    overwrite = "-f" in tokens or "--force" in tokens
    
    # Get source and destination
    args = [arg for arg in tokens[1:] if not arg.startswith("-")]
    
    if len(args) >= 2:
        source = args[0]
        destination = args[-1]  # Last argument is always the destination
    else:
        # Not enough arguments
        raise ValueError("mv command requires source and destination")
    
    return "move_file", {
        "source": source,
        "destination": destination,
        "overwrite": overwrite
    }


# Operation extractors mapping
OPERATION_EXTRACTORS = {
    "mkdir": extract_mkdir_operation,
    "rmdir": extract_rmdir_operation,
    "rm": extract_rmdir_operation if "-r" in "{command}" or "--recursive" in "{command}" else extract_rm_operation,
    "touch": extract_touch_operation,
    "cat": extract_cat_operation,
    "less": extract_cat_operation,
    "more": extract_cat_operation,
    "head": extract_cat_operation,
    "tail": extract_cat_operation,
    "echo": extract_echo_write_operation,
    "cp": extract_cp_operation,
    "mv": extract_mv_operation,
}


async def extract_file_operation(command: str) -> Optional[Tuple[str, Dict[str, Any]]]:
    """
    Extract file operation details from a command string.
    
    Args:
        command: The shell command to analyze.
        
    Returns:
        A tuple of (operation_type, parameters) or None if not a file operation.
    """
    try:
        # Get the base command
        tokens = shlex.split(command)
        if not tokens:
            return None
        
        base_cmd = tokens[0]
        
        # Check if this is a known file operation
        if base_cmd in OPERATION_EXTRACTORS:
            # Use the specific extractor for this command
            extractor = OPERATION_EXTRACTORS[base_cmd]
            
            # For rm, we need to check if it's recursive
            if base_cmd == "rm":
                if any(flag in tokens for flag in ["-r", "-rf", "--recursive", "-R"]):
                    return await extract_rmdir_operation(command)
                else:
                    return await extract_rm_operation(command)
            
            # For other commands, use the registered extractor
            return await extractor(command)
        
        # Fall back to pattern matching
        for pattern, operation_type in FILE_OPERATION_PATTERNS:
            match = re.match(pattern, command)
            if match:
                # Basic extraction based on pattern groups
                if operation_type == "create_directory":
                    return operation_type, {"path": match.group(2), "parents": bool(match.group(1))}
                elif operation_type == "delete_directory":
                    return operation_type, {"path": match.group(1), "recursive": "-r" in command or "-rf" in command}
                elif operation_type == "create_file":
                    return operation_type, {"path": match.group(1), "content": None}
                elif operation_type == "read_file":
                    return operation_type, {"path": match.group(2)}
                elif operation_type == "write_file":
                    return operation_type, {"path": match.group(2), "content": match.group(1), "append": False}
                elif operation_type == "append_file":
                    return operation_type, {"path": match.group(2), "content": match.group(1), "append": True}
                elif operation_type == "delete_file":
                    return operation_type, {"path": match.group(1)}
                elif operation_type == "copy_file":
                    parts = match.group(1).rsplit(" ", 1)
                    if len(parts) == 2:
                        return operation_type, {"source": parts[0], "destination": parts[1]}
                elif operation_type == "move_file":
                    parts = match.group(1).rsplit(" ", 1)
                    if len(parts) == 2:
                        return operation_type, {"source": parts[0], "destination": parts[1]}
        
        # Not a file operation
        return None
    
    except Exception as e:
        logger.exception(f"Error extracting file operation from '{command}': {str(e)}")
        return None


async def execute_file_operation(
    operation_type: str, 
    parameters: Dict[str, Any],
    dry_run: bool = False
) -> Dict[str, Any]:
    """
    Execute a file operation based on type and parameters.
    
    Args:
        operation_type: The type of file operation.
        parameters: Parameters for the operation.
        dry_run: Whether to simulate the operation without making changes.
        
    Returns:
        A dictionary with the operation results.
    """
    try:
        logger.info(f"Executing file operation: {operation_type}")
        logger.debug(f"Parameters: {parameters}")
        
        result = {
            "operation": operation_type,
            "parameters": parameters,
            "success": False,
            "dry_run": dry_run,
        }
        
        # Execute the appropriate operation
        if operation_type == "create_directory":
            path = parameters.get("path")
            parents = parameters.get("parents", True)
            
            success = await create_directory(path, parents=parents, dry_run=dry_run)
            result["success"] = success
            
        elif operation_type == "delete_directory":
            path = parameters.get("path")
            recursive = parameters.get("recursive", False)
            force = parameters.get("force", False)
            
            success = await delete_directory(
                path, recursive=recursive, force=force, dry_run=dry_run
            )
            result["success"] = success
            
        elif operation_type == "create_file":
            path = parameters.get("path")
            content = parameters.get("content")
            
            success = await create_file(path, content=content, dry_run=dry_run)
            result["success"] = success
            
        elif operation_type == "read_file":
            path = parameters.get("path")
            binary = parameters.get("binary", False)
            
            content = await read_file(path, binary=binary)
            result["content"] = content
            result["success"] = True
            
        elif operation_type == "write_file":
            path = parameters.get("path")
            content = parameters.get("content", "")
            append = parameters.get("append", False)
            
            success = await write_file(
                path, content, append=append, dry_run=dry_run
            )
            result["success"] = success
            
        elif operation_type == "delete_file":
            path = parameters.get("path")
            force = parameters.get("force", False)
            
            success = await delete_file(path, force=force, dry_run=dry_run)
            result["success"] = success
            
        elif operation_type == "copy_file":
            source = parameters.get("source")
            destination = parameters.get("destination")
            overwrite = parameters.get("overwrite", False)
            
            success = await copy_file(
                source, destination, overwrite=overwrite, dry_run=dry_run
            )
            result["success"] = success
            
        elif operation_type == "move_file":
            source = parameters.get("source")
            destination = parameters.get("destination")
            overwrite = parameters.get("overwrite", False)
            
            success = await move_file(
                source, destination, overwrite=overwrite, dry_run=dry_run
            )
            result["success"] = success
            
        else:
            logger.warning(f"Unknown file operation: {operation_type}")
            result["error"] = f"Unknown file operation: {operation_type}"
        
        return result
        
    except FileSystemError as e:
        logger.exception(f"Error executing file operation: {str(e)}")
        return {
            "operation": operation_type,
            "parameters": parameters,
            "success": False,
            "error": str(e),
            "dry_run": dry_run,
        }
    except Exception as e:
        logger.exception(f"Unexpected error in file operation: {str(e)}")
        return {
            "operation": operation_type,
            "parameters": parameters,
            "success": False,
            "error": f"Unexpected error: {str(e)}",
            "dry_run": dry_run,
        }
</file>

<file path="angela/ai/intent_analyzer.py">
# angela/ai/intent_analyzer.py

import re
import difflib
from typing import Dict, Any, List, Tuple, Optional
from pydantic import BaseModel

from angela.ai.confidence import confidence_scorer
from angela.context.history import history_manager
from angela.utils.logging import get_logger

logger = get_logger(__name__)

class IntentAnalysisResult(BaseModel):
    """Model for intent analysis results."""
    original_request: str
    normalized_request: str
    intent_type: str
    confidence: float
    entities: Dict[str, Any] = {}
    disambiguation_needed: bool = False
    possible_intents: List[Tuple[str, float]] = []

class IntentAnalyzer:
    """
    Enhanced intent analyzer with fuzzy matching and tolerance for
    misspellings and ambiguity.
    """
    
    # Define known intent patterns with examples
    INTENT_PATTERNS = {
        "file_search": [
            "find files", "search for files", "locate files", 
            "show me files", "list files matching"
        ],
        "directory_operation": [
            "create directory", "make folder", "create folder", 
            "remove directory", "delete folder"
        ],
        "file_operation": [
            "create file", "edit file", "delete file", "write to file",
            "read file", "show file contents", "copy file", "move file"
        ],
        "system_info": [
            "show system info", "check disk space", "memory usage",
            "cpu usage", "system status", "show processes"
        ],
        "git_operation": [
            "git status", "git commit", "git push", "git pull",
            "create branch", "switch branch", "merge branch"
        ],
        # Add more intent patterns as needed
    }
    
    # Common misspellings and variations
    SPELLING_VARIATIONS = {
        "directory": ["dir", "folder", "direcotry", "directroy"],
        "file": ["flie", "fil", "document"],
        "create": ["make", "creat", "crate", "new"],
        "delete": ["remove", "del", "rm", "erase"],
        "search": ["find", "look for", "locate", "seek"],
        # Add more variations
    }
    
    def __init__(self):
        """Initialize the intent analyzer."""
        self._logger = logger
    
    def normalize_request(self, request: str) -> str:
        """
        Normalize the request by fixing common misspellings and variations.
        
        Args:
            request: The original request string
            
        Returns:
            Normalized request string
        """
        normalized = request.lower()
        
        # Replace common variations with standard terms
        for standard, variations in self.SPELLING_VARIATIONS.items():
            for variation in variations:
                # Use word boundary regex to avoid partial replacements
                pattern = r'\b' + re.escape(variation) + r'\b'
                normalized = re.sub(pattern, standard, normalized)
        
        self._logger.debug(f"Normalized request: '{request}' -> '{normalized}'")
        return normalized
    
    def analyze_intent(self, request: str) -> IntentAnalysisResult:
        """
        Analyze the intent of a request with enhanced tolerance for
        variations and ambiguity.
        
        Args:
            request: The original request string
            
        Returns:
            IntentAnalysisResult with the analysis
        """
        # Normalize the request
        normalized = self.normalize_request(request)
        
        # Find closest matching intents
        matches = []
        for intent_type, patterns in self.INTENT_PATTERNS.items():
            # Calculate best match score for this intent type
            best_score = 0
            for pattern in patterns:
                similarity = difflib.SequenceMatcher(None, normalized, pattern).ratio()
                if similarity > best_score:
                    best_score = similarity
            
            # Add to matches if score is above threshold
            if best_score > 0.6:  # Adjust threshold as needed
                matches.append((intent_type, best_score))
        
        # Sort matches by confidence score
        matches.sort(key=lambda x: x[1], reverse=True)
        
        # Check if we have a clear winner or need disambiguation
        if not matches:
            # No clear intent - low confidence fallback to generic
            return IntentAnalysisResult(
                original_request=request,
                normalized_request=normalized,
                intent_type="unknown",
                confidence=0.3,
                disambiguation_needed=True
            )
        
        top_intent, top_score = matches[0]
        
        # Extract entities based on the intent type
        entities = self._extract_entities(normalized, top_intent)
        
        # Check if disambiguation is needed
        disambiguation_needed = False
        if len(matches) > 1:
            second_intent, second_score = matches[1]
            # If top two scores are close, disambiguation might be needed
            if top_score - second_score < 0.15:
                disambiguation_needed = True
                self._logger.debug(f"Ambiguous intent: {top_intent} ({top_score:.2f}) vs {second_intent} ({second_score:.2f})")
        
        # Create the result
        result = IntentAnalysisResult(
            original_request=request,
            normalized_request=normalized,
            intent_type=top_intent,
            confidence=top_score,
            entities=entities,
            disambiguation_needed=disambiguation_needed,
            possible_intents=matches[:3]  # Keep top 3 for disambiguation
        )
        
        self._logger.info(f"Intent analysis: {top_intent} (confidence: {top_score:.2f})")
        return result
    
    def _extract_entities(self, normalized: str, intent_type: str) -> Dict[str, Any]:
        """
        Extract entities from the request based on intent type.
        
        Args:
            normalized: The normalized request string
            intent_type: The type of intent
            
        Returns:
            Dictionary of extracted entities
        """
        entities = {}
        
        # Extract entities based on intent type
        if intent_type == "file_search":
            # Extract file patterns
            pattern_match = re.search(r'matching (.+?)(?: in | with | containing |$)', normalized)
            if pattern_match:
                entities["pattern"] = pattern_match.group(1)
            
            # Extract directory to search in
            dir_match = re.search(r'in (?:directory |folder |)([\w\./]+)', normalized)
            if dir_match:
                entities["directory"] = dir_match.group(1)
                
        elif intent_type == "file_operation" or intent_type == "directory_operation":
            # Extract file/directory names
            path_match = re.search(r'(?:file|directory|folder) (?:called |named |)["\'"]?([\w\./]+)["\'"]?', normalized)
            if path_match:
                entities["path"] = path_match.group(1)
                
            # Extract content if applicable
            content_match = re.search(r'with (?:content |text |)["\'](.*?)["\']', normalized)
            if content_match:
                entities["content"] = content_match.group(1)
        
        # Add more entity extraction rules for other intent types
        
        return entities
    
    async def get_interactive_disambiguation(self, result: IntentAnalysisResult) -> IntentAnalysisResult:
        """
        Get user clarification for ambiguous intent.
        
        Args:
            result: The initial analysis result
            
        Returns:
            Updated analysis result after disambiguation
        """
        # Only disambiguate if confidence is low or explicitly needed
        if result.confidence > 0.7 and not result.disambiguation_needed:
            return result
        
        self._logger.info(f"Getting disambiguation for intent: {result.intent_type}")
        
        # Import here to avoid circular imports
        from prompt_toolkit.shortcuts import radiolist_dialog
        from prompt_toolkit.styles import Style
        
        # Create options for disambiguation
        options = []
        for intent_type, score in result.possible_intents:
            # Create a human-readable description for each intent
            description = self._get_intent_description(intent_type, result.entities)
            options.append((intent_type, description))
        
        # Add a "none of these" option
        options.append(("none", "None of these - let me rephrase"))
        
        # Create dialog style
        dialog_style = Style.from_dict({
            'dialog': 'bg:#222222',
            'dialog.body': 'bg:#222222 #ffffff',
            'dialog.border': '#888888',
            'button': 'bg:#222222 #ffffff',
            'button.focused': 'bg:#0969DA #ffffff',
        })
        
        # Show the dialog
        selected_intent = radiolist_dialog(
            title="Clarification Needed",
            text=f"I'm not sure what you meant by: '{result.original_request}'\nPlease select what you intended:",
            values=options,
            style=dialog_style
        ).run()
        
        # If user selected a specific intent, update the result
        if selected_intent and selected_intent != "none":
            # Find the score for the selected intent
            selected_score = 0.85  # Default to high confidence since user confirmed
            for intent, score in result.possible_intents:
                if intent == selected_intent:
                    selected_score = max(0.85, score)  # At least 0.85 confidence
                    break
                    
            # Update the result
            result.intent_type = selected_intent
            result.confidence = selected_score
            result.disambiguation_needed = False
            
            # Re-extract entities based on the new intent
            result.entities = self._extract_entities(result.normalized_request, selected_intent)
            
            self._logger.info(f"Intent clarified: {selected_intent} (confidence: {selected_score:.2f})")
        
        return result
    
    def _get_intent_description(self, intent_type: str, entities: Dict[str, Any]) -> str:
        """
        Create a human-readable description for an intent.
        
        Args:
            intent_type: The type of intent
            entities: Extracted entities
            
        Returns:
            Human-readable description
        """
        if intent_type == "file_search":
            pattern = entities.get("pattern", "files")
            directory = entities.get("directory", "current directory")
            return f"Search for {pattern} in {directory}"
            
        elif intent_type == "file_operation":
            path = entities.get("path", "a file")
            if "create" in path or "make" in path:
                return f"Create a new file: {path}"
            elif "delete" in path or "remove" in path:
                return f"Delete file: {path}"
            else:
                return f"Perform operation on file: {path}"
                
        elif intent_type == "directory_operation":
            path = entities.get("path", "a directory")
            if "create" in path or "make" in path:
                return f"Create a new directory: {path}"
            elif "delete" in path or "remove" in path:
                return f"Delete directory: {path}"
            else:
                return f"Perform operation on directory: {path}"
                
        elif intent_type == "system_info":
            return "Show system information"
            
        elif intent_type == "git_operation":
            return "Perform Git operation"
            
        # Fallback for unknown intent types
        return f"Intent: {intent_type}"

# Global intent analyzer instance
intent_analyzer = IntentAnalyzer()
</file>

<file path="angela/ai/parser.py">
# angela/ai/parser.py
import json
from typing import Dict, Any, Optional

from pydantic import BaseModel, Field, ValidationError

from angela.utils.logging import get_logger

logger = get_logger(__name__)

class CommandSuggestion(BaseModel):
    """Model for a command suggestion from the AI."""
    intent: str = Field(..., description="The classified intent of the user's request")
    command: str = Field(..., description="The suggested shell command")
    explanation: str = Field(..., description="Explanation of what the command does")
    additional_info: Optional[str] = Field(None, description="Any additional information")

# Update in angela/ai/parser.py
def parse_ai_response(response_text: str) -> CommandSuggestion:
    """Parse the AI response into a structured format."""
    try:
        # Try to extract JSON from the response
        json_str = None
        
        # Check for JSON in markdown code block with language specifier
        if "```json" in response_text and "```" in response_text.split("```json")[1]:
            json_str = response_text.split("```json")[1].split("```")[0].strip()
        # Check for JSON in regular markdown code block
        elif "```" in response_text and "```" in response_text.split("```")[1]:
            # Try without language specifier
            json_str = response_text.split("```")[1].strip()
        else:
            # Assume the entire response is JSON
            json_str = response_text.strip()
        
        # Parse the JSON
        data = json.loads(json_str)
        
        # Validate with Pydantic model
        suggestion = CommandSuggestion(**data)
        
        logger.debug(f"Successfully parsed AI response: {suggestion}")
        return suggestion
    
    except (json.JSONDecodeError, ValidationError) as e:
        logger.error(f"Failed to parse AI response: {str(e)}")
        logger.debug(f"Raw response: {response_text}")
        
        # Fallback: Try to extract just the command if JSON parsing fails
        try:
            import re
            # Improve the regex pattern to better match different formats
            command_match = re.search(r'command["\']?\s*:\s*["\']?(.*?)["\']?[,}]', response_text)
            if command_match:
                # Extract just the command value, not the whole match
                command = command_match.group(1).strip()
                # Remove any trailing quotes
                if command.endswith('"') or command.endswith("'"):
                    command = command[:-1]
                logger.debug(f"Extracted command using regex: {command}")
                return CommandSuggestion(
                    intent="unknown",
                    command=command,
                    explanation="Command extracted from incomplete response."
                )
        except Exception as regex_error:
            logger.error(f"Regex extraction also failed: {str(regex_error)}")
        
        raise ValueError(f"Could not parse AI response: {str(e)}")
</file>

<file path="angela/ai/prompts_update.py">
"""
Enhanced prompt engineering for Angela CLI.

This file contains enhanced prompt templates that incorporate rich project context,
dependency information, recent file activities, and resolved file references.
"""

# Enhanced project context prompt section
ENHANCED_PROJECT_CONTEXT = """
## Enhanced Project Information
Project Type: {project_type}
Frameworks: {frameworks}
Main Dependencies: {dependencies}
Important Files: {important_files}
Project Structure:
- Main Directories: {main_directories}
- Total Files: {total_files}
"""

# Recent file activity prompt section
RECENT_FILES_CONTEXT = """
## Recent File Activity
Recently Accessed Files:
{recent_files}

Most Active Files:
{active_files}
"""

# Resolved file references prompt section
RESOLVED_FILES_CONTEXT = """
## Resolved File References
The following file references were resolved from your request:
{resolved_files}
"""

# Enhanced build_prompt function
def build_prompt(
    request: str, 
    context: Dict[str, Any],
    similar_command: Optional[str] = None,
    intent_result: Optional[Dict[str, Any]] = None
) -> str:
    """Build a prompt for the Gemini API with enhanced context information."""
    # Create a context description
    context_str = "Current context:\n"
    if context.get("cwd"):
        context_str += f"- Current working directory: {context['cwd']}\n"
    if context.get("project_root"):
        context_str += f"- Project root: {context['project_root']}\n"
    if context.get("project_type"):
        context_str += f"- Project type: {context['project_type']}\n"
    if context.get("relative_path"):
        context_str += f"- Path relative to project root: {context['relative_path']}\n"
    
    # Add information about the current file if available
    if context.get("current_file"):
        file_info = context["current_file"]
        context_str += f"- Current file: {file_info.get('path')}\n"
        if file_info.get("language"):
            context_str += f"- File language: {file_info.get('language')}\n"
        if file_info.get("type"):
            context_str += f"- File type: {file_info.get('type')}\n"
    
    # Add enhanced project information if available
    if context.get("enhanced_project"):
        project_info = context["enhanced_project"]
        
        # Format frameworks information
        frameworks_str = "None detected"
        if project_info.get("frameworks"):
            framework_names = list(project_info["frameworks"].keys())
            frameworks_str = ", ".join(framework_names[:5])
            if len(framework_names) > 5:
                frameworks_str += f" and {len(framework_names) - 5} more"
        
        # Format dependencies information
        dependencies_str = "None detected"
        if project_info.get("dependencies") and project_info["dependencies"].get("top_dependencies"):
            dependencies_str = ", ".join(project_info["dependencies"]["top_dependencies"][:5])
            if len(project_info["dependencies"]["top_dependencies"]) > 5:
                dependencies_str += f" and {len(project_info['dependencies']['top_dependencies']) - 5} more"
            
            # Add counts information
            if project_info["dependencies"].get("counts"):
                dependencies_str += f" (Total: {project_info['dependencies'].get('total', 0)})"
        
        # Format important files information
        important_files_str = "None detected"
        if project_info.get("important_files") and project_info["important_files"].get("paths"):
            important_files_str = ", ".join(project_info["important_files"]["paths"][:5])
            if len(project_info["important_files"]["paths"]) > 5:
                important_files_str += f" and {len(project_info['important_files']['paths']) - 5} more"
        
        # Format main directories information
        main_directories_str = "None detected"
        if project_info.get("structure") and project_info["structure"].get("main_directories"):
            main_directories_str = ", ".join(project_info["structure"]["main_directories"])
        
        # Format total files information
        total_files_str = "Unknown"
        if project_info.get("structure") and "total_files" in project_info["structure"]:
            total_files_str = str(project_info["structure"]["total_files"])
        
        # Add to context string
        context_str += ENHANCED_PROJECT_CONTEXT.format(
            project_type=project_info.get("type", "Unknown"),
            frameworks=frameworks_str,
            dependencies=dependencies_str,
            important_files=important_files_str,
            main_directories=main_directories_str,
            total_files=total_files_str
        )
    
    # Add recent file activity if available
    if context.get("recent_files"):
        recent_files = context["recent_files"]
        
        # Format recent files information
        recent_files_str = "None"
        if recent_files.get("accessed"):
            # Extract filenames only for brevity
            recent_filenames = [Path(path).name for path in recent_files["accessed"][:5]]
            recent_files_str = ", ".join(recent_filenames)
            if len(recent_files["accessed"]) > 5:
                recent_files_str += f" and {len(recent_files['accessed']) - 5} more"
        
        # Format active files information
        active_files_str = "None"
        if recent_files.get("activities"):
            active_files_str = ", ".join([a.get("name", "unknown") for a in recent_files["activities"][:3]])
            if len(recent_files["activities"]) > 3:
                active_files_str += f" and {len(recent_files['activities']) - 3} more"
        
        # Add to context string
        context_str += RECENT_FILES_CONTEXT.format(
            recent_files=recent_files_str,
            active_files=active_files_str
        )
    
    # Add resolved file references if available
    if context.get("resolved_files"):
        resolved_files = context["resolved_files"]
        
        # Format resolved files information
        resolved_files_str = ""
        for ref_info in resolved_files:
            reference = ref_info.get("reference", "")
            path = ref_info.get("path", "Not found")
            resolved_files_str += f"- '{reference}' → {path}\n"
        
        # Add to context string
        if resolved_files_str:
            context_str += RESOLVED_FILES_CONTEXT.format(
                resolved_files=resolved_files_str
            )
    
    # Add conversation context
    if "session" in context:
        session = context["session"]
        
        # Add recent commands for continuity
        if session.get("recent_commands"):
            context_str += "Recent commands:\n"
            for i, cmd in enumerate(session.get("recent_commands", []), 1):
                context_str += f"- Command {i}: {cmd}\n"
        
        # Add recent results for reference
        if session.get("recent_results"):
            context_str += "Recent command results:\n"
            for i, result in enumerate(session.get("recent_results", []), 1):
                # Truncate long results
                if len(result) > 200:
                    result = result[:200] + "..."
                context_str += f"- Result {i}: {result}\n"
        
        # Add entities for reference resolution
        if session.get("entities"):
            context_str += "Referenced entities:\n"
            for name, entity in session.get("entities", {}).items():
                context_str += f"- {name}: {entity.get('type')} - {entity.get('value')}\n"
    
    # Add intent analysis if available
    if intent_result:
        context_str += "\nIntent analysis:\n"
        context_str += f"- Intent type: {intent_result.get('intent_type', 'unknown')}\n"
        context_str += f"- Confidence: {intent_result.get('confidence', 0.0):.2f}\n"
        
        # Add extracted entities
        if intent_result.get("entities"):
            context_str += "- Extracted entities:\n"
            for key, value in intent_result.get("entities", {}).items():
                context_str += f"  - {key}: {value}\n"
    
    # Add similar command suggestion if available
    if similar_command:
        context_str += f"\nYou previously suggested this similar command: {similar_command}\n"
    
    # Add examples for few-shot learning
    examples_str = "Examples:\n"
    
    # Add standard examples
    for example in EXAMPLES:
        examples_str += f"\nUser request: {example['request']}\n"
        examples_str += f"Context: {example['context']}\n"
        examples_str += f"Response: {example['response']}\n"
    
    # Add file operation examples
    for example in FILE_OPERATION_EXAMPLES:
        examples_str += f"\nUser request: {example['request']}\n"
        examples_str += f"Context: {example['context']}\n"
        examples_str += f"Response: {example['response']}\n"
    
    # Define the expected response format - now with confidence indicator
    response_format = """
Expected response format (valid JSON):
{
    "intent": "the_classified_intent",
    "command": "the_suggested_command",
    "explanation": "explanation of what the command does",
    "confidence": 0.85, /* Optional confidence score from 0.0 to 1.0 */
    "additional_info": "any additional information (optional)"
}
"""
    
    # Build the complete prompt
    prompt = f"{SYSTEM_INSTRUCTIONS}\n\n{context_str}\n\n{examples_str}\n\n{response_format}\n\nUser request: {request}\n\nResponse:"
    
    logger.debug(f"Built prompt with length: {len(prompt)}")
    return prompt

# Enhanced prompt template for file operations
FILE_OPERATION_PROMPT_TEMPLATE = """
You are asked to perform an operation on a file with enhanced context awareness.

## File Information
Path: {file_path}
Type: {file_type}
Language: {language}
Size: {size}

## Project Context
Project Type: {project_type}
Project Root: {project_root}

## Request
{request}

Consider all this context when generating your response. If the file is part of a project,
consider how this operation might affect the project as a whole.
"""

# Enhanced prompt for file operation
def build_file_operation_prompt(
    operation: str, 
    parameters: Dict[str, Any], 
    context: Dict[str, Any]
) -> str:
    """
    Build an enhanced prompt for generating a file operation command.
    
    Args:
        operation: The type of file operation.
        parameters: Parameters for the operation.
        context: Context information about the current environment.
        
    Returns:
        A prompt string for the Gemini API.
    """
    # Create file information
    file_path = parameters.get("path", "Unknown")
    file_info = context_manager.get_file_info(Path(file_path)) if file_path != "Unknown" else {}
    
    file_info_str = f"""
File: {file_path}
Type: {file_info.get('type', 'Unknown')}
Language: {file_info.get('language', 'Unknown')}
"""
    
    # Create a description of the requested operation
    operation_str = f"""
Requested file operation: {operation}
Parameters:
"""
    for key, value in parameters.items():
        operation_str += f"- {key}: {value}\n"
    
    # Create enhanced project context
    project_context_str = "Project Context:\n"
    
    if context.get("enhanced_project"):
        project_info = context["enhanced_project"]
        project_context_str += f"- Project Type: {project_info.get('type', 'Unknown')}\n"
        
        # Add frameworks if available
        if project_info.get("frameworks"):
            frameworks = list(project_info["frameworks"].keys())
            project_context_str += f"- Frameworks: {', '.join(frameworks[:3])}"
            if len(frameworks) > 3:
                project_context_str += f" and {len(frameworks) - 3} more"
            project_context_str += "\n"
        
        # Add file's relationship to project
        if file_path != "Unknown" and context.get("project_root"):
            try:
                rel_path = Path(file_path).relative_to(Path(context["project_root"]))
                project_context_str += f"- Relative Path: {rel_path}\n"
            except ValueError:
                # File is not within project root
                project_context_str += f"- Note: File is outside project root\n"
    
    # Define the task
    task_str = f"""
Your task is to generate a shell command that will perform the requested file operation.
The command should be safe, efficient, and follow best practices for Linux/Unix shell environments.
Consider the file type, language, and project context when generating the command.
"""
    
    # Define the expected response format
    response_format = """
Expected response format (valid JSON):
{
    "command": "the_shell_command",
    "explanation": "explanation of what the command does",
    "risk_level": "SAFE|LOW|MEDIUM|HIGH|CRITICAL",
    "destructive": true|false
}
"""
    
    # Build the complete prompt
    prompt = f"{SYSTEM_INSTRUCTIONS}\n\n{operation_str}\n\n{file_info_str}\n\n{project_context_str}\n\n{task_str}\n\n{response_format}\n\nResponse:"
    
    logger.debug(f"Built file operation prompt with length: {len(prompt)}")
    return prompt
</file>

<file path="angela/cli/files_extensions.py">
"""
CLI extensions for file resolution and activity tracking.

This module extends the files command with advanced file resolution features.
"""
import asyncio
from pathlib import Path
from typing import Optional, List

import typer
from rich.console import Console
from rich.panel import Panel
from rich.table import Table
from rich.text import Text
from rich import print as rich_print

from angela.context import context_manager, file_resolver, file_activity_tracker, ActivityType
from angela.utils.logging import get_logger

logger = get_logger(__name__)
console = Console()

# Create a Typer app for files extensions
app = typer.Typer(name="files", help="Advanced file operations")


@app.command("resolve")
def resolve_file(
    reference: str = typer.Argument(..., help="File reference to resolve"),
    scope: str = typer.Option(
        "all", "--scope", "-s", 
        help="Search scope (project, directory, all)"
    ),
):
    """Resolve a file reference to an actual file path."""
    # Get the current context
    context = context_manager.get_context_dict()
    
    # Convert scope
    search_scope = None
    if scope == "project":
        search_scope = "project"
    elif scope == "directory":
        search_scope = "directory"
    
    # Run the resolver
    try:
        path = asyncio.run(file_resolver.resolve_reference(
            reference,
            context,
            search_scope=search_scope
        ))
        
        if path:
            # Show the resolved path
            console.print(Panel(
                f"Resolved '[bold]{reference}[/bold]' to:\n[green]{path}[/green]",
                title="File Resolution",
                border_style="green"
            ))
            
            # Track as viewed file
            file_activity_tracker.track_file_viewing(path, None, {
                "reference": reference,
                "resolved_via": "cli"
            })
        else:
            # Show not found message
            console.print(Panel(
                f"Could not resolve '[bold]{reference}[/bold]' to a file or directory.",
                title="File Resolution",
                border_style="yellow"
            ))
            
            # Show suggestions based on the scope
            if search_scope == "project" and context.get("project_root"):
                # Suggest listing files in project
                console.print("Try using 'angela files find' to search for files in the project.")
            elif search_scope == "directory":
                # Suggest listing files in directory
                console.print("Try using 'angela files ls' to list files in the current directory.")
            else:
                # Suggest other scopes
                console.print("Try using '--scope project' or '--scope directory' to narrow the search.")
    
    except Exception as e:
        logger.exception(f"Error resolving file reference: {str(e)}")
        console.print(f"[bold red]Error:[/bold red] {str(e)}")


@app.command("extract")
def extract_references(
    text: str = typer.Argument(..., help="Text containing file references"),
):
    """Extract and resolve file references from text."""
    # Get the current context
    context = context_manager.get_context_dict()
    
    # Run the extractor
    try:
        references = asyncio.run(file_resolver.extract_references(text, context))
        
        if references:
            # Create a table for the results
            table = Table(title="Extracted File References")
            table.add_column("Reference", style="cyan")
            table.add_column("Resolved Path", style="green")
            table.add_column("Status", style="yellow")
            
            for reference, path in references:
                if path:
                    status = "[green]Found[/green]"
                    path_str = str(path)
                    
                    # Track as viewed file
                    file_activity_tracker.track_file_viewing(path, None, {
                        "reference": reference,
                        "extracted_via": "cli"
                    })
                else:
                    status = "[yellow]Not Found[/yellow]"
                    path_str = "[italic]Not resolved[/italic]"
                
                table.add_row(reference, path_str, status)
            
            console.print(table)
        else:
            # Show not found message
            console.print(Panel(
                f"No file references found in the text.",
                title="File References",
                border_style="yellow"
            ))
    
    except Exception as e:
        logger.exception(f"Error extracting file references: {str(e)}")
        console.print(f"[bold red]Error:[/bold red] {str(e)}")


@app.command("recent")
def recent_files(
    limit: int = typer.Option(10, "--limit", "-n", help="Number of files to show"),
    activity_type: Optional[str] = typer.Option(
        None, "--type", "-t", 
        help="Filter by activity type (viewed, created, modified, deleted)"
    ),
):
    """Show recently accessed files."""
    try:
        # Convert activity type
        activity_types = None
        if activity_type:
            try:
                activity_types = [ActivityType(activity_type)]
            except ValueError:
                console.print(f"[yellow]Invalid activity type: {activity_type}[/yellow]")
                console.print("Available types: viewed, created, modified, deleted")
                return
        
        # Get recent activities
        activities = file_activity_tracker.get_recent_activities(
            limit=limit,
            activity_types=activity_types
        )
        
        if activities:
            # Create a table for the results
            table = Table(title=f"Recent File Activities (Last {len(activities)})")
            table.add_column("File", style="cyan")
            table.add_column("Activity", style="green")
            table.add_column("Time", style="yellow")
            table.add_column("Command", style="blue")
            
            for activity in activities:
                # Format the file name and path
                file_name = activity.get("name", "unknown")
                file_path = activity.get("path", "unknown")
                file_str = f"{file_name}\n[dim]{file_path}[/dim]"
                
                # Format activity type
                activity_type = activity.get("activity_type", "unknown")
                if activity_type == "viewed":
                    activity_str = "[blue]Viewed[/blue]"
                elif activity_type == "created":
                    activity_str = "[green]Created[/green]"
                elif activity_type == "modified":
                    activity_str = "[yellow]Modified[/yellow]"
                elif activity_type == "deleted":
                    activity_str = "[red]Deleted[/red]"
                else:
                    activity_str = activity_type.capitalize()
                
                # Format time
                time_str = activity.get("datetime", "unknown")
                if "T" in time_str:
                    time_str = time_str.replace("T", " ").split(".")[0]  # Simplify timestamp
                
                # Format command (truncate if too long)
                command = activity.get("command", "")
                if command and len(command) > 40:
                    command = command[:37] + "..."
                
                table.add_row(file_str, activity_str, time_str, command)
            
            console.print(table)
        else:
            # Show no activities message
            console.print(Panel(
                f"No file activities tracked yet.",
                title="Recent Files",
                border_style="yellow"
            ))
            
            # Show help message
            console.print("File activities are tracked when you interact with files using Angela.")
            console.print("Try viewing or manipulating some files first.")
    
    except Exception as e:
        logger.exception(f"Error retrieving recent files: {str(e)}")
        console.print(f"[bold red]Error:[/bold red] {str(e)}")


@app.command("active")
def most_active_files(
    limit: int = typer.Option(5, "--limit", "-n", help="Number of files to show"),
):
    """Show most actively used files."""
    try:
        # Get most active files
        active_files = file_activity_tracker.get_most_active_files(limit=limit)
        
        if active_files:
            # Create a table for the results
            table = Table(title=f"Most Active Files (Top {len(active_files)})")
            table.add_column("File", style="cyan")
            table.add_column("Activity Count", style="green")
            table.add_column("Last Activity", style="yellow")
            table.add_column("Activity Types", style="blue")
            
            for file_info in active_files:
                # Format the file name and path
                file_name = file_info.get("name", "unknown")
                file_path = file_info.get("path", "unknown")
                file_str = f"{file_name}\n[dim]{file_path}[/dim]"
                
                # Format activity count
                count = file_info.get("count", 0)
                
                # Format last activity time
                last_activity = file_info.get("last_activity", 0)
                if last_activity:
                    from datetime import datetime
                    time_str = datetime.fromtimestamp(last_activity).isoformat()
                    if "T" in time_str:
                        time_str = time_str.replace("T", " ").split(".")[0]  # Simplify timestamp
                else:
                    time_str = "Unknown"
                
                # Format activity types
                activities = file_info.get("activities", [])
                activities_str = ", ".join(a.capitalize() for a in activities)
                
                table.add_row(file_str, str(count), time_str, activities_str)
            
            console.print(table)
        else:
            # Show no activities message
            console.print(Panel(
                f"No file activities tracked yet.",
                title="Most Active Files",
                border_style="yellow"
            ))
            
            # Show help message
            console.print("File activities are tracked when you interact with files using Angela.")
            console.print("Try viewing or manipulating some files first.")
    
    except Exception as e:
        logger.exception(f"Error retrieving most active files: {str(e)}")
        console.print(f"[bold red]Error:[/bold red] {str(e)}")


@app.command("project")
def show_project_info():
    """Show detected project information."""
    try:
        # Get the current context
        context = context_manager.get_context_dict()
        
        # Check if in a project
        if not context.get("project_root"):
            console.print(Panel(
                "Not currently in a project directory.",
                title="Project Information",
                border_style="yellow"
            ))
            return
        
        # Get enhanced project info
        from angela.context.enhancer import context_enhancer
        
        # Enrich context
        enriched = asyncio.run(context_enhancer.enrich_context(context))
        project_info = enriched.get("enhanced_project", {})
        
        if project_info:
            # Create a panel for the project information
            project_type = project_info.get("type", "Unknown")
            project_root = context.get("project_root", "Unknown")
            
            content = f"[bold]Project Type:[/bold] {project_type}\n"
            content += f"[bold]Project Root:[/bold] {project_root}\n\n"
            
            # Add frameworks if available
            if project_info.get("frameworks"):
                frameworks = list(project_info["frameworks"].keys())
                content += f"[bold]Frameworks:[/bold] {', '.join(frameworks)}\n"
            
            # Add dependencies if available
            if project_info.get("dependencies") and project_info["dependencies"].get("top_dependencies"):
                deps = project_info["dependencies"]["top_dependencies"]
                content += f"[bold]Top Dependencies:[/bold] {', '.join(deps[:5])}"
                if len(deps) > 5:
                    content += f" and {len(deps) - 5} more"
                content += f" (Total: {project_info['dependencies'].get('total', 0)})\n"
            
            # Add important files if available
            if project_info.get("important_files") and project_info["important_files"].get("paths"):
                files = project_info["important_files"]["paths"]
                content += f"[bold]Important Files:[/bold]\n"
                for f in files[:5]:
                    content += f"• {f}\n"
                if len(files) > 5:
                    content += f"• ... and {len(files) - 5} more\n"
            
            # Add structure info if available
            if project_info.get("structure"):
                structure = project_info["structure"]
                content += f"\n[bold]Project Structure:[/bold]\n"
                content += f"• Total Files: {structure.get('total_files', 'Unknown')}\n"
                
                if structure.get("main_directories"):
                    content += f"• Main Directories: {', '.join(structure['main_directories'])}\n"
                
                if structure.get("file_counts"):
                    content += f"• Top File Types:\n"
                    sorted_types = sorted(
                        structure["file_counts"].items(), 
                        key=lambda x: x[1], 
                        reverse=True
                    )
                    for ext, count in sorted_types[:5]:
                        content += f"  - {ext}: {count}\n"
            
            console.print(Panel(
                content,
                title=f"Project Information: {project_type}",
                border_style="green",
                expand=False
            ))
        else:
            # Show no project info message
            console.print(Panel(
                f"No enhanced project information available.",
                title="Project Information",
                border_style="yellow"
            ))
            
            # Show basic project info
            console.print(f"[bold]Project Root:[/bold] {context.get('project_root', 'Unknown')}")
            console.print(f"[bold]Project Type:[/bold] {context.get('project_type', 'Unknown')}")
    
    except Exception as e:
        logger.exception(f"Error retrieving project information: {str(e)}")
        console.print(f"[bold red]Error:[/bold red] {str(e)}")
</file>

<file path="angela/cli/files.py">
"""
File operation commands for Angela CLI.

This module provides CLI commands for file and directory operations.
"""
import os
import sys
import asyncio
from pathlib import Path
from typing import List, Optional, Tuple

import typer
from rich.console import Console
from rich.table import Table
from rich.panel import Panel
from rich.syntax import Syntax
from rich.markup import escape
from rich.text import Text
from rich.prompt import Prompt, Confirm
from rich.filesize import decimal as format_size

from angela.context import context_manager
from angela.execution.filesystem import (
    create_directory, delete_directory, create_file, read_file,
    write_file, delete_file, copy_file, move_file, FileSystemError
)
from angela.execution.rollback import rollback_manager
from angela.utils.logging import get_logger

logger = get_logger(__name__)
console = Console()

# Create the file operations app
app = typer.Typer(help="Angela's file operations")


@app.command("ls")
def list_directory(
    path: str = typer.Argument(
        None, help="Directory to list (defaults to current directory)"
    ),
    all: bool = typer.Option(
        False, "--all", "-a", help="Include hidden files"
    ),
    long: bool = typer.Option(
        False, "--long", "-l", help="Show detailed information"
    ),
):
    """List directory contents with enhanced formatting."""
    try:
        dir_path = Path(path) if path else context_manager.cwd
        
        if not dir_path.exists():
            console.print(f"[bold red]Error:[/bold red] Path not found: {dir_path}")
            sys.exit(1)
        
        if not dir_path.is_dir():
            console.print(f"[bold red]Error:[/bold red] Not a directory: {dir_path}")
            sys.exit(1)
        
        # Get directory contents
        contents = context_manager.get_directory_contents(dir_path, include_hidden=all)
        
        if not contents:
            console.print(f"Directory is empty: {dir_path}")
            return
        
        # Display in table format if long listing is requested
        if long:
            table = Table(title=f"Contents of {dir_path}")
            table.add_column("Name", style="cyan")
            table.add_column("Type", style="green")
            table.add_column("Size", style="blue", justify="right")
            table.add_column("Language", style="magenta")
            
            for item in contents:
                name = item["name"]
                
                # Add indicator for directories
                if item["is_dir"]:
                    name = f"{name}/"
                
                # Format size
                size = format_size(item["size"]) if "size" in item else ""
                
                # Get type and language
                item_type = item.get("type", "unknown")
                language = item.get("language", "")
                
                table.add_row(name, item_type, size, language)
            
            console.print(table)
        
        # Simple listing
        else:
            for item in contents:
                name = item["name"]
                
                # Color and format based on type
                if item["is_dir"]:
                    console.print(f"[bold blue]{name}/[/bold blue]", end="  ")
                elif item.get("language"):
                    console.print(f"[green]{name}[/green]", end="  ")
                elif item.get("binary", False):
                    console.print(f"[dim]{name}[/dim]", end="  ")
                else:
                    console.print(name, end="  ")
            
            # End with a newline
            console.print()
        
    except Exception as e:
        logger.exception(f"Error listing directory: {str(e)}")
        console.print(f"[bold red]Error:[/bold red] {str(e)}")
        sys.exit(1)


@app.command("mkdir")
def make_directory(
    path: str = typer.Argument(..., help="Directory to create"),
    parents: bool = typer.Option(
        True, "--parents/--no-parents", "-p", help="Create parent directories if needed"
    ),
    dry_run: bool = typer.Option(
        False, "--dry-run", help="Show what would happen without making changes"
    ),
):
    """Create a directory."""
    try:
        # Run the operation
        success = asyncio.run(create_directory(path, parents=parents, dry_run=dry_run))
        
        if success:
            if dry_run:
                console.print(f"[bold blue]DRY RUN:[/bold blue] Would create directory: {path}")
            else:
                console.print(f"[bold green]Created directory:[/bold green] {path}")
        else:
            console.print(f"[bold yellow]Operation cancelled.[/bold yellow]")
            sys.exit(1)
        
    except FileSystemError as e:
        logger.exception(f"Error creating directory: {str(e)}")
        console.print(f"[bold red]Error:[/bold red] {str(e)}")
        sys.exit(1)


@app.command("rmdir")
def remove_directory(
    path: str = typer.Argument(..., help="Directory to remove"),
    recursive: bool = typer.Option(
        False, "--recursive", "-r", help="Recursively remove directories and their contents"
    ),
    force: bool = typer.Option(
        False, "--force", "-f", help="Ignore nonexistent files"
    ),
    dry_run: bool = typer.Option(
        False, "--dry-run", help="Show what would happen without making changes"
    ),
):
    """Remove a directory."""
    try:
        # Run the operation
        success = asyncio.run(delete_directory(
            path, recursive=recursive, force=force, dry_run=dry_run
        ))
        
        if success:
            if dry_run:
                console.print(f"[bold blue]DRY RUN:[/bold blue] Would remove directory: {path}")
            else:
                console.print(f"[bold green]Removed directory:[/bold green] {path}")
        else:
            console.print(f"[bold yellow]Operation cancelled.[/bold yellow]")
            sys.exit(1)
        
    except FileSystemError as e:
        logger.exception(f"Error removing directory: {str(e)}")
        console.print(f"[bold red]Error:[/bold red] {str(e)}")
        sys.exit(1)


@app.command("touch")
def touch_file(
    path: str = typer.Argument(..., help="File to create or update timestamp"),
    dry_run: bool = typer.Option(
        False, "--dry-run", help="Show what would happen without making changes"
    ),
):
    """Create a new file or update file timestamp."""
    try:
        # Run the operation (with no content for touch)
        success = asyncio.run(create_file(path, content=None, dry_run=dry_run))
        
        if success:
            if dry_run:
                console.print(f"[bold blue]DRY RUN:[/bold blue] Would touch file: {path}")
            else:
                console.print(f"[bold green]Touched file:[/bold green] {path}")
        else:
            console.print(f"[bold yellow]Operation cancelled.[/bold yellow]")
            sys.exit(1)
        
    except FileSystemError as e:
        logger.exception(f"Error touching file: {str(e)}")
        console.print(f"[bold red]Error:[/bold red] {str(e)}")
        sys.exit(1)


@app.command("cat")
def cat_file(
    path: str = typer.Argument(..., help="File to display"),
    binary: bool = typer.Option(
        False, "--binary", "-b", help="Display binary content"
    ),
    syntax: bool = typer.Option(
        True, "--syntax/--no-syntax", help="Syntax highlighting"
    ),
):
    """Display file contents."""
    try:
        file_path = Path(path)
        
        # Get file info to determine syntax highlighting
        file_info = context_manager.get_file_info(file_path)
        
        # Read the file
        content = asyncio.run(read_file(path, binary=binary))
        
        # Display the content
        if binary:
            # For binary files, just show a hexdump-like output
            console.print(Panel(
                escape(repr(content[:1000])) + ("..." if len(content) > 1000 else ""),
                title=f"Binary content of {path}",
                subtitle=f"Showing first 1000 bytes of {len(content)} total bytes",
                expand=False
            ))
        elif syntax and file_info.get("language") and not binary:
            # Determine the language for syntax highlighting
            lang = file_info.get("language", "").lower()
            if "python" in lang:
                lang = "python"
            elif "javascript" in lang:
                lang = "javascript"
            elif "html" in lang:
                lang = "html"
            elif "css" in lang:
                lang = "css"
            elif "json" in lang:
                lang = "json"
            elif "yaml" in lang:
                lang = "yaml"
            elif "markdown" in lang:
                lang = "markdown"
            elif "bash" in lang or "shell" in lang:
                lang = "bash"
            else:
                lang = "text"
            
            # Display with syntax highlighting
            console.print(Syntax(
                content,
                lang,
                theme="monokai",
                line_numbers=True,
                word_wrap=True
            ))
        else:
            # Simple text display
            console.print(content)
        
    except FileSystemError as e:
        logger.exception(f"Error reading file: {str(e)}")
        console.print(f"[bold red]Error:[/bold red] {str(e)}")
        sys.exit(1)


@app.command("rm")
def remove_file(
    path: str = typer.Argument(..., help="File to remove"),
    force: bool = typer.Option(
        False, "--force", "-f", help="Ignore nonexistent files"
    ),
    dry_run: bool = typer.Option(
        False, "--dry-run", help="Show what would happen without making changes"
    ),
):
    """Remove a file."""
    try:
        # Run the operation
        success = asyncio.run(delete_file(path, force=force, dry_run=dry_run))
        
        if success:
            if dry_run:
                console.print(f"[bold blue]DRY RUN:[/bold blue] Would remove file: {path}")
            else:
                console.print(f"[bold green]Removed file:[/bold green] {path}")
        else:
            console.print(f"[bold yellow]Operation cancelled.[/bold yellow]")
            sys.exit(1)
        
    except FileSystemError as e:
        logger.exception(f"Error removing file: {str(e)}")
        console.print(f"[bold red]Error:[/bold red] {str(e)}")
        sys.exit(1)


@app.command("cp")
def copy_file_command(
    source: str = typer.Argument(..., help="Source file to copy"),
    destination: str = typer.Argument(..., help="Destination path"),
    force: bool = typer.Option(
        False, "--force", "-f", help="Overwrite destination if it exists"
    ),
    dry_run: bool = typer.Option(
        False, "--dry-run", help="Show what would happen without making changes"
    ),
):
    """Copy a file."""
    try:
        # Run the operation
        success = asyncio.run(copy_file(
            source, destination, overwrite=force, dry_run=dry_run
        ))
        
        if success:
            if dry_run:
                console.print(f"[bold blue]DRY RUN:[/bold blue] Would copy {source} to {destination}")
            else:
                console.print(f"[bold green]Copied:[/bold green] {source} -> {destination}")
        else:
            console.print(f"[bold yellow]Operation cancelled.[/bold yellow]")
            sys.exit(1)
        
    except FileSystemError as e:
        logger.exception(f"Error copying file: {str(e)}")
        console.print(f"[bold red]Error:[/bold red] {str(e)}")
        sys.exit(1)


@app.command("mv")
def move_file_command(
    source: str = typer.Argument(..., help="Source file to move"),
    destination: str = typer.Argument(..., help="Destination path"),
    force: bool = typer.Option(
        False, "--force", "-f", help="Overwrite destination if it exists"
    ),
    dry_run: bool = typer.Option(
        False, "--dry-run", help="Show what would happen without making changes"
    ),
):
    """Move a file."""
    try:
        # Run the operation
        success = asyncio.run(move_file(
            source, destination, overwrite=force, dry_run=dry_run
        ))
        
        if success:
            if dry_run:
                console.print(f"[bold blue]DRY RUN:[/bold blue] Would move {source} to {destination}")
            else:
                console.print(f"[bold green]Moved:[/bold green] {source} -> {destination}")
        else:
            console.print(f"[bold yellow]Operation cancelled.[/bold yellow]")
            sys.exit(1)
        
    except FileSystemError as e:
        logger.exception(f"Error moving file: {str(e)}")
        console.print(f"[bold red]Error:[/bold red] {str(e)}")
        sys.exit(1)


@app.command("write")
def write_file_command(
    path: str = typer.Argument(..., help="File to write to"),
    content: str = typer.Option(
        None, "--content", "-c", help="Content to write (if not provided, will prompt)"
    ),
    append: bool = typer.Option(
        False, "--append", "-a", help="Append to file instead of overwriting"
    ),
    dry_run: bool = typer.Option(
        False, "--dry-run", help="Show what would happen without making changes"
    ),
):
    """Write content to a file."""
    try:
        # If content is not provided, prompt for it
        if content is None:
            console.print(f"Enter content for {path} (press Ctrl+D on a new line to finish):")
            lines = []
            try:
                while True:
                    line = input()
                    lines.append(line)
            except EOFError:
                pass
            content = "\n".join(lines)
        
        # Run the operation
        success = asyncio.run(write_file(
            path, content, append=append, dry_run=dry_run
        ))
        
        if success:
            if dry_run:
                mode = "append to" if append else "write to"
                console.print(f"[bold blue]DRY RUN:[/bold blue] Would {mode} file: {path}")
            else:
                mode = "Appended to" if append else "Wrote to"
                console.print(f"[bold green]{mode} file:[/bold green] {path}")
        else:
            console.print(f"[bold yellow]Operation cancelled.[/bold yellow]")
            sys.exit(1)
        
    except FileSystemError as e:
        logger.exception(f"Error writing file: {str(e)}")
        console.print(f"[bold red]Error:[/bold red] {str(e)}")
        sys.exit(1)


@app.command("find")
def find_files(
    pattern: str = typer.Argument(..., help="Pattern to search for"),
    path: str = typer.Option(
        ".", "--path", "-p", help="Directory to search in"
    ),
    include_hidden: bool = typer.Option(
        False, "--hidden", "-a", help="Include hidden files"
    ),
):
    """Find files matching a pattern."""
    try:
        base_dir = Path(path)
        if not base_dir.exists() or not base_dir.is_dir():
            console.print(f"[bold red]Error:[/bold red] Not a valid directory: {path}")
            sys.exit(1)
        
        # Find files matching the pattern
        matches = context_manager.find_files(
            pattern, base_dir=base_dir, include_hidden=include_hidden
        )
        
        if not matches:
            console.print(f"No files found matching pattern: {pattern}")
            return
        
        # Display results
        console.print(f"Found {len(matches)} files matching '{pattern}':")
        
        for match in matches:
            # Get file info
            file_info = context_manager.get_file_info(match)
            
            # Format the output
            if file_info.get("is_dir", False):
                console.print(f"[bold blue]{match}/[/bold blue]")
            elif file_info.get("language"):
                lang = file_info.get("language", "")
                console.print(f"[green]{match}[/green] - [magenta]{lang}[/magenta]")
            else:
                console.print(str(match))
        
    except Exception as e:
        logger.exception(f"Error finding files: {str(e)}")
        console.print(f"[bold red]Error:[/bold red] {str(e)}")
        sys.exit(1)


@app.command("rollback")
def rollback_command(
    list_only: bool = typer.Option(
        False, "--list", "-l", help="List recent operations without rolling back"
    ),
    operation_id: int = typer.Option(
        None, "--id", help="ID of the specific operation to roll back"
    ),
):
    """Roll back a previous file operation."""
    try:
        # Get recent operations
        operations = asyncio.run(rollback_manager.get_recent_operations())
        
        if not operations:
            console.print("No operations available for rollback.")
            return
        
        # If list_only is specified, just show the operations
        if list_only:
            table = Table(title="Recent Operations")
            table.add_column("ID", style="cyan")
            table.add_column("Timestamp", style="blue")
            table.add_column("Operation", style="green")
            table.add_column("Description")
            table.add_column("Can Rollback", style="magenta")
            
            for op in operations:
                can_rollback = "✓" if op["can_rollback"] else "✗"
                table.add_row(
                    str(op["id"]),
                    op["timestamp"],
                    op["operation_type"],
                    op["description"],
                    can_rollback
                )
            
            console.print(table)
            return
        
        # If no ID is provided, show the list and prompt for one
        if operation_id is None:
            table = Table(title="Recent Operations")
            table.add_column("ID", style="cyan")
            table.add_column("Timestamp", style="blue")
            table.add_column("Description")
            table.add_column("Can Rollback", style="magenta")
            
            for op in operations:
                can_rollback = "✓" if op["can_rollback"] else "✗"
                table.add_row(
                    str(op["id"]),
                    op["timestamp"],
                    op["description"],
                    can_rollback
                )
            
            console.print(table)
            
            # Prompt for the operation ID
            operation_id = int(Prompt.ask("Enter the ID of the operation to roll back"))
        
        # Check if the operation ID is valid
        valid_ids = [op["id"] for op in operations]
        if operation_id not in valid_ids:
            console.print(f"[bold red]Error:[/bold red] Invalid operation ID: {operation_id}")
            sys.exit(1)
        
        # Check if the operation can be rolled back
        operation = next(op for op in operations if op["id"] == operation_id)
        if not operation["can_rollback"]:
            console.print(f"[bold red]Error:[/bold red] Operation cannot be rolled back: {operation['description']}")
            sys.exit(1)
        
        # Confirm the rollback
        confirmed = Confirm.ask(f"Roll back operation: {operation['description']}?")
        if not confirmed:
            console.print("Rollback cancelled.")
            return
        
        # Perform the rollback
        success = asyncio.run(rollback_manager.rollback_operation(operation_id))
        
        if success:
            console.print(f"[bold green]Successfully rolled back:[/bold green] {operation['description']}")
        else:
            console.print(f"[bold red]Failed to roll back operation.[/bold red]")
            sys.exit(1)
        
    except Exception as e:
        logger.exception(f"Error during rollback: {str(e)}")
        console.print(f"[bold red]Error:[/bold red] {str(e)}")
        sys.exit(1)


@app.command("info")
def file_info(
    path: str = typer.Argument(
        None, help="File to get information about (defaults to current directory)"
    ),
    preview: bool = typer.Option(
        True, "--preview/--no-preview", help="Show file content preview"
    ),
):
    """Show detailed information about a file or directory."""
    try:
        file_path = Path(path) if path else context_manager.cwd
        
        if not file_path.exists():
            console.print(f"[bold red]Error:[/bold red] Path not found: {file_path}")
            sys.exit(1)
        
        # Get file information
        file_info = context_manager.get_file_info(file_path)
        
        # Display information
        console.print(Panel(
            f"[bold]Path:[/bold] {file_info['path']}\n"
            f"[bold]Type:[/bold] {file_info.get('type', 'unknown')}\n"
            + (f"[bold]Language:[/bold] {file_info.get('language', 'N/A')}\n" if file_info.get('language') else "")
            + (f"[bold]MIME Type:[/bold] {file_info.get('mime_type', 'N/A')}\n" if file_info.get('mime_type') else "")
            + (f"[bold]Size:[/bold] {format_size(file_info.get('size', 0))}\n" if 'size' in file_info else "")
            + (f"[bold]Binary:[/bold] {'Yes' if file_info.get('binary', False) else 'No'}\n" if not file_info.get('is_dir', False) else ""),
            title=f"Information for {file_path.name}",
            expand=False
        ))
        
        # Show content preview for files
        if preview and not file_info.get('is_dir', False) and not file_info.get('binary', False):
            content_preview = context_manager.get_file_preview(file_path)
            
            if content_preview:
                # Try to determine the language for syntax highlighting
                lang = "text"
                if file_info.get("language"):
                    lang_name = file_info.get("language", "").lower()
                    if "python" in lang_name:
                        lang = "python"
                    elif "javascript" in lang_name:
                        lang = "javascript"
                    elif "html" in lang_name:
                        lang = "html"
                    elif "css" in lang_name:
                        lang = "css"
                    elif "json" in lang_name:
                        lang = "json"
                    elif "yaml" in lang_name:
                        lang = "yaml"
                    elif "markdown" in lang_name:
                        lang = "markdown"
                    elif "bash" in lang_name or "shell" in lang_name:
                        lang = "bash"
                
                console.print(Panel(
                    Syntax(
                        content_preview,
                        lang,
                        theme="monokai",
                        line_numbers=True,
                        word_wrap=True
                    ),
                    title="Content Preview",
                    expand=False
                ))
        
    except Exception as e:
        logger.exception(f"Error getting file information: {str(e)}")
        console.print(f"[bold red]Error:[/bold red] {str(e)}")
        sys.exit(1)
</file>

<file path="angela/context/enhancer.py">
"""
Enhanced project context management for Angela CLI.

This module provides advanced context enrichment by integrating project inference,
dependency detection, and file activity tracking to provide a richer context
for AI interactions.
"""
import asyncio
from typing import Dict, Any, Optional, List, Set
from pathlib import Path

from angela.context import context_manager
from angela.context.project_inference import project_inference
from angela.context.session import session_manager
from angela.config import config_manager
from angela.utils.logging import get_logger

logger = get_logger(__name__)

class ContextEnhancer:
    """
    Enhanced context manager that enriches context with project information,
    dependency detection, and recent activity tracking.
    """
    
    def __init__(self):
        """Initialize the context enhancer."""
        self._logger = logger
        self._project_info_cache = {}  # Cache project info by path
        self._file_activity_cache = {}  # Cache recent file activity
    
    async def enrich_context(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """
        Enrich the context with additional information.
        
        Args:
            context: The base context to enrich
            
        Returns:
            The enriched context
        """
        self._logger.debug("Enriching context with additional information")
        
        # Start with a copy of the original context
        enriched = dict(context)
        
        # Add enhanced project information if in a project
        if context.get("project_root"):
            await self._add_project_info(enriched, context["project_root"])
        
        # Add recent file activity
        await self._add_recent_file_activity(enriched)
        
        # Add file reference context
        await self._add_file_reference_context(enriched)
        
        self._logger.debug(f"Context enriched with {len(enriched) - len(context)} additional keys")
        return enriched
    
    async def _add_project_info(self, context: Dict[str, Any], project_root: str) -> None:
        """
        Add enhanced project information to the context.
        
        Args:
            context: The context to enrich
            project_root: The path to the project root
        """
        self._logger.debug(f"Adding project info for {project_root}")
        
        try:
            # Check cache first
            if project_root in self._project_info_cache:
                project_info = self._project_info_cache[project_root]
                self._logger.debug(f"Using cached project info for {project_root}")
            else:
                # Get project info from project_inference
                project_info = await project_inference.infer_project_info(Path(project_root))
                # Cache the result
                self._project_info_cache[project_root] = project_info
                self._logger.debug(f"Inferred project info for {project_root}")
            
            # Add project info to context
            context["enhanced_project"] = {
                "type": project_info.get("project_type", "unknown"),
                "frameworks": project_info.get("detected_frameworks", {}),
                "dependencies": self._format_dependencies(project_info.get("dependencies", [])),
                "important_files": self._format_important_files(project_info.get("detected_files", [])),
                "structure": self._summarize_structure(project_info.get("structure", {}))
            }
            
            self._logger.debug(f"Added enhanced project info to context: {context['enhanced_project']['type']}")
        except Exception as e:
            self._logger.error(f"Error adding project info: {str(e)}")
    
    def _format_dependencies(self, dependencies: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        Format dependencies for inclusion in context.
        
        Args:
            dependencies: List of dependency information
            
        Returns:
            Formatted dependency information
        """
        # Group dependencies by type
        dependency_types = {}
        
        for dep in dependencies:
            dep_type = dep.get("type", "unknown")
            if dep_type not in dependency_types:
                dependency_types[dep_type] = []
            
            dependency_types[dep_type].append({
                "name": dep.get("name", "unknown"),
                "version": dep.get("version_spec", "")
            })
        
        # Return summary with counts
        return {
            "types": list(dependency_types.keys()),
            "counts": {t: len(deps) for t, deps in dependency_types.items()},
            "total": len(dependencies),
            "top_dependencies": [d.get("name", "unknown") for d in dependencies[:10]]
        }
    
    def _format_important_files(self, files: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        Format important files for inclusion in context.
        
        Args:
            files: List of important file information
            
        Returns:
            Formatted important file information
        """
        # Group files by type
        file_types = {}
        paths = []
        
        for file in files:
            file_type = file.get("type", "unknown")
            if file_type not in file_types:
                file_types[file_type] = []
            
            file_types[file_type].append(file.get("path", "unknown"))
            paths.append(file.get("path", "unknown"))
        
        # Return summary
        return {
            "types": list(file_types.keys()),
            "counts": {t: len(files) for t, files in file_types.items()},
            "total": len(files),
            "paths": paths
        }
    
    def _summarize_structure(self, structure: Dict[str, Any]) -> Dict[str, Any]:
        """
        Summarize project structure for inclusion in context.
        
        Args:
            structure: Project structure information
            
        Returns:
            Summarized structure information
        """
        # Extract key information from structure
        if not structure:
            return {}
        
        # Return summary
        return {
            "file_counts": structure.get("file_counts", {}),
            "total_files": structure.get("total_files", 0),
            "main_directories": [d.get("name", "") for d in structure.get("main_directories", [])]
        }
    
    async def _add_recent_file_activity(self, context: Dict[str, Any]) -> None:
        """
        Add recent file activity to the context.
        
        Args:
            context: The context to enrich
        """
        self._logger.debug("Adding recent file activity to context")
        
        try:
            # Get recent file activities from session
            session = session_manager.get_context()
            entities = session.get("entities", {})
            
            # Filter for file-related entities
            file_entities = {
                name: entity for name, entity in entities.items()
                if entity.get("type") in ["file", "directory", "recent_file"]
            }
            
            # Get recent file activities from history
            recent_activities = []
            
            # Format and add to context
            context["recent_files"] = {
                "accessed": [entity.get("value", "") for name, entity in file_entities.items()],
                "activities": recent_activities,
                "count": len(file_entities)
            }
            
            self._logger.debug(f"Added {len(file_entities)} recent files to context")
        except Exception as e:
            self._logger.error(f"Error adding recent file activity: {str(e)}")
    
    async def _add_file_reference_context(self, context: Dict[str, Any]) -> None:
        """
        Add file reference context information.
        
        Args:
            context: The context to enrich
        """
        self._logger.debug("Adding file reference context")
        
        try:
            # Get current working directory
            cwd = context.get("cwd", "")
            if not cwd:
                return
            
            # List files in the current directory
            cwd_path = Path(cwd)
            files = list(cwd_path.glob("*"))
            
            # Format file information
            file_info = {
                "files": [f.name for f in files if f.is_file()],
                "directories": [f.name for f in files if f.is_dir()],
                "total": len(files)
            }
            
            # Add to context
            context["file_reference"] = file_info
            
            self._logger.debug(f"Added file reference context with {len(files)} files")
        except Exception as e:
            self._logger.error(f"Error adding file reference context: {str(e)}")
    
    def clear_cache(self) -> None:
        """Clear the context enhancer cache."""
        self._logger.debug("Clearing context enhancer cache")
        self._project_info_cache.clear()
        self._file_activity_cache.clear()

# Global context enhancer instance
context_enhancer = ContextEnhancer()
</file>

<file path="angela/context/file_activity.py">
"""
File activity tracking for Angela CLI.

This module provides functionality to track file activities such as access,
modification, and creation, and to maintain a history of these activities.
"""
import os
import time
from datetime import datetime
from enum import Enum
from pathlib import Path
from typing import Dict, Any, Optional, List, Set, Union

from angela.context.session import session_manager
from angela.utils.logging import get_logger

logger = get_logger(__name__)

class ActivityType(str, Enum):
    """Types of file activities."""
    CREATED = "created"
    MODIFIED = "modified"
    DELETED = "deleted"
    VIEWED = "viewed"
    EXECUTED = "executed"
    ANALYZED = "analyzed"
    OTHER = "other"

class FileActivity:
    """Represents a file activity with related metadata."""
    
    def __init__(
        self,
        path: Union[str, Path],
        activity_type: ActivityType,
        timestamp: Optional[float] = None,
        command: Optional[str] = None,
        details: Optional[Dict[str, Any]] = None
    ):
        """
        Initialize a file activity.
        
        Args:
            path: Path to the file/directory
            activity_type: Type of activity
            timestamp: Optional timestamp (defaults to current time)
            command: Optional command that triggered the activity
            details: Optional additional details
        """
        self.path = Path(path) if isinstance(path, str) else path
        self.activity_type = activity_type
        self.timestamp = timestamp or time.time()
        self.command = command
        self.details = details or {}
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for serialization."""
        return {
            "path": str(self.path),
            "name": self.path.name,
            "activity_type": self.activity_type.value,
            "timestamp": self.timestamp,
            "datetime": datetime.fromtimestamp(self.timestamp).isoformat(),
            "command": self.command,
            "details": self.details
        }
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'FileActivity':
        """Create from dictionary."""
        return cls(
            path=data["path"],
            activity_type=ActivityType(data["activity_type"]),
            timestamp=data["timestamp"],
            command=data.get("command"),
            details=data.get("details", {})
        )


class FileActivityTracker:
    """
    Tracker for file activities with session integration.
    
    Provides methods to:
    1. Track file activities (creation, modification, etc.)
    2. Retrieve recent file activities
    3. Integrate with session management
    """
    
    def __init__(self, max_activities: int = 100):
        """
        Initialize the file activity tracker.
        
        Args:
            max_activities: Maximum number of activities to track
        """
        self._logger = logger
        self._activities: List[FileActivity] = []
        self._max_activities = max_activities
    
    def track_activity(
        self,
        path: Union[str, Path],
        activity_type: ActivityType,
        command: Optional[str] = None,
        details: Optional[Dict[str, Any]] = None
    ) -> None:
        """
        Track a file activity.
        
        Args:
            path: Path to the file/directory
            activity_type: Type of activity
            command: Optional command that triggered the activity
            details: Optional additional details
        """
        # Create a new activity
        activity = FileActivity(
            path=path,
            activity_type=activity_type,
            command=command,
            details=details
        )
        
        # Add to the activity list
        self._activities.append(activity)
        
        # Trim if needed
        if len(self._activities) > self._max_activities:
            self._activities = self._activities[-self._max_activities:]
        
        # Update session
        self._update_session(activity)
        
        self._logger.debug(f"Tracked {activity_type.value} activity for {path}")
    
    def track_file_creation(
        self,
        path: Union[str, Path],
        command: Optional[str] = None,
        details: Optional[Dict[str, Any]] = None
    ) -> None:
        """
        Track file creation.
        
        Args:
            path: Path to the created file
            command: Optional command that triggered the creation
            details: Optional additional details
        """
        self.track_activity(
            path=path,
            activity_type=ActivityType.CREATED,
            command=command,
            details=details
        )
    
    def track_file_modification(
        self,
        path: Union[str, Path],
        command: Optional[str] = None,
        details: Optional[Dict[str, Any]] = None
    ) -> None:
        """
        Track file modification.
        
        Args:
            path: Path to the modified file
            command: Optional command that triggered the modification
            details: Optional additional details
        """
        self.track_activity(
            path=path,
            activity_type=ActivityType.MODIFIED,
            command=command,
            details=details
        )
    
    def track_file_deletion(
        self,
        path: Union[str, Path],
        command: Optional[str] = None,
        details: Optional[Dict[str, Any]] = None
    ) -> None:
        """
        Track file deletion.
        
        Args:
            path: Path to the deleted file
            command: Optional command that triggered the deletion
            details: Optional additional details
        """
        self.track_activity(
            path=path,
            activity_type=ActivityType.DELETED,
            command=command,
            details=details
        )
    
    def track_file_viewing(
        self,
        path: Union[str, Path],
        command: Optional[str] = None,
        details: Optional[Dict[str, Any]] = None
    ) -> None:
        """
        Track file viewing.
        
        Args:
            path: Path to the viewed file
            command: Optional command that triggered the viewing
            details: Optional additional details
        """
        self.track_activity(
            path=path,
            activity_type=ActivityType.VIEWED,
            command=command,
            details=details
        )
    
    def get_recent_activities(
        self,
        limit: int = 10,
        activity_types: Optional[List[ActivityType]] = None
    ) -> List[Dict[str, Any]]:
        """
        Get recent file activities.
        
        Args:
            limit: Maximum number of activities to return
            activity_types: Optional filter for activity types
            
        Returns:
            List of activities as dictionaries
        """
        # Apply filters
        filtered = self._activities
        if activity_types:
            filtered = [a for a in filtered if a.activity_type in activity_types]
        
        # Sort by timestamp (newest first)
        sorted_activities = sorted(filtered, key=lambda a: a.timestamp, reverse=True)
        
        # Convert to dictionaries
        return [a.to_dict() for a in sorted_activities[:limit]]
    
    def get_activities_for_path(
        self,
        path: Union[str, Path],
        limit: int = 10
    ) -> List[Dict[str, Any]]:
        """
        Get activities for a specific path.
        
        Args:
            path: Path to get activities for
            limit: Maximum number of activities to return
            
        Returns:
            List of activities as dictionaries
        """
        path_obj = Path(path) if isinstance(path, str) else path
        
        # Filter by path
        path_activities = [a for a in self._activities if a.path == path_obj]
        
        # Sort by timestamp (newest first)
        sorted_activities = sorted(path_activities, key=lambda a: a.timestamp, reverse=True)
        
        # Convert to dictionaries
        return [a.to_dict() for a in sorted_activities[:limit]]
    
    def get_most_active_files(self, limit: int = 5) -> List[Dict[str, Any]]:
        """
        Get the most actively used files.
        
        Args:
            limit: Maximum number of files to return
            
        Returns:
            List of files with activity counts
        """
        # Count activities by path
        path_counts = {}
        for activity in self._activities:
            path_str = str(activity.path)
            if path_str not in path_counts:
                path_counts[path_str] = {
                    "path": path_str,
                    "name": activity.path.name,
                    "count": 0,
                    "last_activity": None,
                    "activities": set()
                }
            
            path_counts[path_str]["count"] += 1
            path_counts[path_str]["activities"].add(activity.activity_type.value)
            
            # Update last activity if newer
            if path_counts[path_str]["last_activity"] is None or \
               activity.timestamp > path_counts[path_str]["last_activity"]:
                path_counts[path_str]["last_activity"] = activity.timestamp
        
        # Convert to list and sort by count (highest first)
        result = []
        for path_info in path_counts.values():
            # Convert activities set to list
            path_info["activities"] = list(path_info["activities"])
            result.append(path_info)
        
        result.sort(key=lambda x: x["count"], reverse=True)
        
        return result[:limit]
    
    def clear_activities(self) -> None:
        """Clear all tracked activities."""
        self._activities.clear()
        self._logger.debug("Cleared all file activities")
    
    def _update_session(self, activity: FileActivity) -> None:
        """
        Update session with file activity.
        
        Args:
            activity: The file activity to add to the session
        """
        try:
            # Add to session as an entity
            path_name = activity.path.name
            entity_name = f"file:{path_name}"
            
            session_manager.add_entity(
                name=entity_name,
                entity_type="file",
                value=str(activity.path)
            )
            
            # Also add with activity type
            activity_entity_name = f"{activity.activity_type.value}_file:{path_name}"
            session_manager.add_entity(
                name=activity_entity_name,
                entity_type=f"{activity.activity_type.value}_file",
                value=str(activity.path)
            )
        except Exception as e:
            self._logger.error(f"Error updating session with file activity: {str(e)}")

# Global file activity tracker instance
file_activity_tracker = FileActivityTracker()
</file>

<file path="angela/context/file_resolver.py">
"""
File reference resolution for Angela CLI.

This module provides functionality to resolve file references from natural
language queries, using a combination of techniques such as exact matching,
fuzzy matching, pattern matching, and context-aware resolution.
"""
import os
import re
import difflib
from pathlib import Path
from typing import Dict, Any, Optional, List, Tuple, Union, Set

from angela.context import context_manager
from angela.context.session import session_manager
from angela.utils.logging import get_logger

logger = get_logger(__name__)

class FileResolver:
    """
    Resolver for file references in natural language queries.
    
    Provides multiple strategies for resolving file references:
    1. Exact path matching
    2. Fuzzy name matching
    3. Pattern matching
    4. Context-aware resolution (recent files, project files)
    5. Special references (current file, last modified file)
    """
    
    def __init__(self):
        """Initialize the file resolver."""
        self._logger = logger
        self._threshold = 0.6  # Threshold for fuzzy matching
    
    async def resolve_reference(
        self, 
        reference: str, 
        context: Dict[str, Any],
        search_scope: Optional[str] = None
    ) -> Optional[Path]:
        """
        Resolve a file reference to an actual file path.
        
        Args:
            reference: The file reference to resolve
            context: Context information
            search_scope: Optional scope for the search (project, directory, recent)
            
        Returns:
            Path object if found, None otherwise
        """
        self._logger.info(f"Resolving file reference: '{reference}'")
        
        # Strip quotes if present
        reference = reference.strip('\'"')
        
        # If reference is empty or None, return None
        if not reference:
            return None
        
        # Try resolving with each strategy in order
        resolved = await self._resolve_exact_path(reference, context)
        if resolved:
            self._logger.debug(f"Resolved via exact path: {resolved}")
            self._record_resolution(reference, resolved, "exact_path")
            return resolved
        
        resolved = await self._resolve_special_reference(reference, context)
        if resolved:
            self._logger.debug(f"Resolved via special reference: {resolved}")
            self._record_resolution(reference, resolved, "special_reference")
            return resolved
        
        resolved = await self._resolve_recent_file(reference, context)
        if resolved:
            self._logger.debug(f"Resolved via recent file: {resolved}")
            self._record_resolution(reference, resolved, "recent_file")
            return resolved
        
        resolved = await self._resolve_fuzzy_match(reference, context, search_scope)
        if resolved:
            self._logger.debug(f"Resolved via fuzzy match: {resolved}")
            self._record_resolution(reference, resolved, "fuzzy_match")
            return resolved
        
        resolved = await self._resolve_pattern_match(reference, context, search_scope)
        if resolved:
            self._logger.debug(f"Resolved via pattern match: {resolved}")
            self._record_resolution(reference, resolved, "pattern_match")
            return resolved
        
        # If all strategies fail, log and return None
        self._logger.warning(f"Could not resolve file reference: '{reference}'")
        return None
    
    async def extract_references(
        self, 
        text: str,
        context: Dict[str, Any]
    ) -> List[Tuple[str, Optional[Path]]]:
        """
        Extract and resolve file references from text.
        
        Args:
            text: The text to extract references from
            context: Context information
            
        Returns:
            List of (reference, resolved_path) tuples
        """
        self._logger.info(f"Extracting file references from: '{text}'")
        
        # Define patterns for finding file references
        patterns = [
            # Quoted paths
            r'["\']([^"\']+?\.[a-zA-Z0-9]+)["\']',
            # File/folder with extension
            r'\b([a-zA-Z0-9_\-/\.]+\.[a-zA-Z0-9]+)\b',
            # References to files/folders
            r'(?:file|folder|directory|script|code|document)\s+["\']?([^\s"\']+)["\']?',
            # References with in/from/to
            r'(?:in|from|to)\s+["\']?([^\s"\']+)["\']?',
        ]
        
        references = []
        
        # Extract references using each pattern
        for pattern in patterns:
            for match in re.finditer(pattern, text, re.IGNORECASE):
                reference = match.group(1)
                # Skip if reference is already in the list
                if any(ref == reference for ref, _ in references):
                    continue
                
                # Try to resolve the reference
                resolved = await self.resolve_reference(reference, context)
                references.append((reference, resolved))
        
        self._logger.debug(f"Extracted {len(references)} file references")
        return references
    
    async def _resolve_exact_path(
        self, 
        reference: str, 
        context: Dict[str, Any]
    ) -> Optional[Path]:
        """
        Resolve a reference as an exact path.
        
        Args:
            reference: The reference to resolve
            context: Context information
            
        Returns:
            Path object if found, None otherwise
        """
        # Try as absolute path
        path = Path(reference)
        if path.is_absolute() and path.exists():
            return path
        
        # Try relative to current directory
        cwd_path = Path(context["cwd"]) / reference
        if cwd_path.exists():
            return cwd_path
        
        # Try relative to project root if available
        if context.get("project_root"):
            proj_path = Path(context["project_root"]) / reference
            if proj_path.exists():
                return proj_path
        
        return None
    
    async def _resolve_special_reference(
        self, 
        reference: str, 
        context: Dict[str, Any]
    ) -> Optional[Path]:
        """
        Resolve special references like "current file", "last modified", etc.
        
        Args:
            reference: The reference to resolve
            context: Context information
            
        Returns:
            Path object if found, None otherwise
        """
        # Handle various special references
        lowercase_ref = reference.lower()
        
        # Current file
        if lowercase_ref in ["current file", "this file", "current"]:
            if context.get("current_file") and "path" in context["current_file"]:
                return Path(context["current_file"]["path"])
        
        # Last modified file (via session)
        if lowercase_ref in ["last file", "last modified", "previous file"]:
            session = session_manager.get_context()
            entities = session.get("entities", {})
            
            # Look for the most recent file entity
            last_file = None
            last_time = None
            
            for name, entity in entities.items():
                if entity.get("type") in ["file", "recent_file"]:
                    entity_time = entity.get("created")
                    if entity_time and (not last_time or entity_time > last_time):
                        last_time = entity_time
                        last_file = entity.get("value")
            
            if last_file:
                return Path(last_file)
        
        return None
    
    async def _resolve_recent_file(
        self, 
        reference: str, 
        context: Dict[str, Any]
    ) -> Optional[Path]:
        """
        Resolve a reference against recently used files.
        
        Args:
            reference: The reference to resolve
            context: Context information
            
        Returns:
            Path object if found, None otherwise
        """
        # Get recent files from session
        session = session_manager.get_context()
        entities = session.get("entities", {})
        
        # Filter for file entities
        file_entities = {
            name: entity for name, entity in entities.items()
            if entity.get("type") in ["file", "directory", "recent_file"]
        }
        
        # Look for exact matches first
        for name, entity in file_entities.items():
            path = Path(entity.get("value", ""))
            if path.name.lower() == reference.lower():
                return path
        
        # Then try fuzzy matches
        for name, entity in file_entities.items():
            path = Path(entity.get("value", ""))
            similarity = difflib.SequenceMatcher(None, path.name.lower(), reference.lower()).ratio()
            if similarity >= self._threshold:
                return path
        
        return None
    
    async def _resolve_fuzzy_match(
        self, 
        reference: str, 
        context: Dict[str, Any],
        search_scope: Optional[str] = None
    ) -> Optional[Path]:
        """
        Resolve a reference using fuzzy matching.
        
        Args:
            reference: The reference to resolve
            context: Context information
            search_scope: Optional scope for the search
            
        Returns:
            Path object if found, None otherwise
        """
        paths_to_check = []
        
        # Get paths based on search scope
        if search_scope == "project" and context.get("project_root"):
            # Get all files in the project
            project_path = Path(context["project_root"])
            paths_to_check.extend(project_path.glob("**/*"))
        elif search_scope == "directory":
            # Get all files in the current directory
            cwd_path = Path(context["cwd"])
            paths_to_check.extend(cwd_path.glob("*"))
        else:
            # Default: check both current directory and project root
            cwd_path = Path(context["cwd"])
            paths_to_check.extend(cwd_path.glob("*"))
            
            if context.get("project_root"):
                project_path = Path(context["project_root"])
                
                # Only search project root if different from current directory
                if project_path != cwd_path:
                    # Get direct children of project root
                    paths_to_check.extend(project_path.glob("*"))
                    
                    # Add common directories like src, lib, test
                    common_dirs = ["src", "lib", "test", "tests", "docs", "app", "bin"]
                    for dirname in common_dirs:
                        dir_path = project_path / dirname
                        if dir_path.exists() and dir_path.is_dir():
                            paths_to_check.extend(dir_path.glob("*"))
        
        # Deduplicate paths
        paths_to_check = list(set(paths_to_check))
        
        # Sort paths by the similarity of their name to the reference
        matches = []
        for path in paths_to_check:
            similarity = difflib.SequenceMatcher(None, path.name.lower(), reference.lower()).ratio()
            if similarity >= self._threshold:
                matches.append((path, similarity))
        
        # Sort by similarity (highest first)
        matches.sort(key=lambda x: x[1], reverse=True)
        
        # Return the best match if any
        if matches:
            return matches[0][0]
        
        return None
    
    async def _resolve_pattern_match(
        self, 
        reference: str, 
        context: Dict[str, Any],
        search_scope: Optional[str] = None
    ) -> Optional[Path]:
        """
        Resolve a reference using pattern matching.
        
        Args:
            reference: The reference to resolve
            context: Context information
            search_scope: Optional scope for the search
            
        Returns:
            Path object if found, None otherwise
        """
        # Try to interpret the reference as a glob pattern
        base_paths = []
        
        # Determine base paths based on search scope
        if search_scope == "project" and context.get("project_root"):
            base_paths.append(Path(context["project_root"]))
        elif search_scope == "directory":
            base_paths.append(Path(context["cwd"]))
        else:
            # Default: check both current directory and project root
            base_paths.append(Path(context["cwd"]))
            if context.get("project_root"):
                project_path = Path(context["project_root"])
                if project_path != Path(context["cwd"]):
                    base_paths.append(project_path)
        
        # Try each base path
        for base_path in base_paths:
            # Try with wildcard prefix/suffix if needed
            patterns_to_try = [
                reference,  # As-is
                f"*{reference}*",  # Wildcard prefix and suffix
                f"*{reference}",  # Wildcard prefix
                f"{reference}*"  # Wildcard suffix
            ]
            
            for pattern in patterns_to_try:
                try:
                    # Use glob to find matching files
                    matches = list(base_path.glob(pattern))
                    if matches:
                        return matches[0]  # Return the first match
                except Exception:
                    # Invalid pattern, try the next one
                    continue
        
        return None
    
    def _record_resolution(
        self, 
        reference: str, 
        resolved_path: Path, 
        method: str
    ) -> None:
        """
        Record a successful resolution for learning.
        
        Args:
            reference: The original reference
            resolved_path: The resolved path
            method: The method used for resolution
        """
        # Store in session for future reference
        try:
            session_manager.add_entity(
                name=f"file_ref:{reference}",
                entity_type="file_reference",
                value=str(resolved_path)
            )
            
            # Also store as a recent file
            session_manager.add_entity(
                name=f"recent_file:{resolved_path.name}",
                entity_type="recent_file",
                value=str(resolved_path)
            )
        except Exception as e:
            self._logger.error(f"Error recording resolution: {str(e)}")

# Global file resolver instance
file_resolver = FileResolver()
</file>

<file path="angela/context/history.py">
# angela/context/history.py

import json
import os
import re
from pathlib import Path
from typing import Dict, Any, Optional, List, Tuple
from datetime import datetime, timedelta
from collections import Counter, defaultdict

from angela.config import config_manager
from angela.utils.logging import get_logger
from angela.context.preferences import preferences_manager

logger = get_logger(__name__)

class CommandRecord:
    """Record of a command execution."""
    
    def __init__(
        self,
        command: str,
        natural_request: str,
        success: bool,
        timestamp: Optional[datetime] = None,
        output: Optional[str] = None,
        error: Optional[str] = None,
        risk_level: int = 0
    ):
        self.command = command
        self.natural_request = natural_request
        self.success = success
        self.timestamp = timestamp or datetime.now()
        self.output = output
        self.error = error
        self.risk_level = risk_level
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert the record to a dictionary for storage."""
        return {
            "command": self.command,
            "natural_request": self.natural_request,
            "success": self.success,
            "timestamp": self.timestamp.isoformat(),
            "output": self.output,
            "error": self.error,
            "risk_level": self.risk_level
        }
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'CommandRecord':
        """Create a record from a dictionary."""
        return cls(
            command=data["command"],
            natural_request=data["natural_request"],
            success=data["success"],
            timestamp=datetime.fromisoformat(data["timestamp"]),
            output=data.get("output"),
            error=data.get("error"),
            risk_level=data.get("risk_level", 0)
        )


class CommandPattern:
    """Pattern of commands executed by the user."""
    
    def __init__(self, base_command: str, count: int = 1, success_rate: float = 1.0):
        self.base_command = base_command
        self.count = count
        self.success_rate = success_rate
        self.last_used = datetime.now()
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert the pattern to a dictionary for storage."""
        return {
            "base_command": self.base_command,
            "count": self.count,
            "success_rate": self.success_rate,
            "last_used": self.last_used.isoformat()
        }
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'CommandPattern':
        """Create a pattern from a dictionary."""
        pattern = cls(
            base_command=data["base_command"],
            count=data["count"],
            success_rate=data["success_rate"]
        )
        pattern.last_used = datetime.fromisoformat(data["last_used"])
        return pattern


class HistoryManager:
    """Manager for command history and pattern analysis."""
    
    def __init__(self):
        """Initialize the history manager."""
        self._history_file = config_manager.CONFIG_DIR / "command_history.json"
        self._patterns_file = config_manager.CONFIG_DIR / "command_patterns.json"
        self._history: List[CommandRecord] = []
        self._patterns: Dict[str, CommandPattern] = {}
        self._load_history()
        self._load_patterns()
    
    def _load_history(self) -> None:
        """Load history from file."""
        try:
            if self._history_file.exists():
                with open(self._history_file, "r") as f:
                    data = json.load(f)
                    self._history = [CommandRecord.from_dict(item) for item in data]
                logger.debug(f"Loaded {len(self._history)} history items")
                
                # Trim history if needed
                max_items = preferences_manager.preferences.context.max_history_items
                if len(self._history) > max_items:
                    self._history = self._history[-max_items:]
                    self._save_history()  # Save the trimmed history
            else:
                logger.debug("No history file found")
        except Exception as e:
            logger.error(f"Error loading history: {e}")
            self._history = []
    
    def _load_patterns(self) -> None:
        """Load command patterns from file."""
        try:
            if self._patterns_file.exists():
                with open(self._patterns_file, "r") as f:
                    data = json.load(f)
                    self._patterns = {k: CommandPattern.from_dict(v) for k, v in data.items()}
                logger.debug(f"Loaded {len(self._patterns)} command patterns")
            else:
                logger.debug("No patterns file found")
        except Exception as e:
            logger.error(f"Error loading patterns: {e}")
            self._patterns = {}
    
    def _save_history(self) -> None:
        """Save history to file."""
        try:
            self._history_file.parent.mkdir(parents=True, exist_ok=True)
            with open(self._history_file, "w") as f:
                json.dump([item.to_dict() for item in self._history], f, indent=2)
            logger.debug(f"Saved history with {len(self._history)} items")
        except Exception as e:
            logger.error(f"Error saving history: {e}")
    
    def _save_patterns(self) -> None:
        """Save command patterns to file."""
        try:
            self._patterns_file.parent.mkdir(parents=True, exist_ok=True)
            with open(self._patterns_file, "w") as f:
                json.dump({k: v.to_dict() for k, v in self._patterns.items()}, f, indent=2)
            logger.debug(f"Saved {len(self._patterns)} command patterns")
        except Exception as e:
            logger.error(f"Error saving patterns: {e}")
    
    def add_command(
        self, 
        command: str, 
        natural_request: str, 
        success: bool,
        output: Optional[str] = None,
        error: Optional[str] = None,
        risk_level: int = 0
    ) -> None:
        """
        Add a command to the history.
        
        Args:
            command: The shell command executed
            natural_request: The natural language request
            success: Whether the command executed successfully
            output: Command output (if any)
            error: Command error (if any)
            risk_level: Risk level of the command
        """
        # Create and add the record
        record = CommandRecord(
            command=command,
            natural_request=natural_request,
            success=success,
            output=output,
            error=error,
            risk_level=risk_level
        )
        self._history.append(record)
        
        # Save the updated history
        self._save_history()
        
        # Update patterns if enabled
        if preferences_manager.preferences.context.auto_learn_patterns:
            self._update_patterns(record)
    
    def _extract_base_command(self, command: str) -> str:
        """
        Extract the base command without arguments.
        
        Args:
            command: The full command string
            
        Returns:
            The base command
        """
        # Extract the first word (command name)
        base = command.strip().split()[0]
        
        # For some commands, include the first argument if it's an operation
        if base in ["git", "docker", "npm", "pip", "apt", "apt-get"]:
            parts = command.strip().split()
            if len(parts) > 1 and not parts[1].startswith("-"):
                base = f"{base} {parts[1]}"
        
        return base
    
    def _update_patterns(self, record: CommandRecord) -> None:
        """
        Update command patterns based on a new record.
        
        Args:
            record: The command record
        """
        base_command = self._extract_base_command(record.command)
        
        if base_command in self._patterns:
            # Update existing pattern
            pattern = self._patterns[base_command]
            pattern.count += 1
            pattern.last_used = record.timestamp
            
            # Update success rate
            success_weight = 1.0 / pattern.count  # Weight of the new record
            pattern.success_rate = (
                (pattern.success_rate * (1 - success_weight)) + 
                (1.0 if record.success else 0.0) * success_weight
            )
        else:
            # Create new pattern
            self._patterns[base_command] = CommandPattern(
                base_command=base_command,
                count=1,
                success_rate=1.0 if record.success else 0.0
            )
        
        # Save updated patterns
        self._save_patterns()
    
    def get_recent_commands(self, limit: int = 10) -> List[CommandRecord]:
        """
        Get the most recent commands.
        
        Args:
            limit: Maximum number of commands to return
            
        Returns:
            List of recent CommandRecord objects
        """
        return self._history[-limit:]
    
    def get_command_frequency(self, command: str) -> int:
        """
        Get the frequency of a command.
        
        Args:
            command: The command to check
            
        Returns:
            The number of times the command has been executed
        """
        base_command = self._extract_base_command(command)
        pattern = self._patterns.get(base_command)
        return pattern.count if pattern else 0
    
    def get_command_success_rate(self, command: str) -> float:
        """
        Get the success rate of a command.
        
        Args:
            command: The command to check
            
        Returns:
            The success rate (0.0-1.0) or 0.0 if command not found
        """
        base_command = self._extract_base_command(command)
        pattern = self._patterns.get(base_command)
        return pattern.success_rate if pattern else 0.0
    
    def search_similar_command(self, request: str) -> Optional[str]:
        """
        Search for a similar command in history based on natural language request.
        
        Args:
            request: The natural language request
            
        Returns:
            A similar command if found, None otherwise
        """
        # Simple similarity: lowercase and remove punctuation
        request = re.sub(r'[^\w\s]', '', request.lower())
        
        for record in reversed(self._history):  # Start from most recent
            historical_request = re.sub(r'[^\w\s]', '', record.natural_request.lower())
            
            # Check for significant overlap in words
            request_words = set(request.split())
            historical_words = set(historical_request.split())
            
            # Calculate Jaccard similarity
            if request_words and historical_words:
                intersection = request_words.intersection(historical_words)
                union = request_words.union(historical_words)
                similarity = len(intersection) / len(union)
                
                # If similarity is high enough, return this command
                if similarity > 0.6:
                    return record.command
        
        return None
    
    def find_error_patterns(self, error: str) -> List[Tuple[str, str]]:
        """
        Find patterns in error messages and corresponding fixes.
        
        Args:
            error: The error message
            
        Returns:
            List of (failed_command, successful_fix) tuples
        """
        error_patterns = []
        
        # Find failed commands with this error
        for i, record in enumerate(self._history):
            if not record.success and record.error and error in record.error:
                # Look ahead for successful fixes
                for j in range(i+1, min(i+5, len(self._history))):
                    if self._history[j].success:
                        error_patterns.append((record.command, self._history[j].command))
                        break
        
        return error_patterns
    
    def get_common_command_contexts(self) -> Dict[str, List[str]]:
        """
        Get common command sequences or contexts.
        
        Returns:
            Dict mapping commands to commonly following commands
        """
        context_map = defaultdict(Counter)
        
        # Analyze command sequences
        for i in range(1, len(self._history)):
            prev_cmd = self._extract_base_command(self._history[i-1].command)
            curr_cmd = self._extract_base_command(self._history[i].command)
            context_map[prev_cmd][curr_cmd] += 1
        
        # Convert to more usable format
        result = {}
        for cmd, followers in context_map.items():
            # Get the most common followers
            result[cmd] = [cmd for cmd, count in followers.most_common(3)]
        
        return result

# Global history manager instance
history_manager = HistoryManager()
</file>

<file path="angela/context/preferences.py">
# angela/context/preferences.py

import json
import os
from pathlib import Path
from typing import Dict, Any, Optional, List
from pydantic import BaseModel, Field

from angela.config import config_manager
from angela.constants import RISK_LEVELS
from angela.utils.logging import get_logger

logger = get_logger(__name__)

class TrustPreferences(BaseModel):
    """Model for user trust preferences."""
    default_trust_level: int = Field(4, description="Default trust level (0-4)")
    auto_execute_safe: bool = Field(True, description="Auto-execute SAFE operations")
    auto_execute_low: bool = Field(True, description="Auto-execute LOW risk operations")
    auto_execute_medium: bool = Field(False, description="Auto-execute MEDIUM risk operations")
    auto_execute_high: bool = Field(False, description="Auto-execute HIGH risk operations")
    auto_execute_critical: bool = Field(False, description="Auto-execute CRITICAL risk operations")
    trusted_commands: List[str] = Field(default_factory=list, description="Commands that are always trusted")
    untrusted_commands: List[str] = Field(default_factory=list, description="Commands that require confirmation")

class UIPreferences(BaseModel):
    """Model for UI preferences."""
    show_command_preview: bool = Field(True, description="Show command preview before execution")
    show_impact_analysis: bool = Field(True, description="Show impact analysis for commands")
    use_rich_output: bool = Field(True, description="Use rich formatted output")
    verbose_feedback: bool = Field(True, description="Show detailed execution feedback")
    use_spinners: bool = Field(True, description="Show spinners for long-running operations")

class ContextPreferences(BaseModel):
    """Model for context preferences."""
    remember_session_context: bool = Field(True, description="Maintain context between commands")
    max_history_items: int = Field(50, description="Maximum number of history items to remember")
    auto_learn_patterns: bool = Field(True, description="Automatically learn command patterns")

class UserPreferences(BaseModel):
    """User preferences model."""
    trust: TrustPreferences = Field(default_factory=TrustPreferences, description="Trust settings")
    ui: UIPreferences = Field(default_factory=UIPreferences, description="UI settings")
    context: ContextPreferences = Field(default_factory=ContextPreferences, description="Context settings")

class PreferencesManager:
    """Manager for user preferences."""
    
    def __init__(self):
        """Initialize the preferences manager."""
        self._prefs = UserPreferences()
        self._prefs_file = config_manager.CONFIG_DIR / "preferences.json"
        self._load_preferences()
    
    def _load_preferences(self) -> None:
        """Load preferences from file."""
        try:
            if self._prefs_file.exists():
                with open(self._prefs_file, "r") as f:
                    data = json.load(f)
                    self._prefs = UserPreferences.parse_obj(data)
                logger.debug(f"Loaded preferences from {self._prefs_file}")
            else:
                logger.debug("No preferences file found, using defaults")
                self._save_preferences()  # Create the file with defaults
        except Exception as e:
            logger.error(f"Error loading preferences: {e}")
    
    def _save_preferences(self) -> None:
        """Save preferences to file."""
        try:
            with open(self._prefs_file, "w") as f:
                json.dump(self._prefs.dict(), f, indent=2)
            logger.debug(f"Saved preferences to {self._prefs_file}")
        except Exception as e:
            logger.error(f"Error saving preferences: {e}")
    
    def update_preferences(self, **kwargs) -> None:
        """Update preferences with provided values."""
        if not kwargs:
            return
            
        # Handle nested preferences
        for section in ["trust", "ui", "context"]:
            section_data = kwargs.pop(section, None)
            if section_data and isinstance(section_data, dict):
                for k, v in section_data.items():
                    setattr(getattr(self._prefs, section), k, v)
        
        # Handle top-level preferences
        for k, v in kwargs.items():
            setattr(self._prefs, k, v)
        
        self._save_preferences()
    
    def should_auto_execute(self, risk_level: int, command: str) -> bool:
        """
        Determine if a command should be auto-executed based on risk level
        and user preferences.
        
        Args:
            risk_level: The risk level of the command
            command: The command string
            
        Returns:
            True if should auto-execute, False if confirmation needed
        """
        # Check if the command is explicitly trusted or untrusted
        if command in self._prefs.trust.trusted_commands:
            return True
        if command in self._prefs.trust.untrusted_commands:
            return False
        
        # Check based on risk level
        if risk_level == RISK_LEVELS["SAFE"]:
            return self._prefs.trust.auto_execute_safe
        elif risk_level == RISK_LEVELS["LOW"]:
            return self._prefs.trust.auto_execute_low
        elif risk_level == RISK_LEVELS["MEDIUM"]:
            return self._prefs.trust.auto_execute_medium
        elif risk_level == RISK_LEVELS["HIGH"]:
            return self._prefs.trust.auto_execute_high
        elif risk_level == RISK_LEVELS["CRITICAL"]:
            return self._prefs.trust.auto_execute_critical
        
        # Default to require confirmation
        return False
    
    def add_trusted_command(self, command: str) -> None:
        """Add a command to the trusted commands list."""
        if command not in self._prefs.trust.trusted_commands:
            self._prefs.trust.trusted_commands.append(command)
            # Remove from untrusted if present
            if command in self._prefs.trust.untrusted_commands:
                self._prefs.trust.untrusted_commands.remove(command)
            self._save_preferences()
    
    def add_untrusted_command(self, command: str) -> None:
        """Add a command to the untrusted commands list."""
        if command not in self._prefs.trust.untrusted_commands:
            self._prefs.trust.untrusted_commands.append(command)
            # Remove from trusted if present
            if command in self._prefs.trust.trusted_commands:
                self._prefs.trust.trusted_commands.remove(command)
            self._save_preferences()
    
    @property
    def preferences(self) -> UserPreferences:
        """Get the current preferences."""
        return self._prefs

# Create a global instance of the preferences manager
preferences_manager = PreferencesManager()
</file>

<file path="angela/context/project_inference.py">
# angela/context/project_inference.py

import os
import glob
import json
import re
import asyncio
from pathlib import Path
from typing import Dict, Any, List, Set, Optional, Tuple

from angela.utils.logging import get_logger

logger = get_logger(__name__)

class ProjectInference:
    """
    Advanced project type and structure inference.
    
    This class provides:
    1. Detection of project type based on files and structure
    2. Inference of project dependencies
    3. Identification of important project files
    4. Framework and technology detection
    """
    
    # Project type signatures
    PROJECT_SIGNATURES = {
        "python": {
            "files": ["requirements.txt", "setup.py", "pyproject.toml", "Pipfile"],
            "directories": ["venv", ".venv", "env", ".env"],
            "extensions": [".py", ".pyi", ".pyx"]
        },
        "node": {
            "files": ["package.json", "package-lock.json", "yarn.lock", "node_modules"],
            "extensions": [".js", ".jsx", ".ts", ".tsx"]
        },
        "rust": {
            "files": ["Cargo.toml", "Cargo.lock"],
            "directories": ["src", "target"],
            "extensions": [".rs"]
        },
        "go": {
            "files": ["go.mod", "go.sum"],
            "directories": ["pkg", "cmd"],
            "extensions": [".go"]
        },
        "java": {
            "files": ["pom.xml", "build.gradle", "gradlew", "settings.gradle"],
            "directories": ["src/main/java", "target", "build"],
            "extensions": [".java", ".class", ".jar"]
        },
        "dotnet": {
            "files": [".csproj", ".sln", "packages.config"],
            "directories": ["bin", "obj"],
            "extensions": [".cs", ".vb", ".fs"]
        },
        "php": {
            "files": ["composer.json", "composer.lock"],
            "directories": ["vendor"],
            "extensions": [".php"]
        },
        "ruby": {
            "files": ["Gemfile", "Gemfile.lock", "Rakefile"],
            "directories": ["lib", "bin"],
            "extensions": [".rb"]
        },
        "flutter": {
            "files": ["pubspec.yaml", "pubspec.lock"],
            "directories": ["lib", "android", "ios"],
            "extensions": [".dart"]
        },
        "docker": {
            "files": ["Dockerfile", "docker-compose.yml", "docker-compose.yaml"],
            "extensions": []
        },
        "web": {
            "files": ["index.html", "style.css", "script.js"],
            "extensions": [".html", ".htm", ".css", ".js"]
        }
    }
    
    # Framework signatures
    FRAMEWORK_SIGNATURES = {
        "python": {
            "django": ["manage.py", "settings.py", "wsgi.py", "asgi.py"],
            "flask": ["app.py", "wsgi.py", "requirements.txt"],
            "fastapi": ["main.py", "app.py", "api.py"],
            "tornado": ["server.py", "app.py"],
            "sqlalchemy": ["models.py", "database.py"],
            "pytest": ["conftest.py", "test_*.py", "pytest.ini"],
            "jupyter": [".ipynb"],
            "pandas": ["*.csv", "*.xlsx"],
            "tensorflow": ["model.h5", "keras"]
        },
        "node": {
            "react": ["react", "jsx", "tsx", "components"],
            "vue": ["vue.config.js", "Vue", "components"],
            "angular": ["angular.json", "app.module.ts"],
            "express": ["app.js", "routes", "middleware"],
            "nextjs": ["next.config.js", "pages", "public"],
            "gatsby": ["gatsby-config.js", "gatsby-node.js"],
            "electron": ["electron", "main.js", "renderer.js"]
        },
        "web": {
            "bootstrap": ["bootstrap"],
            "tailwind": ["tailwind.config.js", "tailwindcss"],
            "jquery": ["jquery"]
        }
    }
    
    def __init__(self):
        """Initialize the project inference system."""
        self._logger = logger
        self._cache = {}  # Cache inference results
    
    async def infer_project_info(self, project_root: Path) -> Dict[str, Any]:
        """
        Infer detailed information about a project.
        
        Args:
            project_root: The project root directory
            
        Returns:
            Dictionary with project information
        """
        # Check cache first
        cache_key = str(project_root)
        if cache_key in self._cache:
            return self._cache[cache_key]
        
        self._logger.info(f"Inferring project information for {project_root}")
        
        # Basic project type detection
        project_type = await self._detect_project_type(project_root)
        
        # Get more detailed information
        result = {
            "project_root": str(project_root),
            "project_type": project_type,
            "detected_files": await self._list_important_files(project_root, project_type),
            "detected_frameworks": await self._detect_frameworks(project_root, project_type),
            "dependencies": await self._detect_dependencies(project_root, project_type),
            "structure": await self._analyze_project_structure(project_root, project_type)
        }
        
        # Cache the result
        self._cache[cache_key] = result
        
        return result
    
    async def _detect_project_type(self, project_root: Path) -> str:
        """
        Detect the primary type of a project.
        
        Args:
            project_root: The project root directory
            
        Returns:
            Project type string
        """
        # Count signature matches for each project type
        scores = {}
        
        for project_type, signature in self.PROJECT_SIGNATURES.items():
            score = 0
            
            # Check for signature files
            for file_pattern in signature.get("files", []):
                # Handle glob patterns
                if "*" in file_pattern:
                    matches = list(project_root.glob(file_pattern))
                    score += len(matches)
                else:
                    if (project_root / file_pattern).exists():
                        score += 3  # Higher weight for exact file matches
                        
            # Check for signature directories
            for dir_pattern in signature.get("directories", []):
                # Handle glob patterns
                if "*" in dir_pattern:
                    matches = list(project_root.glob(dir_pattern))
                    score += len(matches)
                else:
                    if (project_root / dir_pattern).exists() and (project_root / dir_pattern).is_dir():
                        score += 2  # Medium weight for directory matches
            
            # Check for file extensions
            for ext in signature.get("extensions", []):
                # Count files with this extension
                count = len(list(project_root.glob(f"**/*{ext}")))
                score += min(count, 10)  # Cap at 10 to avoid skewing
            
            scores[project_type] = score
        
        # Get the project type with the highest score
        if not scores:
            return "unknown"
        
        # If multiple project types have similar scores, handle mixed projects
        max_score = max(scores.values())
        candidates = [pt for pt, score in scores.items() if score >= max_score * 0.7]
        
        if len(candidates) > 1:
            # Special case: For web + node, prefer node as it's more specific
            if "web" in candidates and "node" in candidates:
                return "node"
            
            # Return composite project type for truly mixed projects
            return "+".join(candidates)
        
        # Return the highest scoring project type
        return max(scores.items(), key=lambda x: x[1])[0]
    
    async def _list_important_files(self, project_root: Path, project_type: str) -> List[Dict[str, Any]]:
        """
        List important files in the project.
        
        Args:
            project_root: The project root directory
            project_type: The detected project type
            
        Returns:
            List of important file information
        """
        important_files = []
        
        # Handle composite project types
        if "+" in project_type:
            types = project_type.split("+")
            for pt in types:
                important_files.extend(await self._list_important_files(project_root, pt))
            return important_files
        
        # Get signatures for this project type
        signature = self.PROJECT_SIGNATURES.get(project_type, {})
        
        # Check for signature files
        for file_pattern in signature.get("files", []):
            # Handle glob patterns
            if "*" in file_pattern:
                for file_path in project_root.glob(file_pattern):
                    if file_path.is_file():
                        important_files.append({
                            "path": str(file_path.relative_to(project_root)),
                            "type": "signature_file",
                            "project_type": project_type
                        })
            else:
                file_path = project_root / file_pattern
                if file_path.exists() and file_path.is_file():
                    important_files.append({
                        "path": str(file_path.relative_to(project_root)),
                        "type": "signature_file",
                        "project_type": project_type
                    })
        
        # Add common important files for any project
        common_files = ["README.md", "LICENSE", ".gitignore", "CHANGELOG.md"]
        for file_name in common_files:
            file_path = project_root / file_name
            if file_path.exists() and file_path.is_file():
                important_files.append({
                    "path": file_name,
                    "type": "documentation"
                })
        
        # Add project-specific logic
        if project_type == "python":
            # Look for main Python modules
            for file_path in project_root.glob("**/*.py"):
                if file_path.name == "__main__.py" or file_path.name == "main.py":
                    important_files.append({
                        "path": str(file_path.relative_to(project_root)),
                        "type": "entry_point"
                    })
        
        elif project_type == "node":
            # Look for main JavaScript/TypeScript files
            for pattern in ["index.js", "main.js", "server.js", "app.js", "index.ts", "main.ts"]:
                for file_path in project_root.glob(f"**/{pattern}"):
                    # Skip node_modules
                    if "node_modules" not in str(file_path):
                        important_files.append({
                            "path": str(file_path.relative_to(project_root)),
                            "type": "entry_point"
                        })
        
        # Add more project-specific logic as needed
        
        return important_files
    
    async def _detect_frameworks(self, project_root: Path, project_type: str) -> Dict[str, float]:
        """
        Detect frameworks and technologies used in the project.
        
        Args:
            project_root: The project root directory
            project_type: The detected project type
            
        Returns:
            Dictionary of framework names and confidence scores
        """
        frameworks = {}
        
        # Handle composite project types
        if "+" in project_type:
            types = project_type.split("+")
            for pt in types:
                frameworks.update(await self._detect_frameworks(project_root, pt))
            return frameworks
        
        # Get framework signatures for this project type
        if project_type in self.FRAMEWORK_SIGNATURES:
            for framework, patterns in self.FRAMEWORK_SIGNATURES[project_type].items():
                matches = 0
                total_patterns = len(patterns)
                
                for pattern in patterns:
                    # Handle glob patterns
                    if "*" in pattern:
                        files = list(project_root.glob(f"**/{pattern}"))
                        if files:
                            matches += 1
                    else:
                        # Check for exact file match
                        for file_path in project_root.glob("**/*"):
                            if pattern in file_path.name or pattern in str(file_path):
                                matches += 1
                                break
                
                # Calculate confidence score
                if total_patterns > 0 and matches > 0:
                    confidence = min(matches / total_patterns, 1.0)
                    if confidence >= 0.3:  # Threshold for reporting
                        frameworks[framework] = confidence
        
        # Check dependencies if we have appropriate files
        if project_type == "python":
            requirements_path = project_root / "requirements.txt"
            if requirements_path.exists():
                frameworks.update(await self._analyze_python_requirements(requirements_path))
            
        elif project_type == "node":
            package_json_path = project_root / "package.json"
            if package_json_path.exists():
                frameworks.update(await self._analyze_package_json(package_json_path))
        
        return frameworks
    
    async def _detect_dependencies(self, project_root: Path, project_type: str) -> List[Dict[str, Any]]:
        """
        Detect dependencies of the project.
        
        Args:
            project_root: The project root directory
            project_type: The detected project type
            
        Returns:
            List of dependencies with metadata
        """
        dependencies = []
        
        # Handle composite project types
        if "+" in project_type:
            types = project_type.split("+")
            for pt in types:
                dependencies.extend(await self._detect_dependencies(project_root, pt))
            return dependencies
        
        # Extract dependencies based on project type
        if project_type == "python":
            # Check requirements.txt
            requirements_path = project_root / "requirements.txt"
            if requirements_path.exists():
                dependencies.extend(await self._extract_python_requirements(requirements_path))
            
            # Check setup.py
            setup_py_path = project_root / "setup.py"
            if setup_py_path.exists():
                dependencies.extend(await self._extract_python_setup_dependencies(setup_py_path))
                
            # Check pyproject.toml
            pyproject_path = project_root / "pyproject.toml"
            if pyproject_path.exists():
                dependencies.extend(await self._extract_pyproject_dependencies(pyproject_path))
                
        elif project_type == "node":
            # Check package.json
            package_json_path = project_root / "package.json"
            if package_json_path.exists():
                dependencies.extend(await self._extract_node_dependencies(package_json_path))
                
        # Add more project types as needed
        
        return dependencies
    
    async def _analyze_project_structure(self, project_root: Path, project_type: str) -> Dict[str, Any]:
        """
        Analyze the structure of the project.
        
        Args:
            project_root: The project root directory
            project_type: The detected project type
            
        Returns:
            Dictionary with structure information
        """
        # Count files by type
        file_counts = {}
        
        # Walk the directory tree
        for root, dirs, files in os.walk(project_root):
            # Skip hidden directories and common exclude patterns
            dirs[:] = [d for d in dirs if not d.startswith(".") and d not in ["node_modules", "venv", "__pycache__", "build", "dist"]]
            
            for file in files:
                # Get file extension
                _, ext = os.path.splitext(file)
                if ext:
                    if ext not in file_counts:
                        file_counts[ext] = 0
                    file_counts[ext] += 1
        
        # Identify main directories
        main_dirs = []
        for item in project_root.iterdir():
            if item.is_dir() and not item.name.startswith(".") and item.name not in ["node_modules", "venv", "__pycache__"]:
                main_dirs.append({
                    "name": item.name,
                    "path": str(item.relative_to(project_root)),
                    "file_count": sum(1 for _ in item.glob("**/*") if _.is_file())
                })
        
        # Sort by file count
        main_dirs.sort(key=lambda x: x["file_count"], reverse=True)
        
        return {
            "file_counts": file_counts,
            "main_directories": main_dirs[:5],  # Top 5 directories
            "total_files": sum(file_counts.values()),
            "directory_structure": await self._generate_directory_structure(project_root)
        }
    
    async def _generate_directory_structure(self, project_root: Path, max_depth: int = 3) -> Dict[str, Any]:
        """
        Generate a hierarchical representation of the directory structure.
        
        Args:
            project_root: The project root directory
            max_depth: Maximum depth to traverse
            
        Returns:
            Dictionary representing the directory structure
        """
        def _build_tree(path: Path, current_depth: int) -> Dict[str, Any]:
            if current_depth > max_depth:
                return {"type": "directory", "name": path.name, "truncated": True}
            
            result = {"type": "directory", "name": path.name, "children": []}
            
            try:
                # List directory contents
                items = list(path.iterdir())
                
                # Skip large directories
                if len(items) > 50:
                    result["children"].append({"type": "info", "name": f"{len(items)} items (too many to show)"})
                    return result
                
                # Add directories first
                for item in sorted([i for i in items if i.is_dir()], key=lambda x: x.name):
                    # Skip hidden directories and common excludes
                    if item.name.startswith(".") or item.name in ["node_modules", "venv", "__pycache__", "build", "dist"]:
                        continue
                    
                    child = _build_tree(item, current_depth + 1)
                    result["children"].append(child)
                
                # Then add files
                for item in sorted([i for i in items if i.is_file()], key=lambda x: x.name):
                    # Skip hidden files
                    if item.name.startswith("."):
                        continue
                    
                    result["children"].append({"type": "file", "name": item.name})
                
                return result
            except PermissionError:
                result["children"].append({"type": "error", "name": "Permission denied"})
                return result
        
        return _build_tree(project_root, 0)
    
    async def _analyze_python_requirements(self, requirements_path: Path) -> Dict[str, float]:
        """
        Analyze Python requirements.txt for frameworks.
        
        Args:
            requirements_path: Path to requirements.txt
            
        Returns:
            Dictionary of framework names and confidence scores
        """
        frameworks = {}
        
        # Framework indicators in requirements
        framework_indicators = {
            "django": "django",
            "flask": "flask",
            "fastapi": "fastapi",
            "tornado": "tornado",
            "sqlalchemy": "sqlalchemy",
            "pytest": "pytest",
            "pandas": "pandas",
            "numpy": "numpy",
            "tensorflow": "tensorflow",
            "pytorch": "torch",
            "jupyter": "jupyter"
        }
        
        try:
            with open(requirements_path, "r") as f:
                requirements = f.read()
                
            for framework, indicator in framework_indicators.items():
                pattern = rf"\b{re.escape(indicator)}[>=<~!]"
                if re.search(pattern, requirements, re.IGNORECASE):
                    frameworks[framework] = 1.0  # High confidence for direct dependencies
        except Exception as e:
            self._logger.error(f"Error analyzing requirements.txt: {str(e)}")
        
        return frameworks
    
    async def _analyze_package_json(self, package_json_path: Path) -> Dict[str, float]:
        """
        Analyze package.json for frameworks.
        
        Args:
            package_json_path: Path to package.json
            
        Returns:
            Dictionary of framework names and confidence scores
        """
        frameworks = {}
        
        # Framework indicators in package.json
        framework_indicators = {
            "react": ["react", "react-dom"],
            "vue": ["vue"],
            "angular": ["@angular/core"],
            "express": ["express"],
            "nextjs": ["next"],
            "gatsby": ["gatsby"],
            "electron": ["electron"]
        }
        
        try:
            with open(package_json_path, "r") as f:
                package_data = json.load(f)
            
            # Check dependencies and devDependencies
            all_deps = {}
            all_deps.update(package_data.get("dependencies", {}))
            all_deps.update(package_data.get("devDependencies", {}))
            
            for framework, indicators in framework_indicators.items():
                if any(dep in all_deps for dep in indicators):
                    frameworks[framework] = 1.0  # High confidence for direct dependencies
        except Exception as e:
            self._logger.error(f"Error analyzing package.json: {str(e)}")
        
        return frameworks
    
    async def _extract_python_requirements(self, requirements_path: Path) -> List[Dict[str, Any]]:
        """
        Extract dependencies from requirements.txt.
        
        Args:
            requirements_path: Path to requirements.txt
            
        Returns:
            List of dependencies
        """
        dependencies = []
        
        try:
            with open(requirements_path, "r") as f:
                for line in f:
                    line = line.strip()
                    
                    # Skip comments and empty lines
                    if not line or line.startswith("#"):
                        continue
                    
                    # Parse requirement
                    parts = re.split(r"[>=<~!]", line, 1)
                    name = parts[0].strip()
                    version_spec = line[len(name):].strip() if len(parts) > 1 else ""
                    
                    dependencies.append({
                        "name": name,
                        "version_spec": version_spec,
                        "type": "python",
                        "source": "requirements.txt"
                    })
        except Exception as e:
            self._logger.error(f"Error extracting Python requirements: {str(e)}")
        
        return dependencies
    
    async def _extract_python_setup_dependencies(self, setup_py_path: Path) -> List[Dict[str, Any]]:
        """
        Extract dependencies from setup.py.
        
        Args:
            setup_py_path: Path to setup.py
            
        Returns:
            List of dependencies
        """
        dependencies = []
        
        try:
            with open(setup_py_path, "r") as f:
                setup_content = f.read()
            
            # Look for install_requires
            install_requires_match = re.search(r"install_requires\s*=\s*\[(.*?)\]", setup_content, re.DOTALL)
            if install_requires_match:
                requires_text = install_requires_match.group(1)
                
                # Extract individual requirements
                for req_match in re.finditer(r"[\"']([^\"']+)[\"']", requires_text):
                    req = req_match.group(1)
                    
                    # Parse requirement
                    parts = re.split(r"[>=<~!]", req, 1)
                    name = parts[0].strip()
                    version_spec = req[len(name):].strip() if len(parts) > 1 else ""
                    
                    dependencies.append({
                        "name": name,
                        "version_spec": version_spec,
                        "type": "python",
                        "source": "setup.py"
                    })
        except Exception as e:
            self._logger.error(f"Error extracting setup.py dependencies: {str(e)}")
        
        return dependencies
    
    async def _extract_pyproject_dependencies(self, pyproject_path: Path) -> List[Dict[str, Any]]:
        """
        Extract dependencies from pyproject.toml.
        
        Args:
            pyproject_path: Path to pyproject.toml
            
        Returns:
            List of dependencies
        """
        dependencies = []
        
        try:
            # Simple parsing of dependencies from pyproject.toml
            with open(pyproject_path, "r") as f:
                content = f.read()
            
            # Look for dependencies section
            deps_match = re.search(r"dependencies\s*=\s*\[(.*?)\]", content, re.DOTALL)
            if deps_match:
                deps_text = deps_match.group(1)
                
                # Extract individual dependencies
                for dep_match in re.finditer(r"[\"']([^\"']+)[\"']", deps_text):
                    dep = dep_match.group(1)
                    
                    # Parse requirement
                    parts = re.split(r"[>=<~!]", dep, 1)
                    name = parts[0].strip()
                    version_spec = dep[len(name):].strip() if len(parts) > 1 else ""
                    
                    dependencies.append({
                        "name": name,
                        "version_spec": version_spec,
                        "type": "python",
                        "source": "pyproject.toml"
                    })
        except Exception as e:
            self._logger.error(f"Error extracting pyproject.toml dependencies: {str(e)}")
        
        return dependencies
    
    async def _extract_node_dependencies(self, package_json_path: Path) -> List[Dict[str, Any]]:
        """
        Extract dependencies from package.json.
        
        Args:
            package_json_path: Path to package.json
            
        Returns:
            List of dependencies
        """
        dependencies = []
        
        try:
            with open(package_json_path, "r") as f:
                package_data = json.load(f)
            
            # Process dependencies
            for dep_type in ["dependencies", "devDependencies"]:
                deps = package_data.get(dep_type, {})
                for name, version in deps.items():
                    dependencies.append({
                        "name": name,
                        "version_spec": version,
                        "type": "node",
                        "dev": dep_type == "devDependencies",
                        "source": "package.json"
                    })
        except Exception as e:
            self._logger.error(f"Error extracting Node.js dependencies: {str(e)}")
        
        return dependencies

# Global project inference instance
project_inference = ProjectInference()
</file>

<file path="angela/context/session.py">
# angela/context/session.py

import json
import time
from typing import Dict, Any, Optional, List, Set
from datetime import datetime, timedelta
from dataclasses import dataclass, field, asdict

from angela.context.preferences import preferences_manager
from angela.utils.logging import get_logger

logger = get_logger(__name__)

@dataclass
class EntityReference:
    """A reference to an entity in the session context."""
    name: str  # Name or identifier of the entity
    type: str  # Type of entity (file, directory, command, result, etc.)
    value: str  # Actual value or path 
    created: datetime = field(default_factory=datetime.now)
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for serialization."""
        return {
            "name": self.name,
            "type": self.type,
            "value": self.value,
            "created": self.created.isoformat()
        }
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'EntityReference':
        """Create from dictionary."""
        return cls(
            name=data["name"],
            type=data["type"],
            value=data["value"],
            created=datetime.fromisoformat(data["created"])
        )


class SessionMemory:
    """Memory for a single conversation session."""
    
    def __init__(self):
        """Initialize session memory."""
        self.entities: Dict[str, EntityReference] = {}
        self.recent_commands: List[str] = []
        self.recent_results: List[str] = []
        self.created = datetime.now()
        self.last_accessed = datetime.now()
    
    def add_entity(self, name: str, entity_type: str, value: str) -> None:
        """
        Add an entity to the session memory.
        
        Args:
            name: The name or identifier of the entity
            entity_type: The type of entity
            value: The value or path of the entity
        """
        self.entities[name] = EntityReference(name, entity_type, value)
        self.last_accessed = datetime.now()
    
    def get_entity(self, name: str) -> Optional[EntityReference]:
        """
        Get an entity from the session memory.
        
        Args:
            name: The name or identifier of the entity
            
        Returns:
            The entity reference, or None if not found
        """
        self.last_accessed = datetime.now()
        return self.entities.get(name)
    
    def add_command(self, command: str) -> None:
        """
        Add a command to the recent commands list.
        
        Args:
            command: The command string
        """
        self.recent_commands.append(command)
        self.last_accessed = datetime.now()
        
        # Keep only the last 10 commands
        if len(self.recent_commands) > 10:
            self.recent_commands.pop(0)
    
    def add_result(self, result: str) -> None:
        """
        Add a result to the recent results list.
        
        Args:
            result: The result string
        """
        self.recent_results.append(result)
        self.last_accessed = datetime.now()
        
        # Keep only the last 5 results
        if len(self.recent_results) > 5:
            self.recent_results.pop(0)
    
    def get_context_dict(self) -> Dict[str, Any]:
        """
        Get the session memory as a dictionary.
        
        Returns:
            A dictionary representation of the session memory
        """
        return {
            "entities": {k: v.to_dict() for k, v in self.entities.items()},
            "recent_commands": self.recent_commands,
            "recent_results": self.recent_results,
            "created": self.created.isoformat(),
            "last_accessed": self.last_accessed.isoformat()
        }
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'SessionMemory':
        """
        Create a session memory from a dictionary.
        
        Args:
            data: The dictionary representation
            
        Returns:
            A new SessionMemory instance
        """
        session = cls()
        session.entities = {
            k: EntityReference.from_dict(v) 
            for k, v in data.get("entities", {}).items()
        }
        session.recent_commands = data.get("recent_commands", [])
        session.recent_results = data.get("recent_results", [])
        session.created = datetime.fromisoformat(data["created"])
        session.last_accessed = datetime.fromisoformat(data["last_accessed"])
        return session


class SessionManager:
    """Manager for conversation session memories."""
    
    def __init__(self):
        """Initialize the session manager."""
        self._current_session = SessionMemory()
        self._logger = logger
    
    def refresh_session(self) -> None:
        """Refresh the current session or create a new one if expired."""
        # Check if session is enabled in preferences
        if not preferences_manager.preferences.context.remember_session_context:
            self._current_session = SessionMemory()
            return
        
        # Check if the session has expired (2 hours of inactivity)
        now = datetime.now()
        session_timeout = timedelta(hours=2)
        
        if now - self._current_session.last_accessed > session_timeout:
            self._logger.debug("Session expired, creating new session")
            self._current_session = SessionMemory()
    
    def add_entity(self, name: str, entity_type: str, value: str) -> None:
        """
        Add an entity to the current session.
        
        Args:
            name: The name or identifier of the entity
            entity_type: The type of entity
            value: The value or path of the entity
        """
        self.refresh_session()
        self._current_session.add_entity(name, entity_type, value)
    
    def get_entity(self, name: str) -> Optional[EntityReference]:
        """
        Get an entity from the current session.
        
        Args:
            name: The name or identifier of the entity
            
        Returns:
            The entity reference, or None if not found
        """
        self.refresh_session()
        return self._current_session.get_entity(name)
    
    def add_command(self, command: str) -> None:
        """
        Add a command to the current session.
        
        Args:
            command: The command string
        """
        self.refresh_session()
        self._current_session.add_command(command)
    
    def add_result(self, result: str) -> None:
        """
        Add a result to the current session.
        
        Args:
            result: The result string
        """
        self.refresh_session()
        self._current_session.add_result(result)
    
    def get_context(self) -> Dict[str, Any]:
        """
        Get the current session context as a dictionary.
        
        Returns:
            A dictionary representation of the current session context
        """
        self.refresh_session()
        return self._current_session.get_context_dict()
    
    def clear_session(self) -> None:
        """Clear the current session."""
        self._current_session = SessionMemory()

# Global session manager instance
session_manager = SessionManager()
</file>

<file path="angela/execution/adaptive_engine.py">
# angela/execution/adaptive_engine.py

import asyncio
import os
import sys
import signal
import time
from typing import Dict, Any, Optional, List, Tuple, Union
from pathlib import Path

from rich.console import Console
from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn, TimeElapsedColumn

from angela.safety.classifier import classify_command_risk, analyze_command_impact
from angela.safety.adaptive_confirmation import get_adaptive_confirmation
from angela.execution.engine import execution_engine
from angela.context.history import history_manager
from angela.context.preferences import preferences_manager
from angela.context.session import session_manager
from angela.ai.analyzer import error_analyzer
from angela.utils.logging import get_logger

logger = get_logger(__name__)
console = Console()

class AdaptiveExecutionEngine:
    """
    Context-aware command execution engine.
    
    This engine adapts its behavior based on user history, preferences,
    and command characteristics.
    """
    
    def __init__(self):
        """Initialize the adaptive execution engine."""
        self._logger = logger
    
    async def execute_command(
        self, 
        command: str,
        natural_request: str,
        explanation: Optional[str] = None,
        dry_run: bool = False
    ) -> Dict[str, Any]:
        """
        Execute a command with adaptive behavior based on user context.
        
        Args:
            command: The command to execute
            natural_request: The original natural language request
            explanation: AI explanation of what the command does
            dry_run: Whether to simulate the command without execution
            
        Returns:
            Dictionary with execution results
        """
        self._logger.info(f"Preparing to execute command: {command}")
        
        # Analyze command risk and impact
        risk_level, risk_reason = classify_command_risk(command)
        impact = analyze_command_impact(command)
        
        # Add to session context
        session_manager.add_command(command)
        
        # Generate command preview if needed
        from angela.safety.preview import generate_preview
        preview = await generate_preview(command) if preferences_manager.preferences.ui.show_command_preview else None
        
        # Get adaptive confirmation based on risk level and user history
        confirmed = await get_adaptive_confirmation(
            command=command,
            risk_level=risk_level,
            risk_reason=risk_reason,
            impact=impact,
            preview=preview,
            explanation=explanation,
            natural_request=natural_request,
            dry_run=dry_run
        )
        
        if not confirmed and not dry_run:
            self._logger.info(f"Command execution cancelled by user: {command}")
            return {
                "command": command,
                "success": False,
                "cancelled": True,
                "stdout": "",
                "stderr": "Command execution cancelled by user",
                "return_code": 1,
                "dry_run": dry_run
            }
        
        # Execute the command
        result = await self._execute_with_feedback(command, dry_run)
        
        # Add to history
        history_manager.add_command(
            command=command,
            natural_request=natural_request,
            success=result["success"],
            output=result.get("stdout", ""),
            error=result.get("stderr", ""),
            risk_level=risk_level
        )
        
        # If execution failed, analyze error and suggest fixes
        if not result["success"] and result.get("stderr"):
            result["error_analysis"] = error_analyzer.analyze_error(command, result["stderr"])
            result["fix_suggestions"] = error_analyzer.generate_fix_suggestions(command, result["stderr"])
        
        # Offer to learn from successful executions
        if result["success"] and risk_level > 0:
            from angela.safety.adaptive_confirmation import offer_command_learning
            await offer_command_learning(command)
        
        return result
    
    async def _execute_with_feedback(self, command: str, dry_run: bool) -> Dict[str, Any]:
        """
        Execute a command with rich feedback.
        
        Args:
            command: The command to execute
            dry_run: Whether to simulate the command without execution
            
        Returns:
            Dictionary with execution results
        """
        use_spinners = preferences_manager.preferences.ui.use_spinners
        
        # For dry runs, return the preview directly
        if dry_run:
            # Execute in dry-run mode
            stdout, stderr, return_code = await execution_engine.dry_run_command(command)
            
            return {
                "command": command,
                "success": True,
                "stdout": stdout,
                "stderr": stderr,
                "return_code": return_code,
                "dry_run": True
            }
        
        # Show execution spinner if enabled
        if use_spinners:
            with Progress(
                SpinnerColumn(),
                TextColumn("[bold blue]Executing command...[/bold blue]"),
                TimeElapsedColumn(),
                console=console
            ) as progress:
                task = progress.add_task("Executing", total=None)
                
                # Execute the command
                stdout, stderr, return_code = await execution_engine.execute_command(
                    command,
                    check_safety=False  # We've already done safety checks
                )
                
                # Complete the progress
                progress.update(task, completed=True)
        else:
            # Execute without spinner
            console.print("[bold blue]Executing command...[/bold blue]")
            stdout, stderr, return_code = await execution_engine.execute_command(
                command,
                check_safety=False  # We've already done safety checks
            )
        
        # Store result in session for reference
        if stdout.strip():
            session_manager.add_result(stdout.strip())
        
        return {
            "command": command,
            "success": return_code == 0,
            "stdout": stdout,
            "stderr": stderr,
            "return_code": return_code,
            "dry_run": False
        }

# Global adaptive execution engine instance
adaptive_engine = AdaptiveExecutionEngine()
</file>

<file path="angela/execution/error_recovery.py">
# angela/execution/error_recovery.py

import os
import re
import asyncio
import tempfile
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple, Set, Union
from enum import Enum

from angela.ai.client import gemini_client, GeminiRequest
from angela.ai.analyzer import error_analyzer
from angela.context import context_manager
from angela.utils.logging import get_logger
from angela.shell.formatter import terminal_formatter, OutputType

logger = get_logger(__name__)

class RecoveryStrategy(Enum):
    """Types of error recovery strategies."""
    RETRY = "retry"                 # Simple retry
    MODIFY_COMMAND = "modify"       # Modify the command and retry
    ALTERNATIVE_COMMAND = "alternative"  # Try an alternative command
    PREPARE_ENV = "prepare"         # Prepare the environment and retry
    REVERT_CHANGES = "revert"       # Revert changes and retry
    SKIP = "skip"                   # Skip the step and continue
    ABORT = "abort"                 # Abort the plan execution

class ErrorRecoveryManager:
    """
    Manager for error recovery during multi-step execution.
    
    This class provides:
    1. Analysis of execution errors
    2. Automatic recovery strategies
    3. Guided recovery with user input
    4. Learning from successful recoveries
    """
    
    def __init__(self):
        """Initialize the error recovery manager."""
        self._logger = logger
        self._recovery_history = {}  # Track successful recoveries
    
    async def handle_error(
        self, 
        step: Any, 
        error_result: Dict[str, Any], 
        context: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Handle an error during step execution.
        
        Args:
            step: The step that failed
            error_result: The execution result with error information
            context: Context information
            
        Returns:
            Updated execution result with recovery information
        """
        self._logger.info(f"Handling error for step {step.id if hasattr(step, 'id') else 'unknown'}")
        
        # Extract error information
        command = error_result.get("command") or getattr(step, "command", None)
        stderr = error_result.get("stderr", "")
        error_msg = error_result.get("error", "")
        
        if not command or (not stderr and not error_msg):
            # Not enough information to recover
            self._logger.warning("Insufficient information for error recovery")
            return error_result
        
        # Analyze the error
        analysis = await self._analyze_error(command, stderr or error_msg)
        
        # Generate recovery strategies
        strategies = await self._generate_recovery_strategies(command, analysis, context)
        
        # Record error analysis and strategies
        result = dict(error_result)
        result["error_analysis"] = analysis
        result["recovery_strategies"] = strategies
        
        # Check if we can auto-recover
        if strategies and self._can_auto_recover(strategies[0]):
            # Attempt automatic recovery
            recovery_result = await self._execute_recovery_strategy(
                strategies[0], step, error_result, context
            )
            
            # Update the result
            result["recovery_attempted"] = True
            result["recovery_strategy"] = strategies[0]["type"]
            result["recovery_success"] = recovery_result.get("success", False)
            
            # If recovery succeeded, replace the result
            if recovery_result.get("success", False):
                self._logger.info(f"Automatic recovery succeeded for step {getattr(step, 'id', 'unknown')}")
                result.update(recovery_result)
        else:
            # Guided recovery with user input
            recovery_result = await self._guided_recovery(strategies, step, error_result, context)
            
            # Update the result
            result["recovery_attempted"] = recovery_result is not None
            if recovery_result:
                result["recovery_strategy"] = recovery_result.get("strategy", {}).get("type")
                result["recovery_success"] = recovery_result.get("success", False)
                
                # If recovery succeeded, replace the result
                if recovery_result.get("success", False):
                    self._logger.info(f"Guided recovery succeeded for step {getattr(step, 'id', 'unknown')}")
                    result.update(recovery_result)
        
        return result
    
    async def _analyze_error(self, command: str, error: str) -> Dict[str, Any]:
        """
        Analyze an error to determine its cause and possible fixes.
        
        Args:
            command: The command that failed
            error: The error message
            
        Returns:
            Error analysis result
        """
        # Use the error analyzer to analyze the error
        analysis = error_analyzer.analyze_error(command, error)
        
        # Generate fix suggestions
        suggestions = error_analyzer.generate_fix_suggestions(command, error)
        analysis["fix_suggestions"] = suggestions
        
        # Add error patterns
        error_patterns = analysis.get("error_patterns", [])
        if not error_patterns:
            # Try to match common error patterns
            for pattern in self._get_common_error_patterns():
                if re.search(pattern["pattern"], error, re.IGNORECASE):
                    error_patterns.append({
                        "pattern": pattern["pattern"],
                        "description": pattern["description"],
                        "fixes": pattern["fixes"]
                    })
        
        analysis["error_patterns"] = error_patterns
        
        return analysis
    
    async def _generate_recovery_strategies(
        self, 
        command: str, 
        analysis: Dict[str, Any], 
        context: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """
        Generate recovery strategies based on error analysis.
        
        Args:
            command: The command that failed
            analysis: Error analysis result
            context: Context information
            
        Returns:
            List of recovery strategies
        """
        strategies = []
        
        # Check if we have fix suggestions
        if analysis.get("fix_suggestions"):
            for suggestion in analysis["fix_suggestions"]:
                # Parse the suggestion to generate a recovery strategy
                strategy = self._parse_fix_suggestion(suggestion, command)
                if strategy:
                    strategies.append(strategy)
        
        # Check for error patterns
        if analysis.get("error_patterns"):
            for pattern in analysis["error_patterns"]:
                for fix in pattern.get("fixes", []):
                    strategy = self._create_strategy_from_pattern_fix(fix, command)
                    if strategy and not any(s["command"] == strategy["command"] for s in strategies):
                        strategies.append(strategy)
        
        # If no strategies yet, use AI to generate strategies
        if not strategies:
            ai_strategies = await self._generate_ai_recovery_strategies(command, analysis, context)
            strategies.extend(ai_strategies)
        
        # Always add retry and skip as fallback strategies
        if not any(s["type"] == RecoveryStrategy.RETRY.value for s in strategies):
            strategies.append({
                "type": RecoveryStrategy.RETRY.value,
                "command": command,
                "description": "Retry the command without changes",
                "confidence": 0.3
            })
        
        # Add skip strategy
        strategies.append({
            "type": RecoveryStrategy.SKIP.value,
            "command": None,
            "description": "Skip this step and continue with the plan",
            "confidence": 0.2
        })
        
        # Sort strategies by confidence
        strategies.sort(key=lambda s: s.get("confidence", 0), reverse=True)
        
        return strategies
    
    def _parse_fix_suggestion(self, suggestion: str, command: str) -> Optional[Dict[str, Any]]:
        """
        Parse a fix suggestion into a recovery strategy.
        
        Args:
            suggestion: The fix suggestion
            command: The original command
            
        Returns:
            Recovery strategy or None if parsing fails
        """
        # Check for suggested commands in the form of "Try: command"
        command_match = re.search(r'try:?\s*`?([^`]+)`?', suggestion, re.IGNORECASE)
        if command_match:
            suggested_command = command_match.group(1).strip()
            return {
                "type": RecoveryStrategy.MODIFY_COMMAND.value,
                "command": suggested_command,
                "description": suggestion,
                "confidence": 0.8
            }
        
        # Check for permission issues
        if "permission" in suggestion.lower():
            if "sudo" not in command.lower() and not command.strip().startswith("sudo "):
                # Add sudo to the command
                sudo_command = f"sudo {command}"
                return {
                    "type": RecoveryStrategy.MODIFY_COMMAND.value,
                    "command": sudo_command,
                    "description": "Add sudo to the command for elevated privileges",
                    "confidence": 0.7
                }
        
        # Check for missing file or directory suggestions
        if "file not found" in suggestion.lower() or "directory not found" in suggestion.lower():
            mkdir_match = re.search(r'mkdir\s+([^\s]+)', suggestion, re.IGNORECASE)
            if mkdir_match:
                dir_path = mkdir_match.group(1)
                return {
                    "type": RecoveryStrategy.PREPARE_ENV.value,
                    "command": f"mkdir -p {dir_path}",
                    "description": f"Create the directory {dir_path} and retry",
                    "confidence": 0.7,
                    "retry_original": True
                }
        
        # General suggestion without a specific command
        return {
            "type": RecoveryStrategy.ALTERNATIVE_COMMAND.value,
            "command": None,  # Will be filled in by AI
            "description": suggestion,
            "confidence": 0.5
        }
    
    def _create_strategy_from_pattern_fix(self, fix: str, command: str) -> Optional[Dict[str, Any]]:
        """
        Create a recovery strategy from a pattern fix.
        
        Args:
            fix: The fix description
            command: The original command
            
        Returns:
            Recovery strategy or None if parsing fails
        """
        # Check for command suggestions
        command_match = re.search(r'`([^`]+)`', fix)
        if command_match:
            suggested_command = command_match.group(1).strip()
            return {
                "type": RecoveryStrategy.ALTERNATIVE_COMMAND.value,
                "command": suggested_command,
                "description": fix,
                "confidence": 0.7
            }
        
        # Check for common actions
        if "install" in fix.lower():
            # Extract package name
            pkg_match = re.search(r'install\s+(\w+)', fix, re.IGNORECASE)
            if pkg_match:
                pkg_name = pkg_match.group(1)
                return {
                    "type": RecoveryStrategy.PREPARE_ENV.value,
                    "command": f"apt-get install -y {pkg_name}",
                    "description": f"Install missing package: {pkg_name}",
                    "confidence": 0.7,
                    "retry_original": True
                }
        
        return None
    
    async def _generate_ai_recovery_strategies(
        self, 
        command: str, 
        analysis: Dict[str, Any], 
        context: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """
        Generate recovery strategies using the AI.
        
        Args:
            command: The command that failed
            analysis: Error analysis result
            context: Context information
            
        Returns:
            List of AI-generated recovery strategies
        """
        # Build a prompt for strategy generation
        error_summary = analysis.get("error_summary", "Unknown error")
        possible_cause = analysis.get("possible_cause", "Unknown cause")
        
        prompt = f"""
Generate recovery strategies for a failed command execution.

Failed command: `{command}`
Error summary: {error_summary}
Possible cause: {possible_cause}

Generate 2-3 specific recovery strategies, each with:
1. A specific command to execute
2. A description of what the strategy does
3. A confidence level (0.0-1.0)

Format your response as JSON:
[
  {{
    "type": "modify",
    "command": "modified command",
    "description": "Description of the strategy",
    "confidence": 0.8
  }},
  {{
    "type": "prepare",
    "command": "preparation command",
    "description": "Prepare the environment",
    "confidence": 0.7,
    "retry_original": true
  }}
]

Valid strategy types:
- modify: Modify the original command
- alternative: Use an alternative command
- prepare: Prepare the environment and retry
- revert: Revert changes and retry
"""
        
        # Call the AI service
        api_request = GeminiRequest(prompt=prompt, max_tokens=1000)
        api_response = await gemini_client.generate_text(api_request)
        
        # Parse the response
        try:
            # Extract JSON from the response
            import re
            import json
            
            json_match = re.search(r'```(?:json)?\s*(.*?)\s*```', api_response.text, re.DOTALL)
            if json_match:
                json_str = json_match.group(1)
            else:
                json_str = api_response.text
            
            # Parse the JSON
            strategies = json.loads(json_str)
            
            # Validate and normalize strategies
            valid_strategies = []
            for strategy in strategies:
                if isinstance(strategy, dict) and "type" in strategy and "command" in strategy:
                    # Ensure type is valid
                    try:
                        RecoveryStrategy(strategy["type"])
                        valid_strategies.append(strategy)
                    except ValueError:
                        # Invalid strategy type, skip it
                        pass
            
            return valid_strategies
            
        except Exception as e:
            self._logger.error(f"Error parsing AI recovery strategies: {str(e)}")
            return []
    
    def _can_auto_recover(self, strategy: Dict[str, Any]) -> bool:
        """
        Determine if a strategy can be applied automatically.
        
        Args:
            strategy: The recovery strategy
            
        Returns:
            True if auto-recovery is possible, False otherwise
        """
        # High confidence strategies can be auto-applied
        if strategy.get("confidence", 0) >= 0.8:
            return True
        
        # Certain strategy types can always be auto-applied
        auto_types = [
            RecoveryStrategy.RETRY.value
        ]
        
        if strategy.get("type") in auto_types:
            return True
        
        # Check if the strategy has been successful in the past
        strategy_key = f"{strategy.get('type')}:{strategy.get('command')}"
        if self._recovery_history.get(strategy_key, {}).get("success_count", 0) > 0:
            return True
        
        return False
    
    async def _execute_recovery_strategy(
        self, 
        strategy: Dict[str, Any], 
        step: Any, 
        error_result: Dict[str, Any], 
        context: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Execute a recovery strategy.
        
        Args:
            strategy: The recovery strategy to execute
            step: The step that failed
            error_result: The original error result
            context: Context information
            
        Returns:
            Updated execution result
        """
        self._logger.info(f"Executing recovery strategy: {strategy.get('type')}")
        
        # Import here to avoid circular imports
        from angela.execution.engine import execution_engine
        
        result = {
            "strategy": strategy,
            "original_error": error_result.get("error"),
            "original_stderr": error_result.get("stderr")
        }
        
        try:
            # Execute based on strategy type
            strategy_type = strategy.get("type")
            
            if strategy_type == RecoveryStrategy.RETRY.value:
                # Simple retry of the original command
                command = getattr(step, "command", None) or error_result.get("command")
                if command:
                    stdout, stderr, return_code = await execution_engine.execute_command(
                        command,
                        check_safety=False  # Skip safety checks for retry
                    )
                    
                    result.update({
                        "command": command,
                        "stdout": stdout,
                        "stderr": stderr,
                        "return_code": return_code,
                        "success": return_code == 0
                    })
                else:
                    result.update({
                        "error": "No command available for retry",
                        "success": False
                    })
            
            elif strategy_type in [RecoveryStrategy.MODIFY_COMMAND.value, 
                                RecoveryStrategy.ALTERNATIVE_COMMAND.value]:
                # Execute modified or alternative command
                command = strategy.get("command")
                if command:
                    stdout, stderr, return_code = await execution_engine.execute_command(
                        command,
                        check_safety=True  # Safety checks for new commands
                    )
                    
                    result.update({
                        "command": command,
                        "stdout": stdout,
                        "stderr": stderr,
                        "return_code": return_code,
                        "success": return_code == 0
                    })
                    
                    # If successful and requested, retry the original command
                    if return_code == 0 and strategy.get("retry_original"):
                        original_command = getattr(step, "command", None) or error_result.get("command")
                        if original_command:
                            self._logger.info(f"Retrying original command: {original_command}")
                            stdout, stderr, return_code = await execution_engine.execute_command(
                                original_command,
                                check_safety=False
                            )
                            
                            result.update({
                                "original_retry": {
                                    "command": original_command,
                                    "stdout": stdout,
                                    "stderr": stderr,
                                    "return_code": return_code,
                                    "success": return_code == 0
                                }
                            })
                            
                            # Update overall success based on original command retry
                            result["success"] = return_code == 0
                else:
                    result.update({
                        "error": "No command specified in strategy",
                        "success": False
                    })
            
            elif strategy_type == RecoveryStrategy.PREPARE_ENV.value:
                # Execute preparation command
                command = strategy.get("command")
                if command:
                    stdout, stderr, return_code = await execution_engine.execute_command(
                        command,
                        check_safety=True
                    )
                    
                    result.update({
                        "preparation": {
                            "command": command,
                            "stdout": stdout,
                            "stderr": stderr,
                            "return_code": return_code,
                            "success": return_code == 0
                        }
                    })
                    
                    # If preparation succeeded and requested, retry the original command
                    if return_code == 0 and strategy.get("retry_original"):
                        original_command = getattr(step, "command", None) or error_result.get("command")
                        if original_command:
                            self._logger.info(f"Retrying original command after preparation: {original_command}")
                            stdout, stderr, return_code = await execution_engine.execute_command(
                                original_command,
                                check_safety=False
                            )
                            
                            result.update({
                                "command": original_command,
                                "stdout": stdout,
                                "stderr": stderr,
                                "return_code": return_code,
                                "success": return_code == 0
                            })
                        else:
                            result.update({
                                "error": "No original command available for retry",
                                "success": False
                            })
                    else:
                        # No retry requested or preparation failed
                        result["success"] = return_code == 0
                else:
                    result.update({
                        "error": "No preparation command specified in strategy",
                        "success": False
                    })
            
            elif strategy_type == RecoveryStrategy.REVERT_CHANGES.value:
                # Revert changes (simplified implementation)
                result.update({
                    "message": "Revert changes not implemented",
                    "success": False
                })
            
            elif strategy_type == RecoveryStrategy.SKIP.value:
                # Skip the step
                result.update({
                    "message": "Step skipped",
                    "success": True,
                    "skipped": True
                })
            
            else:
                result.update({
                    "error": f"Unknown strategy type: {strategy_type}",
                    "success": False
                })
            
            # Update recovery history
            if result.get("success"):
                strategy_key = f"{strategy.get('type')}:{strategy.get('command')}"
                if strategy_key in self._recovery_history:
                    self._recovery_history[strategy_key]["success_count"] += 1
                    self._recovery_history[strategy_key]["last_success"] = datetime.now()
                else:
                    self._recovery_history[strategy_key] = {
                        "success_count": 1,
                        "failure_count": 0,
                        "last_success": datetime.now()
                    }
            
            return result
            
        except Exception as e:
            self._logger.exception(f"Error executing recovery strategy: {str(e)}")
            return {
                "strategy": strategy,
                "error": str(e),
                "success": False
            }
    
    async def _guided_recovery(
        self, 
        strategies: List[Dict[str, Any]], 
        step: Any, 
        error_result: Dict[str, Any], 
        context: Dict[str, Any]
    ) -> Optional[Dict[str, Any]]:
        """
        Guide the user through recovery options.
        
        Args:
            strategies: Available recovery strategies
            step: The step that failed
            error_result: The original error result
            context: Context information
            
        Returns:
            Recovery result or None if aborted
        """
        if not strategies:
            return None
        
        # Display recovery options
        terminal_formatter.print_output(
            "Command execution failed. The following recovery options are available:",
            OutputType.WARNING,
            title="Recovery Options"
        )
        
        # Display the error
        terminal_formatter.print_output(
            error_result.get("stderr", "") or error_result.get("error", "Unknown error"),
            OutputType.ERROR,
            title="Error"
        )
        
        # Show strategies
        for i, strategy in enumerate(strategies):
            description = strategy.get("description", "No description")
            command = strategy.get("command", "No command")
            
            if strategy.get("type") == RecoveryStrategy.SKIP.value:
                terminal_formatter.print_output(
                    f"Option {i+1}: [Skip] {description}",
                    OutputType.INFO
                )
            else:
                terminal_formatter.print_output(
                    f"Option {i+1}: {description}\n  Command: {command}",
                    OutputType.INFO
                )
        
        # Add abort option
        terminal_formatter.print_output(
            f"Option {len(strategies)+1}: [Abort] Abort execution",
            OutputType.WARNING
        )
        
        # Get user selection
        from prompt_toolkit.shortcuts import input_dialog
        selection = input_dialog(
            title="Select Recovery Option",
            text="Enter option number:",
        ).run()
        
        if not selection or not selection.isdigit():
            return None
        
        option = int(selection)
        
        # Handle abort option
        if option == len(strategies) + 1:
            return {
                "message": "Execution aborted by user",
                "success": False,
                "aborted": True
            }
        
        # Handle strategy selection
        if 1 <= option <= len(strategies):
            selected_strategy = strategies[option - 1]
            
            # Execute the selected strategy
            return await self._execute_recovery_strategy(
                selected_strategy, step, error_result, context
            )
        
        return None
    
    def _get_common_error_patterns(self) -> List[Dict[str, Any]]:
        """
        Get common error patterns and fix suggestions.
        
        Returns:
            List of error pattern dictionaries
        """
        return [
            {
                "pattern": r'permission denied|cannot access|operation not permitted',
                "description": "Permission denied error",
                "fixes": [
                    "Try running the command with sudo: `sudo {command}`",
                    "Check file permissions with `ls -l {path}`",
                    "Change file permissions with `chmod +x {path}`"
                ]
            },
            {
                "pattern": r'command not found|not installed|no such file or directory',
                "description": "Command or file not found",
                "fixes": [
                    "Install the package containing the command",
                    "Check if the path is correct",
                    "Use `which {command}` to check if the command is in PATH"
                ]
            },
            {
                "pattern": r'syntax error|invalid option|unrecognized option',
                "description": "Command syntax error",
                "fixes": [
                    "Check the command syntax with `man {command}`",
                    "Remove problematic options or flags",
                    "Ensure quotes and brackets are properly matched"
                ]
            },
            {
                "pattern": r'cannot connect|connection refused|network is unreachable',
                "description": "Network connection error",
                "fixes": [
                    "Check if the host is reachable with `ping {host}`",
                    "Verify network connectivity",
                    "Ensure the service is running with `systemctl status {service}`"
                ]
            },
            {
                "pattern": r'disk quota exceeded|no space left on device|file system is full',
                "description": "Disk space issue",
                "fixes": [
                    "Free up disk space with `df -h` to check and `rm` to remove files",
                    "Clean up temporary files with `apt-get clean` or `yum clean all`",
                    "Compress large files with `gzip {file}`"
                ]
            },
            {
                "pattern": r'resource temporarily unavailable|resource busy|device or resource busy',
                "description": "Resource busy error",
                "fixes": [
                    "Wait and try again later",
                    "Check what processes are using the resource with `lsof {path}`",
                    "Terminate competing processes with `kill {pid}`"
                ]
            }
        ]
</file>

<file path="angela/execution/filesystem.py">
"""
File system operations for Angela CLI.

This module provides high-level file and directory operations with safety checks
and proper error handling.
"""
import os
import shutil
import tempfile
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, Optional, List, Tuple, Union, BinaryIO, TextIO

from angela.utils.logging import get_logger
from angela.safety import check_operation_safety

logger = get_logger(__name__)

# Directory for storing backup files for rollback operations
BACKUP_DIR = Path(tempfile.gettempdir()) / "angela-backups"


class FileSystemError(Exception):
    """Exception raised for file system operation errors."""
    pass


# Ensure backup directory exists
def _ensure_backup_dir():
    """Create the backup directory if it doesn't exist."""
    BACKUP_DIR.mkdir(parents=True, exist_ok=True)


async def create_directory(
    path: Union[str, Path], 
    parents: bool = True,
    dry_run: bool = False
) -> bool:
    """
    Create a directory at the specified path.
    
    Args:
        path: The path where the directory should be created.
        parents: Whether to create parent directories if they don't exist.
        dry_run: Whether to simulate the operation without making changes.
        
    Returns:
        True if the operation was successful, False otherwise.
    """
    path_obj = Path(path)
    operation_params = {
        'path': str(path_obj),
        'parents': parents
    }
    
    try:
        # Check if the operation is safe
        if not await check_operation_safety('create_directory', operation_params, dry_run):
            return False
        
        # If this is a dry run, stop here
        if dry_run:
            logger.info(f"DRY RUN: Would create directory at {path_obj}")
            return True
        
        # Create the directory
        if parents:
            path_obj.mkdir(parents=True, exist_ok=True)
        else:
            path_obj.mkdir(exist_ok=False)
        
        logger.info(f"Created directory at {path_obj}")
        return True
    
    except Exception as e:
        logger.exception(f"Error creating directory at {path_obj}: {str(e)}")
        raise FileSystemError(f"Failed to create directory: {str(e)}")


async def delete_directory(
    path: Union[str, Path], 
    recursive: bool = False,
    force: bool = False,
    dry_run: bool = False
) -> bool:
    """
    Delete a directory at the specified path.
    
    Args:
        path: The path of the directory to delete.
        recursive: Whether to recursively delete contents (rmdir vs rm -r).
        force: Whether to ignore errors if the directory doesn't exist.
        dry_run: Whether to simulate the operation without making changes.
        
    Returns:
        True if the operation was successful, False otherwise.
    """
    path_obj = Path(path)
    operation_params = {
        'path': str(path_obj),
        'recursive': recursive,
        'force': force
    }
    
    try:
        # Check if the directory exists
        if not path_obj.exists():
            if force:
                logger.info(f"Directory does not exist, but force=True: {path_obj}")
                return True
            else:
                raise FileSystemError(f"Directory does not exist: {path_obj}")
        
        # Verify it's actually a directory
        if not path_obj.is_dir():
            raise FileSystemError(f"Path is not a directory: {path_obj}")
        
        # Check if the operation is safe
        if not await check_operation_safety('delete_directory', operation_params, dry_run):
            return False
        
        # Create a backup for rollback if needed
        if not dry_run and recursive:
            await _backup_directory(path_obj)
        
        # If this is a dry run, stop here
        if dry_run:
            logger.info(f"DRY RUN: Would delete directory at {path_obj}")
            return True
        
        # Delete the directory
        if recursive:
            shutil.rmtree(path_obj)
            logger.info(f"Recursively deleted directory at {path_obj}")
        else:
            path_obj.rmdir()
            logger.info(f"Deleted directory at {path_obj}")
        
        return True
    
    except Exception as e:
        logger.exception(f"Error deleting directory at {path_obj}: {str(e)}")
        raise FileSystemError(f"Failed to delete directory: {str(e)}")


async def create_file(
    path: Union[str, Path], 
    content: Optional[str] = None,
    dry_run: bool = False
) -> bool:
    """
    Create a file at the specified path with optional content.
    
    Args:
        path: The path where the file should be created.
        content: Optional content to write to the file (if None, like touch).
        dry_run: Whether to simulate the operation without making changes.
        
    Returns:
        True if the operation was successful, False otherwise.
    """
    path_obj = Path(path)
    operation_params = {
        'path': str(path_obj),
        'content': content is not None
    }
    
    try:
        # Check if the operation is safe
        if not await check_operation_safety('create_file', operation_params, dry_run):
            return False
        
        # If this is a dry run, stop here
        if dry_run:
            if content:
                logger.info(f"DRY RUN: Would create file with content at {path_obj}")
            else:
                logger.info(f"DRY RUN: Would touch file at {path_obj}")
            return True
        
        # Make sure parent directory exists
        if not path_obj.parent.exists():
            await create_directory(path_obj.parent, dry_run=False)
        
        # Handle backup if the file already exists
        if path_obj.exists():
            await _backup_file(path_obj)
        
        # Create/write the file
        if content is not None:
            with open(path_obj, 'w') as f:
                f.write(content)
            logger.info(f"Created file with content at {path_obj}")
        else:
            path_obj.touch()
            logger.info(f"Touched file at {path_obj}")
        
        return True
    
    except Exception as e:
        logger.exception(f"Error creating file at {path_obj}: {str(e)}")
        raise FileSystemError(f"Failed to create file: {str(e)}")


async def read_file(
    path: Union[str, Path], 
    binary: bool = False
) -> Union[str, bytes]:
    """
    Read the content of a file.
    
    Args:
        path: The path of the file to read.
        binary: Whether to read the file in binary mode.
        
    Returns:
        The content of the file as a string or bytes.
    """
    path_obj = Path(path)
    operation_params = {
        'path': str(path_obj),
        'binary': binary
    }
    
    try:
        # Check if the file exists
        if not path_obj.exists():
            raise FileSystemError(f"File does not exist: {path_obj}")
        
        # Verify it's actually a file
        if not path_obj.is_file():
            raise FileSystemError(f"Path is not a file: {path_obj}")
        
        # Check if the operation is safe
        if not await check_operation_safety('read_file', operation_params, False):
            raise FileSystemError("Operation not permitted due to safety constraints")
        
        # Read the file
        if binary:
            with open(path_obj, 'rb') as f:
                content = f.read()
        else:
            with open(path_obj, 'r', errors='replace') as f:
                content = f.read()
        
        logger.info(f"Read file at {path_obj}")
        return content
    
    except Exception as e:
        logger.exception(f"Error reading file at {path_obj}: {str(e)}")
        raise FileSystemError(f"Failed to read file: {str(e)}")


async def write_file(
    path: Union[str, Path], 
    content: Union[str, bytes],
    append: bool = False,
    dry_run: bool = False
) -> bool:
    """
    Write content to a file.
    
    Args:
        path: The path of the file to write.
        content: The content to write to the file.
        append: Whether to append to the file instead of overwriting.
        dry_run: Whether to simulate the operation without making changes.
        
    Returns:
        True if the operation was successful, False otherwise.
    """
    path_obj = Path(path)
    is_binary = isinstance(content, bytes)
    operation_params = {
        'path': str(path_obj),
        'append': append,
        'binary': is_binary
    }
    
    try:
        # Check if the operation is safe
        if not await check_operation_safety('write_file', operation_params, dry_run):
            return False
        
        # If this is a dry run, stop here
        if dry_run:
            mode = "append to" if append else "write to"
            logger.info(f"DRY RUN: Would {mode} file at {path_obj}")
            return True
        
        # Make sure parent directory exists
        if not path_obj.parent.exists():
            await create_directory(path_obj.parent, dry_run=False)
        
        # Handle backup if the file already exists
        if path_obj.exists():
            await _backup_file(path_obj)
        
        # Write the file
        mode = 'ab' if append and is_binary else 'wb' if is_binary else 'a' if append else 'w'
        with open(path_obj, mode) as f:
            f.write(content)
        
        action = "Appended to" if append else "Wrote to"
        logger.info(f"{action} file at {path_obj}")
        return True
    
    except Exception as e:
        logger.exception(f"Error writing to file at {path_obj}: {str(e)}")
        raise FileSystemError(f"Failed to write file: {str(e)}")


async def delete_file(
    path: Union[str, Path], 
    force: bool = False,
    dry_run: bool = False
) -> bool:
    """
    Delete a file at the specified path.
    
    Args:
        path: The path of the file to delete.
        force: Whether to ignore errors if the file doesn't exist.
        dry_run: Whether to simulate the operation without making changes.
        
    Returns:
        True if the operation was successful, False otherwise.
    """
    path_obj = Path(path)
    operation_params = {
        'path': str(path_obj),
        'force': force
    }
    
    try:
        # Check if the file exists
        if not path_obj.exists():
            if force:
                logger.info(f"File does not exist, but force=True: {path_obj}")
                return True
            else:
                raise FileSystemError(f"File does not exist: {path_obj}")
        
        # Verify it's actually a file
        if not path_obj.is_file():
            raise FileSystemError(f"Path is not a file: {path_obj}")
        
        # Check if the operation is safe
        if not await check_operation_safety('delete_file', operation_params, dry_run):
            return False
        
        # Create a backup for rollback if needed
        if not dry_run:
            await _backup_file(path_obj)
        
        # If this is a dry run, stop here
        if dry_run:
            logger.info(f"DRY RUN: Would delete file at {path_obj}")
            return True
        
        # Delete the file
        path_obj.unlink()
        logger.info(f"Deleted file at {path_obj}")
        
        return True
    
    except Exception as e:
        logger.exception(f"Error deleting file at {path_obj}: {str(e)}")
        raise FileSystemError(f"Failed to delete file: {str(e)}")


async def copy_file(
    source: Union[str, Path], 
    destination: Union[str, Path],
    overwrite: bool = False,
    dry_run: bool = False
) -> bool:
    """
    Copy a file from source to destination.
    
    Args:
        source: The path of the file to copy.
        destination: The destination path.
        overwrite: Whether to overwrite the destination if it exists.
        dry_run: Whether to simulate the operation without making changes.
        
    Returns:
        True if the operation was successful, False otherwise.
    """
    source_obj = Path(source)
    dest_obj = Path(destination)
    operation_params = {
        'source': str(source_obj),
        'destination': str(dest_obj),
        'overwrite': overwrite
    }
    
    try:
        # Check if the source file exists
        if not source_obj.exists():
            raise FileSystemError(f"Source file does not exist: {source_obj}")
        
        # Verify source is actually a file
        if not source_obj.is_file():
            raise FileSystemError(f"Source path is not a file: {source_obj}")
        
        # Check if the destination exists and handle overwrite
        if dest_obj.exists():
            if not overwrite:
                raise FileSystemError(f"Destination already exists: {dest_obj}")
            
            # Create a backup of the destination file
            if not dry_run:
                await _backup_file(dest_obj)
        
        # Check if the operation is safe
        if not await check_operation_safety('copy_file', operation_params, dry_run):
            return False
        
        # If this is a dry run, stop here
        if dry_run:
            logger.info(f"DRY RUN: Would copy {source_obj} to {dest_obj}")
            return True
        
        # Make sure parent directory exists
        if not dest_obj.parent.exists():
            await create_directory(dest_obj.parent, dry_run=False)
        
        # Copy the file
        shutil.copy2(source_obj, dest_obj)
        logger.info(f"Copied {source_obj} to {dest_obj}")
        
        return True
    
    except Exception as e:
        logger.exception(f"Error copying {source_obj} to {dest_obj}: {str(e)}")
        raise FileSystemError(f"Failed to copy file: {str(e)}")


async def move_file(
    source: Union[str, Path], 
    destination: Union[str, Path],
    overwrite: bool = False,
    dry_run: bool = False
) -> bool:
    """
    Move a file from source to destination.
    
    Args:
        source: The path of the file to move.
        destination: The destination path.
        overwrite: Whether to overwrite the destination if it exists.
        dry_run: Whether to simulate the operation without making changes.
        
    Returns:
        True if the operation was successful, False otherwise.
    """
    source_obj = Path(source)
    dest_obj = Path(destination)
    operation_params = {
        'source': str(source_obj),
        'destination': str(dest_obj),
        'overwrite': overwrite
    }
    
    try:
        # Check if the source file exists
        if not source_obj.exists():
            raise FileSystemError(f"Source file does not exist: {source_obj}")
        
        # Verify source is actually a file
        if not source_obj.is_file():
            raise FileSystemError(f"Source path is not a file: {source_obj}")
        
        # Check if the destination exists and handle overwrite
        if dest_obj.exists():
            if not overwrite:
                raise FileSystemError(f"Destination already exists: {dest_obj}")
            
            # Create a backup of the destination file
            if not dry_run:
                await _backup_file(dest_obj)
        
        # Check if the operation is safe
        if not await check_operation_safety('move_file', operation_params, dry_run):
            return False
        
        # Create a backup of the source file
        if not dry_run:
            await _backup_file(source_obj)
        
        # If this is a dry run, stop here
        if dry_run:
            logger.info(f"DRY RUN: Would move {source_obj} to {dest_obj}")
            return True
        
        # Make sure parent directory exists
        if not dest_obj.parent.exists():
            await create_directory(dest_obj.parent, dry_run=False)
        
        # Move the file
        shutil.move(str(source_obj), str(dest_obj))
        logger.info(f"Moved {source_obj} to {dest_obj}")
        
        return True
    
    except Exception as e:
        logger.exception(f"Error moving {source_obj} to {dest_obj}: {str(e)}")
        raise FileSystemError(f"Failed to move file: {str(e)}")


# --- Helper functions for backups and rollbacks ---

async def _backup_file(path: Path) -> Path:
    """
    Create a backup of a file for potential rollback.
    
    Args:
        path: The path of the file to back up.
        
    Returns:
        The path of the backup file.
    """
    try:
        _ensure_backup_dir()
        
        # Create a unique backup filename
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_name = f"{path.name}.{timestamp}.bak"
        backup_path = BACKUP_DIR / backup_name
        
        # Copy the file to the backup location
        shutil.copy2(path, backup_path)
        logger.debug(f"Created backup of {path} at {backup_path}")
        
        return backup_path
    
    except Exception as e:
        logger.warning(f"Failed to create backup of {path}: {str(e)}")
        # Not raising an exception here as this is a non-critical operation
        return None


async def _backup_directory(path: Path) -> Path:
    """
    Create a backup of a directory for potential rollback.
    
    Args:
        path: The path of the directory to back up.
        
    Returns:
        The path of the backup directory.
    """
    try:
        _ensure_backup_dir()
        
        # Create a unique backup directory name
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_name = f"{path.name}.{timestamp}.bak"
        backup_path = BACKUP_DIR / backup_name
        
        # Copy the directory to the backup location
        shutil.copytree(path, backup_path)
        logger.debug(f"Created backup of directory {path} at {backup_path}")
        
        return backup_path
    
    except Exception as e:
        logger.warning(f"Failed to create backup of directory {path}: {str(e)}")
        # Not raising an exception here as this is a non-critical operation
        return None
</file>

<file path="angela/execution/hooks.py">
"""
Execution hooks for file operations and command execution.

This module provides hooks for tracking file activities during execution
and enriching context in real-time.
"""
import os
import re
from pathlib import Path
from typing import Dict, Any, Optional, List, Tuple, Union

from angela.context.file_activity import file_activity_tracker, ActivityType
from angela.utils.logging import get_logger

logger = get_logger(__name__)

class ExecutionHooks:
    """
    Hooks for file operations and command execution.
    
    Provides hooks to track file activities during execution:
    1. Pre-execution hooks (before a command/operation is executed)
    2. Post-execution hooks (after a command/operation is executed)
    """
    
    def __init__(self):
        """Initialize the execution hooks."""
        self._logger = logger
    
    async def pre_execute_command(
        self, 
        command: str,
        context: Dict[str, Any]
    ) -> None:
        """
        Pre-execution hook for commands.
        
        Args:
            command: The command to be executed
            context: Context information
        """
        self._logger.debug(f"Pre-execute hook for command: {command}")
        
        # Extract potential file operations from the command
        await self._analyze_command_for_files(command, context)
    
    async def post_execute_command(
        self, 
        command: str,
        result: Dict[str, Any],
        context: Dict[str, Any]
    ) -> None:
        """
        Post-execution hook for commands.
        
        Args:
            command: The executed command
            result: The execution result
            context: Context information
        """
        self._logger.debug(f"Post-execute hook for command: {command}")
        
        # Check if the command succeeded
        if not result.get("success", False):
            return
        
        # Analyze command output for file information
        await self._analyze_command_output(command, result.get("stdout", ""), context)
        
        # Track file activities based on the command type
        base_command = command.split()[0] if command else ""
        
        # Handle common file operation commands
        if base_command in ["cat", "less", "more", "head", "tail"]:
            await self._track_file_viewing(command, context)
        elif base_command in ["touch", "echo", "tee"]:
            await self._track_file_creation(command, context)
        elif base_command in ["rm", "rmdir"]:
            await self._track_file_deletion(command, context)
        elif base_command in ["cp", "mv", "rsync"]:
            await self._track_file_copy_move(command, context)
        elif base_command in ["sed", "awk", "perl", "nano", "vim", "emacs"]:
            await self._track_file_modification(command, context)
    
    async def pre_execute_file_operation(
        self, 
        operation_type: str,
        parameters: Dict[str, Any],
        context: Dict[str, Any]
    ) -> None:
        """
        Pre-execution hook for file operations.
        
        Args:
            operation_type: The type of file operation
            parameters: The operation parameters
            context: Context information
        """
        self._logger.debug(f"Pre-execute hook for file operation: {operation_type}")
        
        # Nothing specific to do before file operations for now
        pass
    
    async def post_execute_file_operation(
        self, 
        operation_type: str,
        parameters: Dict[str, Any],
        result: Dict[str, Any],
        context: Dict[str, Any]
    ) -> None:
        """
        Post-execution hook for file operations.
        
        Args:
            operation_type: The type of file operation
            parameters: The operation parameters
            result: The operation result
            context: Context information
        """
        self._logger.debug(f"Post-execute hook for file operation: {operation_type}")
        
        # Check if the operation succeeded
        if not result.get("success", False):
            return
        
        # Track file activity based on operation type
        if operation_type == "create_file":
            file_path = parameters.get("path")
            if file_path:
                file_activity_tracker.track_file_creation(
                    Path(file_path),
                    None,  # No command
                    {"operation": operation_type}
                )
        elif operation_type == "write_file":
            file_path = parameters.get("path")
            if file_path:
                file_activity_tracker.track_file_modification(
                    Path(file_path),
                    None,  # No command
                    {"operation": operation_type, "append": parameters.get("append", False)}
                )
        elif operation_type == "delete_file":
            file_path = parameters.get("path")
            if file_path:
                file_activity_tracker.track_file_deletion(
                    Path(file_path),
                    None,  # No command
                    {"operation": operation_type}
                )
        elif operation_type == "read_file":
            file_path = parameters.get("path")
            if file_path:
                file_activity_tracker.track_file_viewing(
                    Path(file_path),
                    None,  # No command
                    {"operation": operation_type}
                )
        elif operation_type == "copy_file":
            source = parameters.get("source")
            destination = parameters.get("destination")
            if source and destination:
                # Track the source as read
                file_activity_tracker.track_file_viewing(
                    Path(source),
                    None,  # No command
                    {"operation": operation_type}
                )
                # Track the destination as created/modified
                file_activity_tracker.track_file_creation(
                    Path(destination),
                    None,  # No command
                    {"operation": operation_type, "source": source}
                )
        elif operation_type == "move_file":
            source = parameters.get("source")
            destination = parameters.get("destination")
            if source and destination:
                # Track the source as deleted
                file_activity_tracker.track_file_deletion(
                    Path(source),
                    None,  # No command
                    {"operation": operation_type}
                )
                # Track the destination as created
                file_activity_tracker.track_file_creation(
                    Path(destination),
                    None,  # No command
                    {"operation": operation_type, "source": source}
                )
    
    async def _analyze_command_for_files(
        self, 
        command: str,
        context: Dict[str, Any]
    ) -> None:
        """
        Analyze a command for potential file operations.
        
        Args:
            command: The command to analyze
            context: Context information
        """
        # Split command into tokens
        tokens = command.split()
        
        if not tokens:
            return
        
        base_command = tokens[0]
        
        # Check for file paths in tokens
        paths = []
        
        for token in tokens[1:]:
            # Skip options
            if token.startswith('-'):
                continue
            
            # Skip redirection operators
            if token in ['>', '<', '>>', '2>', '&>']:
                continue
            
            # Check if token looks like a path
            if '/' in token or '.' in token:
                # Resolve relative to CWD
                path = Path(context.get("cwd", ".")) / token
                if path.exists():
                    paths.append(path)
        
        # Track potential file accesses
        for path in paths:
            if path.is_file():
                file_activity_tracker.track_file_viewing(
                    path,
                    command,
                    {"pre_execution": True}
                )
    
    async def _analyze_command_output(
        self, 
        command: str,
        output: str,
        context: Dict[str, Any]
    ) -> None:
        """
        Analyze command output for file information.
        
        Args:
            command: The executed command
            output: The command output
            context: Context information
        """
        # Check for file paths in output
        paths = set()
        
        # Look for patterns that might be file paths
        path_patterns = [
            r'[\'"]([/\w\-\.]+\.\w+)[\'"]',  # Quoted paths with extension
            r'\b(/[/\w\-\.]+\.\w+)\b',       # Absolute paths with extension
            r'\b(\./[/\w\-\.]+\.\w+)\b',     # Relative paths with ./ prefix
        ]
        
        for pattern in path_patterns:
            for match in re.finditer(pattern, output):
                potential_path = match.group(1)
                
                # Resolve relative to CWD
                if not potential_path.startswith('/'):
                    potential_path = os.path.join(context.get("cwd", "."), potential_path)
                
                path = Path(potential_path)
                if path.exists() and path.is_file():
                    paths.add(path)
        
        # Track found files
        for path in paths:
            if path.is_file():
                file_activity_tracker.track_file_viewing(
                    path,
                    command,
                    {"from_output": True}
                )
    
    async def _track_file_viewing(
        self, 
        command: str,
        context: Dict[str, Any]
    ) -> None:
        """
        Track file viewing for commands like cat, less, more, etc.
        
        Args:
            command: The executed command
            context: Context information
        """
        tokens = command.split()
        
        if len(tokens) < 2:
            return
        
        # Find potential file paths
        for token in tokens[1:]:
            # Skip options
            if token.startswith('-'):
                continue
            
            # Skip if it looks like a pipe or redirection
            if token in ['|', '>', '<', '>>', '2>', '&>']:
                break
            
            # Resolve path
            path = Path(token)
            if not path.is_absolute():
                path = Path(context.get("cwd", ".")) / token
            
            if path.exists() and path.is_file():
                file_activity_tracker.track_file_viewing(
                    path,
                    command,
                    {}
                )
    
    async def _track_file_creation(
        self, 
        command: str,
        context: Dict[str, Any]
    ) -> None:
        """
        Track file creation for commands like touch, etc.
        
        Args:
            command: The executed command
            context: Context information
        """
        tokens = command.split()
        
        if len(tokens) < 2:
            return
        
        # Handle touch command
        if tokens[0] == 'touch':
            for token in tokens[1:]:
                # Skip options
                if token.startswith('-'):
                    continue
                
                # Resolve path
                path = Path(token)
                if not path.is_absolute():
                    path = Path(context.get("cwd", ".")) / token
                
                if path.exists() and path.is_file():
                    file_activity_tracker.track_file_creation(
                        path,
                        command,
                        {}
                    )
        
        # Handle echo/redirection
        elif tokens[0] == 'echo':
            # Look for redirection
            redirect_idx = -1
            for i, token in enumerate(tokens):
                if token in ['>', '>>']:
                    redirect_idx = i
                    break
            
            if redirect_idx > 0 and redirect_idx < len(tokens) - 1:
                file_path = tokens[redirect_idx + 1]
                
                # Resolve path
                path = Path(file_path)
                if not path.is_absolute():
                    path = Path(context.get("cwd", ".")) / file_path
                
                operation = ActivityType.CREATED
                if tokens[redirect_idx] == '>>':
                    operation = ActivityType.MODIFIED
                
                if path.exists() and path.is_file():
                    if operation == ActivityType.CREATED:
                        file_activity_tracker.track_file_creation(
                            path,
                            command,
                            {"redirect": tokens[redirect_idx]}
                        )
                    else:
                        file_activity_tracker.track_file_modification(
                            path,
                            command,
                            {"redirect": tokens[redirect_idx]}
                        )
    
    async def _track_file_deletion(
        self, 
        command: str,
        context: Dict[str, Any]
    ) -> None:
        """
        Track file deletion for commands like rm, etc.
        
        Args:
            command: The executed command
            context: Context information
        """
        tokens = command.split()
        
        if len(tokens) < 2:
            return
        
        # Skip arguments until we find a non-option
        for token in tokens[1:]:
            # Skip options
            if token.startswith('-'):
                continue
            
            # This is probably a path
            path = Path(token)
            if not path.is_absolute():
                path = Path(context.get("cwd", ".")) / token
            
            # We can't check if it exists since it was deleted
            file_activity_tracker.track_file_deletion(
                path,
                command,
                {}
            )
    
    async def _track_file_copy_move(
        self, 
        command: str,
        context: Dict[str, Any]
    ) -> None:
        """
        Track file copy/move for commands like cp, mv, etc.
        
        Args:
            command: The executed command
            context: Context information
        """
        tokens = command.split()
        
        if len(tokens) < 3:
            return
        
        base_command = tokens[0]
        
        # Find the source and destination
        source_tokens = []
        dest_token = tokens[-1]  # Last token is destination
        
        # Collect all tokens that aren't options as source tokens
        for token in tokens[1:-1]:
            if not token.startswith('-'):
                source_tokens.append(token)
        
        # Resolve destination path
        dest_path = Path(dest_token)
        if not dest_path.is_absolute():
            dest_path = Path(context.get("cwd", ".")) / dest_token
        
        # Track each source
        for source_token in source_tokens:
            source_path = Path(source_token)
            if not source_path.is_absolute():
                source_path = Path(context.get("cwd", ".")) / source_token
            
            if base_command == 'cp':
                # For cp, source is viewed and destination is created
                file_activity_tracker.track_file_viewing(
                    source_path,
                    command,
                    {"operation": "copy", "destination": str(dest_path)}
                )
                
                # If destination is a directory, the file goes inside it
                if dest_path.is_dir():
                    actual_dest = dest_path / source_path.name
                else:
                    actual_dest = dest_path
                
                file_activity_tracker.track_file_creation(
                    actual_dest,
                    command,
                    {"operation": "copy", "source": str(source_path)}
                )
            elif base_command == 'mv':
                # For mv, source is deleted and destination is created
                file_activity_tracker.track_file_deletion(
                    source_path,
                    command,
                    {"operation": "move", "destination": str(dest_path)}
                )
                
                # If destination is a directory, the file goes inside it
                if dest_path.is_dir():
                    actual_dest = dest_path / source_path.name
                else:
                    actual_dest = dest_path
                
                file_activity_tracker.track_file_creation(
                    actual_dest,
                    command,
                    {"operation": "move", "source": str(source_path)}
                )
    
    async def _track_file_modification(
        self, 
        command: str,
        context: Dict[str, Any]
    ) -> None:
        """
        Track file modification for commands like sed, etc.
        
        Args:
            command: The executed command
            context: Context information
        """
        tokens = command.split()
        
        if len(tokens) < 2:
            return
        
        base_command = tokens[0]
        
        # Handle sed command
        if base_command == 'sed':
            # Find the file token (usually the last one)
            file_token = None
            for i in range(len(tokens) - 1, 0, -1):
                if not tokens[i].startswith('-') and not tokens[i].startswith("'") and not tokens[i].startswith('"'):
                    file_token = tokens[i]
                    break
            
            if file_token:
                path = Path(file_token)
                if not path.is_absolute():
                    path = Path(context.get("cwd", ".")) / file_token
                
                if path.exists() and path.is_file():
                    file_activity_tracker.track_file_modification(
                        path,
                        command,
                        {}
                    )

# Global execution hooks instance
execution_hooks = ExecutionHooks()
</file>

<file path="angela/execution/rollback.py">
"""
Rollback functionality for Angela CLI operations.

This module provides the ability to undo previous operations
by restoring files and directories from backups.
"""
import os
import json
import shutil
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, Optional, List, Tuple, Union

from angela.utils.logging import get_logger
from angela.execution.filesystem import BACKUP_DIR

logger = get_logger(__name__)

# File to store operation history for rollback
HISTORY_FILE = BACKUP_DIR / "operation_history.json"


class OperationRecord:
    """Record of an operation for rollback purposes."""
    
    def __init__(
        self,
        operation_type: str,
        params: Dict[str, Any],
        timestamp: Optional[datetime] = None,
        backup_path: Optional[str] = None
    ):
        self.operation_type = operation_type
        self.params = params
        self.timestamp = timestamp or datetime.now()
        self.backup_path = backup_path
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert the record to a dictionary for storage."""
        return {
            "operation_type": self.operation_type,
            "params": self.params,
            "timestamp": self.timestamp.isoformat(),
            "backup_path": str(self.backup_path) if self.backup_path else None
        }
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'OperationRecord':
        """Create a record from a dictionary."""
        return cls(
            operation_type=data["operation_type"],
            params=data["params"],
            timestamp=datetime.fromisoformat(data["timestamp"]),
            backup_path=data["backup_path"]
        )


class RollbackManager:
    """Manager for operation history and rollback functionality."""
    
    def __init__(self):
        """Initialize the rollback manager."""
        self._ensure_history_file()
        self._operations = self._load_history()
    
    def _ensure_history_file(self):
        """Ensure the history file and directory exist."""
        BACKUP_DIR.mkdir(parents=True, exist_ok=True)
        if not HISTORY_FILE.exists():
            self._save_history([])
    
    def _load_history(self) -> List[OperationRecord]:
        """Load operation history from the history file."""
        try:
            with open(HISTORY_FILE, 'r') as f:
                data = json.load(f)
            
            return [OperationRecord.from_dict(item) for item in data]
        
        except Exception as e:
            logger.error(f"Error loading operation history: {str(e)}")
            return []
    
    def _save_history(self, operations: List[OperationRecord]):
        """Save operation history to the history file."""
        try:
            with open(HISTORY_FILE, 'w') as f:
                json.dump([op.to_dict() for op in operations], f, indent=2)
        
        except Exception as e:
            logger.error(f"Error saving operation history: {str(e)}")
    
    async def record_operation(
        self,
        operation_type: str,
        params: Dict[str, Any],
        backup_path: Optional[Union[str, Path]] = None
    ):
        """
        Record an operation for potential rollback.
        
        Args:
            operation_type: The type of operation.
            params: Parameters of the operation.
            backup_path: Path to the backup, if one was created.
        """
        try:
            record = OperationRecord(
                operation_type=operation_type,
                params=params,
                backup_path=str(backup_path) if backup_path else None
            )
            
            self._operations.append(record)
            self._save_history(self._operations)
            
            logger.debug(f"Recorded operation: {operation_type}")
        
        except Exception as e:
            logger.error(f"Error recording operation: {str(e)}")
    
    async def get_recent_operations(self, limit: int = 10) -> List[Dict[str, Any]]:
        """
        Get a list of recent operations that can be rolled back.
        
        Args:
            limit: Maximum number of operations to return.
            
        Returns:
            A list of operation details.
        """
        try:
            # Get the most recent operations, up to the limit
            recent = self._operations[-limit:] if self._operations else []
            
            # Convert to a more user-friendly format
            result = []
            for i, op in enumerate(reversed(recent)):
                # Create a more readable description
                description = self._get_operation_description(op)
                
                result.append({
                    "id": len(self._operations) - i - 1,  # Original index in the full list
                    "timestamp": op.timestamp.strftime("%Y-%m-%d %H:%M:%S"),
                    "operation_type": op.operation_type,
                    "description": description,
                    "can_rollback": bool(op.backup_path)
                })
            
            return result
        
        except Exception as e:
            logger.error(f"Error getting recent operations: {str(e)}")
            return []
    
    def _get_operation_description(self, op: OperationRecord) -> str:
        """Generate a human-readable description of an operation."""
        try:
            if op.operation_type == "create_file":
                return f"Created file: {op.params.get('path', 'unknown')}"
            
            elif op.operation_type == "write_file":
                return f"Wrote to file: {op.params.get('path', 'unknown')}"
            
            elif op.operation_type == "delete_file":
                return f"Deleted file: {op.params.get('path', 'unknown')}"
            
            elif op.operation_type == "create_directory":
                return f"Created directory: {op.params.get('path', 'unknown')}"
            
            elif op.operation_type == "delete_directory":
                return f"Deleted directory: {op.params.get('path', 'unknown')}"
            
            elif op.operation_type == "copy_file":
                return f"Copied file from {op.params.get('source', 'unknown')} to {op.params.get('destination', 'unknown')}"
            
            elif op.operation_type == "move_file":
                return f"Moved file from {op.params.get('source', 'unknown')} to {op.params.get('destination', 'unknown')}"
            
            elif op.operation_type == "execute_command":
                return f"Executed command: {op.params.get('command', 'unknown')}"
            
            else:
                return f"{op.operation_type}: {op.params}"
        
        except Exception as e:
            logger.error(f"Error generating operation description: {str(e)}")
            return "Unknown operation"
    
    async def rollback_operation(self, operation_id: int) -> bool:
        """
        Roll back an operation by its ID.
        
        Args:
            operation_id: The ID of the operation to roll back.
            
        Returns:
            True if the rollback was successful, False otherwise.
        """
        try:
            # Validate the operation ID
            if operation_id < 0 or operation_id >= len(self._operations):
                logger.error(f"Invalid operation ID: {operation_id}")
                return False
            
            # Get the operation record
            op = self._operations[operation_id]
            
            # Check if a backup exists
            if not op.backup_path:
                logger.error(f"No backup available for operation: {op.operation_type}")
                return False
            
            backup_path = Path(op.backup_path)
            if not backup_path.exists():
                logger.error(f"Backup file not found: {backup_path}")
                return False
            
            # Perform the rollback based on operation type
            if op.operation_type == "create_file":
                # For file creation, delete the created file
                path = Path(op.params.get("path", ""))
                if path.exists() and path.is_file():
                    path.unlink()
                    logger.info(f"Rolled back file creation: {path}")
            
            elif op.operation_type == "write_file" or op.operation_type == "delete_file":
                # For file writing/deletion, restore from backup
                path = Path(op.params.get("path", ""))
                # Create parent directory if it doesn't exist
                path.parent.mkdir(parents=True, exist_ok=True)
                
                shutil.copy2(backup_path, path)
                logger.info(f"Restored file from backup: {path}")
            
            elif op.operation_type == "create_directory":
                # For directory creation, delete the created directory
                path = Path(op.params.get("path", ""))
                if path.exists() and path.is_dir():
                    # Use rmtree to handle non-empty directories
                    shutil.rmtree(path)
                    logger.info(f"Rolled back directory creation: {path}")
            
            elif op.operation_type == "delete_directory":
                # For directory deletion, restore from backup
                path = Path(op.params.get("path", ""))
                # Create parent directory if it doesn't exist
                path.parent.mkdir(parents=True, exist_ok=True)
                
                shutil.copytree(backup_path, path)
                logger.info(f"Restored directory from backup: {path}")
            
            elif op.operation_type == "copy_file" or op.operation_type == "move_file":
                # For copy/move, multiple files may need to be restored
                destination = Path(op.params.get("destination", ""))
                
                if destination.exists():
                    # Restore the destination if it was overwritten
                    if Path(op.backup_path).is_file():
                        shutil.copy2(backup_path, destination)
                        logger.info(f"Restored destination file: {destination}")
                    else:
                        shutil.rmtree(destination, ignore_errors=True)
                        shutil.copytree(backup_path, destination)
                        logger.info(f"Restored destination directory: {destination}")
                
                # For move operations, also restore the source
                if op.operation_type == "move_file":
                    source = Path(op.params.get("source", ""))
                    if not source.exists() and backup_path.exists():
                        # Create parent directory if it doesn't exist
                        source.parent.mkdir(parents=True, exist_ok=True)
                        
                        shutil.copy2(backup_path, source)
                        logger.info(f"Restored source file: {source}")
            
            else:
                logger.error(f"Unsupported operation type for rollback: {op.operation_type}")
                return False
            
            # Remove the operation and all later operations from history
            self._operations = self._operations[:operation_id]
            self._save_history(self._operations)
            
            logger.info(f"Successfully rolled back operation {operation_id}: {op.operation_type}")
            return True
        
        except Exception as e:
            logger.exception(f"Error rolling back operation {operation_id}: {str(e)}")
            return False


# Global rollback manager instance
rollback_manager = RollbackManager()
</file>

<file path="angela/generation/architecture.py">
# angela/generation/architecture.py
"""
Architectural analysis and improvements for Angela CLI.

This module provides capabilities for analyzing project architecture,
detecting anti-patterns, and suggesting improvements.
"""
import os
import asyncio
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple, Union
import json
import re

from angela.ai.client import gemini_client, GeminiRequest
from angela.utils.logging import get_logger

logger = get_logger(__name__)

class ArchitecturalPattern:
    """Base class for architectural patterns."""
    
    def __init__(self, name: str, description: str):
        """
        Initialize the architectural pattern.
        
        Args:
            name: Pattern name
            description: Pattern description
        """
        self.name = name
        self.description = description
    
    async def detect(self, project_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """
        Detect if the pattern is present in the project.
        
        Args:
            project_analysis: Analysis of the project
            
        Returns:
            Dictionary with detection results
        """
        raise NotImplementedError("Subclasses must implement detect")
    
    def get_recommendations(self, detection_result: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        Get recommendations based on detection results.
        
        Args:
            detection_result: Results from the detect method
            
        Returns:
            List of recommendation dictionaries
        """
        raise NotImplementedError("Subclasses must implement get_recommendations")

class AntiPattern:
    """Base class for architectural anti-patterns."""
    
    def __init__(self, name: str, description: str, severity: str):
        """
        Initialize the anti-pattern.
        
        Args:
            name: Anti-pattern name
            description: Anti-pattern description
            severity: Severity level (low, medium, high)
        """
        self.name = name
        self.description = description
        self.severity = severity
    
    async def detect(self, project_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """
        Detect if the anti-pattern is present in the project.
        
        Args:
            project_analysis: Analysis of the project
            
        Returns:
            Dictionary with detection results
        """
        raise NotImplementedError("Subclasses must implement detect")
    
    def get_recommendations(self, detection_result: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        Get recommendations to fix the anti-pattern.
        
        Args:
            detection_result: Results from the detect method
            
        Returns:
            List of recommendation dictionaries
        """
        raise NotImplementedError("Subclasses must implement get_recommendations")

class MvcPattern(ArchitecturalPattern):
    """Model-View-Controller pattern detector."""
    
    def __init__(self):
        """Initialize the MVC pattern detector."""
        super().__init__(
            name="Model-View-Controller",
            description="Separates application logic into three components: Model (data), View (presentation), and Controller (logic)"
        )
    
    async def detect(self, project_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """
        Detect if MVC pattern is used in the project.
        
        Args:
            project_analysis: Analysis of the project
            
        Returns:
            Dictionary with detection results
        """
        logger.debug("Detecting MVC pattern")
        
        # Default result
        result = {
            "pattern": self.name,
            "present": False,
            "confidence": 0.0,
            "components": {
                "models": [],
                "views": [],
                "controllers": []
            }
        }
        
        # Check file structure for MVC pattern
        models = []
        views = []
        controllers = []
        
        for file_info in project_analysis.get("files", []):
            file_path = file_info.get("path", "").lower()
            file_content = file_info.get("content", "")
            
            # Check for models
            if "model" in file_path or "/models/" in file_path:
                models.append(file_path)
            elif file_content and re.search(r'class\s+\w*Model\b', file_content):
                models.append(file_path)
            
            # Check for views
            if "view" in file_path or "/views/" in file_path:
                views.append(file_path)
            elif file_path.endswith((".html", ".jsx", ".tsx", ".vue")) or "template" in file_path:
                views.append(file_path)
            elif file_content and re.search(r'class\s+\w*View\b', file_content):
                views.append(file_path)
            
            # Check for controllers
            if "controller" in file_path or "/controllers/" in file_path:
                controllers.append(file_path)
            elif file_content and re.search(r'class\s+\w*Controller\b', file_content):
                controllers.append(file_path)
        
        # Update result with components found
        result["components"]["models"] = models
        result["components"]["views"] = views
        result["components"]["controllers"] = controllers
        
        # Calculate confidence
        if models and views and controllers:
            result["present"] = True
            result["confidence"] = 0.9  # High confidence if all three components found
        elif (models and views) or (models and controllers) or (views and controllers):
            result["present"] = True
            result["confidence"] = 0.6  # Medium confidence if two components found
        elif models or views or controllers:
            result["present"] = False
            result["confidence"] = 0.3  # Low confidence if only one component found
        
        return result
    
    def get_recommendations(self, detection_result: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        Get recommendations for improving MVC pattern usage.
        
        Args:
            detection_result: Results from the detect method
            
        Returns:
            List of recommendation dictionaries
        """
        recommendations = []
        
        if detection_result["present"]:
            # MVC pattern is present, check if all components are balanced
            models = detection_result["components"]["models"]
            views = detection_result["components"]["views"]
            controllers = detection_result["components"]["controllers"]
            
            if len(models) < len(controllers) / 2:
                recommendations.append({
                    "title": "Insufficient Model Separation",
                    "description": "There are significantly fewer Model files than Controllers, which may indicate business logic leaking into Controllers.",
                    "action": "Consider extracting data models from Controllers into separate Model classes.",
                    "priority": "medium"
                })
            
            if not controllers and models and views:
                recommendations.append({
                    "title": "Missing Controller Layer",
                    "description": "Models and Views are present, but no clear Controller layer was detected.",
                    "action": "Implement Controllers to handle the interaction between Models and Views.",
                    "priority": "high"
                })
        else:
            # MVC pattern is not present
            if detection_result["confidence"] > 0.0:
                # Some components found, but not all
                recommendations.append({
                    "title": "Incomplete MVC Implementation",
                    "description": "Some MVC components are present, but the pattern is not fully implemented.",
                    "action": "Consider fully adopting the MVC pattern by adding the missing components.",
                    "priority": "medium"
                })
            else:
                # No components found
                recommendations.append({
                    "title": "Consider MVC Pattern",
                    "description": "The project doesn't appear to use the MVC pattern, which can help with code organization and maintainability.",
                    "action": "Consider refactoring to separate concerns into Model, View, and Controller components.",
                    "priority": "low"
                })
        
        return recommendations

class SingleResponsibilityAntiPattern(AntiPattern):
    """Detects violations of the Single Responsibility Principle."""
    
    def __init__(self):
        """Initialize the single responsibility anti-pattern detector."""
        super().__init__(
            name="Single Responsibility Violation",
            description="Classes or modules that have more than one reason to change",
            severity="medium"
        )
    
    async def detect(self, project_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """
        Detect violations of the Single Responsibility Principle.
        
        Args:
            project_analysis: Analysis of the project
            
        Returns:
            Dictionary with detection results
        """
        logger.debug("Detecting Single Responsibility violations")
        
        # Default result
        result = {
            "anti_pattern": self.name,
            "detected": False,
            "instances": [],
            "severity": self.severity
        }
        
        # Check for large classes with many methods
        for file_info in project_analysis.get("files", []):
            if not file_info.get("content"):
                continue
            
            file_path = file_info.get("path", "")
            content = file_info.get("content", "")
            
            # Skip non-source files
            if file_info.get("type") != "source_code":
                continue
            
            language = file_info.get("language", "").lower()
            
            if language in ["python", "java", "javascript", "typescript"]:
                # Check for classes with too many methods
                classes = self._extract_classes(content, language)
                
                for class_info in classes:
                    # Check number of methods
                    if len(class_info["methods"]) > 10:  # Arbitrary threshold
                        # Check for different categories of methods
                        categories = self._categorize_methods(class_info["methods"], language)
                        
                        if len(categories) >= 3:  # If methods fall into 3+ categories, likely has multiple responsibilities
                            result["instances"].append({
                                "file": file_path,
                                "class": class_info["name"],
                                "method_count": len(class_info["methods"]),
                                "categories": categories,
                                "confidence": min(0.5 + (len(categories) - 3) * 0.1, 0.9)  # Higher confidence with more categories
                            })
        
        if result["instances"]:
            result["detected"] = True
        
        return result
    
    def _extract_classes(self, content: str, language: str) -> List[Dict[str, Any]]:
        """
        Extract classes and their methods from code.
        
        Args:
            content: Source code content
            language: Programming language
            
        Returns:
            List of dictionaries with class info
        """
        classes = []
        
        if language == "python":
            # Extract Python classes
            class_pattern = r'class\s+(\w+)(?:\(.*?\))?:'
            method_pattern = r'\s+def\s+(\w+)\s*\('
            
            class_matches = re.finditer(class_pattern, content)
            
            for class_match in class_matches:
                class_name = class_match.group(1)
                class_start = class_match.end()
                
                # Find the end of the class (indentation level)
                class_content = ""
                for line in content[class_start:].splitlines():
                    if line.strip() and not line.startswith(" ") and not line.startswith("\t"):
                        break
                    class_content += line + "\n"
                
                # Extract methods
                methods = []
                for method_match in re.finditer(method_pattern, class_content):
                    method_name = method_match.group(1)
                    if method_name != "__init__":  # Skip constructor
                        methods.append(method_name)
                
                classes.append({
                    "name": class_name,
                    "methods": methods
                })
        
        elif language in ["java", "javascript", "typescript"]:
            # Extract classes (simplified)
            class_pattern = r'class\s+(\w+)(?:\s+extends\s+\w+)?(?:\s+implements\s+[\w,\s]+)?\s*{'
            method_pattern = r'(?:public|private|protected)?\s+(?:static\s+)?(?:\w+\s+)?(\w+)\s*\([^)]*\)\s*{(?:[^{}]|{[^{}]*})*}'
            
            class_matches = re.finditer(class_pattern, content)
            
            for class_match in class_matches:
                class_name = class_match.group(1)
                class_start = class_match.start()
                
                # Find the class block by counting braces
                brace_count = 0
                class_end = class_start
                in_class = False
                
                for i, c in enumerate(content[class_start:]):
                    if c == '{':
                        if not in_class:
                            in_class = True
                        brace_count += 1
                    elif c == '}':
                        brace_count -= 1
                        if in_class and brace_count == 0:
                            class_end = class_start + i + 1
                            break
                
                class_content = content[class_start:class_end]
                
                # Extract methods
                methods = []
                for method_match in re.finditer(method_pattern, class_content):
                    method_name = method_match.group(1)
                    if method_name != "constructor" and not method_name.startswith("get") and not method_name.startswith("set"):
                        methods.append(method_name)
                
                classes.append({
                    "name": class_name,
                    "methods": methods
                })
        
        return classes
    
    def _categorize_methods(self, methods: List[str], language: str) -> Dict[str, List[str]]:
        """
        Categorize methods into different responsibilities.
        
        Args:
            methods: List of method names
            language: Programming language
            
        Returns:
            Dictionary mapping categories to method names
        """
        categories = {}
        
        # Common categories and their keywords
        categories_keywords = {
            "data_access": ["save", "load", "read", "write", "fetch", "store", "retrieve", "query", "find", "get", "set", "select", "insert", "update", "delete", "persist", "repository", "dao"],
            "business_logic": ["calculate", "compute", "process", "validate", "check", "verify", "evaluate", "analyze", "generate", "create", "build", "make", "service"],
            "presentation": ["display", "show", "render", "view", "draw", "paint", "print", "format", "transform", "convert", "ui", "gui", "interface"],
            "networking": ["connect", "disconnect", "send", "receive", "post", "get", "put", "delete", "request", "response", "url", "uri", "http", "api", "rest", "soap", "websocket"],
            "file_io": ["file", "stream", "open", "close", "read", "write", "input", "output", "io", "path", "directory", "folder"],
            "concurrency": ["thread", "async", "await", "parallel", "concurrent", "lock", "mutex", "semaphore", "synchronize", "task", "job", "worker", "pool"],
            "utility": ["util", "helper", "common", "shared", "factory", "builder", "converter", "mapper", "utils"]
        }
        
        # Categorize methods based on name
        for method in methods:
            method_lower = method.lower()
            categorized = False
            
            for category, keywords in categories_keywords.items():
                for keyword in keywords:
                    if keyword in method_lower:
                        if category not in categories:
                            categories[category] = []
                        categories[category].append(method)
                        categorized = True
                        break
                
                if categorized:
                    break
            
            # If method doesn't match any category, put it in "other"
            if not categorized:
                if "other" not in categories:
                    categories["other"] = []
                categories["other"].append(method)
        
        return categories
    
    def get_recommendations(self, detection_result: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        Get recommendations for fixing Single Responsibility Principle violations.
        
        Args:
            detection_result: Results from the detect method
            
        Returns:
            List of recommendation dictionaries
        """
        recommendations = []
        
        if detection_result["detected"]:
            for instance in detection_result["instances"]:
                # Create a recommendation for each instance
                categories_str = ", ".join(instance["categories"].keys())
                
                recommendations.append({
                    "title": f"Refactor Class: {instance['class']}",
                    "description": f"This class has multiple responsibilities: {categories_str}",
                    "action": f"Consider splitting '{instance['class']}' into multiple classes, each with a single responsibility.",
                    "priority": "medium" if instance["confidence"] > 0.7 else "low"
                })
        
        return recommendations

class GodObjectAntiPattern(AntiPattern):
    """Detects God Objects - classes that know or do too much."""
    
    def __init__(self):
        """Initialize the God Object anti-pattern detector."""
        super().__init__(
            name="God Object",
            description="Classes that know or do too much, often with excessive size and responsibilities",
            severity="high"
        )
    
    async def detect(self, project_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """
        Detect God Objects in the project.
        
        Args:
            project_analysis: Analysis of the project
            
        Returns:
            Dictionary with detection results
        """
        logger.debug("Detecting God Objects")
        
        # Default result
        result = {
            "anti_pattern": self.name,
            "detected": False,
            "instances": [],
            "severity": self.severity
        }
        
        # Define thresholds for various metrics
        thresholds = {
            "lines": 500,  # Lines of code
            "methods": 20,  # Number of methods
            "fields": 15,   # Number of fields/properties
            "imports": 15,  # Number of imports
            "dependencies": 10  # Number of dependencies on other classes
        }
        
        # Check each file for potential God Objects
        for file_info in project_analysis.get("files", []):
            if not file_info.get("content"):
                continue
            
            file_path = file_info.get("path", "")
            content = file_info.get("content", "")
            
            # Skip non-source files
            if file_info.get("type") != "source_code":
                continue
            
            language = file_info.get("language", "").lower()
            
            if language in ["python", "java", "javascript", "typescript"]:
                # Extract classes
                classes = self._extract_classes_with_metrics(content, language)
                
                for class_info in classes:
                    # Check if any metric exceeds thresholds
                    violations = {}
                    for metric, value in class_info["metrics"].items():
                        if metric in thresholds and value > thresholds[metric]:
                            violations[metric] = value
                    
                    if violations:
                        # Calculate violation severity
                        violation_count = len(violations)
                        violation_ratio = sum(violations[m] / thresholds[m] for m in violations) / len(violations)
                        confidence = min(0.5 + (violation_count * 0.1) + (violation_ratio * 0.2), 0.95)
                        
                        result["instances"].append({
                            "file": file_path,
                            "class": class_info["name"],
                            "violations": violations,
                            "metrics": class_info["metrics"],
                            "confidence": confidence
                        })
        
        if result["instances"]:
            result["detected"] = True
        
        return result
    
    def _extract_classes_with_metrics(self, content: str, language: str) -> List[Dict[str, Any]]:
        """
        Extract classes and calculate metrics.
        
        Args:
            content: Source code content
            language: Programming language
            
        Returns:
            List of dictionaries with class info and metrics
        """
        classes = []
        
        if language == "python":
            # Extract Python classes
            class_pattern = r'class\s+(\w+)(?:\(.*?\))?:'
            method_pattern = r'\s+def\s+(\w+)\s*\('
            field_pattern = r'\s+self\.(\w+)\s*='
            import_pattern = r'(?:import|from)\s+[\w.]+'
            
            # Count imports
            imports = len(re.findall(import_pattern, content))
            
            class_matches = re.finditer(class_pattern, content)
            
            for class_match in class_matches:
                class_name = class_match.group(1)
                class_start = class_match.end()
                
                # Find the end of the class (indentation level)
                class_content = ""
                class_lines = 0
                for line in content[class_start:].splitlines():
                    if line.strip() and not line.startswith(" ") and not line.startswith("\t"):
                        break
                    class_content += line + "\n"
                    class_lines += 1
                
                # Extract methods
                methods = re.findall(method_pattern, class_content)
                
                # Extract fields
                fields = re.findall(field_pattern, class_content)
                
                # Calculate other dependencies (simplified)
                dependencies = set()
                for line in class_content.splitlines():
                    # Look for other class instantiations
                    instance_matches = re.findall(r'=\s*(\w+)\(', line)
                    for instance in instance_matches:
                        if instance != class_name and instance[0].isupper():  # Potential class
                            dependencies.add(instance)
                
                classes.append({
                    "name": class_name,
                    "content": class_content,
                    "metrics": {
                        "lines": class_lines,
                        "methods": len(methods),
                        "fields": len(fields),
                        "imports": imports,
                        "dependencies": len(dependencies)
                    }
                })
        
        elif language in ["java", "javascript", "typescript"]:
            # Extract classes (simplified)
            class_pattern = r'class\s+(\w+)(?:\s+extends\s+\w+)?(?:\s+implements\s+[\w,\s]+)?\s*{'
            method_pattern = r'(?:public|private|protected)?\s+(?:static\s+)?(?:\w+\s+)?(\w+)\s*\(['
            field_pattern = r'(?:public|private|protected)?\s+(?:static\s+)?(?:final\s+)?[\w<>[\],\s]+\s+(\w+)\s*[;=]'
            import_pattern = r'import\s+[\w.]+'
            
            # Count imports
            imports = len(re.findall(import_pattern, content))
            
            class_matches = re.finditer(class_pattern, content)
            
            for class_match in class_matches:
                class_name = class_match.group(1)
                class_start = class_match.start()
                
                # Find the class block by counting braces
                brace_count = 0
                class_end = class_start
                in_class = False
                
                for i, c in enumerate(content[class_start:]):
                    if c == '{':
                        if not in_class:
                            in_class = True
                        brace_count += 1
                    elif c == '}':
                        brace_count -= 1
                        if in_class and brace_count == 0:
                            class_end = class_start + i + 1
                            break
                
                class_content = content[class_start:class_end]
                class_lines = class_content.count('\n')
                
                # Extract methods
                methods = re.findall(method_pattern, class_content)
                
                # Extract fields
                fields = re.findall(field_pattern, class_content)
                
                # Calculate other dependencies (simplified)
                dependencies = set()
                for line in class_content.splitlines():
                    # Look for other class instantiations
                    instance_matches = re.findall(r'new\s+(\w+)\(', line)
                    for instance in instance_matches:
                        if instance != class_name:
                            dependencies.add(instance)
                
                classes.append({
                    "name": class_name,
                    "content": class_content,
                    "metrics": {
                        "lines": class_lines,
                        "methods": len(methods),
                        "fields": len(fields),
                        "imports": imports,
                        "dependencies": len(dependencies)
                    }
                })
        
        return classes
    
    def get_recommendations(self, detection_result: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        Get recommendations for fixing God Objects.
        
        Args:
            detection_result: Results from the detect method
            
        Returns:
            List of recommendation dictionaries
        """
        recommendations = []
        
        if detection_result["detected"]:
            for instance in detection_result["instances"]:
                # Create specific recommendations based on violations
                violations_msg = []
                
                if "lines" in instance["violations"]:
                    violations_msg.append(f"excessive size ({instance['violations']['lines']} lines)")
                if "methods" in instance["violations"]:
                    violations_msg.append(f"too many methods ({instance['violations']['methods']})")
                if "fields" in instance["violations"]:
                    violations_msg.append(f"too many fields ({instance['violations']['fields']})")
                if "dependencies" in instance["violations"]:
                    violations_msg.append(f"too many dependencies ({instance['violations']['dependencies']})")
                
                violations_str = ", ".join(violations_msg)
                
                # Main recommendation
                recommendations.append({
                    "title": f"Refactor God Object: {instance['class']}",
                    "description": f"This class exhibits God Object symptoms: {violations_str}",
                    "action": f"Break '{instance['class']}' into smaller, more focused classes following the Single Responsibility Principle.",
                    "priority": "high" if instance["confidence"] > 0.8 else "medium"
                })
                
                # Add specific tactical recommendations
                if "methods" in instance["violations"] and instance["violations"]["methods"] > 25:
                    recommendations.append({
                        "title": f"Extract Classes from {instance['class']}",
                        "description": f"This class has an excessive number of methods ({instance['violations']['methods']}).",
                        "action": "Group related methods and extract them into new classes with clear responsibilities.",
                        "priority": "high"
                    })
                
                if "dependencies" in instance["violations"] and instance["violations"]["dependencies"] > 12:
                    recommendations.append({
                        "title": f"Reduce Dependencies in {instance['class']}",
                        "description": f"This class depends on too many other classes ({instance['violations']['dependencies']}).",
                        "action": "Use dependency injection or introduce service locators to reduce direct dependencies.",
                        "priority": "medium"
                    })
        
        return recommendations

class ArchitecturalAnalyzer:
    """
    Analyzer for project architecture, detecting patterns and anti-patterns.
    """
    
    def __init__(self):
        """Initialize the architectural analyzer."""
        self._logger = logger
        
        # Register patterns and anti-patterns
        self._patterns = [
            MvcPattern(),
            # Add more patterns here
        ]
        
        self._anti_patterns = [
            SingleResponsibilityAntiPattern(),
            GodObjectAntiPattern(),
            # Add more anti-patterns here
        ]
    
    async def analyze_architecture(
        self, 
        project_path: Union[str, Path],
        context: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Analyze project architecture.
        
        Args:
            project_path: Path to the project
            context: Additional context information
            
        Returns:
            Dictionary with analysis results
        """
        self._logger.info(f"Analyzing architecture of project at {project_path}")
        
        # Analyze project structure
        project_analysis = await self._analyze_project_structure(project_path, context)
        
        # Detect patterns
        patterns_results = await self._detect_patterns(project_analysis)
        
        # Detect anti-patterns
        anti_patterns_results = await self._detect_anti_patterns(project_analysis)
        
        # Generate recommendations
        recommendations = await self._generate_recommendations(patterns_results, anti_patterns_results)
        
        return {
            "project_path": str(project_path),
            "patterns": patterns_results,
            "anti_patterns": anti_patterns_results,
            "recommendations": recommendations,
            "project_analysis": project_analysis
        }
    
    async def _analyze_project_structure(
        self, 
        project_path: Union[str, Path],
        context: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Analyze project structure.
        
        Args:
            project_path: Path to the project
            context: Additional context information
            
        Returns:
            Dictionary with project analysis
        """
        self._logger.info(f"Analyzing project structure at {project_path}")
        
        # Convert to Path object
        project_path = Path(project_path)
        
        # Use context if available
        if context and "enhanced_project" in context:
            return {
                "project_type": context["enhanced_project"].get("type", "unknown"),
                "frameworks": context["enhanced_project"].get("frameworks", {}),
                "dependencies": context["enhanced_project"].get("dependencies", {}),
                "files": context.get("files", []),
                "path": str(project_path)
            }
        
        # Perform a simplified project analysis
        project_analysis = {
            "project_type": "unknown",
            "frameworks": {},
            "dependencies": {},
            "files": [],
            "path": str(project_path)
        }
        
        # Determine project type
        if (project_path / "requirements.txt").exists() or (project_path / "setup.py").exists() or (project_path / "pyproject.toml").exists():
            project_analysis["project_type"] = "python"
        elif (project_path / "package.json").exists():
            project_analysis["project_type"] = "node"
        elif (project_path / "pom.xml").exists() or (project_path / "build.gradle").exists():
            project_analysis["project_type"] = "java"
        
        # Collect file information
        for root, _, filenames in os.walk(project_path):
            for filename in filenames:
                # Skip common directories to ignore
                if any(ignored in root for ignored in [".git", "__pycache__", "node_modules", "venv", ".idea", ".vscode"]):
                    continue
                
                file_path = Path(root) / filename
                rel_path = file_path.relative_to(project_path)
                
                # Get basic file info
                file_info = {
                    "path": str(rel_path),
                    "full_path": str(file_path),
                    "type": None,
                    "language": None,
                    "content": None
                }
                
                # Try to determine file type and language
                try:
                    from angela.context.file_detector import detect_file_type
                    type_info = detect_file_type(file_path)
                    file_info["type"] = type_info.get("type")
                    file_info["language"] = type_info.get("language")
                    
                    # Read content for source code files (limit to prevent memory issues)
                    if type_info.get("type") == "source_code" and file_path.stat().st_size < 100000:
                        with open(file_path, 'r', encoding='utf-8', errors='replace') as f:
                            file_info["content"] = f.read()
                except Exception as e:
                    self._logger.debug(f"Error analyzing file {file_path}: {str(e)}")
                
                project_analysis["files"].append(file_info)
        
        return project_analysis
    
    async def _detect_patterns(self, project_analysis: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        Detect architectural patterns in the project.
        
        Args:
            project_analysis: Analysis of the project
            
        Returns:
            List of pattern detection results
        """
        self._logger.info("Detecting architectural patterns")
        
        results = []
        
        # Run all pattern detectors
        for pattern in self._patterns:
            try:
                pattern_result = await pattern.detect(project_analysis)
                results.append(pattern_result)
                self._logger.debug(f"Pattern '{pattern.name}' detected: {pattern_result['present']}")
            except Exception as e:
                self._logger.error(f"Error detecting pattern '{pattern.name}': {str(e)}")
        
        return results
    
    async def _detect_anti_patterns(self, project_analysis: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        Detect architectural anti-patterns in the project.
        
        Args:
            project_analysis: Analysis of the project
            
        Returns:
            List of anti-pattern detection results
        """
        self._logger.info("Detecting architectural anti-patterns")
        
        results = []
        
        # Run all anti-pattern detectors
        for anti_pattern in self._anti_patterns:
            try:
                anti_pattern_result = await anti_pattern.detect(project_analysis)
                results.append(anti_pattern_result)
                self._logger.debug(f"Anti-pattern '{anti_pattern.name}' detected: {anti_pattern_result['detected']}")
            except Exception as e:
                self._logger.error(f"Error detecting anti-pattern '{anti_pattern.name}': {str(e)}")
        
        return results
    
    async def _generate_recommendations(
        self, 
        patterns_results: List[Dict[str, Any]],
        anti_patterns_results: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """
        Generate recommendations based on detected patterns and anti-patterns.
        
        Args:
            patterns_results: Pattern detection results
            anti_patterns_results: Anti-pattern detection results
            
        Returns:
            List of recommendations
        """
        self._logger.info("Generating architecture recommendations")
        
        recommendations = []
        
        # Generate recommendations from patterns
        for pattern_result in patterns_results:
            pattern_name = pattern_result["pattern"]
            pattern = next((p for p in self._patterns if p.name == pattern_name), None)
            
            if pattern:
                pattern_recommendations = pattern.get_recommendations(pattern_result)
                recommendations.extend(pattern_recommendations)
        
        # Generate recommendations from anti-patterns
        for anti_pattern_result in anti_patterns_results:
            anti_pattern_name = anti_pattern_result["anti_pattern"]
            anti_pattern = next((ap for ap in self._anti_patterns if ap.name == anti_pattern_name), None)
            
            if anti_pattern and anti_pattern_result["detected"]:
                anti_pattern_recommendations = anti_pattern.get_recommendations(anti_pattern_result)
                recommendations.extend(anti_pattern_recommendations)
        
        # Generate general recommendations using AI for more complex analysis
        if patterns_results or anti_patterns_results:
            ai_recommendations = await self._generate_ai_recommendations(patterns_results, anti_patterns_results)
            recommendations.extend(ai_recommendations)
        
        return recommendations
    
    async def _generate_ai_recommendations(
        self, 
        patterns_results: List[Dict[str, Any]],
        anti_patterns_results: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """
        Generate additional recommendations using AI.
        
        Args:
            patterns_results: Pattern detection results
            anti_patterns_results: Anti-pattern detection results
            
        Returns:
            List of AI-generated recommendations
        """
        self._logger.debug("Generating AI recommendations")
        
        # Build prompt for AI
        prompt = """
You are an expert software architect tasked with providing architectural recommendations based on detected patterns and anti-patterns in a project.

Here are the detected architectural patterns:
"""
        
        # Add pattern information
        for pattern_result in patterns_results:
            prompt += f"\n- Pattern: {pattern_result['pattern']}"
            prompt += f"\n  Present: {pattern_result['present']}"
            prompt += f"\n  Confidence: {pattern_result['confidence']:.2f}"
            
            if "components" in pattern_result:
                prompt += "\n  Components:"
                for component_type, components in pattern_result["components"].items():
                    prompt += f"\n    - {component_type}: {len(components)} files"
        
        prompt += "\n\nHere are the detected architectural anti-patterns:"
        
        # Add anti-pattern information
        for anti_pattern_result in anti_patterns_results:
            prompt += f"\n- Anti-pattern: {anti_pattern_result['anti_pattern']}"
            prompt += f"\n  Detected: {anti_pattern_result['detected']}"
            prompt += f"\n  Severity: {anti_pattern_result['severity']}"
            
            if anti_pattern_result["detected"] and "instances" in anti_pattern_result:
                prompt += f"\n  Instances: {len(anti_pattern_result['instances'])}"
                
                # Add details for the first few instances
                for i, instance in enumerate(anti_pattern_result["instances"][:3]):
                    prompt += f"\n    {i+1}. Class: {instance.get('class', 'Unknown')}"
                    if "violations" in instance:
                        violations = ", ".join(f"{v}: {val}" for v, val in instance["violations"].items())
                        prompt += f"\n       Violations: {violations}"
        
        prompt += """

Based on the above information, provide 3-5 high-level architectural recommendations that would improve the project's design.
For each recommendation, include:
1. A title (concise description)
2. A detailed explanation
3. Concrete action steps
4. Priority (high, medium, low)

Format your response as a JSON array of recommendation objects, like this:
[
  {
    "title": "Clear recommendation title",
    "description": "Detailed explanation of the issue",
    "action": "Specific action steps to implement the recommendation",
    "priority": "high|medium|low"
  },
  ...
]
"""
        
        # Call AI service
        api_request = GeminiRequest(
            prompt=prompt,
            max_tokens=2000,
            temperature=0.3
        )
        
        try:
            response = await gemini_client.generate_text(api_request)
            
            # Parse the response
            recommendations = []
            
            # Extract JSON
            json_match = re.search(r'```(?:json)?\s*(.*?)\s*```', response.text, re.DOTALL)
            if json_match:
                json_str = json_match.group(1)
            else:
                # Try to find JSON without code blocks
                json_match = re.search(r'(\[.*\])', response.text, re.DOTALL)
                if json_match:
                    json_str = json_match.group(1)
                else:
                    # Assume the entire response is JSON
                    json_str = response.text
            
            try:
                recommendations = json.loads(json_str)
            except json.JSONDecodeError:
                self._logger.error("Failed to parse AI recommendations as JSON")
                recommendations = []
            
            # Add source information
            for rec in recommendations:
                rec["source"] = "ai"
            
            return recommendations
            
        except Exception as e:
            self._logger.error(f"Error generating AI recommendations: {str(e)}")
            return []

async def analyze_project_architecture(
    project_path: Union[str, Path],
    context: Optional[Dict[str, Any]] = None
) -> Dict[str, Any]:
    """
    Analyze project architecture.
    
    Args:
        project_path: Path to the project
        context: Additional context information
        
    Returns:
        Dictionary with analysis results
    """
    analyzer = ArchitecturalAnalyzer()
    return await analyzer.analyze_architecture(project_path, context)

# Global architectural analyzer instance
architectural_analyzer = ArchitecturalAnalyzer()
</file>

<file path="angela/generation/documentation.py">
# angela/generation/documentation.py
"""
Documentation generation for Angela CLI.

This module provides capabilities for generating documentation for projects,
including READMEs, API docs, and user guides.
"""
import os
import asyncio
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple, Union
import json
import re

from angela.ai.client import gemini_client, GeminiRequest
from angela.utils.logging import get_logger
from angela.context import context_manager

logger = get_logger(__name__)

class DocumentationGenerator:
    """
    Generator for project documentation.
    """
    
    def __init__(self):
        """Initialize the documentation generator."""
        self._logger = logger
    
    async def generate_readme(
        self, 
        project_path: Union[str, Path],
        project_info: Optional[Dict[str, Any]] = None,
        context: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Generate a README file for a project.
        
        Args:
            project_path: Path to the project
            project_info: Optional project information
            context: Additional context information
            
        Returns:
            Dictionary with the generated README
        """
        self._logger.info(f"Generating README for project at {project_path}")
        
        # Analyze project if project_info not provided
        if not project_info:
            project_info = await self._analyze_project(project_path, context)
        
        # Build prompt for the AI
        prompt = self._build_readme_prompt(project_info)
        
        # Call AI service
        api_request = GeminiRequest(
            prompt=prompt,
            max_tokens=4000,
            temperature=0.3
        )
        
        self._logger.debug("Sending README generation request to AI service")
        response = await gemini_client.generate_text(api_request)
        
        # Extract README content from the response
        readme_content = self._extract_markdown_content(response.text)
        
        return {
            "content": readme_content,
            "file_name": "README.md",
            "project_path": str(project_path)
        }
    
    async def generate_api_docs(
        self, 
        project_path: Union[str, Path],
        files: Optional[List[Dict[str, Any]]] = None,
        context: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Generate API documentation for a project.
        
        Args:
            project_path: Path to the project
            files: Optional list of files to document
            context: Additional context information
            
        Returns:
            Dictionary with the generated API docs
        """
        self._logger.info(f"Generating API docs for project at {project_path}")
        
        # Convert to Path object
        project_path = Path(project_path)
        
        # Get files if not provided
        if not files:
            project_info = await self._analyze_project(project_path, context)
            files = project_info.get("files", [])
        
        # Filter for source code files
        source_files = [f for f in files if f.get("type") == "source_code"]
        
        # Determine project type
        project_type = "unknown"
        if context and "enhanced_project" in context:
            project_type = context["enhanced_project"].get("type", "unknown")
        elif any(f.get("path", "").endswith(".py") for f in source_files):
            project_type = "python"
        elif any(f.get("path", "").endswith((".js", ".jsx", ".ts", ".tsx")) for f in source_files):
            project_type = "node"
        elif any(f.get("path", "").endswith(".java") for f in source_files):
            project_type = "java"
        
        # Generate docs based on project type
        if project_type == "python":
            return await self._generate_python_api_docs(project_path, source_files)
        elif project_type == "node":
            return await self._generate_js_api_docs(project_path, source_files)
        elif project_type == "java":
            return await self._generate_java_api_docs(project_path, source_files)
        else:
            return await self._generate_generic_api_docs(project_path, source_files)
    
    async def generate_user_guide(
        self, 
        project_path: Union[str, Path],
        project_info: Optional[Dict[str, Any]] = None,
        context: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Generate a user guide for a project.
        
        Args:
            project_path: Path to the project
            project_info: Optional project information
            context: Additional context information
            
        Returns:
            Dictionary with the generated user guide
        """
        self._logger.info(f"Generating user guide for project at {project_path}")
        
        # Analyze project if project_info not provided
        if not project_info:
            project_info = await self._analyze_project(project_path, context)
        
        # Build prompt for the AI
        prompt = self._build_user_guide_prompt(project_info)
        
        # Call AI service
        api_request = GeminiRequest(
            prompt=prompt,
            max_tokens=6000,
            temperature=0.3
        )
        
        self._logger.debug("Sending user guide generation request to AI service")
        response = await gemini_client.generate_text(api_request)
        
        # Extract user guide content from the response
        guide_content = self._extract_markdown_content(response.text)
        
        return {
            "content": guide_content,
            "file_name": "USER_GUIDE.md",
            "project_path": str(project_path)
        }
    
    async def generate_contributing_guide(
        self, 
        project_path: Union[str, Path],
        project_info: Optional[Dict[str, Any]] = None,
        context: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Generate a CONTRIBUTING guide for a project.
        
        Args:
            project_path: Path to the project
            project_info: Optional project information
            context: Additional context information
            
        Returns:
            Dictionary with the generated contributing guide
        """
        self._logger.info(f"Generating contributing guide for project at {project_path}")
        
        # Analyze project if project_info not provided
        if not project_info:
            project_info = await self._analyze_project(project_path, context)
        
        # Build prompt for the AI
        prompt = self._build_contributing_prompt(project_info)
        
        # Call AI service
        api_request = GeminiRequest(
            prompt=prompt,
            max_tokens=4000,
            temperature=0.3
        )
        
        self._logger.debug("Sending contributing guide generation request to AI service")
        response = await gemini_client.generate_text(api_request)
        
        # Extract contributing guide content from the response
        guide_content = self._extract_markdown_content(response.text)
        
        return {
            "content": guide_content,
            "file_name": "CONTRIBUTING.md",
            "project_path": str(project_path)
        }
    
    async def _analyze_project(
        self, 
        project_path: Union[str, Path],
        context: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Analyze a project to gather information for documentation.
        
        Args:
            project_path: Path to the project
            context: Additional context information
            
        Returns:
            Dictionary with project information
        """
        self._logger.info(f"Analyzing project at {project_path}")
        
        # Convert to Path object
        project_path = Path(project_path)
        
        # Use context if available
        if context and "enhanced_project" in context:
            return {
                "project_type": context["enhanced_project"].get("type", "unknown"),
                "frameworks": context["enhanced_project"].get("frameworks", {}),
                "dependencies": context["enhanced_project"].get("dependencies", {}),
                "files": context.get("files", []),
                "path": str(project_path),
                "name": project_path.name
            }
        
        # Perform a simplified project analysis
        project_info = {
            "project_type": "unknown",
            "frameworks": {},
            "dependencies": {},
            "files": [],
            "path": str(project_path),
            "name": project_path.name
        }
        
        # Determine project type
        if (project_path / "requirements.txt").exists() or (project_path / "setup.py").exists() or (project_path / "pyproject.toml").exists():
            project_info["project_type"] = "python"
        elif (project_path / "package.json").exists():
            project_info["project_type"] = "node"
        elif (project_path / "pom.xml").exists() or (project_path / "build.gradle").exists():
            project_info["project_type"] = "java"
        
        # Get dependencies
        if project_info["project_type"] == "python":
            if (project_path / "requirements.txt").exists():
                try:
                    with open(project_path / "requirements.txt", 'r') as f:
                        deps = [line.strip() for line in f if line.strip() and not line.strip().startswith('#')]
                        project_info["dependencies"] = {"runtime": deps}
                except Exception as e:
                    self._logger.error(f"Error reading requirements.txt: {str(e)}")
        elif project_info["project_type"] == "node":
            if (project_path / "package.json").exists():
                try:
                    with open(project_path / "package.json", 'r') as f:
                        package_data = json.load(f)
                        project_info["dependencies"] = {
                            "runtime": list(package_data.get("dependencies", {}).keys()),
                            "development": list(package_data.get("devDependencies", {}).keys())
                        }
                        # Get project name
                        if "name" in package_data:
                            project_info["name"] = package_data["name"]
                except Exception as e:
                    self._logger.error(f"Error reading package.json: {str(e)}")
        
        # Collect file information
        for root, _, filenames in os.walk(project_path):
            for filename in filenames:
                # Skip common directories to ignore
                if any(ignored in root for ignored in [".git", "__pycache__", "node_modules", "venv", ".idea", ".vscode"]):
                    continue
                
                file_path = Path(root) / filename
                rel_path = file_path.relative_to(project_path)
                
                # Skip files over 1MB
                if file_path.stat().st_size > 1000000:
                    continue
                
                # Get basic file info
                file_info = {
                    "path": str(rel_path),
                    "full_path": str(file_path),
                    "type": None,
                    "language": None,
                    "content": None
                }
                
                # Try to determine file type and language
                try:
                    from angela.context.file_detector import detect_file_type
                    type_info = detect_file_detector.detect_file_type(file_path)
                    file_info["type"] = type_info.get("type")
                    file_info["language"] = type_info.get("language")
                    
                    # Read content for source code files and documentation files
                    if file_info["type"] in ["source_code", "document"] and file_path.stat().st_size < 100000:
                        with open(file_path, 'r', encoding='utf-8', errors='replace') as f:
                            file_info["content"] = f.read()
                except Exception as e:
                    self._logger.debug(f"Error analyzing file {file_path}: {str(e)}")
                
                project_info["files"].append(file_info)
        
        # Try to find entry points
        project_info["entry_points"] = self._find_entry_points(project_info)
        
        return project_info
    
    def _find_entry_points(self, project_info: Dict[str, Any]) -> List[str]:
        """
        Find potential entry points for the project.
        
        Args:
            project_info: Project information
            
        Returns:
            List of potential entry point files
        """
        entry_points = []
        
        if project_info["project_type"] == "python":
            # Look for Python entry points
            main_files = [f for f in project_info["files"] if f.get("path") in [
                "main.py", "__main__.py", "app.py", "server.py", "run.py"
            ]]
            
            # If no common entry point, look for files with main function
            if not main_files:
                for file_info in project_info["files"]:
                    if file_info.get("content") and "if __name__ == '__main__'" in file_info.get("content"):
                        main_files.append(file_info)
            
            entry_points.extend([f.get("path") for f in main_files])
            
        elif project_info["project_type"] == "node":
            # Look for Node.js entry points
            main_files = [f for f in project_info["files"] if f.get("path") in [
                "index.js", "server.js", "app.js", "main.js"
            ]]
            
            # Check package.json main field
            for file_info in project_info["files"]:
                if file_info.get("path") == "package.json" and file_info.get("content"):
                    try:
                        package_data = json.loads(file_info.get("content"))
                        if "main" in package_data:
                            main_file = package_data["main"]
                            # Add to entry points if not already there
                            if main_file not in [f.get("path") for f in main_files]:
                                main_files.append({"path": main_file})
                    except Exception:
                        pass
            
            entry_points.extend([f.get("path") for f in main_files])
        
        return entry_points
    
    def _build_readme_prompt(self, project_info: Dict[str, Any]) -> str:
        """
        Build a prompt for README generation.
        
        Args:
            project_info: Project information
            
        Returns:
            Prompt string for the AI service
        """
        prompt = f"""
You are an expert technical writer tasked with creating a comprehensive README.md file for a {project_info.get('project_type', 'software')} project named "{project_info.get('name', 'Project')}".

Project details:
- Type: {project_info.get('project_type', 'Unknown')}
"""
        
        # Add dependencies information
        if "dependencies" in project_info:
            prompt += "- Dependencies:\n"
            
            if "runtime" in project_info["dependencies"]:
                runtime_deps = project_info["dependencies"]["runtime"]
                if runtime_deps:
                    prompt += f"  - Runtime: {', '.join(runtime_deps[:10])}"
                    if len(runtime_deps) > 10:
                        prompt += f" and {len(runtime_deps) - 10} more"
                    prompt += "\n"
            
            if "development" in project_info["dependencies"]:
                dev_deps = project_info["dependencies"]["development"]
                if dev_deps:
                    prompt += f"  - Development: {', '.join(dev_deps[:10])}"
                    if len(dev_deps) > 10:
                        prompt += f" and {len(dev_deps) - 10} more"
                    prompt += "\n"
        
        # Add entry points information
        if "entry_points" in project_info and project_info["entry_points"]:
            prompt += f"- Entry points: {', '.join(project_info['entry_points'])}\n"
        
        # Add file structure information
        file_types = {}
        for file_info in project_info.get("files", []):
            file_type = file_info.get("type")
            if file_type:
                if file_type not in file_types:
                    file_types[file_type] = []
                file_types[file_type].append(file_info.get("path"))
        
        prompt += "- File structure summary:\n"
        for file_type, files in file_types.items():
            prompt += f"  - {file_type} files: {len(files)}\n"
        
        # Add main source files
        source_files = file_types.get("source_code", [])
        if source_files:
            prompt += "- Main source files (up to 10):\n"
            for file in source_files[:10]:
                prompt += f"  - {file}\n"
        
        prompt += """
Create a comprehensive README.md file that follows these best practices:
1. Clear project title and description
2. Installation instructions
3. Usage examples
4. Features list
5. API documentation overview (if applicable)
6. Project structure explanation
7. Contributing guidelines reference
8. License information
9. Badges for build status, version, etc. (if applicable)

The README should be well-formatted with Markdown, including:
- Proper headings (# for main title, ## for sections, etc.)
- Code blocks with appropriate syntax highlighting
- Lists (ordered and unordered)
- Links to important resources
- Tables where appropriate

Make the README user-friendly, comprehensive, and professional.
"""
        
        return prompt
    
    async def _generate_python_api_docs(
        self, 
        project_path: Path,
        source_files: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """
        Generate API documentation for a Python project.
        
        Args:
            project_path: Path to the project
            source_files: List of source files
            
        Returns:
            Dictionary with the generated API docs
        """
        self._logger.info("Generating Python API docs")
        
        # Filter for Python files
        python_files = [f for f in source_files if f.get("path", "").endswith(".py")]
        
        # Organize files by module/package
        modules = {}
        
        for file_info in python_files:
            file_path = file_info.get("path", "")
            
            # Skip __init__.py files with no content
            if file_path.endswith("__init__.py") and not file_info.get("content", "").strip():
                continue
            
            # Determine module name
            if "/" in file_path:
                # File in a package
                package_parts = file_path.split("/")
                module_name = ".".join(package_parts[:-1])
                if module_name not in modules:
                    modules[module_name] = []
                modules[module_name].append(file_info)
            else:
                # File in root
                module_name = "root"
                if module_name not in modules:
                    modules[module_name] = []
                modules[module_name].append(file_info)
        
        # Generate docs for each module
        module_docs = {}
        
        for module_name, files in modules.items():
            module_docs[module_name] = await self._generate_python_module_docs(module_name, files)
        
        # Create docs structure
        docs_structure = {
            "index.md": self._generate_python_docs_index(modules, project_path.name),
            "modules": module_docs
        }
        
        return {
            "structure": docs_structure,
            "format": "markdown",
            "project_path": str(project_path)
        }
    
    async def _generate_python_module_docs(
        self, 
        module_name: str,
        files: List[Dict[str, Any]]
    ) -> Dict[str, str]:
        """
        Generate documentation for a Python module.
        
        Args:
            module_name: Name of the module
            files: List of files in the module
            
        Returns:
            Dictionary mapping file names to documentation content
        """
        module_docs = {}
        
        for file_info in files:
            file_path = file_info.get("path", "")
            content = file_info.get("content", "")
            
            if not content:
                continue
            
            # Extract file name without extension
            file_name = os.path.basename(file_path)
            doc_name = os.path.splitext(file_name)[0]
            
            # Parse Python content
            doc_content = await self._parse_python_file(file_path, content)
            
            # Generate markdown
            markdown = f"""# {doc_name}

{doc_content.get('module_docstring', 'No description available.')}

## Classes

"""
            # Add classes
            for class_name, class_info in doc_content.get("classes", {}).items():
                markdown += f"### {class_name}\n\n{class_info.get('docstring', 'No description available.')}\n\n"
                
                # Add methods
                if class_info.get("methods"):
                    markdown += "#### Methods\n\n"
                    for method_name, method_info in class_info.get("methods", {}).items():
                        markdown += f"##### `{method_name}{method_info.get('signature', '()') }`\n\n{method_info.get('docstring', 'No description available.')}\n\n"
            
            # Add functions
            if doc_content.get("functions"):
                markdown += "## Functions\n\n"
                for func_name, func_info in doc_content.get("functions", {}).items():
                    markdown += f"### `{func_name}{func_info.get('signature', '()') }`\n\n{func_info.get('docstring', 'No description available.')}\n\n"
            
            module_docs[doc_name + ".md"] = markdown
        
        return module_docs
    
    async def _parse_python_file(self, file_path: str, content: str) -> Dict[str, Any]:
        """
        Parse Python file for documentation.
        
        Args:
            file_path: Path to the file
            content: File content
            
        Returns:
            Dictionary with parsed documentation
        """
        # Basic structure
        doc_info = {
            "module_docstring": "",
            "classes": {},
            "functions": {}
        }
        
        # Extract module docstring
        module_docstring_match = re.search(r'"""(.*?)"""', content, re.DOTALL)
        if module_docstring_match:
            doc_info["module_docstring"] = module_docstring_match.group(1).strip()
        
        # Extract classes
        class_pattern = r'class\s+(\w+)(?:\([^)]*\))?:\s*(?:"""(.*?)""")?'
        for match in re.finditer(class_pattern, content, re.DOTALL):
            class_name = match.group(1)
            class_docstring = match.group(2) if match.group(2) else ""
            
            # Get class content
            class_start = match.end()
            class_content = ""
            
            # Find the end of the class (by indentation)
            for line in content[class_start:].splitlines():
                if line.strip() and not line.startswith(" ") and not line.startswith("\t"):
                    break
                class_content += line + "\n"
            
            # Extract methods
            methods = {}
            method_pattern = r'^\s+def\s+(\w+)\s*\((self(?:,\s*[^)]*)?)\):\s*(?:"""(.*?)""")?'
            for method_match in re.finditer(method_pattern, class_content, re.MULTILINE | re.DOTALL):
                method_name = method_match.group(1)
                method_signature = method_match.group(2).strip()
                method_docstring = method_match.group(3) if method_match.group(3) else ""
                
                methods[method_name] = {
                    "signature": f"({method_signature})",
                    "docstring": method_docstring.strip()
                }
            
            doc_info["classes"][class_name] = {
                "docstring": class_docstring.strip(),
                "methods": methods
            }
        
        # Extract functions
        function_pattern = r'^def\s+(\w+)\s*\(([^)]*)\):\s*(?:"""(.*?)""")?'
        for match in re.finditer(function_pattern, content, re.MULTILINE | re.DOTALL):
            func_name = match.group(1)
            func_signature = match.group(2).strip()
            func_docstring = match.group(3) if match.group(3) else ""
            
            doc_info["functions"][func_name] = {
                "signature": f"({func_signature})",
                "docstring": func_docstring.strip()
            }
        
        return doc_info
    
    def _generate_python_docs_index(self, modules: Dict[str, List[Dict[str, Any]]], project_name: str) -> str:
        """
        Generate index page for Python API docs.
        
        Args:
            modules: Dictionary mapping module names to files
            project_name: Name of the project
            
        Returns:
            Markdown content for index page
        """
        markdown = f"""# {project_name} API Documentation

## Modules

"""
        # Add module links
        for module_name, files in modules.items():
            if module_name == "root":
                markdown += "### Root Module\n\n"
            else:
                markdown += f"### {module_name}\n\n"
            
            for file_info in files:
                file_path = file_info.get("path", "")
                file_name = os.path.basename(file_path)
                doc_name = os.path.splitext(file_name)[0]
                
                markdown += f"- [{doc_name}](modules/{doc_name}.md)\n"
            
            markdown += "\n"
        
        return markdown
    
    async def _generate_js_api_docs(
        self, 
        project_path: Path,
        source_files: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """
        Generate API documentation for a JavaScript/TypeScript project.
        
        Args:
            project_path: Path to the project
            source_files: List of source files
            
        Returns:
            Dictionary with the generated API docs
        """
        self._logger.info("Generating JavaScript/TypeScript API docs")
        
        # Filter for JS/TS files
        js_files = [f for f in source_files if f.get("path", "").endswith((".js", ".jsx", ".ts", ".tsx"))]
        
        # Organize files by directory
        directories = {}
        
        for file_info in js_files:
            file_path = file_info.get("path", "")
            
            if "/" in file_path:
                # File in a directory
                dir_parts = file_path.split("/")
                dir_name = dir_parts[0]
                if dir_name not in directories:
                    directories[dir_name] = []
                directories[dir_name].append(file_info)
            else:
                # File in root
                dir_name = "root"
                if dir_name not in directories:
                    directories[dir_name] = []
                directories[dir_name].append(file_info)
        
        # Generate docs for each directory
        dir_docs = {}
        
        for dir_name, files in directories.items():
            dir_docs[dir_name] = await self._generate_js_directory_docs(dir_name, files)
        
        # Create docs structure
        docs_structure = {
            "index.md": self._generate_js_docs_index(directories, project_path.name),
            "modules": dir_docs
        }
        
        return {
            "structure": docs_structure,
            "format": "markdown",
            "project_path": str(project_path)
        }
    
    async def _generate_js_directory_docs(
        self, 
        dir_name: str,
        files: List[Dict[str, Any]]
    ) -> Dict[str, str]:
        """
        Generate documentation for a JavaScript/TypeScript directory.
        
        Args:
            dir_name: Name of the directory
            files: List of files in the directory
            
        Returns:
            Dictionary mapping file names to documentation content
        """
        dir_docs = {}
        
        for file_info in files:
            file_path = file_info.get("path", "")
            content = file_info.get("content", "")
            
            if not content:
                continue
            
            # Extract file name without extension
            file_name = os.path.basename(file_path)
            doc_name = os.path.splitext(file_name)[0]
            
            # Parse JS/TS content
            is_typescript = file_path.endswith((".ts", ".tsx"))
            doc_content = await self._parse_js_file(file_path, content, is_typescript)
            
            # Generate markdown
            markdown = f"""# {doc_name}

{doc_content.get('file_description', 'No description available.')}

## Exports

"""
            # Add classes
            for class_name, class_info in doc_content.get("classes", {}).items():
                markdown += f"### Class: {class_name}\n\n{class_info.get('description', 'No description available.')}\n\n"
                
                # Add methods
                if class_info.get("methods"):
                    markdown += "#### Methods\n\n"
                    for method_name, method_info in class_info.get("methods", {}).items():
                        markdown += f"##### `{method_name}{method_info.get('signature', '()') }`\n\n{method_info.get('description', 'No description available.')}\n\n"
            
            # Add functions
            if doc_content.get("functions"):
                markdown += "## Functions\n\n"
                for func_name, func_info in doc_content.get("functions", {}).items():
                    markdown += f"### `{func_name}{func_info.get('signature', '()') }`\n\n{func_info.get('description', 'No description available.')}\n\n"
            
            # Add interfaces (TypeScript only)
            if is_typescript and doc_content.get("interfaces"):
                markdown += "## Interfaces\n\n"
                for interface_name, interface_info in doc_content.get("interfaces", {}).items():
                    markdown += f"### Interface: {interface_name}\n\n{interface_info.get('description', 'No description available.')}\n\n"
                    
                    if interface_info.get("properties"):
                        markdown += "#### Properties\n\n"
                        for prop_name, prop_info in interface_info.get("properties", {}).items():
                            markdown += f"- `{prop_name}: {prop_info.get('type', 'any')}` - {prop_info.get('description', 'No description available.')}\n"
                    
                    markdown += "\n"
            
            dir_docs[doc_name + ".md"] = markdown
        
        return dir_docs
    
    async def _parse_js_file(
        self, 
        file_path: str, 
        content: str, 
        is_typescript: bool = False
    ) -> Dict[str, Any]:
        """
        Parse JavaScript/TypeScript file for documentation.
        
        Args:
            file_path: Path to the file
            content: File content
            is_typescript: Whether the file is TypeScript
            
        Returns:
            Dictionary with parsed documentation
        """
        # Basic structure
        doc_info = {
            "file_description": "",
            "classes": {},
            "functions": {}
        }
        
        if is_typescript:
            doc_info["interfaces"] = {}
        
        # Extract file description from initial comment block
        file_comment_match = re.search(r'/\*\*(.*?)\*/', content, re.DOTALL)
        if file_comment_match:
            doc_info["file_description"] = self._parse_js_comment(file_comment_match.group(1))
        
        # Extract classes
        class_pattern = r'(?:export\s+)?class\s+(\w+)(?:\s+extends\s+\w+)?(?:\s+implements\s+[\w,\s]+)?\s*{([^}]*\/\*\*.*?\*\/)?'
        for match in re.finditer(class_pattern, content, re.DOTALL):
            class_name = match.group(1)
            class_content = match.group(0)
            
            # Extract class comment
            class_comment_match = re.search(r'/\*\*(.*?)\*/', class_content, re.DOTALL)
            class_description = self._parse_js_comment(class_comment_match.group(1)) if class_comment_match else ""
            
            # Extract methods
            methods = {}
            method_pattern = r'(?:public|private|protected)?\s*(\w+)\s*\(([^)]*)\)(?:\s*:\s*[\w<>[\],\s]+)?(?:\s*{)?(?:\s*/\*\*(.*?)\*\/)?'
            for method_match in re.finditer(method_pattern, class_content, re.DOTALL):
                method_name = method_match.group(1)
                method_signature = method_match.group(2).strip()
                method_comment = method_match.group(3) if method_match.group(3) else ""
                
                methods[method_name] = {
                    "signature": f"({method_signature})",
                    "description": self._parse_js_comment(method_comment)
                }
            
            doc_info["classes"][class_name] = {
                "description": class_description,
                "methods": methods
            }
        
        # Extract functions
        function_pattern = r'(?:export\s+)?(?:function|const|let|var)\s+(\w+)\s*(?:=\s*(?:function)?\s*)?(?:\([^)]*\))(?:\s*:\s*[\w<>[\],\s]+)?(?:\s*=>)?(?:\s*{)?(?:\s*/\*\*(.*?)\*\/)?'
        for match in re.finditer(function_pattern, content, re.DOTALL):
            func_name = match.group(1)
            func_comment = match.group(2) if match.group(2) else ""
            
            # Determine signature (simplified)
            func_signature_match = re.search(r'\(([^)]*)\)', match.group(0))
            func_signature = func_signature_match.group(1) if func_signature_match else ""
            
            doc_info["functions"][func_name] = {
                "signature": f"({func_signature})",
                "description": self._parse_js_comment(func_comment)
            }
        
        # Extract TypeScript interfaces
        if is_typescript:
            interface_pattern = r'(?:export\s+)?interface\s+(\w+)(?:\s+extends\s+[\w,\s]+)?\s*{([^}]*)}'
            for match in re.finditer(interface_pattern, content, re.DOTALL):
                interface_name = match.group(1)
                interface_content = match.group(2)
                
                # Extract interface comment
                interface_comment_match = re.search(r'/\*\*(.*?)\*/', match.group(0), re.DOTALL)
                interface_description = self._parse_js_comment(interface_comment_match.group(1)) if interface_comment_match else ""
                
                # Extract properties
                properties = {}
                property_pattern = r'(\w+)(?:\?)?:\s*([\w<>[\],\s|]+)(?:;)?\s*(?://\s*(.*))?'
                for prop_match in re.finditer(property_pattern, interface_content):
                    prop_name = prop_match.group(1)
                    prop_type = prop_match.group(2).strip()
                    prop_comment = prop_match.group(3) if prop_match.group(3) else ""
                    
                    properties[prop_name] = {
                        "type": prop_type,
                        "description": prop_comment.strip()
                    }
                
                doc_info["interfaces"][interface_name] = {
                    "description": interface_description,
                    "properties": properties
                }
        
        return doc_info
    
    def _parse_js_comment(self, comment: str) -> str:
        """
        Parse JSDoc comment.
        
        Args:
            comment: JSDoc comment
            
        Returns:
            Parsed description
        """
        # Remove * at the beginning of lines
        lines = [line.strip().lstrip('*') for line in comment.splitlines()]
        
        # Join and clean up
        description = " ".join(line for line in lines if line and not line.startswith('@'))
        
        return description.strip()
    
    def _generate_js_docs_index(self, directories: Dict[str, List[Dict[str, Any]]], project_name: str) -> str:
        """
        Generate index page for JavaScript/TypeScript API docs.
        
        Args:
            directories: Dictionary mapping directory names to files
            project_name: Name of the project
            
        Returns:
            Markdown content for index page
        """
        markdown = f"""# {project_name} API Documentation

## Modules

"""
        # Add directory links
        for dir_name, files in directories.items():
            if dir_name == "root":
                markdown += "### Root Module\n\n"
            else:
                markdown += f"### {dir_name}\n\n"
            
            for file_info in files:
                file_path = file_info.get("path", "")
                file_name = os.path.basename(file_path)
                doc_name = os.path.splitext(file_name)[0]
                
                markdown += f"- [{doc_name}](modules/{doc_name}.md)\n"
            
            markdown += "\n"
        
        return markdown
    
    async def _generate_java_api_docs(
        self, 
        project_path: Path,
        source_files: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """
        Generate API documentation for a Java project.
        
        Args:
            project_path: Path to the project
            source_files: List of source files
            
        Returns:
            Dictionary with the generated API docs
        """
        self._logger.info("Generating Java API docs")
        
        # Filter for Java files
        java_files = [f for f in source_files if f.get("path", "").endswith(".java")]
        
        # Organize files by package
        packages = {}
        
        for file_info in java_files:
            file_path = file_info.get("path", "")
            content = file_info.get("content", "")
            
            if not content:
                continue
            
            # Extract package name
            package_match = re.search(r'package\s+([\w.]+);', content)
            if package_match:
                package_name = package_match.group(1)
            else:
                package_name = "default"
            
            if package_name not in packages:
                packages[package_name] = []
            packages[package_name].append(file_info)
        
        # Generate docs for each package
        package_docs = {}
        
        for package_name, files in packages.items():
            package_docs[package_name] = await self._generate_java_package_docs(package_name, files)
        
        # Create docs structure
        docs_structure = {
            "index.md": self._generate_java_docs_index(packages, project_path.name),
            "packages": package_docs
        }
        
        return {
            "structure": docs_structure,
            "format": "markdown",
            "project_path": str(project_path)
        }
    
    async def _generate_java_package_docs(
        self, 
        package_name: str,
        files: List[Dict[str, Any]]
    ) -> Dict[str, str]:
        """
        Generate documentation for a Java package.
        
        Args:
            package_name: Name of the package
            files: List of files in the package
            
        Returns:
            Dictionary mapping file names to documentation content
        """
        package_docs = {}
        
        for file_info in files:
            file_path = file_info.get("path", "")
            content = file_info.get("content", "")
            
            if not content:
                continue
            
            # Extract file name without extension
            file_name = os.path.basename(file_path)
            class_name = os.path.splitext(file_name)[0]
            
            # Parse Java content
            doc_content = await self._parse_java_file(file_path, content)
            
            # Generate markdown
            markdown = f"""# {class_name}

Package: `{package_name}`

{doc_content.get('class_javadoc', 'No description available.')}

"""
            # Add class info
            if "is_interface" in doc_content and doc_content["is_interface"]:
                markdown += "## Interface Methods\n\n"
            else:
                markdown += "## Methods\n\n"
            
            for method_name, method_info in doc_content.get("methods", {}).items():
                markdown += f"### `{method_name}{method_info.get('signature', '()') }`\n\n{method_info.get('javadoc', 'No description available.')}\n\n"
                
                # Add parameters documentation
                if method_info.get("params"):
                    markdown += "#### Parameters\n\n"
                    for param_name, param_desc in method_info.get("params", {}).items():
                        markdown += f"- `{param_name}` - {param_desc}\n"
                    markdown += "\n"
                
                # Add return documentation
                if method_info.get("returns"):
                    markdown += f"#### Returns\n\n{method_info.get('returns')}\n\n"
                
                # Add throws documentation
                if method_info.get("throws"):
                    markdown += "#### Throws\n\n"
                    for exception, desc in method_info.get("throws", {}).items():
                        markdown += f"- `{exception}` - {desc}\n"
                    markdown += "\n"
            
            # Add fields
            if doc_content.get("fields"):
                markdown += "## Fields\n\n"
                for field_name, field_info in doc_content.get("fields", {}).items():
                    markdown += f"### `{field_info.get('type', 'Object')} {field_name}`\n\n{field_info.get('javadoc', 'No description available.')}\n\n"
            
            package_docs[class_name + ".md"] = markdown
        
        return package_docs
    
    async def _parse_java_file(self, file_path: str, content: str) -> Dict[str, Any]:
        """
        Parse Java file for documentation.
        
        Args:
            file_path: Path to the file
            content: File content
            
        Returns:
            Dictionary with parsed documentation
        """
        # Basic structure
        doc_info = {
            "class_javadoc": "",
            "methods": {},
            "fields": {}
        }
        
        # Check if it's an interface
        if re.search(r'(?:public\s+)?interface\s+\w+', content):
            doc_info["is_interface"] = True
        
        # Extract class javadoc
        class_javadoc_match = re.search(r'/\*\*(.*?)\*/', content, re.DOTALL)
        if class_javadoc_match:
            doc_info["class_javadoc"] = self._parse_javadoc(class_javadoc_match.group(1))
        
        # Extract methods
        method_pattern = r'(?:/\*\*(.*?)\*/\s*)?(?:public|private|protected)?\s+(?:static\s+)?(?:final\s+)?(?:[\w<>[\],\s]+)\s+(\w+)\s*\(([^)]*)\)(?:\s+throws\s+[\w,\s]+)?(?:\s*{)?'
        for match in re.finditer(method_pattern, content, re.DOTALL):
            javadoc = match.group(1)
            method_name = match.group(2)
            params_str = match.group(3)
            
            # Skip constructor if it has the same name as the class
            class_name = os.path.splitext(os.path.basename(file_path))[0]
            if method_name == class_name:
                continue
            
            # Parse javadoc
            javadoc_info = self._parse_javadoc_with_tags(javadoc) if javadoc else {}
            
            # Format parameters
            formatted_params = []
            for param in params_str.split(","):
                param = param.strip()
                if param:
                    parts = param.split()
                    if len(parts) >= 2:
                        param_type = " ".join(parts[:-1])
                        param_name = parts[-1]
                        formatted_params.append(f"{param_type} {param_name}")
            
            # Extract throws info
            throws_match = re.search(r'throws\s+([\w,\s]+)', match.group(0))
            throws = throws_match.group(1).split(",") if throws_match else []
            
            method_info = {
                "signature": f"({', '.join(formatted_params)})",
                "javadoc": javadoc_info.get("description", ""),
                "params": javadoc_info.get("params", {}),
                "returns": javadoc_info.get("returns", ""),
                "throws": javadoc_info.get("throws", {})
            }
            
            doc_info["methods"][method_name] = method_info
        
        # Extract fields
        field_pattern = r'(?:/\*\*(.*?)\*/\s*)?(?:public|private|protected)?\s+(?:static\s+)?(?:final\s+)?(?:[\w<>[\],\s]+)\s+(\w+)\s*(?:=\s*[^;]+)?;'
        for match in re.finditer(field_pattern, content, re.DOTALL):
            javadoc = match.group(1)
            field_declaration = match.group(0)
            
            # Extract field name and type
            field_name = match.group(2)
            
            # Extract field type
            type_match = re.search(r'(?:public|private|protected)?\s+(?:static\s+)?(?:final\s+)?([\w<>[\],\s]+)\s+\w+\s*(?:=|;)', field_declaration)
            field_type = type_match.group(1).strip() if type_match else "Object"
            
            # Parse javadoc
            field_javadoc = self._parse_javadoc(javadoc) if javadoc else ""
            
            doc_info["fields"][field_name] = {
                "type": field_type,
                "javadoc": field_javadoc
            }
        
        return doc_info
    
    def _parse_javadoc(self, javadoc: str) -> str:
        """
        Parse basic Javadoc comment.
        
        Args:
            javadoc: Javadoc comment
            
        Returns:
            Parsed description
        """
        if not javadoc:
            return ""
        
        # Remove * at the beginning of lines
        lines = [line.strip().lstrip('*') for line in javadoc.splitlines()]
        
        # Join and clean up
        description = " ".join(line for line in lines if line and not line.startswith('@'))
        
        return description.strip()
    
    def _parse_javadoc_with_tags(self, javadoc: str) -> Dict[str, Any]:
        """
        Parse Javadoc comment with tags.
        
        Args:
            javadoc: Javadoc comment
            
        Returns:
            Dictionary with parsed javadoc
        """
        if not javadoc:
            return {"description": ""}
        
        result = {
            "description": "",
            "params": {},
            "returns": "",
            "throws": {}
        }
        
        # Remove * at the beginning of lines
        lines = [line.strip().lstrip('*') for line in javadoc.splitlines()]
        
        # Extract description (text before tags)
        description_lines = []
        tag_lines = []
        in_description = True
        
        for line in lines:
            if line.startswith('@'):
                in_description = False
                tag_lines.append(line)
            elif in_description:
                description_lines.append(line)
            else:
                tag_lines.append(line)
        
        result["description"] = " ".join(line for line in description_lines if line).strip()
        
        # Process tags
        current_tag = None
        current_text = []
        
        for line in tag_lines:
            if line.startswith('@'):
                # Save previous tag
                if current_tag and current_text:
                    self._add_tag_to_result(result, current_tag, " ".join(current_text).strip())
                
                # Start new tag
                parts = line.split(' ', 1)
                current_tag = parts[0][1:]  # Remove @ and get tag name
                current_text = [parts[1].strip()] if len(parts) > 1 else []
            elif current_tag:
                current_text.append(line)
        
        # Save last tag
        if current_tag and current_text:
            self._add_tag_to_result(result, current_tag, " ".join(current_text).strip())
        
        return result
    
    def _add_tag_to_result(self, result: Dict[str, Any], tag: str, text: str) -> None:
        """
        Add a parsed javadoc tag to the result.
        
        Args:
            result: Result dictionary
            tag: Tag name
            text: Tag text
        """
        if tag == "param":
            # Extract parameter name
            parts = text.split(' ', 1)
            if len(parts) > 1:
                param_name = parts[0]
                param_description = parts[1]
                result["params"][param_name] = param_description
        elif tag == "return":
            result["returns"] = text
        elif tag in ["throws", "exception"]:
            # Extract exception class
            parts = text.split(' ', 1)
            if len(parts) > 1:
                exception_class = parts[0]
                exception_description = parts[1]
                result["throws"][exception_class] = exception_description
    
    def _generate_java_docs_index(self, packages: Dict[str, List[Dict[str, Any]]], project_name: str) -> str:
        """
        Generate index page for Java API docs.
        
        Args:
            packages: Dictionary mapping package names to files
            project_name: Name of the project
            
        Returns:
            Markdown content for index page
        """
        markdown = f"""# {project_name} API Documentation

## Packages

"""
        # Add package links
        for package_name, files in packages.items():
            markdown += f"### {package_name}\n\n"
            
            for file_info in files:
                file_path = file_info.get("path", "")
                file_name = os.path.basename(file_path)
                class_name = os.path.splitext(file_name)[0]
                
                markdown += f"- [{class_name}](packages/{class_name}.md)\n"
            
            markdown += "\n"
        
        return markdown
    
    async def _generate_generic_api_docs(
        self, 
        project_path: Path,
        source_files: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """
        Generate generic API documentation for an unknown project type.
        
        Args:
            project_path: Path to the project
            source_files: List of source files
            
        Returns:
            Dictionary with the generated API docs
        """
        self._logger.info("Generating generic API docs")
        
        # Organize files by directory
        directories = {}
        
        for file_info in source_files:
            file_path = file_info.get("path", "")
            
            if "/" in file_path:
                # File in a directory
                dir_parts = file_path.split("/")
                dir_name = dir_parts[0]
                if dir_name not in directories:
                    directories[dir_name] = []
                directories[dir_name].append(file_info)
            else:
                # File in root
                dir_name = "root"
                if dir_name not in directories:
                    directories[dir_name] = []
                directories[dir_name].append(file_info)
        
        # Generate docs for each directory
        dir_docs = {}
        
        for dir_name, files in directories.items():
            dir_docs[dir_name] = await self._generate_generic_directory_docs(dir_name, files)
        
        # Create docs structure
        docs_structure = {
            "index.md": self._generate_generic_docs_index(directories, project_path.name),
            "modules": dir_docs
        }
        
        return {
            "structure": docs_structure,
            "format": "markdown",
            "project_path": str(project_path)
        }
    
    async def _generate_generic_directory_docs(
        self, 
        dir_name: str,
        files: List[Dict[str, Any]]
    ) -> Dict[str, str]:
        """
        Generate documentation for a generic directory.
        
        Args:
            dir_name: Name of the directory
            files: List of files in the directory
            
        Returns:
            Dictionary mapping file names to documentation content
        """
        dir_docs = {}
        
        for file_info in files:
            file_path = file_info.get("path", "")
            content = file_info.get("content", "")
            
            if not content:
                continue
            
            # Extract file name without extension
            file_name = os.path.basename(file_path)
            doc_name = os.path.splitext(file_name)[0]
            
            # Generate markdown using AI
            markdown = await self._generate_file_docs_with_ai(file_info)
            
            dir_docs[doc_name + ".md"] = markdown
        
        return dir_docs
    
    def _generate_generic_docs_index(self, directories: Dict[str, List[Dict[str, Any]]], project_name: str) -> str:
        """
        Generate index page for generic API docs.
        
        Args:
            directories: Dictionary mapping directory names to files
            project_name: Name of the project
            
        Returns:
            Markdown content for index page
        """
        markdown = f"""# {project_name} API Documentation

## Modules

"""
        # Add directory links
        for dir_name, files in directories.items():
            if dir_name == "root":
                markdown += "### Root Module\n\n"
            else:
                markdown += f"### {dir_name}\n\n"
            
            for file_info in files:
                file_path = file_info.get("path", "")
                file_name = os.path.basename(file_path)
                doc_name = os.path.splitext(file_name)[0]
                
                markdown += f"- [{doc_name}](modules/{doc_name}.md)\n"
            
            markdown += "\n"
        
        return markdown
    
    async def _generate_file_docs_with_ai(self, file_info: Dict[str, Any]) -> str:
        """
        Generate documentation for a file using AI.
        
        Args:
            file_info: File information
            
        Returns:
            Markdown documentation
        """
        file_path = file_info.get("path", "")
        content = file_info.get("content", "")
        language = file_info.get("language", "Unknown")
        
        # Build prompt for AI
        prompt = f"""
You are an expert technical writer tasked with documenting a {language} file.

File path: {file_path}

File content:
```
{content[:5000] if len(content) > 5000 else content}
```
{f"..." if len(content) > 5000 else ""}

Create comprehensive documentation in Markdown format for this file, including:
1. File overview/purpose
2. Main functions/classes/components
3. Usage examples (if applicable)
4. Any dependencies or relationships with other files (if detectable)

Follow these formatting guidelines:
- Use Markdown headings appropriately (# for title, ## for sections, etc.)
- Use code blocks with appropriate syntax highlighting
- Document parameters, return values, and exceptions where applicable
- Be concise but thorough

DO NOT reproduce the entire file content - focus on documenting functionality and usage.
"""
        
        # Call AI service
        api_request = GeminiRequest(
            prompt=prompt,
            max_tokens=3000,
            temperature=0.2
        )
        
        self._logger.debug(f"Sending file documentation request to AI for {file_path}")
        response = await gemini_client.generate_text(api_request)
        
        # Extract documentation
        return response.text
    
    def _build_user_guide_prompt(self, project_info: Dict[str, Any]) -> str:
        """
        Build a prompt for user guide generation.
        
        Args:
            project_info: Project information
            
        Returns:
            Prompt string for the AI service
        """
        prompt = f"""
You are an expert technical writer tasked with creating a comprehensive user guide for a {project_info.get('project_type', 'software')} project named "{project_info.get('name', 'Project')}".

Project details:
- Type: {project_info.get('project_type', 'Unknown')}
"""
        
        # Add dependencies information
        if "dependencies" in project_info:
            prompt += "- Dependencies:\n"
            
            if "runtime" in project_info["dependencies"]:
                runtime_deps = project_info["dependencies"]["runtime"]
                if runtime_deps:
                    prompt += f"  - Runtime: {', '.join(runtime_deps[:10])}"
                    if len(runtime_deps) > 10:
                        prompt += f" and {len(runtime_deps) - 10} more"
                    prompt += "\n"
        
        # Add entry points information
        if "entry_points" in project_info and project_info["entry_points"]:
            prompt += f"- Entry points: {', '.join(project_info['entry_points'])}\n"
        
        # Add important source files
        source_files = [f for f in project_info.get("files", []) if f.get("type") == "source_code"]
        if source_files:
            prompt += "- Important source files:\n"
            for file in source_files[:5]:  # Limit to 5 files
                prompt += f"  - {file.get('path')}\n"
        
        prompt += """
Create a comprehensive user guide that follows these best practices:
1. Introduction and overview
2. Getting started (installation, setup)
3. Basic usage
4. Advanced features
5. Troubleshooting
6. API/command reference
7. Examples and use cases

The user guide should be well-formatted with Markdown, including:
- Proper headings (# for main title, ## for sections, etc.)
- Code blocks with appropriate syntax highlighting
- Lists (ordered and unordered)
- Tables where appropriate
- Screenshots (described with placeholders)

Make the user guide user-friendly, comprehensive, and suitable for users with varying levels of technical expertise.
"""
        
        return prompt
    
    def _build_contributing_prompt(self, project_info: Dict[str, Any]) -> str:
        """
        Build a prompt for contributing guide generation.
        
        Args:
            project_info: Project information
            
        Returns:
            Prompt string for the AI service
        """
        prompt = f"""
You are an expert technical writer tasked with creating a comprehensive CONTRIBUTING.md file for a {project_info.get('project_type', 'software')} project named "{project_info.get('name', 'Project')}".

Project details:
- Type: {project_info.get('project_type', 'Unknown')}
"""
        
        # Add file structure information
        file_types = {}
        for file_info in project_info.get("files", []):
            file_type = file_info.get("type")
            if file_type:
                if file_type not in file_types:
                    file_types[file_type] = []
                file_types[file_type].append(file_info.get("path"))
        
        prompt += "- File structure summary:\n"
        for file_type, files in file_types.items():
            prompt += f"  - {file_type} files: {len(files)}\n"
        
        prompt += """
Create a comprehensive CONTRIBUTING.md file that follows these best practices:
1. Introduction and welcome message
2. Code of conduct reference
3. Getting started for contributors
4. Development environment setup
5. Coding standards and conventions
6. Pull request process
7. Issue reporting guidelines
8. Testing instructions
9. Documentation guidelines

The contributing guide should be well-formatted with Markdown, including:
- Proper headings (# for main title, ## for sections, etc.)
- Code blocks with appropriate syntax highlighting
- Lists (ordered and unordered)
- Links to important resources

Make the contributing guide friendly, comprehensive, and helpful for new contributors.
"""
        
        return prompt
    
    def _extract_markdown_content(self, content: str) -> str:
        """
        Extract markdown content from AI response.
        
        Args:
            content: AI response text
            
        Returns:
            Markdown content
        """
        # Check if content is already markdown
        if content.startswith('#') or content.startswith('# '):
            return content
        
        # Try to extract markdown from code blocks
        markdown_match = re.search(r'```(?:markdown)?\s*(.*?)\s*```', content, re.DOTALL)
        if markdown_match:
            return markdown_match.group(1)
        
        # Otherwise, just return the response
        return content

# Global documentation generator instance
documentation_generator = DocumentationGenerator()
</file>

<file path="angela/generation/frameworks.py">
# angela/generation/frameworks.py
"""
Specialized framework generators for Angela CLI.

This module provides framework-specific code generation capabilities
for popular frameworks like React, Django, Spring, etc.
"""
import os
import asyncio
from pathlib import Path
from typing import Dict, Any, List, Optional, Union, Tuple
import json
import re
import logging

from angela.ai.client import gemini_client, GeminiRequest
from angela.utils.logging import get_logger
from angela.generation.engine import CodeFile

logger = get_logger(__name__)

class FrameworkGenerator:
    """
    Generator for framework-specific code structures.
    
    This class provides specialized generation capabilities for various
    web and application frameworks, creating standardized project structures
    with appropriate files, configurations, and boilerplate code.
    """
    
    def __init__(self):
        """Initialize the framework generator with registered framework handlers."""
        self._logger = logger
        self._logger.info("Initializing FrameworkGenerator")
        
        # Register specialized framework generators
        self._framework_generators = {
            "react": self._generate_react,
            "django": self._generate_django,
            "flask": self._generate_flask,
            "spring": self._generate_spring,
            "express": self._generate_express,
            "fastapi": self._generate_fastapi,
            "vue": self._generate_vue,
            "angular": self._generate_angular
        }
        
        # Framework to project type mapping for better type inference
        self._framework_project_types = {
            "react": "node",
            "vue": "node",
            "angular": "node",
            "express": "node",
            "django": "python",
            "flask": "python",
            "fastapi": "python",
            "spring": "java",
            "rails": "ruby",
            "laravel": "php"
        }
        
        self._logger.debug(f"Registered frameworks: {', '.join(self._framework_generators.keys())}")
    
    async def generate_framework_structure(
        self, 
        framework: str,
        description: str,
        output_dir: Union[str, Path],
        options: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Generate a framework-specific project structure.
        
        Args:
            framework: Framework to generate for (e.g., "react", "django")
            description: Description of the project
            output_dir: Directory where the project should be generated
            options: Additional options for the framework (e.g., typescript, variant)
            
        Returns:
            Dictionary with generation results containing:
            - success: Whether generation was successful
            - framework: The framework that was generated
            - files: List of CodeFile objects
            - project_type: Type of project (node, python, etc.)
            - Additional framework-specific information
            
        Raises:
            ValueError: If the framework is not recognized and generic generation fails
        """
        options = options or {}
        framework = framework.lower()
        self._logger.info(f"Generating {framework} structure for: {description}")
        
        try:
            # Get the specialized generator function if available
            generator_func = self._framework_generators.get(framework)
            
            if generator_func:
                # Use specialized generator
                self._logger.debug(f"Using specialized generator for {framework}")
                return await generator_func(description, output_dir, options)
            else:
                # Fallback to generic generator
                self._logger.debug(f"No specialized generator for {framework}, using generic generator")
                return await self._generate_generic(framework, description, output_dir, options)
        except Exception as e:
            self._logger.error(f"Error generating {framework} project: {str(e)}", exc_info=True)
            return {
                "success": False,
                "framework": framework,
                "error": f"Failed to generate {framework} project: {str(e)}",
                "files": []
            }
    
    async def list_supported_frameworks(self) -> List[Dict[str, Any]]:
        """
        Get a list of supported frameworks with details.
        
        Returns:
            List of framework information dictionaries
        """
        frameworks = []
        
        # Add specialized frameworks
        for framework in self._framework_generators.keys():
            frameworks.append({
                "name": framework,
                "type": "specialized",
                "project_type": self._framework_project_types.get(framework, "unknown")
            })
        
        # We could add more supported frameworks here that would use the generic generator
        additional_frameworks = [
            {"name": "svelte", "project_type": "node"},
            {"name": "rails", "project_type": "ruby"},
            {"name": "laravel", "project_type": "php"},
            {"name": "dotnet", "project_type": "csharp"}
        ]
        
        for framework in additional_frameworks:
            if framework["name"] not in self._framework_generators:
                framework["type"] = "generic"
                frameworks.append(framework)
        
        return frameworks
    
    async def _generate_react(
        self,
        description: str,
        output_dir: Union[str, Path],
        options: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Generate a React project structure.
        
        Args:
            description: Description of the project
            output_dir: Output directory
            options: Additional options
            
        Returns:
            Dictionary with generation results
        """
        self._logger.info(f"Generating React project: {description}")
        
        # Determine React variant (Next.js, Create React App, etc.)
        variant = options.get("variant", "cra").lower()
        
        if variant == "nextjs":
            # Call Next.js generator
            return await self._generate_nextjs(description, output_dir, options)
        
        # Default: Create React App structure
        files = []
        
        # Define project structure
        structure = [
            {
                "path": "public/index.html",
                "content": await self._generate_content("react/index.html", description, options),
                "purpose": "Main HTML file",
                "language": "html"
            },
            {
                "path": "src/index.js",
                "content": await self._generate_content("react/index.js", description, options),
                "purpose": "Application entry point",
                "language": "javascript"
            },
            {
                "path": "src/App.js",
                "content": await self._generate_content("react/App.js", description, options),
                "purpose": "Main application component",
                "language": "javascript"
            },
            {
                "path": "src/App.css",
                "content": await self._generate_content("react/App.css", description, options),
                "purpose": "Application styles",
                "language": "css"
            },
            {
                "path": "package.json",
                "content": await self._generate_content("react/package.json", description, options),
                "purpose": "NPM package configuration",
                "language": "json"
            },
            {
                "path": "README.md",
                "content": await self._generate_content("react/README.md", description, options),
                "purpose": "Project documentation",
                "language": "markdown"
            }
        ]
        
        # TypeScript support
        if options.get("typescript", False):
            # Replace .js files with .tsx
            structure = [
                {
                    "path": f.get("path").replace(".js", ".tsx") if f.get("path").endswith(".js") else f.get("path"),
                    "content": await self._generate_content(f.get("path").replace(".js", ".tsx") if f.get("path").endswith(".js") else f.get("path"), description, options),
                    "purpose": f.get("purpose"),
                    "language": "typescript" if f.get("language") == "javascript" else f.get("language")
                }
                for f in structure
            ]
            
            # Add TypeScript config
            structure.append({
                "path": "tsconfig.json",
                "content": await self._generate_content("react/tsconfig.json", description, options),
                "purpose": "TypeScript configuration",
                "language": "json"
            })
        
        # Add testing setup if requested
        if options.get("testing", False):
            test_files = await self._generate_react_testing_files(description, options)
            structure.extend(test_files)
        
        # Generate files
        for file_info in structure:
            files.append(CodeFile(
                path=file_info["path"],
                content=file_info["content"],
                purpose=file_info["purpose"],
                dependencies=[],
                language=file_info["language"]
            ))
        
        return {
            "success": True,
            "framework": "react",
            "variant": variant,
            "files": files,
            "project_type": "node"
        }
    
    async def _generate_nextjs(
        self,
        description: str,
        output_dir: Union[str, Path],
        options: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Generate a Next.js project structure.
        
        Args:
            description: Description of the project
            output_dir: Output directory
            options: Additional options
            
        Returns:
            Dictionary with generation results
        """
        self._logger.info(f"Generating Next.js project: {description}")
        
        files = []
        
        # Define project structure
        structure = [
            {
                "path": "pages/index.js",
                "content": await self._generate_content("nextjs/pages/index.js", description, options),
                "purpose": "Home page component",
                "language": "javascript"
            },
            {
                "path": "pages/_app.js",
                "content": await self._generate_content("nextjs/pages/_app.js", description, options),
                "purpose": "Application wrapper component",
                "language": "javascript"
            },
            {
                "path": "styles/globals.css",
                "content": await self._generate_content("nextjs/styles/globals.css", description, options),
                "purpose": "Global styles",
                "language": "css"
            },
            {
                "path": "package.json",
                "content": await self._generate_content("nextjs/package.json", description, options),
                "purpose": "NPM package configuration",
                "language": "json"
            },
            {
                "path": "next.config.js",
                "content": await self._generate_content("nextjs/next.config.js", description, options),
                "purpose": "Next.js configuration",
                "language": "javascript"
            },
            {
                "path": "README.md",
                "content": await self._generate_content("nextjs/README.md", description, options),
                "purpose": "Project documentation",
                "language": "markdown"
            }
        ]
        
        # Add app directory structure if using the new App Router pattern
        if options.get("app_router", False):
            structure.extend([
                {
                    "path": "app/page.js",
                    "content": await self._generate_content("nextjs/app/page.js", description, options),
                    "purpose": "Home page using App Router",
                    "language": "javascript"
                },
                {
                    "path": "app/layout.js",
                    "content": await self._generate_content("nextjs/app/layout.js", description, options),
                    "purpose": "Root layout using App Router",
                    "language": "javascript"
                }
            ])
        
        # TypeScript support
        if options.get("typescript", False):
            # Replace .js files with .tsx
            structure = [
                {
                    "path": f.get("path").replace(".js", ".tsx") if f.get("path").endswith(".js") else f.get("path"),
                    "content": await self._generate_content(f.get("path").replace(".js", ".tsx") if f.get("path").endswith(".js") else f.get("path"), description, options),
                    "purpose": f.get("purpose"),
                    "language": "typescript" if f.get("language") == "javascript" else f.get("language")
                }
                for f in structure
            ]
            
            # Add TypeScript config
            structure.append({
                "path": "tsconfig.json",
                "content": await self._generate_content("nextjs/tsconfig.json", description, options),
                "purpose": "TypeScript configuration",
                "language": "json"
            })
        
        # Generate files
        for file_info in structure:
            files.append(CodeFile(
                path=file_info["path"],
                content=file_info["content"],
                purpose=file_info["purpose"],
                dependencies=[],
                language=file_info["language"]
            ))
        
        return {
            "success": True,
            "framework": "react",
            "variant": "nextjs",
            "files": files,
            "project_type": "node"
        }
    
    async def _generate_react_testing_files(
        self,
        description: str,
        options: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """
        Generate React testing files.
        
        Args:
            description: Project description
            options: Additional options
            
        Returns:
            List of file information dictionaries
        """
        # Determine testing framework
        testing_framework = options.get("testing_framework", "jest").lower()
        
        files = []
        file_extension = ".tsx" if options.get("typescript", False) else ".js"
        
        if testing_framework == "jest":
            files = [
                {
                    "path": f"src/App.test{file_extension}",
                    "content": await self._generate_content(f"react/App.test{file_extension}", description, options),
                    "purpose": "App component tests",
                    "language": "typescript" if options.get("typescript", False) else "javascript"
                },
                {
                    "path": "jest.config.js",
                    "content": await self._generate_content("react/jest.config.js", description, options),
                    "purpose": "Jest configuration",
                    "language": "javascript"
                }
            ]
        elif testing_framework == "cypress":
            files = [
                {
                    "path": "cypress/e2e/home.cy.js",
                    "content": await self._generate_content("react/cypress/e2e/home.cy.js", description, options),
                    "purpose": "Home page E2E tests",
                    "language": "javascript"
                },
                {
                    "path": "cypress.config.js",
                    "content": await self._generate_content("react/cypress.config.js", description, options),
                    "purpose": "Cypress configuration",
                    "language": "javascript"
                }
            ]
        elif testing_framework == "testing-library":
            files = [
                {
                    "path": f"src/App.test{file_extension}",
                    "content": await self._generate_content(f"react/App.test-rtl{file_extension}", description, options),
                    "purpose": "App component tests with React Testing Library",
                    "language": "typescript" if options.get("typescript", False) else "javascript"
                }
            ]
        
        return files
    
    async def _generate_django(
        self,
        description: str,
        output_dir: Union[str, Path],
        options: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Generate a Django project structure.
        
        Args:
            description: Description of the project
            output_dir: Output directory
            options: Additional options
            
        Returns:
            Dictionary with generation results
        """
        self._logger.info(f"Generating Django project: {description}")
        
        # Get project name
        project_name = options.get("project_name", "django_project")
        project_name = re.sub(r'[^a-zA-Z0-9_]', '_', project_name)
        
        # Get app name
        app_name = options.get("app_name", "main")
        app_name = re.sub(r'[^a-zA-Z0-9_]', '_', app_name)
        
        files = []
        
        # Define project structure
        structure = [
            {
                "path": f"{project_name}/settings.py",
                "content": await self._generate_content("django/settings.py", description, {"project_name": project_name, "app_name": app_name, **options}),
                "purpose": "Django settings",
                "language": "python"
            },
            {
                "path": f"{project_name}/urls.py",
                "content": await self._generate_content("django/urls.py", description, {"project_name": project_name, "app_name": app_name, **options}),
                "purpose": "URL configuration",
                "language": "python"
            },
            {
                "path": f"{project_name}/wsgi.py",
                "content": await self._generate_content("django/wsgi.py", description, {"project_name": project_name, **options}),
                "purpose": "WSGI configuration",
                "language": "python"
            },
            {
                "path": f"{project_name}/asgi.py",
                "content": await self._generate_content("django/asgi.py", description, {"project_name": project_name, **options}),
                "purpose": "ASGI configuration",
                "language": "python"
            },
            {
                "path": f"{project_name}/__init__.py",
                "content": "",
                "purpose": "Package initialization",
                "language": "python"
            },
            {
                "path": f"{app_name}/models.py",
                "content": await self._generate_content("django/models.py", description, {"app_name": app_name, **options}),
                "purpose": "Data models",
                "language": "python"
            },
            {
                "path": f"{app_name}/views.py",
                "content": await self._generate_content("django/views.py", description, {"app_name": app_name, **options}),
                "purpose": "View functions",
                "language": "python"
            },
            {
                "path": f"{app_name}/urls.py",
                "content": await self._generate_content("django/app_urls.py", description, {"app_name": app_name, **options}),
                "purpose": "App URL configuration",
                "language": "python"
            },
            {
                "path": f"{app_name}/__init__.py",
                "content": "",
                "purpose": "App package initialization",
                "language": "python"
            },
            {
                "path": f"{app_name}/templates/{app_name}/index.html",
                "content": await self._generate_content("django/index.html", description, {"app_name": app_name, **options}),
                "purpose": "Main template",
                "language": "html"
            },
            {
                "path": "manage.py",
                "content": await self._generate_content("django/manage.py", description, {"project_name": project_name, **options}),
                "purpose": "Django management script",
                "language": "python"
            },
            {
                "path": "requirements.txt",
                "content": await self._generate_content("django/requirements.txt", description, options),
                "purpose": "Python dependencies",
                "language": "text"
            },
            {
                "path": "README.md",
                "content": await self._generate_content("django/README.md", description, {"project_name": project_name, "app_name": app_name, **options}),
                "purpose": "Project documentation",
                "language": "markdown"
            }
        ]
        
        # Add tests if requested
        if options.get("tests", True):
            structure.append({
                "path": f"{app_name}/tests.py",
                "content": await self._generate_content("django/tests.py", description, {"app_name": app_name, **options}),
                "purpose": "Test cases",
                "language": "python"
            })
        
        # Add forms if requested
        if options.get("forms", False):
            structure.append({
                "path": f"{app_name}/forms.py",
                "content": await self._generate_content("django/forms.py", description, {"app_name": app_name, **options}),
                "purpose": "Form definitions",
                "language": "python"
            })
        
        # Generate files
        for file_info in structure:
            files.append(CodeFile(
                path=file_info["path"],
                content=file_info["content"],
                purpose=file_info["purpose"],
                dependencies=[],
                language=file_info["language"]
            ))
        
        return {
            "success": True,
            "framework": "django",
            "files": files,
            "project_type": "python",
            "project_name": project_name,
            "app_name": app_name
        }
    
    async def _generate_flask(
        self,
        description: str,
        output_dir: Union[str, Path],
        options: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Generate a Flask project structure.
        
        Args:
            description: Description of the project
            output_dir: Output directory
            options: Additional options
            
        Returns:
            Dictionary with generation results
        """
        self._logger.info(f"Generating Flask project: {description}")
        
        # Get app name
        app_name = options.get("app_name", "app")
        app_name = re.sub(r'[^a-zA-Z0-9_]', '_', app_name)
        
        files = []
        
        # Define project structure
        structure = [
            {
                "path": "app.py",
                "content": await self._generate_content("flask/app.py", description, {"app_name": app_name, **options}),
                "purpose": "Main application",
                "language": "python"
            },
            {
                "path": "config.py",
                "content": await self._generate_content("flask/config.py", description, options),
                "purpose": "Configuration",
                "language": "python"
            },
            {
                "path": f"{app_name}/__init__.py",
                "content": await self._generate_content("flask/init.py", description, {"app_name": app_name, **options}),
                "purpose": "Application initialization",
                "language": "python"
            },
            {
                "path": f"{app_name}/routes.py",
                "content": await self._generate_content("flask/routes.py", description, options),
                "purpose": "Route definitions",
                "language": "python"
            },
            {
                "path": f"{app_name}/models.py",
                "content": await self._generate_content("flask/models.py", description, options),
                "purpose": "Data models",
                "language": "python"
            },
            {
                "path": "templates/index.html",
                "content": await self._generate_content("flask/index.html", description, options),
                "purpose": "Main template",
                "language": "html"
            },
            {
                "path": "templates/layout.html",
                "content": await self._generate_content("flask/layout.html", description, options),
                "purpose": "Base template",
                "language": "html"
            },
            {
                "path": "static/css/style.css",
                "content": await self._generate_content("flask/style.css", description, options),
                "purpose": "Main stylesheet",
                "language": "css"
            },
            {
                "path": "requirements.txt",
                "content": await self._generate_content("flask/requirements.txt", description, options),
                "purpose": "Python dependencies",
                "language": "text"
            },
            {
                "path": "README.md",
                "content": await self._generate_content("flask/README.md", description, {"app_name": app_name, **options}),
                "purpose": "Project documentation",
                "language": "markdown"
            }
        ]
        
        # Add Docker support if requested
        if options.get("docker", False):
            structure.extend([
                {
                    "path": "Dockerfile",
                    "content": await self._generate_content("flask/Dockerfile", description, {"app_name": app_name, **options}),
                    "purpose": "Docker configuration",
                    "language": "dockerfile"
                },
                {
                    "path": "docker-compose.yml",
                    "content": await self._generate_content("flask/docker-compose.yml", description, {"app_name": app_name, **options}),
                    "purpose": "Docker Compose configuration",
                    "language": "yaml"
                }
            ])
        
        # Generate files
        for file_info in structure:
            files.append(CodeFile(
                path=file_info["path"],
                content=file_info["content"],
                purpose=file_info["purpose"],
                dependencies=[],
                language=file_info["language"]
            ))
        
        return {
            "success": True,
            "framework": "flask",
            "files": files,
            "project_type": "python",
            "app_name": app_name
        }
    
    async def _generate_express(
        self,
        description: str,
        output_dir: Union[str, Path],
        options: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Generate an Express.js project structure.
        
        Args:
            description: Description of the project
            output_dir: Output directory
            options: Additional options
            
        Returns:
            Dictionary with generation results
        """
        self._logger.info(f"Generating Express project: {description}")
        
        files = []
        
        # Define project structure
        structure = [
            {
                "path": "app.js",
                "content": await self._generate_content("express/app.js", description, options),
                "purpose": "Main application",
                "language": "javascript"
            },
            {
                "path": "routes/index.js",
                "content": await self._generate_content("express/routes/index.js", description, options),
                "purpose": "Main routes",
                "language": "javascript"
            },
            {
                "path": "routes/users.js",
                "content": await self._generate_content("express/routes/users.js", description, options),
                "purpose": "User routes",
                "language": "javascript"
            },
            {
                "path": "views/index.ejs",
                "content": await self._generate_content("express/views/index.ejs", description, options),
                "purpose": "Main view template",
                "language": "html"
            },
            {
                "path": "views/error.ejs",
                "content": await self._generate_content("express/views/error.ejs", description, options),
                "purpose": "Error view template",
                "language": "html"
            },
            {
                "path": "public/stylesheets/style.css",
                "content": await self._generate_content("express/public/stylesheets/style.css", description, options),
                "purpose": "Main stylesheet",
                "language": "css"
            },
            {
                "path": "package.json",
                "content": await self._generate_content("express/package.json", description, options),
                "purpose": "NPM package configuration",
                "language": "json"
            },
            {
                "path": "README.md",
                "content": await self._generate_content("express/README.md", description, options),
                "purpose": "Project documentation",
                "language": "markdown"
            }
        ]
        
        # Add configuration file
        structure.append({
            "path": "config/config.js",
            "content": await self._generate_content("express/config/config.js", description, options),
            "purpose": "Configuration settings",
            "language": "javascript"
        })
        
        # Add middleware directory
        structure.append({
            "path": "middleware/auth.js",
            "content": await self._generate_content("express/middleware/auth.js", description, options),
            "purpose": "Authentication middleware",
            "language": "javascript"
        })
        
        # TypeScript support
        if options.get("typescript", False):
            # Replace .js files with .ts
            structure = [
                {
                    "path": f.get("path").replace(".js", ".ts") if f.get("path").endswith(".js") else f.get("path"),
                    "content": await self._generate_content(f.get("path").replace(".js", ".ts") if f.get("path").endswith(".js") else f.get("path"), description, options),
                    "purpose": f.get("purpose"),
                    "language": "typescript" if f.get("language") == "javascript" else f.get("language")
                }
                for f in structure
            ]
            
            # Add TypeScript config
            structure.append({
                "path": "tsconfig.json",
                "content": await self._generate_content("express/tsconfig.json", description, options),
                "purpose": "TypeScript configuration",
                "language": "json"
            })
        
        # Generate files
        for file_info in structure:
            files.append(CodeFile(
                path=file_info["path"],
                content=file_info["content"],
                purpose=file_info["purpose"],
                dependencies=[],
                language=file_info["language"]
            ))
        
        return {
            "success": True,
            "framework": "express",
            "files": files,
            "project_type": "node"
        }
    
    async def _generate_fastapi(
        self,
        description: str,
        output_dir: Union[str, Path],
        options: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Generate a FastAPI project structure.
        
        Args:
            description: Description of the project
            output_dir: Output directory
            options: Additional options
            
        Returns:
            Dictionary with generation results
        """
        self._logger.info(f"Generating FastAPI project: {description}")
        
        # Get app name
        app_name = options.get("app_name", "app")
        app_name = re.sub(r'[^a-zA-Z0-9_]', '_', app_name)
        
        files = []
        
        # Define project structure
        structure = [
            {
                "path": "main.py",
                "content": await self._generate_content("fastapi/main.py", description, {"app_name": app_name, **options}),
                "purpose": "Main application",
                "language": "python"
            },
            {
                "path": f"{app_name}/__init__.py",
                "content": "",
                "purpose": "Package initialization",
                "language": "python"
            },
            {
                "path": f"{app_name}/routes.py",
                "content": await self._generate_content("fastapi/routes.py", description, options),
                "purpose": "API routes",
                "language": "python"
            },
            {
                "path": f"{app_name}/models.py",
                "content": await self._generate_content("fastapi/models.py", description, options),
                "purpose": "Data models",
                "language": "python"
            },
            {
                "path": f"{app_name}/schemas.py",
                "content": await self._generate_content("fastapi/schemas.py", description, options),
                "purpose": "Pydantic schemas",
                "language": "python"
            },
            {
                "path": f"{app_name}/database.py",
                "content": await self._generate_content("fastapi/database.py", description, options),
                "purpose": "Database connection",
                "language": "python"
            },
            {
                "path": "requirements.txt",
                "content": await self._generate_content("fastapi/requirements.txt", description, options),
                "purpose": "Python dependencies",
                "language": "text"
            },
            {
                "path": "README.md",
                "content": await self._generate_content("fastapi/README.md", description, {"app_name": app_name, **options}),
                "purpose": "Project documentation",
                "language": "markdown"
            }
        ]
        
        # Add dependencies directory for better organization
        structure.append({
            "path": f"{app_name}/dependencies.py",
            "content": await self._generate_content("fastapi/dependencies.py", description, options),
            "purpose": "Dependency injection functions",
            "language": "python"
        })
        
        # Add config module
        structure.append({
            "path": f"{app_name}/config.py",
            "content": await self._generate_content("fastapi/config.py", description, options),
            "purpose": "Configuration settings",
            "language": "python"
        })
        
        # Add Docker support if requested
        if options.get("docker", True):
            structure.extend([
                {
                    "path": "Dockerfile",
                    "content": await self._generate_content("fastapi/Dockerfile", description, {"app_name": app_name, **options}),
                    "purpose": "Docker configuration",
                    "language": "dockerfile"
                },
                {
                    "path": ".dockerignore",
                    "content": await self._generate_content("fastapi/.dockerignore", description, options),
                    "purpose": "Docker ignore file",
                    "language": "text"
                }
            ])
        
        # Generate files
        for file_info in structure:
            files.append(CodeFile(
                path=file_info["path"],
                content=file_info["content"],
                purpose=file_info["purpose"],
                dependencies=[],
                language=file_info["language"]
            ))
        
        return {
            "success": True,
            "framework": "fastapi",
            "files": files,
            "project_type": "python",
            "app_name": app_name
        }
    
    async def _generate_spring(
        self,
        description: str,
        output_dir: Union[str, Path],
        options: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Generate a Spring Boot project structure.
        
        Args:
            description: Description of the project
            output_dir: Output directory
            options: Additional options
            
        Returns:
            Dictionary with generation results
        """
        self._logger.info(f"Generating Spring Boot project: {description}")
        
        # Get package name
        package_name = options.get("package_name", "com.example.demo")
        package_path = package_name.replace(".", "/")
        
        files = []
        
        # Define project structure
        structure = [
            {
                "path": f"src/main/java/{package_path}/Application.java",
                "content": await self._generate_content("spring/Application.java", description, {"package_name": package_name, **options}),
                "purpose": "Main application class",
                "language": "java"
            },
            {
                "path": f"src/main/java/{package_path}/controller/MainController.java",
                "content": await self._generate_content("spring/MainController.java", description, {"package_name": package_name, **options}),
                "purpose": "Main controller",
                "language": "java"
            },
            {
                "path": f"src/main/java/{package_path}/model/User.java",
                "content": await self._generate_content("spring/User.java", description, {"package_name": package_name, **options}),
                "purpose": "User model",
                "language": "java"
            },
            {
                "path": f"src/main/resources/application.properties",
                "content": await self._generate_content("spring/application.properties", description, options),
                "purpose": "Application properties",
                "language": "properties"
            },
            {
                "path": f"src/main/resources/templates/index.html",
                "content": await self._generate_content("spring/index.html", description, options),
                "purpose": "Main template",
                "language": "html"
            },
            {
                "path": "build.gradle",
                "content": await self._generate_content("spring/build.gradle", description, {"package_name": package_name, **options}),
                "purpose": "Gradle build configuration",
                "language": "gradle"
            },
            {
                "path": "README.md",
                "content": await self._generate_content("spring/README.md", description, {"package_name": package_name, **options}),
                "purpose": "Project documentation",
                "language": "markdown"
            }
        ]
        
        # Add repository and service layers for better organization
        structure.extend([
            {
                "path": f"src/main/java/{package_path}/repository/UserRepository.java",
                "content": await self._generate_content("spring/UserRepository.java", description, {"package_name": package_name, **options}),
                "purpose": "User repository interface",
                "language": "java"
            },
            {
                "path": f"src/main/java/{package_path}/service/UserService.java",
                "content": await self._generate_content("spring/UserService.java", description, {"package_name": package_name, **options}),
                "purpose": "User service interface",
                "language": "java"
            },
            {
                "path": f"src/main/java/{package_path}/service/impl/UserServiceImpl.java",
                "content": await self._generate_content("spring/UserServiceImpl.java", description, {"package_name": package_name, **options}),
                "purpose": "User service implementation",
                "language": "java"
            }
        ])
        
        # Add Maven support
        if options.get("maven", False) or options.get("build_tool", "gradle") == "maven":
            structure.append({
                "path": "pom.xml",
                "content": await self._generate_content("spring/pom.xml", description, {"package_name": package_name, **options}),
                "purpose": "Maven build configuration",
                "language": "xml"
            })
        
        # Generate files
        for file_info in structure:
            files.append(CodeFile(
                path=file_info["path"],
                content=file_info["content"],
                purpose=file_info["purpose"],
                dependencies=[],
                language=file_info["language"]
            ))
        
        return {
            "success": True,
            "framework": "spring",
            "files": files,
            "project_type": "java",
            "package_name": package_name
        }
    
    async def _generate_vue(
        self,
        description: str,
        output_dir: Union[str, Path],
        options: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Generate a Vue.js project structure.
        
        Args:
            description: Description of the project
            output_dir: Output directory
            options: Additional options
            
        Returns:
            Dictionary with generation results
        """
        self._logger.info(f"Generating Vue.js project: {description}")
        
        files = []
        
        # Define project structure
        structure = [
            {
                "path": "src/main.js",
                "content": await self._generate_content("vue/main.js", description, options),
                "purpose": "Application entry point",
                "language": "javascript"
            },
            {
                "path": "src/App.vue",
                "content": await self._generate_content("vue/App.vue", description, options),
                "purpose": "Main application component",
                "language": "vue"
            },
            {
                "path": "src/components/HelloWorld.vue",
                "content": await self._generate_content("vue/HelloWorld.vue", description, options),
                "purpose": "Example component",
                "language": "vue"
            },
            {
                "path": "src/router/index.js",
                "content": await self._generate_content("vue/router.js", description, options),
                "purpose": "Vue Router configuration",
                "language": "javascript"
            },
            {
                "path": "src/views/Home.vue",
                "content": await self._generate_content("vue/Home.vue", description, options),
                "purpose": "Home page component",
                "language": "vue"
            },
            {
                "path": "src/views/About.vue",
                "content": await self._generate_content("vue/About.vue", description, options),
                "purpose": "About page component",
                "language": "vue"
            },
            {
                "path": "public/index.html",
                "content": await self._generate_content("vue/index.html", description, options),
                "purpose": "Main HTML file",
                "language": "html"
            },
            {
                "path": "package.json",
                "content": await self._generate_content("vue/package.json", description, options),
                "purpose": "NPM package configuration",
                "language": "json"
            },
            {
                "path": "vue.config.js",
                "content": await self._generate_content("vue/vue.config.js", description, options),
                "purpose": "Vue CLI configuration",
                "language": "javascript"
            },
            {
                "path": "README.md",
                "content": await self._generate_content("vue/README.md", description, options),
                "purpose": "Project documentation",
                "language": "markdown"
            }
        ]
        
        # Add Vuex store if requested
        if options.get("store", True):
            structure.extend([
                {
                    "path": "src/store/index.js",
                    "content": await self._generate_content("vue/store/index.js", description, options),
                    "purpose": "Vuex store configuration",
                    "language": "javascript"
                },
                {
                    "path": "src/store/modules/auth.js",
                    "content": await self._generate_content("vue/store/modules/auth.js", description, options),
                    "purpose": "Auth store module",
                    "language": "javascript"
                }
            ])
        
        # TypeScript support
        if options.get("typescript", False):
            # Replace .js files with .ts
            structure = [
                {
                    "path": f.get("path").replace(".js", ".ts") if f.get("path").endswith(".js") else f.get("path"),
                    "content": await self._generate_content(f.get("path").replace(".js", ".ts") if f.get("path").endswith(".js") else f.get("path"), description, options),
                    "purpose": f.get("purpose"),
                    "language": "typescript" if f.get("language") == "javascript" else f.get("language")
                }
                for f in structure
            ]
            
            # Add TypeScript config
            structure.append({
                "path": "tsconfig.json",
                "content": await self._generate_content("vue/tsconfig.json", description, options),
                "purpose": "TypeScript configuration",
                "language": "json"
            })
        
        # Generate files
        for file_info in structure:
            files.append(CodeFile(
                path=file_info["path"],
                content=file_info["content"],
                purpose=file_info["purpose"],
                dependencies=[],
                language=file_info["language"]
            ))
        
        return {
            "success": True,
            "framework": "vue",
            "files": files,
            "project_type": "node"
        }
    
    async def _generate_angular(
        self,
        description: str,
        output_dir: Union[str, Path],
        options: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Generate an Angular project structure.
        
        Args:
            description: Description of the project
            output_dir: Output directory
            options: Additional options
            
        Returns:
            Dictionary with generation results
        """
        self._logger.info(f"Generating Angular project: {description}")
        
        files = []
        
        # Define project structure
        structure = [
            {
                "path": "src/main.ts",
                "content": await self._generate_content("angular/main.ts", description, options),
                "purpose": "Application entry point",
                "language": "typescript"
            },
            {
                "path": "src/app/app.module.ts",
                "content": await self._generate_content("angular/app.module.ts", description, options),
                "purpose": "Main application module",
                "language": "typescript"
            },
            {
                "path": "src/app/app.component.ts",
                "content": await self._generate_content("angular/app.component.ts", description, options),
                "purpose": "Main application component",
                "language": "typescript"
            },
            {
                "path": "src/app/app.component.html",
                "content": await self._generate_content("angular/app.component.html", description, options),
                "purpose": "Main component template",
                "language": "html"
            },
            {
                "path": "src/app/app.component.css",
                "content": await self._generate_content("angular/app.component.css", description, options),
                "purpose": "Main component styles",
                "language": "css"
            },
            {
                "path": "src/app/app-routing.module.ts",
                "content": await self._generate_content("angular/app-routing.module.ts", description, options),
                "purpose": "Routing configuration",
                "language": "typescript"
            },
            {
                "path": "src/index.html",
                "content": await self._generate_content("angular/index.html", description, options),
                "purpose": "Main HTML file",
                "language": "html"
            },
            {
                "path": "src/styles.css",
                "content": await self._generate_content("angular/styles.css", description, options),
                "purpose": "Global styles",
                "language": "css"
            },
            {
                "path": "angular.json",
                "content": await self._generate_content("angular/angular.json", description, options),
                "purpose": "Angular CLI configuration",
                "language": "json"
            },
            {
                "path": "package.json",
                "content": await self._generate_content("angular/package.json", description, options),
                "purpose": "NPM package configuration",
                "language": "json"
            },
            {
                "path": "tsconfig.json",
                "content": await self._generate_content("angular/tsconfig.json", description, options),
                "purpose": "TypeScript configuration",
                "language": "json"
            },
            {
                "path": "README.md",
                "content": await self._generate_content("angular/README.md", description, options),
                "purpose": "Project documentation",
                "language": "markdown"
            }
        ]
        
        # Add feature module for better organization
        structure.extend([
            {
                "path": "src/app/features/home/home.component.ts",
                "content": await self._generate_content("angular/features/home/home.component.ts", description, options),
                "purpose": "Home feature component",
                "language": "typescript"
            },
            {
                "path": "src/app/features/home/home.component.html",
                "content": await self._generate_content("angular/features/home/home.component.html", description, options),
                "purpose": "Home feature template",
                "language": "html"
            },
            {
                "path": "src/app/features/home/home.module.ts",
                "content": await self._generate_content("angular/features/home/home.module.ts", description, options),
                "purpose": "Home feature module",
                "language": "typescript"
            }
        ])
        
        # Add shared module
        structure.extend([
            {
                "path": "src/app/shared/shared.module.ts",
                "content": await self._generate_content("angular/shared/shared.module.ts", description, options),
                "purpose": "Shared module",
                "language": "typescript"
            },
            {
                "path": "src/app/shared/components/header/header.component.ts",
                "content": await self._generate_content("angular/shared/components/header/header.component.ts", description, options),
                "purpose": "Header component",
                "language": "typescript"
            },
            {
                "path": "src/app/shared/components/header/header.component.html",
                "content": await self._generate_content("angular/shared/components/header/header.component.html", description, options),
                "purpose": "Header template",
                "language": "html"
            }
        ])
        
        # Generate files
        for file_info in structure:
            files.append(CodeFile(
                path=file_info["path"],
                content=file_info["content"],
                purpose=file_info["purpose"],
                dependencies=[],
                language=file_info["language"]
            ))
        
        return {
            "success": True,
            "framework": "angular",
            "files": files,
            "project_type": "node"
        }
    
    async def _generate_generic(
        self,
        framework: str,
        description: str,
        output_dir: Union[str, Path],
        options: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Generate a generic framework structure using AI.
        
        Args:
            framework: Framework name
            description: Description of the project
            output_dir: Output directory
            options: Additional options
            
        Returns:
            Dictionary with generation results
        """
        self._logger.info(f"Generating generic {framework} project: {description}")
        
        # Use AI to generate a project structure
        prompt = f"""
Generate a typical file structure for a {framework} project that matches this description:
"{description}"

Your response should be a JSON object with this structure:
```json
{{
  "files": [
    {{
      "path": "relative/path/to/file.ext",
      "purpose": "brief description of the file's purpose",
      "language": "programming language/file type"
    }}
  ],
  "project_type": "main programming language (e.g., python, node, java)",
  "description": "brief description of the project structure"
}}
Include all essential files for a working {framework} project, including:

Main entry point file(s)
Configuration files
View/template files
Model definitions
Routing or controller files
Package management files (e.g., package.json, requirements.txt)
Documentation

Keep the structure focused on the core framework files, don't include optional or very specific files.
Ensure the structure follows best practices for {framework} projects.
"""
    try:
        # Call AI service
        api_request = GeminiRequest(
            prompt=prompt,
            max_tokens=4000,
            temperature=0.2
        )   response = await gemini_client.generate_text(api_request)
        
        # Extract JSON from the response
        structure_match = re.search(r'```(?:json)?\s*(.*?)\s*```', response.text, re.DOTALL)
        if structure_match:
            structure_json = structure_match.group(1)
        else:
            structure_json = response.text
        
        structure_data = json.loads(structure_json)
        
        # Generate content for each file
        files = []
        
        for file_info in structure_data.get("files", []):
            # Generate content for the file
            content = await self._generate_file_content(
                framework,
                file_info["path"],
                file_info["purpose"],
                description,
                options
            )
            
            files.append(CodeFile(
                path=file_info["path"],
                content=content,
                purpose=file_info["purpose"],
                dependencies=[],
                language=file_info.get("language")
            ))
        
        return {
            "success": True,
            "framework": framework,
            "files": files,
            "project_type": structure_data.get("project_type", self._infer_project_type(framework))
        }
    
    except json.JSONDecodeError as e:
        self._logger.error(f"Error parsing AI response for {framework} project structure: {e}")
        return {
            "success": False,
            "error": f"Could not generate {framework} project structure: Invalid JSON response",
            "framework": framework
        }
    except Exception as e:
        self._logger.error(f"Error generating {framework} project: {str(e)}", exc_info=True)
        return {
            "success": False,
            "error": f"Could not generate {framework} project structure: {str(e)}",
            "framework": framework
        }

def _infer_project_type(self, framework: str) -> str:
    """
    Infer project type from framework name.
    
    Args:
        framework: Framework name
        
    Returns:
        Inferred project type
    """
    return self._framework_project_types.get(framework.lower(), "unknown")

async def _generate_content(
    self, 
    template_path: str,
    description: str,
    options: Dict[str, Any]
) -> str:
    """
    Generate content for a file based on a template path.
    
    Args:
        template_path: Path to template relative to framework
        description: Project description
        options: Additional options
        
    Returns:
        Generated file content
    """
    self._logger.debug(f"Generating content for template: {template_path}")
    
    # Extract framework and file path
    parts = template_path.split("/", 1)
    if len(parts) < 2:
        # Invalid template path
        framework = "generic"
        file_path = template_path
    else:
        framework = parts[0]
        file_path = parts[1]
    
    return await self._generate_file_content(
        framework,
        file_path,
        "Framework-specific file",
        description,
        options
    )

async def _generate_file_content(
    self, 
    framework: str,
    file_path: str,
    purpose: str,
    description: str,
    options: Dict[str, Any]
) -> str:
    """
    Generate content for a file using AI.
    
    Args:
        framework: Framework name
        file_path: Path to the file
        purpose: Purpose of the file
        description: Project description
        options: Additional options
        
    Returns:
        Generated file content
    """
    # Determine the programming language from the file extension
    ext = Path(file_path).suffix.lower()
    language_map = {
        ".py": "Python",
        ".js": "JavaScript",
        ".ts": "TypeScript",
        ".jsx": "JavaScript (React)",
        ".tsx": "TypeScript (React)",
        ".html": "HTML",
        ".css": "CSS",
        ".json": "JSON",
        ".java": "Java",
        ".go": "Go",
        ".rs": "Rust",
        ".vue": "Vue.js",
        ".md": "Markdown",
        ".yml": "YAML",
        ".yaml": "YAML",
        ".gradle": "Gradle",
        ".properties": "Properties",
        ".xml": "XML",
        ".dockerfile": "Dockerfile",
        ".sh": "Bash",
        ".bat": "Batch",
        ".ejs": "EJS template"
    }
    language = language_map.get(ext, "Unknown")
    
    # Build the prompt
    prompt = f"""
Generate content for a {language} file in a {framework} project.
File path: {file_path}
File purpose: {purpose}
Project description: "{description}"
Requirements:

Generate complete, valid code for a {language} file
Ensure the code follows best practices for {framework} projects
Make the code clean, well-structured, and well-commented
Only include code relevant to the file's purpose and path
Match the style and idioms typically used in {framework} projects

Only respond with the file content, nothing else.
"""
    # Add language-specific instructions
    if language == "Python":
        prompt += "\nInclude appropriate imports and docstrings. Follow PEP 8 guidelines."
    elif language in ["JavaScript", "TypeScript"]:
        prompt += "\nUse modern ES6+ syntax. Include appropriate imports/exports."
    elif language in ["JavaScript (React)", "TypeScript (React)"]:
        prompt += "\nUse functional components with hooks. Include appropriate imports."
    elif language == "Java":
        prompt += "\nInclude appropriate package declaration, imports, and JavaDoc comments."
    # Add framework-specific information
    if framework == "react":
        if "variant" in options and options["variant"] == "nextjs":
            prompt += "\nThis is a Next.js project. Use Next.js-specific patterns and API."
        else:
            prompt += "\nThis is a Create React App project. Use appropriate React patterns."
    elif framework == "django":
        prompt += f"\nProject name: {options.get('project_name', 'django_project')}"
        prompt += f"\nApp name: {options.get('app_name', 'main')}"
    
    try:
        # Call AI service
        api_request = GeminiRequest(
            prompt=prompt,
            max_tokens=4000,
            temperature=0.2
        )
        
        response = await gemini_client.generate_text(api_request)
        
        # Extract code from the response
        code_match = re.search(r'```(?:\w+)?\s*(.*?)\s*```', response.text, re.DOTALL)
        if code_match:
            return code_match.group(1)
        
        # No code block found, use the entire response
        return response.text.strip()
    except Exception as e:
        self._logger.error(f"Error generating content for {file_path}: {str(e)}", exc_info=True)
        return f"# Error generating content: {str(e)}\n# Please regenerate this file"
Global framework generator instance
framework_generator = FrameworkGenerator()
</file>

<file path="angela/generation/planner.py">
# angela/generation/planner.py
"""
Project structure planning for Angela CLI.

This module is responsible for planning the structure of a new project,
identifying necessary files, their roles, and interdependencies.
"""
import os
import asyncio
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple, Union
import json
import re

from pydantic import BaseModel, Field

from angela.ai.client import gemini_client, GeminiRequest
from angela.context import context_manager
from angela.utils.logging import get_logger
from angela.generation.engine import CodeProject, CodeFile

logger = get_logger(__name__)

class ArchitectureComponent(BaseModel):
    """Model for a component in the project architecture."""
    name: str = Field(..., description="Name of the component")
    description: str = Field(..., description="Description of the component")
    responsibilities: List[str] = Field(..., description="Responsibilities of the component")
    files: List[str] = Field(default_factory=list, description="Files implementing this component")
    dependencies: List[str] = Field(default_factory=list, description="Components this depends on")

class ProjectArchitecture(BaseModel):
    """Model for a project's architecture."""
    components: List[ArchitectureComponent] = Field(..., description="Components in the architecture")
    layers: List[str] = Field(default_factory=list, description="Architectural layers")
    patterns: List[str] = Field(default_factory=list, description="Design patterns used")
    data_flow: List[str] = Field(default_factory=list, description="Data flow descriptions")

class ProjectPlanner:
    """
    Project planner for designing and organizing code projects.
    """
    
    def __init__(self):
        """Initialize the project planner."""
        self._logger = logger
    
    async def create_project_architecture(
        self, 
        description: str,
        project_type: str,
        context: Optional[Dict[str, Any]] = None
    ) -> ProjectArchitecture:
        """
        Create a high-level architecture for a project.
        
        Args:
            description: Natural language description of the project
            project_type: Type of project to generate
            context: Additional context information
            
        Returns:
            ProjectArchitecture object
        """
        self._logger.info(f"Creating project architecture for: {description}")
        
        # Get context if not provided
        if context is None:
            context = context_manager.get_context_dict()
        
        # Build prompt for architecture planning
        prompt = self._build_architecture_prompt(description, project_type, context)
        
        # Call AI service to generate architecture
        api_request = GeminiRequest(
            prompt=prompt,
            max_tokens=4000,
            temperature=0.2
        )
        
        self._logger.debug("Sending architecture planning request to AI service")
        response = await gemini_client.generate_text(api_request)
        
        # Parse the response to extract the architecture
        architecture = await self._parse_architecture(response.text)
        
        return architecture
    
    async def refine_project_plan(
        self, 
        project: CodeProject,
        architecture: ProjectArchitecture,
        context: Optional[Dict[str, Any]] = None
    ) -> CodeProject:
        """
        Refine a project plan based on architecture.
        
        Args:
            project: Initial CodeProject
            architecture: ProjectArchitecture to use for refinement
            context: Additional context information
            
        Returns:
            Refined CodeProject
        """
        self._logger.info(f"Refining project plan for: {project.name}")
        
        # Get context if not provided
        if context is None:
            context = context_manager.get_context_dict()
        
        # Build prompt for plan refinement
        prompt = self._build_plan_refinement_prompt(project, architecture, context)
        
        # Call AI service to refine plan
        api_request = GeminiRequest(
            prompt=prompt,
            max_tokens=4000,
            temperature=0.2
        )
        
        self._logger.debug("Sending plan refinement request to AI service")
        response = await gemini_client.generate_text(api_request)
        
        # Parse the response to extract the refined plan
        refined_plan = await self._parse_refined_plan(response.text, project)
        
        return refined_plan
    
    def _build_architecture_prompt(
        self, 
        description: str,
        project_type: str,
        context: Dict[str, Any]
    ) -> str:
        """
        Build a prompt for architecture planning.
        
        Args:
            description: Natural language description of the project
            project_type: Type of project to generate
            context: Additional context information
            
        Returns:
            Prompt string for the AI service
        """
        prompt = f"""
As an experienced software architect, design a high-level architecture for a {project_type} project based on this description:

"{description}"

Analyze the requirements and create a comprehensive architecture that is:
- Modular and maintainable
- Follows SOLID principles
- Anticipates future changes/extensions
- Accounts for scalability and performance

Your response should be a JSON object with this structure:

```json
{{
  "components": [
    {{
      "name": "component_name",
      "description": "what this component does",
      "responsibilities": ["resp1", "resp2", ...],
      "files": ["expected/path/to/file.ext", ...],
      "dependencies": ["other_component_names", ...]
    }},
    ...
  ],
  "layers": ["Layer1", "Layer2", ...],
  "patterns": ["Design patterns used in the architecture"],
  "data_flow": ["Descriptions of data flow between components"]
}}
Focus on a clean separation of concerns, appropriate design patterns for {project_type}, and efficient data flow.
"""
    return prompt

async def _parse_architecture(self, response: str) -> ProjectArchitecture:
    """
    Parse the AI response to extract the architecture.
    
    Args:
        response: AI response text
        
    Returns:
        ProjectArchitecture object
    """
    try:
        # Look for JSON block in the response
        json_match = re.search(r'```(?:json)?\s*(.*?)\s*```', response, re.DOTALL)
        if json_match:
            json_str = json_match.group(1)
        else:
            # Try to find JSON without code blocks
            json_match = re.search(r'({.*})', response, re.DOTALL)
            if json_match:
                json_str = json_match.group(1)
            else:
                # Assume the entire response is JSON
                json_str = response
        
        # Parse the JSON
        arch_data = json.loads(json_str)
        
        # Create ArchitectureComponent objects
        components = []
        for comp_data in arch_data.get("components", []):
            components.append(ArchitectureComponent(
                name=comp_data["name"],
                description=comp_data["description"],
                responsibilities=comp_data.get("responsibilities", []),
                files=comp_data.get("files", []),
                dependencies=comp_data.get("dependencies", [])
            ))
        
        # Create ProjectArchitecture object
        architecture = ProjectArchitecture(
            components=components,
            layers=arch_data.get("layers", []),
            patterns=arch_data.get("patterns", []),
            data_flow=arch_data.get("data_flow", [])
        )
        
        return architecture
        
    except Exception as e:
        self._logger.exception(f"Error parsing architecture: {str(e)}")
        
        # Create a minimal fallback architecture
        fallback_component = ArchitectureComponent(
            name="Core",
            description="Core application functionality",
            responsibilities=["Main application logic"],
            files=[],
            dependencies=[]
        )
        
        return ProjectArchitecture(
            components=[fallback_component],
            layers=["Presentation", "Business Logic", "Data Access"],
            patterns=["MVC"],
            data_flow=["User input -> Core processing -> Storage"]
        )

def _build_plan_refinement_prompt(
    self, 
    project: CodeProject,
    architecture: ProjectArchitecture,
    context: Dict[str, Any]
) -> str:
    """
    Build a prompt for plan refinement.
    
    Args:
        project: Initial CodeProject
        architecture: ProjectArchitecture to use for refinement
        context: Additional context information
        
    Returns:
        Prompt string for the AI service
    """
    # Extract architecture info
    arch_json = {}
    arch_json["components"] = [comp.dict() for comp in architecture.components]
    arch_json["layers"] = architecture.layers
    arch_json["patterns"] = architecture.patterns
    arch_json["data_flow"] = architecture.data_flow
    
    # Extract project info
    project_json = {}
    project_json["name"] = project.name
    project_json["description"] = project.description
    project_json["project_type"] = project.project_type
    project_json["dependencies"] = project.dependencies
    project_json["files"] = [
        {
            "path": file.path,
            "purpose": file.purpose,
            "dependencies": file.dependencies
        }
        for file in project.files
    ]
    
    prompt = f"""
You are refining a project plan based on a high-level architecture.
Here is the current project plan:
json{json.dumps(project_json, indent=2)}
And here is the architecture design:
json{json.dumps(arch_json, indent=2)}
Your task is to refine the project plan to better align with the architecture.
This may involve:

1. Adding missing files that would be needed for components in the architecture
2. Updating existing file purposes to match component responsibilities
3. Adjusting file dependencies to match component dependencies
4. Ensuring the project structure follows the architectural layers

Return a refined project plan in this JSON format:
json{{
  "name": "project_name",
  "description": "project description",
  "project_type": "{project.project_type}",
  "dependencies": {{
    "runtime": ["dep1", "dep2"],
    "development": ["dev_dep1", "dev_dep2"]
  }},
  "files": [
    {{
      "path": "path/to/file.ext",
      "purpose": "file purpose",
      "dependencies": ["other/file/paths"],
      "component": "associated_component_name"
    }}
  ],
  "structure_explanation": "explanation of the refined structure"
}}
Make sure the refined plan implements all components and follows all architectural patterns in the design.
"""
    return prompt

async def _parse_refined_plan(
    self, 
    response: str, 
    original_project: CodeProject
) -> CodeProject:
    """
    Parse the AI response to extract the refined plan.
    
    Args:
        response: AI response text
        original_project: The original project to refine
        
    Returns:
        Refined CodeProject
    """
    try:
        # Look for JSON block in the response
        json_match = re.search(r'```(?:json)?\s*(.*?)\s*```', response, re.DOTALL)
        if json_match:
            json_str = json_match.group(1)
        else:
            # Try to find JSON without code blocks
            json_match = re.search(r'({.*})', response, re.DOTALL)
            if json_match:
                json_str = json_match.group(1)
            else:
                # Assume the entire response is JSON
                json_str = response
        
        # Parse the JSON
        plan_data = json.loads(json_str)
        
        # Create CodeFile objects
        files = []
        for file_data in plan_data.get("files", []):
            # Check if this file existed in the original project
            original_file = next((f for f in original_project.files if f.path == file_data["path"]), None)
            
            files.append(CodeFile(
                path=file_data["path"],
                content=original_file.content if original_file else "",
                purpose=file_data["purpose"],
                dependencies=file_data.get("dependencies", []),
                language=original_file.language if original_file else None
            ))
        
        # Create CodeProject object
        project = CodeProject(
            name=plan_data.get("name", original_project.name),
            description=plan_data.get("description", original_project.description),
            root_dir=original_project.root_dir,
            files=files,
            dependencies=plan_data.get("dependencies", original_project.dependencies),
            project_type=original_project.project_type,
            structure_explanation=plan_data.get("structure_explanation", original_project.structure_explanation)
        )
        
        return project
        
    except Exception as e:
        self._logger.exception(f"Error parsing refined plan: {str(e)}")
        
        # Return the original project if parsing failed
        return original_project
        
        
project_planner = ProjectPlanner()
</file>

<file path="angela/generation/validators.py">
# angela/generation/validators.py
"""
Code validation for Angela CLI.

This module provides validators for different programming languages
to ensure generated code is syntactically correct and follows best practices.
"""
import os
import subprocess
import tempfile
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple, Union
import re

from angela.utils.logging import get_logger

logger = get_logger(__name__)

# Map file extensions to language validators
LANGUAGE_VALIDATORS = {
    ".py": "validate_python",
    ".js": "validate_javascript",
    ".jsx": "validate_javascript",
    ".ts": "validate_typescript",
    ".tsx": "validate_typescript",
    ".java": "validate_java",
    ".go": "validate_go",
    ".rb": "validate_ruby",
    ".rs": "validate_rust",
    ".html": "validate_html",
    ".css": "validate_css",
    ".php": "validate_php"
}

def validate_code(content: str, file_path: str) -> Tuple[bool, str]:
    """
    Validate code based on file extension.
    
    Args:
        content: Code content to validate
        file_path: Path of the file (used to determine language)
        
    Returns:
        Tuple of (is_valid, error_message)
    """
    logger.info(f"Validating code for: {file_path}")
    
    _, extension = os.path.splitext(file_path.lower())
    
    # Get the validator function for this extension
    validator_name = LANGUAGE_VALIDATORS.get(extension)
    
    if validator_name and validator_name in globals():
        validator_func = globals()[validator_name]
        return validator_func(content, file_path)
    
    # If no specific validator, do basic checks
    return validate_generic(content, file_path)

def validate_generic(content: str, file_path: str) -> Tuple[bool, str]:
    """
    Generic validator for any code file.
    
    Args:
        content: Code content to validate
        file_path: Path of the file
        
    Returns:
        Tuple of (is_valid, error_message)
    """
    # Check for basic issues like unmatched brackets
    bracket_pairs = [('(', ')'), ('[', ']'), ('{', '}')]
    
    for opening, closing in bracket_pairs:
        if content.count(opening) != content.count(closing):
            return False, f"Unmatched brackets: {opening}{closing}"
    
    return True, ""

def validate_python(content: str, file_path: str) -> Tuple[bool, str]:
    """
    Validate Python code.
    
    Args:
        content: Python code to validate
        file_path: Path of the file
        
    Returns:
        Tuple of (is_valid, error_message)
    """
    # Create a temporary file
    with tempfile.NamedTemporaryFile(suffix='.py', delete=False) as tmp:
        tmp_path = tmp.name
        tmp.write(content.encode('utf-8'))
    
    try:
        # Use Python's compile function to check syntax
        result = subprocess.run(
            ['python', '-m', 'py_compile', tmp_path],
            capture_output=True,
            text=True
        )
        
        if result.returncode != 0:
            # Parse the error message
            error_msg = result.stderr
            
            # Simplify the error message
            error_lines = error_msg.splitlines()
            for line in error_lines:
                if "File " in line and ", line " in line:
                    continue
                if line.strip():
                    error_msg = line.strip()
                    break
            
            return False, f"Python syntax error: {error_msg}"
        
        # Additional checks for common Python issues
        issues = []
        
        # Check for undefined or unused imports
        import_lines = []
        imported_modules = []
        
        for line in content.splitlines():
            if line.strip().startswith(('import ', 'from ')):
                import_lines.append(line)
                
                if line.strip().startswith('import '):
                    modules = line.strip()[7:].split(',')
                    for module in modules:
                        if ' as ' in module:
                            module = module.split(' as ')[1]
                        imported_modules.append(module.strip())
                elif line.strip().startswith('from '):
                    parts = line.strip().split(' import ')
                    if len(parts) == 2:
                        modules = parts[1].split(',')
                        for module in modules:
                            if ' as ' in module:
                                module = module.split(' as ')[1]
                            imported_modules.append(module.strip())
        
        # Check if imports are used
        for module in imported_modules:
            if module not in content.replace(f"import {module}", "").replace(f"from {module}", ""):
                issues.append(f"Potentially unused import: {module}")
        
        # If we found issues but not syntax errors, still consider it valid
        # but report the issues
        if issues:
            return True, f"Code is valid but has issues: {'; '.join(issues)}"
        
        return True, ""
    
    except Exception as e:
        logger.error(f"Error validating Python code: {str(e)}")
        return False, f"Error validating Python code: {str(e)}"
    
    finally:
        # Clean up the temporary file
        try:
            os.unlink(tmp_path)
        except Exception:
            pass

def validate_javascript(content: str, file_path: str) -> Tuple[bool, str]:
    """
    Validate JavaScript code.
    
    Args:
        content: JavaScript code to validate
        file_path: Path of the file
        
    Returns:
        Tuple of (is_valid, error_message)
    """
    # Check if node is available
    try:
        subprocess.run(['node', '--version'], capture_output=True, check=True)
    except (subprocess.SubprocessError, FileNotFoundError):
        logger.warning("Node.js not found, falling back to basic JS validation")
        return validate_javascript_basic(content, file_path)
    
    # Create a temporary file
    with tempfile.NamedTemporaryFile(suffix='.js', delete=False) as tmp:
        tmp_path = tmp.name
        tmp.write(content.encode('utf-8'))
    
    try:
        # Use Node.js to check syntax
        result = subprocess.run(
            ['node', '--check', tmp_path],
            capture_output=True,
            text=True
        )
        
        if result.returncode != 0:
            # Parse the error message
            error_msg = result.stderr
            return False, f"JavaScript syntax error: {error_msg.strip()}"
        
        return True, ""
    
    except Exception as e:
        logger.error(f"Error validating JavaScript code: {str(e)}")
        return False, f"Error validating JavaScript code: {str(e)}"
    
    finally:
        # Clean up the temporary file
        try:
            os.unlink(tmp_path)
        except Exception:
            pass

def validate_javascript_basic(content: str, file_path: str) -> Tuple[bool, str]:
    """
    Basic validation for JavaScript code without using Node.js.
    
    Args:
        content: JavaScript code to validate
        file_path: Path of the file
        
    Returns:
        Tuple of (is_valid, error_message)
    """
    # Check for basic syntax issues
    
    # Check for unmatched brackets
    bracket_pairs = [('(', ')'), ('[', ']'), ('{', '}')]
    
    for opening, closing in bracket_pairs:
        if content.count(opening) != content.count(closing):
            return False, f"Unmatched brackets: {opening}{closing}"
    
    # Check for missing semicolons (simplified)
    lines = content.splitlines()
    for i, line in enumerate(lines):
        line = line.strip()
        if line and not line.endswith(';') and not line.endswith('{') and not line.endswith('}') and \
           not line.endswith(':') and not line.startswith('//') and not line.startswith('/*') and \
           not line.endswith('*/') and not line.startswith('import ') and not line.startswith('export '):
            # This is a simplistic check and might have false positives
            logger.debug(f"Line {i+1} might be missing a semicolon: {line}")
    
    # Check for common React/JSX issues if it seems to be a React file
    if ".jsx" in file_path or "React" in content or "react" in content:
        # Check if JSX elements are closed
        jsx_tags = re.findall(r'<([a-zA-Z0-9]+)(?:\s+[^>]*)?>', content)
        for tag in jsx_tags:
            if tag[0].isupper() and f"</{tag}>" not in content and "/>" not in content:
                return False, f"Unclosed JSX element: <{tag}>"
    
    return True, ""

def validate_typescript(content: str, file_path: str) -> Tuple[bool, str]:
    """
    Validate TypeScript code.
    
    Args:
        content: TypeScript code to validate
        file_path: Path of the file
        
    Returns:
        Tuple of (is_valid, error_message)
    """
    # Check if tsc is available
    try:
        subprocess.run(['tsc', '--version'], capture_output=True, check=True)
    except (subprocess.SubprocessError, FileNotFoundError):
        logger.warning("TypeScript compiler not found, falling back to JavaScript validation")
        return validate_javascript(content, file_path)
    
    # Create a temporary file
    with tempfile.NamedTemporaryFile(suffix='.ts', delete=False) as tmp:
        tmp_path = tmp.name
        tmp.write(content.encode('utf-8'))
    
    try:
        # Use tsc to check syntax (without emitting JS files)
        result = subprocess.run(
            ['tsc', '--noEmit', tmp_path],
            capture_output=True,
            text=True
        )
        
        if result.returncode != 0:
            # Parse the error message
            error_msg = result.stderr or result.stdout
            return False, f"TypeScript error: {error_msg.strip()}"
        
        return True, ""
    
    except Exception as e:
        logger.error(f"Error validating TypeScript code: {str(e)}")
        return False, f"Error validating TypeScript code: {str(e)}"
    
    finally:
        # Clean up the temporary file
        try:
            os.unlink(tmp_path)
        except Exception:
            pass

def validate_java(content: str, file_path: str) -> Tuple[bool, str]:
    """
    Validate Java code.
    
    Args:
        content: Java code to validate
        file_path: Path of the file
        
    Returns:
        Tuple of (is_valid, error_message)
    """
    # Check if javac is available
    try:
        subprocess.run(['javac', '-version'], capture_output=True, check=True)
    except (subprocess.SubprocessError, FileNotFoundError):
        logger.warning("Java compiler not found, falling back to basic Java validation")
        return validate_java_basic(content, file_path)
    
    # Extract class name from the code
    class_name = None
    class_match = re.search(r'public\s+class\s+(\w+)', content)
    if class_match:
        class_name = class_match.group(1)
    else:
        # Try to get class name from file path
        base_name = os.path.basename(file_path)
        if base_name.endswith('.java'):
            class_name = base_name[:-5]
    
    if not class_name:
        return False, "Could not determine Java class name"
    
    # Update content to match class name from file if needed
    if class_match and class_match.group(1) != class_name:
        content = content.replace(f"class {class_match.group(1)}", f"class {class_name}")
    
    # Create a temporary file
    with tempfile.NamedTemporaryFile(suffix='.java', delete=False) as tmp:
        tmp_path = tmp.name
        tmp.write(content.encode('utf-8'))
    
    try:
        # Use javac to check syntax
        result = subprocess.run(
            ['javac', tmp_path],
            capture_output=True,
            text=True
        )
        
        if result.returncode != 0:
            # Parse the error message
            error_msg = result.stderr
            return False, f"Java syntax error: {error_msg.strip()}"
        
        return True, ""
    
    except Exception as e:
        logger.error(f"Error validating Java code: {str(e)}")
        return False, f"Error validating Java code: {str(e)}"
    
    finally:
        # Clean up the temporary file
        try:
            os.unlink(tmp_path)
        except Exception:
            pass

def validate_java_basic(content: str, file_path: str) -> Tuple[bool, str]:
    """
    Basic validation for Java code without using javac.
    
    Args:
        content: Java code to validate
        file_path: Path of the file
        
    Returns:
        Tuple of (is_valid, error_message)
    """
    # Check for basic syntax issues
    
    # Check for unmatched brackets
    bracket_pairs = [('(', ')'), ('[', ']'), ('{', '}')]
    
    for opening, closing in bracket_pairs:
        if content.count(opening) != content.count(closing):
            return False, f"Unmatched brackets: {opening}{closing}"
    
    # Check for missing semicolons (simplified)
    lines = content.splitlines()
    for i, line in enumerate(lines):
        line = line.strip()
        if line and not line.endswith(';') and not line.endswith('{') and not line.endswith('}') and \
           not line.endswith(':') and not line.startswith('//') and not line.startswith('/*') and \
           not line.endswith('*/'):
            if not any(keyword in line for keyword in ['class ', 'interface ', 'enum ', 'import ', 'package ']):
                # This is a simplistic check and might have false positives
                logger.debug(f"Line {i+1} might be missing a semicolon: {line}")
    
    # Check for class name matching file name
    class_match = re.search(r'public\s+class\s+(\w+)', content)
    if class_match:
        class_name = class_match.group(1)
        base_name = os.path.basename(file_path)
        if base_name.endswith('.java') and base_name[:-5] != class_name:
            return False, f"Class name '{class_name}' does not match file name '{base_name[:-5]}'"
    
    return True, ""

def validate_go(content: str, file_path: str) -> Tuple[bool, str]:
    """
    Validate Go code.
    
    Args:
        content: Go code to validate
        file_path: Path of the file
        
    Returns:
        Tuple of (is_valid, error_message)
    """
    # Check if go is available
    try:
        subprocess.run(['go', 'version'], capture_output=True, check=True)
    except (subprocess.SubprocessError, FileNotFoundError):
        logger.warning("Go compiler not found, falling back to basic Go validation")
        return validate_go_basic(content, file_path)
    
    # Create a temporary file
    with tempfile.NamedTemporaryFile(suffix='.go', delete=False) as tmp:
        tmp_path = tmp.name
        tmp.write(content.encode('utf-8'))
    
    try:
        # Use go vet to check syntax and common issues
        result = subprocess.run(
            ['go', 'vet', tmp_path],
            capture_output=True,
            text=True
        )
        
        if result.returncode != 0:
            # Parse the error message
            error_msg = result.stderr
            return False, f"Go error: {error_msg.strip()}"
        
        return True, ""
    
    except Exception as e:
        logger.error(f"Error validating Go code: {str(e)}")
        return False, f"Error validating Go code: {str(e)}"
    
    finally:
        # Clean up the temporary file
        try:
            os.unlink(tmp_path)
        except Exception:
            pass

def validate_go_basic(content: str, file_path: str) -> Tuple[bool, str]:
    """
    Basic validation for Go code without using go compiler.
    
    Args:
        content: Go code to validate
        file_path: Path of the file
        
    Returns:
        Tuple of (is_valid, error_message)
    """
    # Check for basic syntax issues
    
    # Check for unmatched brackets
    bracket_pairs = [('(', ')'), ('[', ']'), ('{', '}')]
    
    for opening, closing in bracket_pairs:
        if content.count(opening) != content.count(closing):
            return False, f"Unmatched brackets: {opening}{closing}"
    
    # Check for package declaration
    if not re.search(r'package\s+\w+', content):
        return False, "Missing package declaration"
    
    return True, ""

def validate_ruby(content: str, file_path: str) -> Tuple[bool, str]:
    """
    Validate Ruby code.
    
    Args:
        content: Ruby code to validate
        file_path: Path of the file
        
    Returns:
        Tuple of (is_valid, error_message)
    """
    # Check if ruby is available
    try:
        subprocess.run(['ruby', '--version'], capture_output=True, check=True)
    except (subprocess.SubprocessError, FileNotFoundError):
        logger.warning("Ruby interpreter not found, falling back to basic Ruby validation")
        return validate_ruby_basic(content, file_path)
    
    # Create a temporary file
    with tempfile.NamedTemporaryFile(suffix='.rb', delete=False) as tmp:
        tmp_path = tmp.name
        tmp.write(content.encode('utf-8'))
    
    try:
        # Use ruby -c to check syntax
        result = subprocess.run(
            ['ruby', '-c', tmp_path],
            capture_output=True,
            text=True
        )
        
        if result.returncode != 0:
            # Parse the error message
            error_msg = result.stderr
            return False, f"Ruby syntax error: {error_msg.strip()}"
        
        return True, ""
    
    except Exception as e:
        logger.error(f"Error validating Ruby code: {str(e)}")
        return False, f"Error validating Ruby code: {str(e)}"
    
    finally:
        # Clean up the temporary file
        try:
            os.unlink(tmp_path)
        except Exception:
            pass

def validate_ruby_basic(content: str, file_path: str) -> Tuple[bool, str]:
    """
    Basic validation for Ruby code without using ruby interpreter.
    
    Args:
        content: Ruby code to validate
        file_path: Path of the file
        
    Returns:
        Tuple of (is_valid, error_message)
    """
    # Check for basic syntax issues
    
    # Check for unmatched brackets
    bracket_pairs = [('(', ')'), ('[', ']'), ('{', '}')]
    
    for opening, closing in bracket_pairs:
        if content.count(opening) != content.count(closing):
            return False, f"Unmatched brackets: {opening}{closing}"
    
    # Check for unmatched 'do' blocks
    do_count = len(re.findall(r'\bdo\b(?:\s*\|.*?\|)?', content))
    end_count = len(re.findall(r'\bend\b', content))
    
    if do_count != end_count:
        return False, f"Unmatched 'do' and 'end' blocks: {do_count} 'do' vs {end_count} 'end'"
    
    return True, ""

def validate_rust(content: str, file_path: str) -> Tuple[bool, str]:
    """
    Validate Rust code.
    
    Args:
        content: Rust code to validate
        file_path: Path of the file
        
    Returns:
        Tuple of (is_valid, error_message)
    """
    # Check if rustc is available
    try:
        subprocess.run(['rustc', '--version'], capture_output=True, check=True)
    except (subprocess.SubprocessError, FileNotFoundError):
        logger.warning("Rust compiler not found, falling back to basic Rust validation")
        return validate_rust_basic(content, file_path)
    
    # Create a temporary file
    with tempfile.NamedTemporaryFile(suffix='.rs', delete=False) as tmp:
        tmp_path = tmp.name
        tmp.write(content.encode('utf-8'))
    
    try:
        # Use rustc to check syntax (with --emit=metadata to avoid producing binaries)
        result = subprocess.run(
            ['rustc', '--emit=metadata', tmp_path],
            capture_output=True,
            text=True
        )
        
        if result.returncode != 0:
            # Parse the error message
            error_msg = result.stderr
            return False, f"Rust syntax error: {error_msg.strip()}"
        
        return True, ""
    
    except Exception as e:
        logger.error(f"Error validating Rust code: {str(e)}")
        return False, f"Error validating Rust code: {str(e)}"
    
    finally:
        # Clean up the temporary files
        try:
            os.unlink(tmp_path)
            # Also try to remove any generated metadata files
            metadata_path = tmp_path.replace('.rs', '')
            if os.path.exists(metadata_path):
                os.unlink(metadata_path)
        except Exception:
            pass

def validate_rust_basic(content: str, file_path: str) -> Tuple[bool, str]:
    """
    Basic validation for Rust code without using rustc.
    
    Args:
        content: Rust code to validate
        file_path: Path of the file
        
    Returns:
        Tuple of (is_valid, error_message)
    """
    # Check for basic syntax issues
    
    # Check for unmatched brackets
    bracket_pairs = [('(', ')'), ('[', ']'), ('{', '}')]
    
    for opening, closing in bracket_pairs:
        if content.count(opening) != content.count(closing):
            return False, f"Unmatched brackets: {opening}{closing}"
    
    return True, ""

def validate_html(content: str, file_path: str) -> Tuple[bool, str]:
    """
    Validate HTML code.
    
    Args:
        content: HTML code to validate
        file_path: Path of the file
        
    Returns:
        Tuple of (is_valid, error_message)
    """
    # Basic HTML validation without external tools
    
    # Check for DOCTYPE declaration
    if not re.search(r'<!DOCTYPE\s+html>', content, re.IGNORECASE):
        logger.warning("HTML file missing DOCTYPE declaration")
    
    # Check for basic required elements
    if not re.search(r'<html', content, re.IGNORECASE):
        return False, "Missing <html> element"
    
    if not re.search(r'<head', content, re.IGNORECASE):
        return False, "Missing <head> element"
    
    if not re.search(r'<body', content, re.IGNORECASE):
        return False, "Missing <body> element"
    
    # Check for unmatched tags (simplified)
    html_tags = re.findall(r'<([a-zA-Z0-9]+)[^>]*>', content)
    void_elements = {'area', 'base', 'br', 'col', 'embed', 'hr', 'img', 'input', 
                     'link', 'meta', 'param', 'source', 'track', 'wbr'}
    
    tag_stack = []
    
    for tag in html_tags:
        tag_lower = tag.lower()
        if tag_lower not in void_elements:
            tag_stack.append(tag_lower)
    
    closing_tags = re.findall(r'</([a-zA-Z0-9]+)>', content)
    
    for tag in closing_tags:
        tag_lower = tag.lower()
        if tag_stack and tag_stack[-1] == tag_lower:
            tag_stack.pop()
        else:
            return False, f"Unmatched closing tag: </
            
            
def validate_css(content: str, file_path: str) -> Tuple[bool, str]:
    """
    Validate CSS code.
    
    Args:
        content: CSS code to validate
        file_path: Path of the file
        
    Returns:
        Tuple of (is_valid, error_message)
    """
    # Basic CSS validation without external tools
    
    # Check for unmatched brackets
    if content.count('{') != content.count('}'):
        return False, "Unmatched brackets in CSS"
    
    # Check for missing semicolons in property declarations
    lines = content.splitlines()
    in_rule_block = False
    
    for i, line in enumerate(lines):
        line = line.strip()
        
        if not line or line.startswith('/*') or line.endswith('*/'):
            continue
        
        if '{' in line:
            in_rule_block = True
            continue
            
        if '}' in line:
            in_rule_block = False
            continue
        
        if in_rule_block and ':' in line and not line.endswith(';') and not line.endswith('{'):
            # This might be a property without a semicolon
            # Check if it's not the last property in a block
            next_line_idx = i + 1
            while next_line_idx < len(lines) and not lines[next_line_idx].strip():
                next_line_idx += 1
                
            if next_line_idx < len(lines) and not lines[next_line_idx].strip().startswith('}'):
                return False, f"Missing semicolon at line {i+1}: {line}"
    
    return True, ""

def validate_php(content: str, file_path: str) -> Tuple[bool, str]:
    """
    Validate PHP code.
    
    Args:
        content: PHP code to validate
        file_path: Path of the file
        
    Returns:
        Tuple of (is_valid, error_message)
    """
    # Check if php is available
    try:
        subprocess.run(['php', '--version'], capture_output=True, check=True)
    except (subprocess.SubprocessError, FileNotFoundError):
        logger.warning("PHP interpreter not found, falling back to basic PHP validation")
        return validate_php_basic(content, file_path)
    
    # Create a temporary file
    with tempfile.NamedTemporaryFile(suffix='.php', delete=False) as tmp:
        tmp_path = tmp.name
        tmp.write(content.encode('utf-8'))
    
    try:
        # Use php -l to check syntax
        result = subprocess.run(
            ['php', '-l', tmp_path],
            capture_output=True,
            text=True
        )
        
        if result.returncode != 0:
            # Parse the error message
            error_msg = result.stderr or result.stdout
            return False, f"PHP syntax error: {error_msg.strip()}"
        
        return True, ""
    
    except Exception as e:
        logger.error(f"Error validating PHP code: {str(e)}")
        return False, f"Error validating PHP code: {str(e)}"
    
    finally:
        # Clean up the temporary file
        try:
            os.unlink(tmp_path)
        except Exception:
            pass

def validate_php_basic(content: str, file_path: str) -> Tuple[bool, str]:
    """
    Basic validation for PHP code without using php interpreter.
    
    Args:
        content: PHP code to validate
        file_path: Path of the file
        
    Returns:
        Tuple of (is_valid, error_message)
    """
    # Check for basic syntax issues
    
    # Check for PHP opening tag
    if not re.search(r'<\?php', content):
        return False, "Missing PHP opening tag (<?php)"
    
    # Check for unmatched brackets
    bracket_pairs = [('(', ')'), ('[', ']'), ('{', '}')]
    
    for opening, closing in bracket_pairs:
        if content.count(opening) != content.count(closing):
            return False, f"Unmatched brackets: {opening}{closing}"
    
    return True, ""
</file>

<file path="angela/integrations/integrations5.py">
# angela/integrations.py

import asyncio
from typing import Dict, Any, List, Optional, Set
from pathlib import Path

from angela.utils.logging import get_logger
from angela.context import context_manager
from angela.context.project_inference import project_inference
from angela.intent.advanced_planner import advanced_task_planner
from angela.ai.content_analyzer import content_analyzer
from angela.monitoring.network_monitor import network_monitor
from angela.workflows.sharing import workflow_sharing_manager
from angela.execution.error_recovery import ErrorRecoveryManager

logger = get_logger(__name__)

class PhaseIntegration:
    """
    Integration module for Phase 5.5 features.
    
    This class provides:
    1. Initialization and setup of Phase 5.5 features
    2. Integration between different components
    3. Helper methods for the orchestrator
    4. Status reporting
    """
    
    def __init__(self):
        """Initialize the integration module."""
        self._logger = logger
        self._error_recovery = ErrorRecoveryManager()
        self._features_enabled = {}
    
    async def initialize(self, config: Dict[str, Any] = None) -> Dict[str, Any]:
        """
        Initialize Phase 5.5 features.
        
        Args:
            config: Optional configuration dictionary
            
        Returns:
            Status of initialization
        """
        config = config or {}
        results = {}
        
        # Initialize project inference
        if config.get("enable_project_inference", True):
            try:
                project_root = context_manager.project_root
                if project_root:
                    project_info = await project_inference.infer_project_info(project_root)
                    results["project_inference"] = {
                        "status": "initialized",
                        "project_type": project_info.get("project_type", "unknown")
                    }
                    self._features_enabled["project_inference"] = True
                else:
                    results["project_inference"] = {
                        "status": "disabled",
                        "reason": "No project root detected"
                    }
            except Exception as e:
                self._logger.error(f"Error initializing project inference: {str(e)}")
                results["project_inference"] = {
                    "status": "error",
                    "error": str(e)
                }
        
        # Initialize network monitoring
        if config.get("enable_network_monitoring", False):
            try:
                network_monitor.start_monitoring()
                results["network_monitoring"] = {
                    "status": "started"
                }
                self._features_enabled["network_monitoring"] = True
            except Exception as e:
                self._logger.error(f"Error starting network monitoring: {str(e)}")
                results["network_monitoring"] = {
                    "status": "error",
                    "error": str(e)
                }
        
        # Log the initialization results
        self._logger.info(f"Phase 5.5 features initialized: {', '.join(k for k, v in self._features_enabled.items() if v)}")
        
        return results
    
    async def get_enhanced_context(self) -> Dict[str, Any]:
        """
        Get enhanced context information for AI prompts.
        
        Returns:
            Enhanced context dictionary
        """
        context = {}
        
        # Add project inference data if available
        if self._features_enabled.get("project_inference"):
            project_root = context_manager.project_root
            if project_root:
                try:
                    project_info = await project_inference.infer_project_info(project_root)
                    context["project_info"] = project_info
                except Exception as e:
                    self._logger.error(f"Error getting project information: {str(e)}")
        
        # Add network status if available
        if self._features_enabled.get("network_monitoring"):
            try:
                # This is a placeholder - in a real implementation, you would
                # get the actual network status from the network monitor
                context["network_status"] = {
                    "internet_connected": True,
                    "local_services": {}
                }
            except Exception as e:
                self._logger.error(f"Error getting network status: {str(e)}")
        
        return context
    
    async def handle_execution_error(
        self, 
        step: Any, 
        error_result: Dict[str, Any], 
        context: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Handle execution errors with recovery.
        
        Args:
            step: The step that failed
            error_result: The execution result with error information
            context: Context information
            
        Returns:
            Updated execution result
        """
        return await self._error_recovery.handle_error(step, error_result, context)
    
    async def analyze_content(
        self, 
        file_path: Path, 
        request: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Analyze file content with enhanced capabilities.
        
        Args:
            file_path: Path to the file to analyze
            request: Optional specific analysis request
            
        Returns:
            Analysis results
        """
        # Get project context for better analysis
        enhanced_context = {}
        if self._features_enabled.get("project_inference"):
            project_root = context_manager.project_root
            if project_root:
                try:
                    project_info = await project_inference.infer_project_info(project_root)
                    enhanced_context["project_type"] = project_info.get("project_type", "unknown")
                    enhanced_context["frameworks"] = project_info.get("detected_frameworks", {})
                except Exception as e:
                    self._logger.error(f"Error getting project information: {str(e)}")
        
        # Use the enhanced content analyzer
        from angela.ai.content_analyzer_extensions import enhanced_content_analyzer
        return await enhanced_content_analyzer.analyze_content(file_path, request)
    
    async def status(self) -> Dict[str, Any]:
        """
        Get status of Phase 5.5 features.
        
        Returns:
            Status dictionary
        """
        status = {
            "enabled_features": {k: v for k, v in self._features_enabled.items() if v},
            "phase": "5.5",
            "description": "Autonomous Task Orchestration & Proactive Assistance"
        }
        
        # Add project information if available
        if self._features_enabled.get("project_inference"):
            project_root = context_manager.project_root
            if project_root:
                try:
                    project_info = await project_inference.infer_project_info(project_root)
                    status["project"] = {
                        "type": project_info.get("project_type", "unknown"),
                        "frameworks": list(project_info.get("detected_frameworks", {}).keys()),
                        "dependencies_count": len(project_info.get("dependencies", []))
                    }
                except Exception as e:
                    self._logger.error(f"Error getting project status: {str(e)}")
        
        # Add network status if available
        if self._features_enabled.get("network_monitoring"):
            try:
                # This is a placeholder - in a real implementation, you would
                # get actual network monitor statistics
                status["network_monitoring"] = {
                    "status": "active",
                    "services_monitored": 0,
                    "dependency_updates": 0
                }
            except Exception as e:
                self._logger.error(f"Error getting network status: {str(e)}")
        
        return status

# Global integration instance
phase_integration = PhaseIntegration()
</file>

<file path="angela/integrations/integrations6.py">
"""
Phase 6 Integration for Enhanced Project Context.

This file provides the necessary integration points for all Phase 6 components.
It should be used to update the existing code in the Angela CLI project.
"""

# Import statements to add to the beginning of orchestrator.py
IMPORT_STATEMENTS = """
from angela.context.enhancer import context_enhancer
from angela.context.file_resolver import file_resolver
from angela.context.file_activity import file_activity_tracker, ActivityType
from angela.execution.hooks import execution_hooks
"""

# Updated process_request method for Orchestrator class
PROCESS_REQUEST_METHOD = """
async def process_request(
    self, 
    request: str, 
    execute: bool = True,
    dry_run: bool = False
) -> Dict[str, Any]:
    '''
    Process a request from the user with enhanced context.
    
    Args:
        request: The user request
        execute: Whether to execute commands
        dry_run: Whether to simulate execution without making changes
        
    Returns:
        Dictionary with processing results
    '''
    # Refresh context to ensure we have the latest information
    context_manager.refresh_context()
    context = context_manager.get_context_dict()
    
    # Add session context for continuity across requests
    session_context = session_manager.get_context()
    context["session"] = session_context
    
    # Enhance context with project information, dependencies, and recent activity
    context = await context_enhancer.enrich_context(context)
    
    self._logger.info(f"Processing request: {request}")
    self._logger.debug(f"Enhanced context with {len(context)} keys")
    
    # Extract and resolve file references
    file_references = await file_resolver.extract_references(request, context)
    if file_references:
        # Add resolved file references to context
        context["resolved_files"] = [
            {"reference": ref, "path": str(path) if path else None}
            for ref, path in file_references
        ]
        self._logger.debug(f"Resolved {len(file_references)} file references")
    
    try:
        # Analyze the request to determine its type
        request_type = await self._determine_request_type(request, context)
        self._logger.info(f"Determined request type: {request_type.value}")
        
        # Process the request based on its type
        if request_type == RequestType.COMMAND:
            # Handle single command request
            return await self._process_command_request(request, context, execute, dry_run)
            
        elif request_type == RequestType.MULTI_STEP:
            # Handle multi-step operation
            return await self._process_multi_step_request(request, context, execute, dry_run)
            
        elif request_type == RequestType.FILE_CONTENT:
            # Handle file content analysis/manipulation
            return await self._process_file_content_request(request, context, execute, dry_run)
            
        elif request_type == RequestType.WORKFLOW_DEFINITION:
            # Handle workflow definition
            return await self._process_workflow_definition(request, context)
            
        elif request_type == RequestType.WORKFLOW_EXECUTION:
            # Handle workflow execution
            return await self._process_workflow_execution(request, context, execute, dry_run)
            
        elif request_type == RequestType.CLARIFICATION:
            # Handle request for clarification
            return await self._process_clarification_request(request, context)
            
        else:
            # Handle unknown request type
            return await self._process_unknown_request(request, context)
        
    except Exception as e:
        self._logger.exception(f"Error processing request: {str(e)}")
        # Fallback behavior
        return {
            "request": request,
            "response": f"An error occurred while processing your request: {str(e)}",
            "error": str(e),
            "context": context,
        }
"""

# Updated _extract_file_path method for Orchestrator class
EXTRACT_FILE_PATH_METHOD = """
async def _extract_file_path(
    self, 
    request: str, 
    context: Dict[str, Any]
) -> Optional[Path]:
    '''
    Extract a file path from a request using file_resolver.
    
    Args:
        request: The user request
        context: Context information
        
    Returns:
        Path object if found, None otherwise
    '''
    self._logger.debug(f"Extracting file path from: {request}")
    
    # Try to extract file references
    file_references = await file_resolver.extract_references(request, context)
    
    # If we found any resolved references, return the first one
    for reference, path in file_references:
        if path:
            # Track as viewed file
            file_activity_tracker.track_file_viewing(path, None, {
                "request": request,
                "reference": reference
            })
            return path
    
    # If we found references but couldn't resolve them, use AI extraction as fallback
    if file_references:
        for reference, _ in file_references:
            # Try to resolve with a broader scope
            path = await file_resolver.resolve_reference(
                reference, 
                context,
                search_scope="project"
            )
            if path:
                # Track as viewed file
                file_activity_tracker.track_file_viewing(path, None, {
                    "request": request,
                    "reference": reference
                })
                return path
    
    # If all else fails, fall back to the original AI method
    prompt = f'''
Extract the most likely file path from this user request:
"{request}"

Current working directory: {context["cwd"]}
Project root (if any): {context.get("project_root", "None")}

Return the most likely file path as just a single word, with no additional explanation or context.
'''
    
    api_request = GeminiRequest(prompt=prompt, max_tokens=100)
    response = await gemini_client.generate_text(api_request)
    
    file_name = response.text.strip()
    
    # Remove quotes if present
    if file_name.startswith('"') and file_name.endswith('"'):
        file_name = file_name[1:-1]
    if file_name.startswith("'") and file_name.endswith("'"):
        file_name = file_name[1:-1]
    
    # Check if this is a valid path
    path = Path(file_name)
    if not path.is_absolute():
        # Check in current directory
        cwd_path = Path(context["cwd"]) / path
        if cwd_path.exists():
            return cwd_path
        
        # Check in project root if available
        if context.get("project_root"):
            proj_path = Path(context["project_root"]) / path
            if proj_path.exists():
                return proj_path
    else:
        # Absolute path
        if path.exists():
            return path
    
    # No valid path found
    return None
"""

# Updates to execution methods to add hooks
EXECUTE_COMMAND_METHOD = """
async def execute_command(
    self, 
    command: str,
    natural_request: str,
    explanation: Optional[str] = None,
    dry_run: bool = False
) -> Dict[str, Any]:
    '''
    Execute a command with adaptive behavior based on user context.
    
    Args:
        command: The command to execute
        natural_request: The original natural language request
        explanation: AI explanation of what the command does
        dry_run: Whether to simulate the command without execution
        
    Returns:
        Dictionary with execution results
    '''
    self._logger.info(f"Preparing to execute command: {command}")
    
    # Get current context for hooks
    context = context_manager.get_context_dict()
    
    # Call pre-execution hook
    await execution_hooks.pre_execute_command(command, context)
    
    # Analyze command risk and impact
    risk_level, risk_reason = classify_command_risk(command)
    impact = analyze_command_impact(command)
    
    # Add to session context
    session_manager.add_command(command)
    
    # Generate command preview if needed
    from angela.safety.preview import generate_preview
    preview = await generate_preview(command) if preferences_manager.preferences.ui.show_command_preview else None
    
    # Get adaptive confirmation based on risk level and user history
    confirmed = await get_adaptive_confirmation(
        command=command,
        risk_level=risk_level,
        risk_reason=risk_reason,
        impact=impact,
        preview=preview,
        explanation=explanation,
        natural_request=natural_request,
        dry_run=dry_run
    )
    
    if not confirmed and not dry_run:
        self._logger.info(f"Command execution cancelled by user: {command}")
        return {
            "command": command,
            "success": False,
            "cancelled": True,
            "stdout": "",
            "stderr": "Command execution cancelled by user",
            "return_code": 1,
            "dry_run": dry_run
        }
    
    # Execute the command
    result = await self._execute_with_feedback(command, dry_run)
    
    # Call post-execution hook
    await execution_hooks.post_execute_command(command, result, context)
    
    # Add to history
    history_manager.add_command(
        command=command,
        natural_request=natural_request,
        success=result["success"],
        output=result.get("stdout", ""),
        error=result.get("stderr", ""),
        risk_level=risk_level
    )
    
    # If execution failed, analyze error and suggest fixes
    if not result["success"] and result.get("stderr"):
        result["error_analysis"] = error_analyzer.analyze_error(command, result["stderr"])
        result["fix_suggestions"] = error_analyzer.generate_fix_suggestions(command, result["stderr"])
    
    # Offer to learn from successful executions
    if result["success"] and risk_level > 0:
        from angela.safety.adaptive_confirmation import offer_command_learning
        await offer_command_learning(command)
    
    return result
"""

# Integration steps for Phase 6
INTEGRATION_STEPS = """
To integrate Phase 6 components:

1. Add the new Python files to the project:
   - angela/context/enhancer.py
   - angela/context/file_resolver.py
   - angela/context/file_activity.py
   - angela/execution/hooks.py

2. Update orchestrator.py:
   - Add the import statements at the top
   - Replace process_request method with the updated version
   - Replace _extract_file_path method with the updated version
   - Update execute_command method in adaptive_engine.py with the updated version

3. Update ai/prompts.py:
   - Add the new prompt templates
   - Replace build_prompt function with the updated version
   - Replace build_file_operation_prompt function with the updated version

4. Update execution/engine.py and execution/adaptive_engine.py:
   - Add hooks integration to execution methods

5. Test all components:
   - Test project inference
   - Test file resolution
   - Test file activity tracking
   - Test enhanced prompts

6. Update documentation to reflect the new capabilities
"""

# Installation and usage instructions
INSTALLATION_INSTRUCTIONS = """
# Phase 6: Enhanced Project Context

## Installation

1. Copy the new Python files to their respective directories:
   ```
   cp angela/context/enhancer.py /path/to/angela/context/
   cp angela/context/file_resolver.py /path/to/angela/context/
   cp angela/context/file_activity.py /path/to/angela/context/
   cp angela/execution/hooks.py /path/to/angela/execution/
   ```

2. Update the existing files with the provided code snippets.

3. Install additional dependencies if needed:
   ```
   pip install difflib
   ```

## Usage

The enhanced project context capabilities are automatically used by the system.
When you make a request, Angela will:

1. Detect project type and dependencies
2. Resolve file references in your natural language query
3. Track file activities
4. Use all this information to provide more contextually relevant responses

Example usage:
```
angela "Find all functions in the main file that handle user input"
```

In this example, Angela will:
- Infer what "main file" means in your project
- Find relevant functions
- Track this file viewing in its history
- Use project context to understand what "user input" means in your specific project type
```
</file>

<file path="angela/intent/advanced_planner.py">
# angela/intent/advanced_planner.py

import re
import json
import shlex
from typing import Dict, Any, List, Tuple, Optional, Set, Union
from pathlib import Path
from enum import Enum
from datetime import datetime

from pydantic import BaseModel, Field

from angela.intent.models import ActionPlan, Intent, IntentType
from angela.intent.planner import TaskPlan, PlanStep, task_planner
from angela.ai.client import gemini_client, GeminiRequest
from angela.context import context_manager
from angela.utils.logging import get_logger

logger = get_logger(__name__)

class PlanStepType(str, Enum):
    """Types of plan steps."""
    COMMAND = "command"  # Shell command
    CODE = "code"        # Code to execute or save
    FILE = "file"        # File operation
    DECISION = "decision"  # Decision point, may branch execution
    API = "api"          # API call
    LOOP = "loop"        # Looping construct

class AdvancedPlanStep(BaseModel):
    """Model for an advanced plan step with additional capabilities."""
    id: str = Field(..., description="Unique identifier for this step")
    type: PlanStepType = Field(..., description="Type of step")
    description: str = Field(..., description="Human-readable description")
    command: Optional[str] = Field(None, description="Command to execute (for command type)")
    code: Optional[str] = Field(None, description="Code to execute or save (for code type)")
    file_path: Optional[str] = Field(None, description="Path for file operations (for file type)")
    file_content: Optional[str] = Field(None, description="Content for file operations (for file type)")
    condition: Optional[str] = Field(None, description="Condition for decision steps (for decision type)")
    true_branch: Optional[List[str]] = Field(None, description="Steps to execute if condition is true")
    false_branch: Optional[List[str]] = Field(None, description="Steps to execute if condition is false")
    api_url: Optional[str] = Field(None, description="URL for API calls (for api type)")
    api_method: Optional[str] = Field(None, description="HTTP method for API calls (for api type)")
    api_payload: Optional[Dict[str, Any]] = Field(None, description="Payload for API calls (for api type)")
    loop_items: Optional[str] = Field(None, description="Items to loop over (for loop type)")
    loop_body: Optional[List[str]] = Field(None, description="Steps to execute in loop (for loop type)")
    dependencies: List[str] = Field(default_factory=list, description="IDs of steps this step depends on")
    estimated_risk: int = Field(0, description="Estimated risk level (0-4)")
    timeout: Optional[int] = Field(None, description="Timeout in seconds")
    retry: Optional[int] = Field(None, description="Number of retries on failure")
    tags: List[str] = Field(default_factory=list, description="Tags for categorization")

class AdvancedTaskPlan(BaseModel):
    """Model for an advanced task plan with branching and complex steps."""
    id: str = Field(..., description="Unique identifier for this plan")
    goal: str = Field(..., description="The original high-level goal")
    description: str = Field(..., description="Detailed description of the plan")
    steps: Dict[str, AdvancedPlanStep] = Field(..., description="Steps to achieve the goal, indexed by ID")
    entry_points: List[str] = Field(..., description="Step IDs to start execution with")
    context: Dict[str, Any] = Field(default_factory=dict, description="Context information")
    created: datetime = Field(default_factory=datetime.now, description="When the plan was created")

class AdvancedTaskPlanner:
    """
    Advanced task planner for complex goal decomposition.
    
    This class extends the basic task planner with:
    1. Branching execution paths
    2. Multiple types of steps (commands, code, file operations)
    3. Conditional execution
    4. Looping constructs
    5. Decision points
    6. Error recovery strategies
    """
    
    def __init__(self):
        """Initialize the advanced task planner."""
        self._logger = logger
        self._basic_planner = task_planner
    
    async def plan_task(
        self, 
        goal: str, 
        context: Dict[str, Any],
        complexity: str = "auto"
    ) -> Union[TaskPlan, AdvancedTaskPlan]:
        """
        Plan a complex task by breaking it down into actionable steps.
        
        Args:
            goal: The high-level goal description
            context: Context information
            complexity: Planning complexity level ("simple", "advanced", or "auto")
            
        Returns:
            A TaskPlan or AdvancedTaskPlan with steps to achieve the goal
        """
        self._logger.info(f"Planning task: {goal} (complexity: {complexity})")
        
        # Determine planning complexity if auto
        if complexity == "auto":
            complexity = await self._determine_complexity(goal)
            self._logger.info(f"Determined complexity: {complexity}")
        
        # Use the appropriate planning strategy
        if complexity == "simple":
            # Use the basic planner for simple tasks
            return await self._basic_planner.plan_task(goal, context)
        else:
            # Use advanced planning for complex tasks
            return await self._generate_advanced_plan(goal, context)
    
    async def _determine_complexity(self, goal: str) -> str:
        """
        Determine the appropriate planning complexity for a goal.
        
        Args:
            goal: The high-level goal
            
        Returns:
            Complexity level ("simple" or "advanced")
        """
        # Simple heuristics based on goal text
        complexity_indicators = [
            "if", "when", "based on", "for each", "foreach", "loop", "iterate",
            "depending on", "decision", "alternative", "otherwise", "create file",
            "write to file", "dynamic", "api", "request", "conditionally",
            "advanced", "complex", "multiple steps", "error handling"
        ]
        
        # Count indicators
        indicator_count = sum(1 for indicator in complexity_indicators 
                              if indicator in goal.lower())
        
        # Check goal length and complexity indicators
        if len(goal.split()) > 20 or indicator_count >= 2:
            return "advanced"
        else:
            return "simple"
    
    async def _generate_advanced_plan(self, goal: str, context: Dict[str, Any]) -> AdvancedTaskPlan:
        """
        Generate an advanced plan using the AI service.
        
        Args:
            goal: The high-level goal
            context: Context information
            
        Returns:
            An AdvancedTaskPlan with steps to achieve the goal
        """
        # Build a planning prompt for advanced planning
        prompt = self._build_advanced_planning_prompt(goal, context)
        
        # Call the AI service
        api_request = GeminiRequest(prompt=prompt, max_tokens=4000)
        api_response = await gemini_client.generate_text(api_request)
        
        # Parse the plan from the response
        plan = self._parse_advanced_plan_response(api_response.text, goal, context)
        
        return plan
    
    def _build_advanced_planning_prompt(self, goal: str, context: Dict[str, Any]) -> str:
        """
        Build a prompt for generating an advanced plan.
        
        Args:
            goal: The high-level goal
            context: Context information
            
        Returns:
            A prompt string for the AI service
        """
        # Create context string
        context_str = "Current context:\n"
        
        if context.get("cwd"):
            context_str += f"- Current working directory: {context['cwd']}\n"
        if context.get("project_root"):
            context_str += f"- Project root: {context['project_root']}\n"
        if context.get("project_type"):
            context_str += f"- Project type: {context['project_type']}\n"
        
        # Add files in current directory for context
        if context.get("cwd"):
            try:
                dir_contents = context_manager.get_directory_contents(Path(context["cwd"]))
                files_str = "\n".join([f"- {item['name']}" for item in dir_contents])
                context_str += f"\nFiles in current directory:\n{files_str}\n"
            except Exception as e:
                self._logger.error(f"Error getting directory contents: {str(e)}")
        
        # Build the prompt
        prompt = f"""
You are Angela, an AI terminal assistant with advanced planning capabilities. Your task is to create a detailed, sophisticated plan for achieving the following complex goal:

GOAL: {goal}

{context_str}

This is a complex goal that may require branching, conditions, loops, or other advanced constructs.

Break down this goal into a comprehensive plan with these advanced features:
1. Different types of steps (commands, code, file operations, API calls, decisions, loops)
2. Branching execution paths based on conditions
3. Dependencies between steps
4. Error recovery strategies
5. Risk assessment for each step

Format your response as JSON:
{{
  "id": "generate a unique plan ID",
  "goal": "the original goal",
  "description": "detailed plan description",
  "steps": {{
    "step1": {{
      "id": "step1",
      "type": "command",
      "description": "Description of step 1",
      "command": "command to execute",
      "dependencies": [],
      "estimated_risk": 1
    }},
    "step2": {{
      "id": "step2",
      "type": "file",
      "description": "Create a file",
      "file_path": "/path/to/file",
      "file_content": "content to write",
      "dependencies": ["step1"],
      "estimated_risk": 2
    }},
    "step3": {{
      "id": "step3",
      "type": "decision",
      "description": "Check if a condition is met",
      "condition": "test condition",
      "true_branch": ["step4a"],
      "false_branch": ["step4b"],
      "dependencies": ["step2"],
      "estimated_risk": 0
    }},
    "step4a": {{
      "id": "step4a",
      "type": "command",
      "description": "Executed if condition is true",
      "command": "command to execute",
      "dependencies": ["step3"],
      "estimated_risk": 1
    }},
    "step4b": {{
      "id": "step4b",
      "type": "command",
      "description": "Executed if condition is false",
      "command": "command to execute",
      "dependencies": ["step3"],
      "estimated_risk": 1
    }},
    "step5": {{
      "id": "step5",
      "type": "loop",
      "description": "Process each item",
      "loop_items": "items to process",
      "loop_body": ["step6"],
      "dependencies": ["step4a", "step4b"],
      "estimated_risk": 2
    }},
    "step6": {{
      "id": "step6",
      "type": "code",
      "description": "Execute some code",
      "code": "code to execute",
      "dependencies": [],
      "estimated_risk": 1
    }}
  }},
  "entry_points": ["step1"]
}}

Ensure each step has a unique ID and clear dependencies. Entry points are the steps that should be executed first.
"""
        
        return prompt
    
    def _parse_advanced_plan_response(
        self, 
        response: str, 
        goal: str, 
        context: Dict[str, Any]
    ) -> AdvancedTaskPlan:
        """
        Parse the AI response into an AdvancedTaskPlan.
        
        Args:
            response: The AI response text
            goal: The original high-level goal
            context: Context information
            
        Returns:
            An AdvancedTaskPlan object
        """
        try:
            # Extract JSON from the response
            import json
            import re
            
            # Find JSON content
            json_match = re.search(r'```(?:json)?\s*(.*?)\s*```', response, re.DOTALL)
            if json_match:
                json_str = json_match.group(1)
            else:
                # Try to find JSON without code blocks
                json_match = re.search(r'({.*})', response, re.DOTALL)
                if json_match:
                    json_str = json_match.group(1)
                else:
                    # Assume the entire response is JSON
                    json_str = response
            
            # Parse the JSON
            plan_data = json.loads(json_str)
            
            # Create an AdvancedTaskPlan object
            return AdvancedTaskPlan(
                id=plan_data.get("id", f"plan_{datetime.now().strftime('%Y%m%d%H%M%S')}"),
                goal=goal,
                description=plan_data.get("description", "Advanced plan for " + goal),
                steps=plan_data["steps"],
                entry_points=plan_data.get("entry_points", [next(iter(plan_data["steps"].keys()))]),
                context=context,
                created=datetime.now()
            )
        
        except Exception as e:
            self._logger.exception(f"Error parsing advanced plan response: {str(e)}")
            # Create a fallback plan
            fallback_step_id = "fallback_step"
            return AdvancedTaskPlan(
                id=f"fallback_plan_{datetime.now().strftime('%Y%m%d%H%M%S')}",
                goal=goal,
                description=f"Fallback plan for: {goal}",
                steps={
                    fallback_step_id: AdvancedPlanStep(
                        id=fallback_step_id,
                        type=PlanStepType.COMMAND,
                        description="Fallback step due to planning error",
                        command=f"echo 'Unable to create detailed plan for: {goal}'",
                        dependencies=[],
                        estimated_risk=0
                    )
                },
                entry_points=[fallback_step_id],
                context=context
            )
    
    async def execute_plan(
        self, 
        plan: AdvancedTaskPlan,
        dry_run: bool = False
    ) -> Dict[str, Any]:
        """
        Execute an advanced task plan.
        
        Args:
            plan: The advanced task plan to execute
            dry_run: Whether to simulate execution without making changes
            
        Returns:
            Dictionary with execution results
        """
        self._logger.info(f"Executing advanced plan for goal: {plan.goal}")
        
        # Track executed steps and results
        executed_steps = set()
        pending_steps = set(plan.entry_points)
        results = {}
        
        while pending_steps:
            # Get next step to execute
            next_step_id = self._select_next_step(plan, pending_steps, executed_steps)
            if not next_step_id:
                break  # No more executable steps
            
            # Get the step
            step = plan.steps[next_step_id]
            
            # Execute the step
            self._logger.info(f"Executing step {next_step_id}: {step.description}")
            result = await self._execute_step(step, results, dry_run)
            
            # Store the result
            results[next_step_id] = result
            
            # Mark step as executed
            executed_steps.add(next_step_id)
            pending_steps.remove(next_step_id)
            
            # Add dependent steps to pending
            if result.get("success", False) or step.type == PlanStepType.DECISION:
                self._update_pending_steps(plan, step, result, pending_steps, executed_steps)
        
        return {
            "plan_id": plan.id,
            "goal": plan.goal,
            "steps_executed": len(executed_steps),
            "steps_total": len(plan.steps),
            "results": results,
            "success": all(results.get(step_id, {}).get("success", False) for step_id in executed_steps),
            "dry_run": dry_run
        }
    
    def _select_next_step(
        self, 
        plan: AdvancedTaskPlan,
        pending_steps: Set[str],
        executed_steps: Set[str]
    ) -> Optional[str]:
        """
        Select the next step to execute.
        
        Args:
            plan: The plan
            pending_steps: Set of pending step IDs
            executed_steps: Set of executed step IDs
            
        Returns:
            ID of the next step to execute, or None if no steps are ready
        """
        for step_id in pending_steps:
            step = plan.steps[step_id]
            
            # Check if all dependencies are satisfied
            if all(dep in executed_steps for dep in step.dependencies):
                return step_id
        
        return None
    
    async def _execute_step(
        self, 
        step: AdvancedPlanStep,
        previous_results: Dict[str, Dict[str, Any]],
        dry_run: bool
    ) -> Dict[str, Any]:
        """
        Execute a single step of the plan.
        
        Args:
            step: The step to execute
            previous_results: Results of previously executed steps
            dry_run: Whether to simulate execution
            
        Returns:
            Dictionary with step execution results
        """
        # Import here to avoid circular imports
        from angela.execution.engine import execution_engine
        
        # Prepare base result
        result = {
            "step_id": step.id,
            "type": step.type,
            "description": step.description,
            "dry_run": dry_run
        }
        
        try:
            # Execute based on step type
            if step.type == PlanStepType.COMMAND:
                if step.command:
                    if dry_run:
                        # Simulate command execution
                        result["stdout"] = f"[DRY RUN] Would execute: {step.command}"
                        result["stderr"] = ""
                        result["return_code"] = 0
                        result["success"] = True
                    else:
                        # Execute the command
                        stdout, stderr, return_code = await execution_engine.execute_command(
                            step.command,
                            check_safety=True
                        )
                        result["stdout"] = stdout
                        result["stderr"] = stderr
                        result["return_code"] = return_code
                        result["success"] = return_code == 0
                else:
                    result["error"] = "Missing command for command step"
                    result["success"] = False
            
            elif step.type == PlanStepType.FILE:
                if step.file_path:
                    if dry_run:
                        # Simulate file operation
                        operation = "write" if step.file_content else "read"
                        result["message"] = f"[DRY RUN] Would {operation} file: {step.file_path}"
                        result["success"] = True
                    else:
                        # Execute file operation
                        if step.file_content:
                            # Write to file
                            await self._write_file(step.file_path, step.file_content)
                            result["message"] = f"Wrote content to {step.file_path}"
                            result["success"] = True
                        else:
                            # Read from file
                            content = await self._read_file(step.file_path)
                            result["content"] = content
                            result["success"] = True
                else:
                    result["error"] = "Missing file path for file step"
                    result["success"] = False
            
            elif step.type == PlanStepType.CODE:
                if step.code:
                    if dry_run:
                        # Simulate code execution
                        result["message"] = f"[DRY RUN] Would execute code: {len(step.code)} characters"
                        result["success"] = True
                    else:
                        # Execute the code
                        # This is a simplified implementation - in a real system,
                        # this would use a sandboxed execution environment
                        code_result = await self._execute_code(step.code)
                        result.update(code_result)
                else:
                    result["error"] = "Missing code for code step"
                    result["success"] = False
            
            elif step.type == PlanStepType.DECISION:
                if step.condition:
                    # Evaluate the condition
                    # This is a simplified implementation - in a real system,
                    # this would use a more sophisticated condition evaluation
                    condition_result = await self._evaluate_condition(
                        step.condition, previous_results, dry_run
                    )
                    result["condition"] = step.condition
                    result["condition_result"] = condition_result
                    result["next_branch"] = "true_branch" if condition_result else "false_branch"
                    result["success"] = True
                else:
                    result["error"] = "Missing condition for decision step"
                    result["success"] = False
            
            elif step.type == PlanStepType.API:
                if step.api_url and step.api_method:
                    if dry_run:
                        # Simulate API call
                        result["message"] = f"[DRY RUN] Would call API: {step.api_method} {step.api_url}"
                        result["success"] = True
                    else:
                        # Execute API call
                        api_result = await self._execute_api_call(
                            step.api_url, step.api_method, step.api_payload
                        )
                        result.update(api_result)
                else:
                    result["error"] = "Missing URL or method for API step"
                    result["success"] = False
            
            elif step.type == PlanStepType.LOOP:
                if step.loop_items and step.loop_body:
                    if dry_run:
                        # Simulate loop execution
                        result["message"] = f"[DRY RUN] Would loop over {step.loop_items}"
                        result["success"] = True
                    else:
                        # This is a placeholder for loop execution
                        # In a real system, this would execute the loop body for each item
                        result["message"] = f"Loop execution not implemented: {step.loop_items}"
                        result["success"] = True
                else:
                    result["error"] = "Missing items or body for loop step"
                    result["success"] = False
            
            else:
                result["error"] = f"Unknown step type: {step.type}"
                result["success"] = False
            
        except Exception as e:
            self._logger.exception(f"Error executing step {step.id}: {str(e)}")
            result["error"] = str(e)
            result["success"] = False
            
            # Handle retry if configured
            if step.retry and step.retry > 0:
                result["retry_count"] = 1
                result["retried"] = True
                
                # Attempt retries (simplified)
                for retry_num in range(1, step.retry + 1):
                    self._logger.info(f"Retrying step {step.id} (attempt {retry_num}/{step.retry})")
                    try:
                        # Wait before retrying
                        await asyncio.sleep(1)
                        
                        # Execute retry logic based on step type
                        # This is a simplified implementation
                        retry_result = await self._execute_step(step, previous_results, dry_run)
                        
                        if retry_result.get("success", False):
                            # Retry succeeded
                            retry_result["retry_count"] = retry_num
                            retry_result["retried"] = True
                            return retry_result
                    except Exception as retry_e:
                        self._logger.error(f"Error in retry {retry_num} for step {step.id}: {str(retry_e)}")
                
                # All retries failed
                result["retry_exhausted"] = True
        
        return result
    
    def _update_pending_steps(
        self, 
        plan: AdvancedTaskPlan,
        executed_step: AdvancedPlanStep,
        result: Dict[str, Any],
        pending_steps: Set[str],
        executed_steps: Set[str]
    ) -> None:
        """
        Update the set of pending steps based on execution result.
        
        Args:
            plan: The plan
            executed_step: The step that was just executed
            result: The execution result
            pending_steps: Set of pending step IDs to update
            executed_steps: Set of executed step IDs
        """
        # For decision steps, add the appropriate branch
        if executed_step.type == PlanStepType.DECISION:
            condition_result = result.get("condition_result", False)
            if condition_result and executed_step.true_branch:
                # Add steps from true branch
                for step_id in executed_step.true_branch:
                    if step_id not in executed_steps and step_id in plan.steps:
                        pending_steps.add(step_id)
            elif not condition_result and executed_step.false_branch:
                # Add steps from false branch
                for step_id in executed_step.false_branch:
                    if step_id not in executed_steps and step_id in plan.steps:
                        pending_steps.add(step_id)
        
        # For normal steps, add all steps that depend on this one
        for step_id, step in plan.steps.items():
            if executed_step.id in step.dependencies and step_id not in executed_steps:
                # Check if all dependencies are satisfied
                if all(dep in executed_steps for dep in step.dependencies):
                    pending_steps.add(step_id)
    
    async def _read_file(self, path: str) -> str:
        """Read content from a file."""
        from angela.execution.filesystem import read_file
        return await read_file(path)
    
    async def _write_file(self, path: str, content: str) -> bool:
        """Write content to a file."""
        from angela.execution.filesystem import write_file
        return await write_file(path, content)
    
    async def _execute_code(self, code: str) -> Dict[str, Any]:
        """
        Execute code (placeholder implementation).
        
        In a real system, this would use a sandboxed execution environment.
        """
        return {
            "message": f"Code execution not implemented: {len(code)} characters",
            "success": True
        }
    
    async def _evaluate_condition(
        self, 
        condition: str,
        previous_results: Dict[str, Dict[str, Any]],
        dry_run: bool
    ) -> bool:
        """
        Evaluate a condition (placeholder implementation).
        
        In a real system, this would use a more sophisticated condition evaluation.
        """
        import re
        
        # Look for simple patterns
        if re.search(r'file exists', condition, re.IGNORECASE):
            # Extract file path
            match = re.search(r'file exists[:\s]+([^\s]+)', condition, re.IGNORECASE)
            if match:
                file_path = match.group(1)
                return Path(file_path).exists()
        
        if re.search(r'command success', condition, re.IGNORECASE):
            # Extract step ID
            match = re.search(r'step[:\s]+([^\s]+)', condition, re.IGNORECASE)
            if match:
                step_id = match.group(1)
                return previous_results.get(step_id, {}).get("success", False)
        
        # Default behavior for dry run
        if dry_run:
            return True
        
        # Default for unknown conditions
        return False
    
    async def _execute_api_call(
        self, 
        url: str,
        method: str,
        payload: Optional[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """
        Execute an API call (placeholder implementation).
        
        In a real system, this would use a proper HTTP client.
        """
        return {
            "message": f"API call not implemented: {method} {url}",
            "success": True
        }

# Global advanced task planner instance
advanced_task_planner = AdvancedTaskPlanner()
</file>

<file path="angela/intent/models.py">
# angela/intent/models.py
from enum import Enum
from typing import Dict, Any, Optional, List

from pydantic import BaseModel, Field

class IntentType(str, Enum):
    """Enumeration of possible intent types."""
    UNKNOWN = "unknown"
    FILE_SEARCH = "file_search"
    DIRECTORY_LIST = "directory_list"
    FILE_VIEW = "file_view"
    SYSTEM_INFO = "system_info"
    NETWORK_INFO = "network_info"
    FILE_EDIT = "file_edit"  # For future phases
    FILE_CREATE = "file_create"  # For future phases
    GIT_OPERATION = "git_operation"  # For future phases
    DOCKER_OPERATION = "docker_operation"  # For future phases

class Intent(BaseModel):
    """Model for user intent."""
    type: IntentType = Field(..., description="The type of intent")
    entities: Dict[str, Any] = Field(default_factory=dict, description="Entities extracted from the request")
    original_request: str = Field(..., description="The original user request")

class ActionPlan(BaseModel):
    """Model for an action plan derived from intent."""
    intent: Intent = Field(..., description="The intent that led to this plan")
    commands: List[str] = Field(..., description="List of commands to execute")
    explanations: List[str] = Field(..., description="Explanations for each command")
    risk_level: int = Field(0, description="Risk level of the plan (0-4)")
</file>

<file path="angela/intent/planner.py">
"""
Task planning and goal decomposition for Angela CLI.

This module handles breaking down complex high-level goals into
executable action plans with dependencies and execution flow.
"""
import re
import shlex
from typing import Dict, Any, List, Tuple, Optional, Set
from pathlib import Path

from pydantic import BaseModel, Field

from angela.intent.models import ActionPlan, Intent, IntentType
from angela.ai.client import gemini_client, GeminiRequest
from angela.context import context_manager
from angela.utils.logging import get_logger

logger = get_logger(__name__)

class PlanStep(BaseModel):
    """Model for a single step in a plan."""
    command: str = Field(..., description="The command to execute")
    explanation: str = Field(..., description="Explanation of the command")
    dependencies: List[int] = Field(default_factory=list, description="Indices of steps this step depends on")
    estimated_risk: int = Field(0, description="Estimated risk level (0-4)")


class TaskPlan(BaseModel):
    """Model for a complete task plan."""
    goal: str = Field(..., description="The original high-level goal")
    steps: List[PlanStep] = Field(..., description="Steps to achieve the goal")
    context: Dict[str, Any] = Field(default_factory=dict, description="Context information")


class TaskPlanner:
    """
    Task planner for breaking down complex goals into actionable steps.
    
    This class handles:
    1. Analyzing high-level goals
    2. Breaking them down into sequences of commands
    3. Determining dependencies between steps
    4. Generating executable action plans
    """
    
    def __init__(self):
        """Initialize the task planner."""
        self._logger = logger
    
    async def plan_task(self, goal: str, context: Dict[str, Any]) -> TaskPlan:
        """
        Plan a complex task by breaking it down into actionable steps.
        
        Args:
            goal: The high-level goal description
            context: Context information
            
        Returns:
            A TaskPlan with the steps to achieve the goal
        """
        self._logger.info(f"Planning task: {goal}")
        
        # Generate a plan using the AI
        plan = await self._generate_plan(goal, context)
        
        self._logger.info(f"Generated plan with {len(plan.steps)} steps")
        return plan
    
    async def _generate_plan(self, goal: str, context: Dict[str, Any]) -> TaskPlan:
        """
        Generate a plan using the AI service.
        
        Args:
            goal: The high-level goal
            context: Context information
            
        Returns:
            A TaskPlan with steps to achieve the goal
        """
        # Build a planning prompt
        prompt = self._build_planning_prompt(goal, context)
        
        # Call the AI service
        api_request = GeminiRequest(prompt=prompt)
        api_response = await gemini_client.generate_text(api_request)
        
        # Parse the plan from the response
        plan = self._parse_plan_response(api_response.text, goal, context)
        
        return plan
    
    def _build_planning_prompt(self, goal: str, context: Dict[str, Any]) -> str:
        """
        Build a prompt for generating a plan.
        
        Args:
            goal: The high-level goal
            context: Context information
            
        Returns:
            A prompt string for the AI service
        """
        # Create context string
        context_str = "Current context:\n"
        
        if context.get("cwd"):
            context_str += f"- Current working directory: {context['cwd']}\n"
        if context.get("project_root"):
            context_str += f"- Project root: {context['project_root']}\n"
        if context.get("project_type"):
            context_str += f"- Project type: {context['project_type']}\n"
        
        # Add files in current directory for context
        if context.get("cwd"):
            try:
                dir_contents = context_manager.get_directory_contents(Path(context["cwd"]))
                files_str = "\n".join([f"- {item['name']}" for item in dir_contents])
                context_str += f"\nFiles in current directory:\n{files_str}\n"
            except Exception as e:
                self._logger.error(f"Error getting directory contents: {str(e)}")
        
        # Build the prompt
        prompt = f"""
You are Angela, an AI terminal assistant. Your task is to create a detailed plan for achieving the following goal:

GOAL: {goal}

{context_str}

Break down this goal into a sequence of shell commands that would accomplish it.
For each command, provide:
1. The exact command to run
2. A brief explanation of what the command does
3. Any dependencies (which previous steps must complete first)
4. An estimated risk level (0: SAFE, 1: LOW, 2: MEDIUM, 3: HIGH, 4: CRITICAL)

Format your response as JSON:
{{
  "steps": [
    {{
      "command": "command_1",
      "explanation": "Explanation of command 1",
      "dependencies": [],
      "estimated_risk": 1
    }},
    {{
      "command": "command_2",
      "explanation": "Explanation of command 2",
      "dependencies": [0],
      "estimated_risk": 2
    }},
    ...
  ]
}}

Ensure each command is valid and appropriate for a Linux/Unix shell environment.
Use the most efficient and standard commands to accomplish the task.
Include error handling where appropriate.
"""
        
        return prompt
    
    def _parse_plan_response(self, response: str, goal: str, context: Dict[str, Any]) -> TaskPlan:
        """
        Parse the AI response into a TaskPlan.
        
        Args:
            response: The AI response text
            goal: The original high-level goal
            context: Context information
            
        Returns:
            A TaskPlan object
        """
        try:
            # Extract JSON from the response
            import json
            import re
            
            # Find JSON content
            json_match = re.search(r'```(?:json)?\s*(.*?)\s*```', response, re.DOTALL)
            if json_match:
                json_str = json_match.group(1)
            else:
                # Try to find JSON without code blocks
                json_match = re.search(r'({.*})', response, re.DOTALL)
                if json_match:
                    json_str = json_match.group(1)
                else:
                    # Assume the entire response is JSON
                    json_str = response
            
            # Parse the JSON
            plan_data = json.loads(json_str)
            
            # Create a TaskPlan object
            steps = []
            for step_data in plan_data.get("steps", []):
                step = PlanStep(
                    command=step_data["command"],
                    explanation=step_data["explanation"],
                    dependencies=step_data.get("dependencies", []),
                    estimated_risk=step_data.get("estimated_risk", 0)
                )
                steps.append(step)
            
            return TaskPlan(
                goal=goal,
                steps=steps,
                context=context
            )
        
        except Exception as e:
            self._logger.exception(f"Error parsing plan response: {str(e)}")
            # Create a fallback single-step plan
            return TaskPlan(
                goal=goal,
                steps=[
                    PlanStep(
                        command=f"echo 'Unable to create detailed plan for: {goal}'",
                        explanation="Fallback step due to planning error",
                        dependencies=[],
                        estimated_risk=0
                    )
                ],
                context=context
            )
    
    def create_action_plan(self, task_plan: TaskPlan) -> ActionPlan:
        """
        Convert a TaskPlan to an executable ActionPlan.
        
        Args:
            task_plan: The task plan to convert
            
        Returns:
            An ActionPlan ready for execution
        """
        # Create an intent for the action plan
        intent = Intent(
            type=IntentType.UNKNOWN,
            original_request=task_plan.goal
        )
        
        # Extract commands and explanations preserving the order
        commands = []
        explanations = []
        
        # For now, we'll execute steps in the order they appear
        # In the future, we could use the dependencies to create a proper execution order
        for step in task_plan.steps:
            commands.append(step.command)
            explanations.append(step.explanation)
        
        # Determine the overall risk level (maximum of all steps)
        risk_level = max([step.estimated_risk for step in task_plan.steps], default=0)
        
        return ActionPlan(
            intent=intent,
            commands=commands,
            explanations=explanations,
            risk_level=risk_level
        )
    
    async def execute_plan(self, task_plan: TaskPlan, dry_run: bool = False) -> List[Dict[str, Any]]:
        """
        Execute a task plan.
        
        Args:
            task_plan: The task plan to execute
            dry_run: Whether to simulate execution without making changes
            
        Returns:
            A list of execution results for each step
        """
        # Convert to action plan
        action_plan = self.create_action_plan(task_plan)
        
        # Import here to avoid circular imports
        from angela.execution.engine import execution_engine
        
        # Execute the plan
        results = await execution_engine.execute_plan(
            action_plan,
            check_safety=True,
            dry_run=dry_run
        )
        
        return results


# Global task planner instance
task_planner = TaskPlanner()
</file>

<file path="angela/monitoring/__init__.py">
"""
Monitoring and proactive assistance for Angela CLI.

This package provides background monitoring capabilities that allow Angela
to offer proactive suggestions and assistance based on system state.
"""

from angela.monitoring.background import background_monitor
</file>

<file path="angela/monitoring/background.py">
"""
Background monitoring and proactive suggestions for Angela CLI.

This module provides background monitoring of system state and user activities
to offer proactive assistance and suggestions.
"""
import os
import sys
import asyncio
import time
import re
import signal
import subprocess
from pathlib import Path
from typing import Dict, Any, Optional, List, Set, Callable, Awaitable
from datetime import datetime, timedelta

from angela.ai.client import gemini_client, GeminiRequest
from angela.context import context_manager
from angela.utils.logging import get_logger
from angela.shell.formatter import terminal_formatter

logger = get_logger(__name__)

class BackgroundMonitor:
    """
    Background monitor for detecting potential issues and offering proactive assistance.
    
    Monitors:
    1. Git status changes
    2. Syntax errors in recently modified files
    3. System resource usage
    4. Process failures
    5. Common error patterns
    """
    
    def __init__(self):
        """Initialize the background monitor."""
        self._logger = logger
        self._monitoring_tasks = set()
        self._monitoring_active = False
        self._suggestions = set()  # To avoid repeating the same suggestions
        self._last_suggestion_time = datetime.now() - timedelta(hours=1)  # Ensure initial delay has passed
        self._suggestion_cooldown = timedelta(minutes=5)  # Minimum time between suggestions
    
    def start_monitoring(self):
        """Start background monitoring tasks."""
        if self._monitoring_active:
            return
            
        self._monitoring_active = True
        
        # Create and start monitoring tasks
        self._create_monitoring_task(self._monitor_git_status(), "git_status")
        self._create_monitoring_task(self._monitor_file_changes(), "file_changes")
        self._create_monitoring_task(self._monitor_system_resources(), "system_resources")
        
        self._logger.info("Background monitoring started")
    
    def stop_monitoring(self):
        """Stop all background monitoring tasks."""
        if not self._monitoring_active:
            return
            
        self._monitoring_active = False
        
        # Cancel all running tasks
        for task in self._monitoring_tasks:
            if not task.done():
                task.cancel()
                
        self._monitoring_tasks.clear()
        self._logger.info("Background monitoring stopped")
    
    def _create_monitoring_task(self, coro: Awaitable, name: str) -> None:
        """
        Create and start a monitoring task.
        
        Args:
            coro: The coroutine to run as a task
            name: A name for the task (for logging)
        """
        task = asyncio.create_task(self._run_monitoring_task(coro, name))
        self._monitoring_tasks.add(task)
        task.add_done_callback(self._monitoring_tasks.discard)
    
    async def _run_monitoring_task(self, coro: Awaitable, name: str) -> None:
        """
        Run a monitoring task with error handling.
        
        Args:
            coro: The coroutine to run
            name: The task name
        """
        try:
            await coro
        except asyncio.CancelledError:
            self._logger.debug(f"Monitoring task {name} cancelled")
        except Exception as e:
            self._logger.exception(f"Error in monitoring task {name}: {str(e)}")
            
            # Restart the task after a delay
            await asyncio.sleep(30)
            if self._monitoring_active:
                self._logger.info(f"Restarting monitoring task {name}")
                if name == "git_status":
                    self._create_monitoring_task(self._monitor_git_status(), name)
                elif name == "file_changes":
                    self._create_monitoring_task(self._monitor_file_changes(), name)
                elif name == "system_resources":
                    self._create_monitoring_task(self._monitor_system_resources(), name)
    
    async def _monitor_git_status(self) -> None:
        """Monitor Git status in the current project."""
        self._logger.debug("Starting Git status monitoring")
        
        while self._monitoring_active:
            try:
                # Check if the current directory is a Git repository
                context = context_manager.get_context_dict()
                if not context.get("project_root"):
                    # No project detected, sleep and try again later
                    await asyncio.sleep(60)
                    continue
                
                project_root = Path(context["project_root"])
                git_dir = project_root / ".git"
                
                if not git_dir.exists():
                    # Not a Git repository, sleep and try again later
                    await asyncio.sleep(60)
                    continue
                
                # Check Git status
                result = await self._run_command("git status -s", cwd=str(project_root))
                
                if result["success"] and result["stdout"].strip():
                    # Check if this is different from the last status we saw
                    status_text = result["stdout"].strip()
                    
                    # Count changes
                    modified_count = status_text.count(" M ")
                    untracked_count = status_text.count("?? ")
                    deleted_count = status_text.count(" D ")
                    
                    # Analyze the status and suggest actions
                    if modified_count > 0 or untracked_count > 0 or deleted_count > 0:
                        suggestion_key = f"git_status:{modified_count}:{untracked_count}:{deleted_count}"
                        
                        if suggestion_key not in self._suggestions:
                            # Create a suggestion based on the status
                            suggestion = await self._generate_git_suggestion(
                                modified_count, 
                                untracked_count, 
                                deleted_count
                            )
                            
                            # Display the suggestion if possible
                            if suggestion and self._can_show_suggestion():
                                terminal_formatter.print_proactive_suggestion(suggestion, "Git Monitor")
                                self._suggestions.add(suggestion_key)
                                self._last_suggestion_time = datetime.now()
                
                # Wait before checking again
                await asyncio.sleep(60)
                
            except Exception as e:
                self._logger.exception(f"Error in Git status monitoring: {str(e)}")
                await asyncio.sleep(60)  # Wait before retrying
    
    async def _monitor_file_changes(self) -> None:
        """Monitor file changes for syntax errors and linting issues."""
        self._logger.debug("Starting file changes monitoring")
        
        # Track the last modified time of each file
        last_modified_times = {}
        
        while self._monitoring_active:
            try:
                # Get current project context
                context = context_manager.get_context_dict()
                if not context.get("project_root"):
                    # No project detected, sleep and try again later
                    await asyncio.sleep(30)
                    continue
                
                project_root = Path(context["project_root"])
                
                # Scan for files that have changed
                changed_files = []
                
                for file_path in self._find_source_files(project_root):
                    try:
                        mtime = file_path.stat().st_mtime
                        
                        # Check if this file is newly modified
                        if file_path in last_modified_times:
                            if mtime > last_modified_times[file_path]:
                                changed_files.append(file_path)
                                last_modified_times[file_path] = mtime
                        else:
                            # New file we haven't seen before
                            last_modified_times[file_path] = mtime
                    except (FileNotFoundError, PermissionError):
                        # File may have been deleted or is inaccessible
                        if file_path in last_modified_times:
                            del last_modified_times[file_path]
                
                # Check changed files for issues
                for file_path in changed_files:
                    # Get file info
                    file_info = context_manager.get_file_info(file_path)
                    
                    # Check file based on language
                    if file_info.get("language") == "Python":
                        await self._check_python_file(file_path)
                    elif file_info.get("language") == "JavaScript":
                        await self._check_javascript_file(file_path)
                    # Add more language checks as needed
                
                # Wait before checking again
                await asyncio.sleep(10)
                
            except Exception as e:
                self._logger.exception(f"Error in file changes monitoring: {str(e)}")
                await asyncio.sleep(30)  # Wait before retrying
    
    async def _monitor_system_resources(self) -> None:
        """Monitor system resources for potential issues."""
        self._logger.debug("Starting system resources monitoring")
        
        # Last values for comparison
        last_values = {
            "disk_usage": 0,
            "memory_usage": 0,
            "cpu_usage": 0
        }
        
        while self._monitoring_active:
            try:
                # Check disk space
                disk_usage = await self._get_disk_usage()
                if disk_usage > 90 and disk_usage > last_values["disk_usage"] + 5:
                    # Disk usage above 90% and increased by 5%
                    if self._can_show_suggestion():
                        suggestion = f"Your disk space is running low ({disk_usage}% used). Consider cleaning up unused files or moving data to free up space."
                        terminal_formatter.print_proactive_suggestion(suggestion, "System Monitor")
                        self._last_suggestion_time = datetime.now()
                last_values["disk_usage"] = disk_usage
                
                # Wait before checking again
                await asyncio.sleep(300)  # Check every 5 minutes
                
            except Exception as e:
                self._logger.exception(f"Error in system resources monitoring: {str(e)}")
                await asyncio.sleep(60)  # Wait before retrying
    
    async def _run_command(self, command: str, cwd: Optional[str] = None) -> Dict[str, Any]:
        """
        Run a shell command and return its output.
        
        Args:
            command: The command to run
            cwd: Optional working directory
            
        Returns:
            Dictionary with command results
        """
        try:
            process = await asyncio.create_subprocess_shell(
                command,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE,
                cwd=cwd
            )
            
            stdout, stderr = await process.communicate()
            
            return {
                "command": command,
                "stdout": stdout.decode('utf-8', errors='replace'),
                "stderr": stderr.decode('utf-8', errors='replace'),
                "return_code": process.returncode,
                "success": process.returncode == 0
            }
        except Exception as e:
            self._logger.error(f"Error running command '{command}': {str(e)}")
            return {
                "command": command,
                "stdout": "",
                "stderr": str(e),
                "return_code": -1,
                "success": False
            }
    
    def _find_source_files(self, base_dir: Path) -> List[Path]:
        """
        Find source code files in a directory.
        
        Args:
            base_dir: The base directory to search
            
        Returns:
            List of file paths
        """
        source_files = []
        
        # File extensions to look for
        extensions = {
            ".py", ".js", ".ts", ".java", ".c", ".cpp", ".h", ".hpp",
            ".rs", ".go", ".rb", ".php", ".html", ".css", ".jsx", ".tsx"
        }
        
        # Directories to ignore
        ignore_dirs = {
            "__pycache__", "node_modules", ".git", "venv", "env",
            "build", "dist", "target", ".idea", ".vscode"
        }
        
        try:
            for root, dirs, files in os.walk(base_dir):
                # Skip ignored directories
                dirs[:] = [d for d in dirs if d not in ignore_dirs]
                
                for file in files:
                    file_ext = os.path.splitext(file)[1]
                    if file_ext in extensions:
                        source_files.append(Path(os.path.join(root, file)))
        except Exception as e:
            self._logger.error(f"Error finding source files: {str(e)}")
        
        return source_files
    
    async def _check_python_file(self, file_path: Path) -> None:
        """
        Check a Python file for syntax errors or linting issues.
        
        Args:
            file_path: Path to the Python file
        """
        # Check for syntax errors
        result = await self._run_command(f"python -m py_compile {file_path}")
        
        if not result["success"]:
            # Found a syntax error, generate a suggestion
            error_text = result["stderr"]
            suggestion_key = f"python_syntax:{file_path}"
            
            if suggestion_key not in self._suggestions and self._can_show_suggestion():
                # Extract the error message
                match = re.search(r"SyntaxError: (.*)", error_text)
                error_msg = match.group(1) if match else "syntax error"
                
                suggestion = f"Syntax error detected in {file_path.name}: {error_msg}"
                terminal_formatter.print_proactive_suggestion(suggestion, "File Monitor")
                self._suggestions.add(suggestion_key)
                self._last_suggestion_time = datetime.now()
        
        # Check with flake8 if available
        flake8_result = await self._run_command(f"flake8 {file_path}")
        
        if flake8_result["success"] and flake8_result["stdout"].strip():
            # Found linting issues
            suggestion_key = f"python_lint:{file_path}"
            
            if suggestion_key not in self._suggestions and self._can_show_suggestion():
                lint_issues = flake8_result["stdout"].strip().count('\n') + 1
                suggestion = f"Found {lint_issues} linting issues in {file_path.name}"
                terminal_formatter.print_proactive_suggestion(suggestion, "File Monitor")
                self._suggestions.add(suggestion_key)
                self._last_suggestion_time = datetime.now()
    
    async def _check_javascript_file(self, file_path: Path) -> None:
        """
        Check a JavaScript file for syntax errors or linting issues.
        
        Args:
            file_path: Path to the JavaScript file
        """
        # Check for syntax errors with Node.js
        result = await self._run_command(f"node --check {file_path}")
        
        if not result["success"]:
            # Found a syntax error, generate a suggestion
            error_text = result["stderr"]
            suggestion_key = f"js_syntax:{file_path}"
            
            if suggestion_key not in self._suggestions and self._can_show_suggestion():
                suggestion = f"Syntax error detected in {file_path.name}"
                terminal_formatter.print_proactive_suggestion(suggestion, "File Monitor")
                self._suggestions.add(suggestion_key)
                self._last_suggestion_time = datetime.now()
        
        # Check with ESLint if available
        eslint_result = await self._run_command(f"eslint {file_path}")
        
        if eslint_result["success"] and eslint_result["stdout"].strip():
            # Found linting issues
            suggestion_key = f"js_lint:{file_path}"
            
            if suggestion_key not in self._suggestions and self._can_show_suggestion():
                lint_issues = eslint_result["stdout"].strip().count('\n') + 1
                suggestion = f"Found {lint_issues} linting issues in {file_path.name}"
                terminal_formatter.print_proactive_suggestion(suggestion, "File Monitor")
                self._suggestions.add(suggestion_key)
                self._last_suggestion_time = datetime.now()
    
    async def _get_disk_usage(self) -> float:
        """
        Get disk usage percentage for the current directory.
        
        Returns:
            Disk usage percentage (0-100)
        """
        if sys.platform == "win32":
            # Windows
            result = await self._run_command("wmic logicaldisk get freespace,size")
            if result["success"]:
                lines = result["stdout"].strip().split('\n')
                if len(lines) >= 2:
                    values = lines[1].split()
                    if len(values) >= 2:
                        try:
                            free_space = int(values[0])
                            total_size = int(values[1])
                            return 100 - (free_space / total_size * 100)
                        except (ValueError, IndexError):
                            pass
            return 0
        else:
            # Unix-like
            result = await self._run_command("df -h .")
            if result["success"]:
                lines = result["stdout"].strip().split('\n')
                if len(lines) >= 2:
                    values = lines[1].split()
                    if len(values) >= 5:
                        try:
                            usage = values[4].rstrip('%')
                            return float(usage)
                        except (ValueError, IndexError):
                            pass
            return 0
    
    async def _generate_git_suggestion(
        self, 
        modified_count: int, 
        untracked_count: int, 
        deleted_count: int
    ) -> Optional[str]:
        """
        Generate a suggestion based on Git status.
        
        Args:
            modified_count: Number of modified files
            untracked_count: Number of untracked files
            deleted_count: Number of deleted files
            
        Returns:
            A suggestion string, or None if no suggestion is needed
        """
        total_changes = modified_count + untracked_count + deleted_count
        
        if total_changes <= 0:
            return None
            
        if total_changes > 10:
            return f"You have {total_changes} uncommitted changes in your Git repository. Consider committing your changes to avoid losing work."
            
        if modified_count > 0 and untracked_count > 0:
            return f"You have {modified_count} modified files and {untracked_count} untracked files. Consider using 'git add' to stage changes and 'git commit' to save your work."
            
        if modified_count > 0:
            return f"You have {modified_count} modified files that aren't committed. Use 'git commit' to save your changes."
            
        if untracked_count > 0:
            return f"You have {untracked_count} untracked files. Use 'git add' to begin tracking them."
            
        if deleted_count > 0:
            return f"You have {deleted_count} deleted files that haven't been committed. Use 'git commit' to record these deletions."
            
        return None
    
    def _can_show_suggestion(self) -> bool:
        """
        Check if we can show a suggestion now (respecting cooldown period).
        
        Returns:
            True if a suggestion can be shown, False otherwise
        """
        now = datetime.now()
        return (now - self._last_suggestion_time) >= self._suggestion_cooldown

# Global background monitor instance
background_monitor = BackgroundMonitor()
</file>

<file path="angela/monitoring/network_monitor.py">
# angela/monitoring/network_monitor.py

import asyncio
import time
import socket
import os
import re
import json
from pathlib import Path
from typing import Dict, Any, List, Set, Optional, Tuple
from datetime import datetime, timedelta

from angela.config import config_manager
from angela.utils.logging import get_logger
from angela.shell.formatter import terminal_formatter
from angela.context import context_manager

logger = get_logger(__name__)

class NetworkMonitor:
    """
    Network monitoring for services, dependencies, and connections.
    
    Monitors:
    1. Local service health (e.g., web servers, databases)
    2. External API status
    3. Network connectivity
    4. Dependency update availability
    """
    
    def __init__(self):
        """Initialize the network monitor."""
        self._logger = logger
        self._monitoring_tasks = set()
        self._monitoring_active = False
        self._suggestions = set()
        self._last_suggestion_time = datetime.now() - timedelta(hours=1)
        self._suggestion_cooldown = timedelta(minutes=15)
        
    def start_monitoring(self):
        """Start network monitoring tasks."""
        if self._monitoring_active:
            return
            
        self._monitoring_active = True
        
        # Create and start monitoring tasks
        self._create_monitoring_task(self._monitor_local_services(), "local_services")
        self._create_monitoring_task(self._monitor_dependency_updates(), "dependency_updates")
        self._create_monitoring_task(self._monitor_network_connectivity(), "network_connectivity")
        
        self._logger.info("Network monitoring started")
    
    def stop_monitoring(self):
        """Stop all network monitoring tasks."""
        if not self._monitoring_active:
            return
            
        self._monitoring_active = False
        
        # Cancel all running tasks
        for task in self._monitoring_tasks:
            if not task.done():
                task.cancel()
                
        self._monitoring_tasks.clear()
        self._logger.info("Network monitoring stopped")
    
    def _create_monitoring_task(self, coro, name):
        """Create and start a monitoring task."""
        task = asyncio.create_task(self._run_monitoring_task(coro, name))
        self._monitoring_tasks.add(task)
        task.add_done_callback(self._monitoring_tasks.discard)
    
    async def _run_monitoring_task(self, coro, name):
        """Run a monitoring task with error handling."""
        try:
            await coro
        except asyncio.CancelledError:
            self._logger.debug(f"Network monitoring task {name} cancelled")
        except Exception as e:
            self._logger.exception(f"Error in network monitoring task {name}: {str(e)}")
            
            # Restart the task after a delay
            await asyncio.sleep(60)
            if self._monitoring_active:
                self._logger.info(f"Restarting network monitoring task {name}")
                if name == "local_services":
                    self._create_monitoring_task(self._monitor_local_services(), name)
                elif name == "dependency_updates":
                    self._create_monitoring_task(self._monitor_dependency_updates(), name)
                elif name == "network_connectivity":
                    self._create_monitoring_task(self._monitor_network_connectivity(), name)
    
    async def _monitor_local_services(self):
        """Monitor local services like web servers and databases."""
        self._logger.debug("Starting local services monitoring")
        
        # Track service status to detect changes
        service_status = {}
        
        while self._monitoring_active:
            try:
                # Get current project context
                context = context_manager.get_context_dict()
                project_type = context.get("project_type")
                
                # Detect potential services based on project type
                services_to_check = self._detect_project_services(project_type)
                
                # Check each service
                for service_name, service_info in services_to_check.items():
                    status = await self._check_service_status(service_info)
                    
                    # Compare with previous status
                    prev_status = service_status.get(service_name, {}).get("status")
                    if prev_status is not None and prev_status != status["status"]:
                        # Status changed
                        if status["status"] == "down" and self._can_show_suggestion():
                            suggestion = f"Service '{service_name}' appears to be down. {status.get('message', '')}"
                            terminal_formatter.print_proactive_suggestion(suggestion, "Network Monitor")
                            self._last_suggestion_time = datetime.now()
                    
                    # Update status
                    service_status[service_name] = status
                
                # Wait before checking again
                await asyncio.sleep(60)
                
            except Exception as e:
                self._logger.exception(f"Error monitoring local services: {str(e)}")
                await asyncio.sleep(120)  # Wait before retrying
    
    async def _monitor_dependency_updates(self):
        """Monitor for available updates to project dependencies."""
        self._logger.debug("Starting dependency updates monitoring")
        
        # Track which dependencies we've already notified about
        notified_updates = set()
        
        while self._monitoring_active:
            try:
                # Get current project context
                context = context_manager.get_context_dict()
                project_root = context.get("project_root")
                project_type = context.get("project_type")
                
                if not project_root:
                    # No project detected, sleep and try again later
                    await asyncio.sleep(3600)  # Check every hour
                    continue
                
                # Check dependencies based on project type
                if project_type == "python":
                    updates = await self._check_python_dependencies(Path(project_root))
                elif project_type == "node":
                    updates = await self._check_node_dependencies(Path(project_root))
                else:
                    # Unknown project type, sleep and try again later
                    await asyncio.sleep(3600)
                    continue
                
                # Notify about new updates
                if updates and self._can_show_suggestion():
                    # Filter out already notified updates
                    new_updates = [u for u in updates if f"{u['name']}:{u['new_version']}" not in notified_updates]
                    
                    if new_updates:
                        count = len(new_updates)
                        pkg_list = ", ".join([f"{u['name']} ({u['current_version']} → {u['new_version']})" 
                                             for u in new_updates[:3]])
                        more = f" and {count - 3} more" if count > 3 else ""
                        
                        suggestion = f"Found {count} dependency updates available: {pkg_list}{more}"
                        terminal_formatter.print_proactive_suggestion(suggestion, "Dependency Monitor")
                        
                        # Mark as notified
                        for update in new_updates:
                            notified_updates.add(f"{update['name']}:{update['new_version']}")
                        
                        self._last_suggestion_time = datetime.now()
                
                # Wait before checking again (dependencies don't change often)
                await asyncio.sleep(86400)  # Check once per day
                
            except Exception as e:
                self._logger.exception(f"Error monitoring dependency updates: {str(e)}")
                await asyncio.sleep(3600)  # Wait before retrying
    
    async def _monitor_network_connectivity(self):
        """Monitor network connectivity to important services."""
        self._logger.debug("Starting network connectivity monitoring")
        
        # Track connectivity status to detect changes
        connectivity_status = {
            "internet": True,  # Assume connected initially
            "last_check": datetime.now()
        }
        
        while self._monitoring_active:
            try:
                # Check internet connectivity
                internet_status = await self._check_internet_connectivity()
                
                # Check if status changed
                if connectivity_status["internet"] != internet_status["connected"]:
                    if not internet_status["connected"] and self._can_show_suggestion():
                        suggestion = f"Internet connectivity appears to be down. {internet_status.get('message', '')}"
                        terminal_formatter.print_proactive_suggestion(suggestion, "Network Monitor")
                        self._last_suggestion_time = datetime.now()
                    elif internet_status["connected"] and not connectivity_status["internet"]:
                        # Internet connection restored
                        elapsed = datetime.now() - connectivity_status["last_check"]
                        if elapsed > timedelta(minutes=5) and self._can_show_suggestion():
                            suggestion = "Internet connectivity has been restored."
                            terminal_formatter.print_proactive_suggestion(suggestion, "Network Monitor")
                            self._last_suggestion_time = datetime.now()
                
                # Update status
                connectivity_status["internet"] = internet_status["connected"]
                connectivity_status["last_check"] = datetime.now()
                
                # Wait before checking again
                await asyncio.sleep(30)
                
            except Exception as e:
                self._logger.exception(f"Error monitoring network connectivity: {str(e)}")
                await asyncio.sleep(60)  # Wait before retrying
    
    def _detect_project_services(self, project_type: Optional[str]) -> Dict[str, Dict[str, Any]]:
        """
        Detect services to monitor based on project type.
        
        Args:
            project_type: The type of project
            
        Returns:
            Dictionary of service information
        """
        services = {}
        
        # Default services to check
        services["localhost:8000"] = {
            "host": "localhost",
            "port": 8000,
            "name": "Web Server (8000)",
            "type": "http"
        }
        
        # Add services based on project type
        if project_type == "node":
            services["localhost:3000"] = {
                "host": "localhost",
                "port": 3000,
                "name": "Node.js Server",
                "type": "http"
            }
        elif project_type == "python":
            services["localhost:5000"] = {
                "host": "localhost",
                "port": 5000,
                "name": "Flask Server",
                "type": "http"
            }
            services["localhost:8000"] = {
                "host": "localhost",
                "port": 8000,
                "name": "Django Server",
                "type": "http"
            }
        
        # Always check database ports
        services["localhost:5432"] = {
            "host": "localhost",
            "port": 5432,
            "name": "PostgreSQL",
            "type": "tcp"
        }
        services["localhost:3306"] = {
            "host": "localhost",
            "port": 3306,
            "name": "MySQL",
            "type": "tcp"
        }
        services["localhost:27017"] = {
            "host": "localhost",
            "port": 27017,
            "name": "MongoDB",
            "type": "tcp"
        }
        services["localhost:6379"] = {
            "host": "localhost",
            "port": 6379,
            "name": "Redis",
            "type": "tcp"
        }
        
        return services
    
    async def _check_service_status(self, service_info: Dict[str, Any]) -> Dict[str, Any]:
        """
        Check the status of a service.
        
        Args:
            service_info: Service information
            
        Returns:
            Status information
        """
        host = service_info.get("host", "localhost")
        port = service_info.get("port", 80)
        service_type = service_info.get("type", "tcp")
        
        # Basic port check
        try:
            # Create a socket
            sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            sock.settimeout(2.0)  # 2 second timeout
            
            # Try to connect
            result = sock.connect_ex((host, port))
            sock.close()
            
            if result == 0:
                # Port is open
                if service_type == "http":
                    # For HTTP services, try to get a response
                    try:
                        import aiohttp
                        async with aiohttp.ClientSession() as session:
                            async with session.get(f"http://{host}:{port}/", timeout=5) as response:
                                if response.status < 400:
                                    return {"status": "up", "message": f"HTTP status: {response.status}"}
                                else:
                                    return {"status": "error", "message": f"HTTP error: {response.status}"}
                    except Exception as e:
                        return {"status": "error", "message": f"HTTP error: {str(e)}"}
                else:
                    # TCP service is up
                    return {"status": "up", "message": "Port is open"}
            else:
                # Port is closed
                return {"status": "down", "message": "Port is closed"}
                
        except Exception as e:
            return {"status": "error", "message": f"Error checking service: {str(e)}"}
    
    async def _check_python_dependencies(self, project_root: Path) -> List[Dict[str, Any]]:
        """
        Check for updates to Python dependencies.
        
        Args:
            project_root: The project root directory
            
        Returns:
            List of available updates
        """
        requirements_path = project_root / "requirements.txt"
        
        if not requirements_path.exists():
            return []
        
        try:
            # Run pip list --outdated
            result = await self._run_command("pip list --outdated --format=json")
            
            if not result["success"]:
                return []
                
            # Parse the output
            outdated = json.loads(result["stdout"])
            
            # Format the updates
            updates = []
            for pkg in outdated:
                updates.append({
                    "name": pkg["name"],
                    "current_version": pkg["version"],
                    "new_version": pkg["latest_version"],
                    "type": "python"
                })
                
            return updates
            
        except Exception as e:
            self._logger.error(f"Error checking Python dependencies: {str(e)}")
            return []
    
    async def _check_node_dependencies(self, project_root: Path) -> List[Dict[str, Any]]:
        """
        Check for updates to Node.js dependencies.
        
        Args:
            project_root: The project root directory
            
        Returns:
            List of available updates
        """
        package_json_path = project_root / "package.json"
        
        if not package_json_path.exists():
            return []
        
        try:
            # Run npm outdated --json
            result = await self._run_command("npm outdated --json", cwd=str(project_root))
            
            if not result["success"] and not result["stdout"]:
                return []
                
            # Parse the output
            try:
                outdated = json.loads(result["stdout"])
            except json.JSONDecodeError:
                # npm outdated returns non-zero exit code when updates are available
                if not result["stdout"]:
                    return []
                outdated = {}
            
            # Format the updates
            updates = []
            for pkg_name, pkg_info in outdated.items():
                updates.append({
                    "name": pkg_name,
                    "current_version": pkg_info.get("current", "unknown"),
                    "new_version": pkg_info.get("latest", "unknown"),
                    "type": "npm"
                })
                
            return updates
            
        except Exception as e:
            self._logger.error(f"Error checking Node.js dependencies: {str(e)}")
            return []
    
    async def _check_internet_connectivity(self) -> Dict[str, Any]:
        """
        Check internet connectivity.
        
        Returns:
            Status information
        """
        # List of reliable domains to check
        check_domains = [
            "google.com",
            "cloudflare.com",
            "amazon.com",
            "microsoft.com"
        ]
        
        successes = 0
        failures = 0
        
        for domain in check_domains:
            try:
                # Try to resolve the domain
                await asyncio.get_event_loop().getaddrinfo(domain, 80)
                successes += 1
            except socket.gaierror:
                failures += 1
        
        # Consider internet connected if at least half of the checks succeeded
        connected = successes >= len(check_domains) / 2
        
        return {
            "connected": connected,
            "message": f"{successes}/{len(check_domains)} connectivity checks succeeded"
        }
    
    async def _run_command(self, command: str, cwd: Optional[str] = None) -> Dict[str, Any]:
        """
        Run a shell command and return its output.
        
        Args:
            command: The command to run
            cwd: Optional working directory
            
        Returns:
            Dictionary with command results
        """
        try:
            process = await asyncio.create_subprocess_shell(
                command,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE,
                cwd=cwd
            )
            
            stdout, stderr = await process.communicate()
            
            return {
                "command": command,
                "stdout": stdout.decode('utf-8', errors='replace'),
                "stderr": stderr.decode('utf-8', errors='replace'),
                "return_code": process.returncode,
                "success": process.returncode == 0
            }
        except Exception as e:
            self._logger.error(f"Error running command '{command}': {str(e)}")
            return {
                "command": command,
                "stdout": "",
                "stderr": str(e),
                "return_code": -1,
                "success": False
            }
    
    def _can_show_suggestion(self) -> bool:
        """
        Check if we can show a suggestion now (respecting cooldown period).
        
        Returns:
            True if a suggestion can be shown, False otherwise
        """
        now = datetime.now()
        return (now - self._last_suggestion_time) >= self._suggestion_cooldown

# Global network monitor instance
network_monitor = NetworkMonitor()
</file>

<file path="angela/review/diff_manager.py">
# angela/review/diff_manager.py
"""
Diff management for Angela CLI.

This module provides functionality for managing and presenting diffs
between original and modified code.
"""
import os
import difflib
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple, Union

from angela.utils.logging import get_logger

logger = get_logger(__name__)

class DiffManager:
    """
    Manager for generating and displaying code diffs.
    """
    
    def __init__(self):
        """Initialize the diff manager."""
        self._logger = logger
    
    def generate_diff(
        self, 
        original: str, 
        modified: str, 
        context_lines: int = 3
    ) -> str:
        """
        Generate a unified diff between original and modified content.
        
        Args:
            original: Original content
            modified: Modified content
            context_lines: Number of context lines to include
            
        Returns:
            Unified diff string
        """
        self._logger.debug("Generating diff")
        
        # Split content into lines
        original_lines = original.splitlines(keepends=True)
        modified_lines = modified.splitlines(keepends=True)
        
        # Generate unified diff
        diff = difflib.unified_diff(
            original_lines, 
            modified_lines,
            fromfile='original',
            tofile='modified',
            n=context_lines
        )
        
        return ''.join(diff)
    
    def generate_html_diff(
        self, 
        original: str, 
        modified: str, 
        context_lines: int = 3
    ) -> str:
        """
        Generate an HTML diff between original and modified content.
        
        Args:
            original: Original content
            modified: Modified content
            context_lines: Number of context lines to include
            
        Returns:
            HTML diff string
        """
        self._logger.debug("Generating HTML diff")
        
        # Split content into lines
        original_lines = original.splitlines()
        modified_lines = modified.splitlines()
        
        # Generate HTML diff
        diff = difflib.HtmlDiff().make_file(
            original_lines, 
            modified_lines,
            fromdesc='Original',
            todesc='Modified',
            context=True,
            numlines=context_lines
        )
        
        return diff
    
    def generate_file_diff(
        self, 
        original_file: Union[str, Path], 
        modified_file: Union[str, Path],
        context_lines: int = 3
    ) -> str:
        """
        Generate a unified diff between original and modified files.
        
        Args:
            original_file: Path to original file
            modified_file: Path to modified file
            context_lines: Number of context lines to include
            
        Returns:
            Unified diff string
        """
        self._logger.debug(f"Generating diff between {original_file} and {modified_file}")
        
        # Read file contents
        try:
            with open(original_file, 'r', encoding='utf-8', errors='replace') as f:
                original_content = f.read()
            
            with open(modified_file, 'r', encoding='utf-8', errors='replace') as f:
                modified_content = f.read()
            
            # Generate diff
            return self.generate_diff(
                original_content, 
                modified_content,
                context_lines
            )
        except Exception as e:
            self._logger.error(f"Error generating file diff: {str(e)}")
            return f"Error generating diff: {str(e)}"
    
    def generate_directory_diff(
        self, 
        original_dir: Union[str, Path], 
        modified_dir: Union[str, Path],
        context_lines: int = 3
    ) -> Dict[str, str]:
        """
        Generate diffs for all files in two directories.
        
        Args:
            original_dir: Path to original directory
            modified_dir: Path to modified directory
            context_lines: Number of context lines to include
            
        Returns:
            Dictionary mapping file paths to diffs
        """
        self._logger.debug(f"Generating diffs between {original_dir} and {modified_dir}")
        
        original_dir = Path(original_dir)
        modified_dir = Path(modified_dir)
        
        # Check if directories exist
        if not original_dir.exists() or not original_dir.is_dir():
            self._logger.error(f"Original directory does not exist: {original_dir}")
            return {}
        
        if not modified_dir.exists() or not modified_dir.is_dir():
            self._logger.error(f"Modified directory does not exist: {modified_dir}")
            return {}
        
        # Find all files in both directories
        original_files = set()
        modified_files = set()
        
        for root, _, files in os.walk(original_dir):
            for file in files:
                file_path = Path(root) / file
                rel_path = file_path.relative_to(original_dir)
                original_files.add(str(rel_path))
        
        for root, _, files in os.walk(modified_dir):
            for file in files:
                file_path = Path(root) / file
                rel_path = file_path.relative_to(modified_dir)
                modified_files.add(str(rel_path))
        
        # Generate diffs for all files
        diffs = {}
        
        # Files in both directories
        for rel_path in original_files.intersection(modified_files):
            original_file = original_dir / rel_path
            modified_file = modified_dir / rel_path
            
            try:
                diff = self.generate_file_diff(
                    original_file, 
                    modified_file,
                    context_lines
                )
                
                # Only include if there are differences
                if diff:
                    diffs[rel_path] = diff
            except Exception as e:
                self._logger.error(f"Error generating diff for {rel_path}: {str(e)}")
        
        # Files only in original directory (deleted)
        for rel_path in original_files - modified_files:
            original_file = original_dir / rel_path
            
            try:
                with open(original_file, 'r', encoding='utf-8', errors='replace') as f:
                    content = f.read()
                
                # Generate diff showing deletion
                diff = self.generate_diff(content, '')
                diffs[rel_path] = diff
            except Exception as e:
                self._logger.error(f"Error generating diff for {rel_path}: {str(e)}")
        
        # Files only in modified directory (added)
        for rel_path in modified_files - original_files:
            modified_file = modified_dir / rel_path
            
            try:
                with open(modified_file, 'r', encoding='utf-8', errors='replace') as f:
                    content = f.read()
                
                # Generate diff showing addition
                diff = self.generate_diff('', content)
                diffs[rel_path] = diff
            except Exception as e:
                self._logger.error(f"Error generating diff for {rel_path}: {str(e)}")
        
        return diffs
    
    def apply_diff(
        self, 
        original: str, 
        diff: str
    ) -> Tuple[str, bool]:
        """
        Apply a unified diff to original content.
        
        Args:
            original: Original content
            diff: Unified diff string
            
        Returns:
            Tuple of (modified_content, success)
        """
        self._logger.debug("Applying diff")
        
        try:
            # Parse the diff
            lines = diff.splitlines()
            
            # Skip header lines (starting with ---, +++, @@)
            i = 0
            while i < len(lines) and (lines[i].startswith('---') or lines[i].startswith('+++') or lines[i].startswith('@@')):
                i += 1
            
            # Apply changes
            result = []
            original_lines = original.splitlines()
            
            line_num = 0
            while line_num < len(original_lines):
                if i < len(lines):
                    if lines[i].startswith('-'):
                        # Line removed, skip in original
                        if not original_lines[line_num] == lines[i][1:]:
                            # Mismatch, can't apply diff
                            return original, False
                        
                        line_num += 1
                        i += 1
                    elif lines[i].startswith('+'):
                        # Line added
                        result.append(lines[i][1:])
                        i += 1
                    elif lines[i].startswith(' '):
                        # Context line
                        if not original_lines[line_num] == lines[i][1:]:
                            # Mismatch, can't apply diff
                            return original, False
                        
                        result.append(original_lines[line_num])
                        line_num += 1
                        i += 1
                    else:
                        # Unknown line in diff
                        return original, False
                else:
                    # No more diff lines, copy remaining original lines
                    result.extend(original_lines[line_num:])
                    break
            
            # Return the modified content
            return '\n'.join(result), True
        except Exception as e:
            self._logger.error(f"Error applying diff: {str(e)}")
            return original, False

# Global diff manager instance
diff_manager = DiffManager()
</file>

<file path="angela/review/feedback.py">
# angela/review/feedback.py
"""
Feedback processing for Angela CLI.

This module provides functionality for processing user feedback
on generated code and refining code based on feedback.
"""
import os
import re
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple, Union
import json

from angela.ai.client import gemini_client, GeminiRequest
from angela.review.diff_manager import diff_manager
from angela.utils.logging import get_logger

logger = get_logger(__name__)

class FeedbackManager:
    """
    Manager for processing user feedback and refining code.
    """
    
    def __init__(self):
        """Initialize the feedback manager."""
        self._logger = logger
    
    async def process_feedback(
        self, 
        feedback: str,
        original_code: str,
        file_path: Optional[str] = None,
        context: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Process feedback on code and generate improved version.
        
        Args:
            feedback: User feedback
            original_code: Original code to improve
            file_path: Optional path to the file
            context: Optional additional context
            
        Returns:
            Dictionary with the improved code and other information
        """
        self._logger.info("Processing feedback for code improvement")
        
        # Extract file extension for language detection
        language = None
        if file_path:
            _, ext = os.path.splitext(file_path)
            language = self._get_language_from_extension(ext)
        
        # Build prompt for code improvement
        prompt = self._build_improvement_prompt(
            feedback, 
            original_code, 
            language,
            file_path,
            context
        )
        
        # Call AI service
        api_request = GeminiRequest(
            prompt=prompt,
            max_tokens=16000,  # Large token limit for code
            temperature=0.2
        )
        
        response = await gemini_client.generate_text(api_request)
        
        # Extract improved code and explanation
        improved_code, explanation = self._extract_improved_code(response.text, original_code)
        
        # Generate diff
        diff = diff_manager.generate_diff(original_code, improved_code)
        
        return {
            "original_code": original_code,
            "improved_code": improved_code,
            "explanation": explanation,
            "diff": diff,
            "file_path": file_path,
            "language": language,
            "feedback": feedback
        }
    
    async def refine_project(
        self, 
        project_dir: Union[str, Path],
        feedback: str,
        focus_files: Optional[List[str]] = None,
        context: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Refine an entire project based on feedback.
        
        Args:
            project_dir: Path to the project directory
            feedback: User feedback
            focus_files: Optional list of files to focus on
            context: Optional additional context
            
        Returns:
            Dictionary with the refinement results
        """
        self._logger.info(f"Refining project in {project_dir} based on feedback")
        
        project_dir = Path(project_dir)
        
        # Check if directory exists
        if not project_dir.exists() or not project_dir.is_dir():
            return {
                "success": False,
                "error": f"Project directory does not exist: {project_dir}",
                "feedback": feedback
            }
        
        # Get list of files to refine
        files_to_refine = []
        
        if focus_files:
            # Refine specific files
            for file_pattern in focus_files:
                # Handle glob patterns
                if '*' in file_pattern or '?' in file_pattern:
                    matches = list(project_dir.glob(file_pattern))
                    for match in matches:
                        if match.is_file():
                            files_to_refine.append(match)
                else:
                    # Direct file path
                    file_path = project_dir / file_pattern
                    if file_path.is_file():
                        files_to_refine.append(file_path)
        else:
            # Auto-detect files to refine based on feedback
            files = self._find_relevant_files(project_dir, feedback)
            files_to_refine.extend(files)
        
        # Process each file
        results = []
        
        for file_path in files_to_refine:
            try:
                with open(file_path, 'r', encoding='utf-8', errors='replace') as f:
                    original_code = f.read()
                
                # Process feedback for this file
                file_result = await self.process_feedback(
                    feedback,
                    original_code,
                    str(file_path.relative_to(project_dir)),
                    context
                )
                
                results.append({
                    "file_path": str(file_path.relative_to(project_dir)),
                    "has_changes": original_code != file_result["improved_code"],
                    "diff": file_result["diff"],
                    "explanation": file_result["explanation"]
                })
            except Exception as e:
                self._logger.error(f"Error processing {file_path}: {str(e)}")
                results.append({
                    "file_path": str(file_path.relative_to(project_dir)),
                    "error": str(e)
                })
        
        return {
            "success": True,
            "project_dir": str(project_dir),
            "feedback": feedback,
            "results": results,
            "files_processed": len(results)
        }
    
    async def apply_refinements(
        self, 
        refinements: Dict[str, Any],
        backup: bool = True
    ) -> Dict[str, Any]:
        """
        Apply refinements to files.
        
        Args:
            refinements: Refinement results from refine_project
            backup: Whether to create backup files
            
        Returns:
            Dictionary with the application results
        """
        self._logger.info("Applying refinements to files")
        
        # Extract project directory and results
        project_dir = Path(refinements["project_dir"])
        results = refinements["results"]
        
        # Apply changes to each file
        applied_results = []
        
        for result in results:
            file_path = project_dir / result["file_path"]
            
            # Skip files with errors
            if "error" in result:
                applied_results.append({
                    "file_path": result["file_path"],
                    "applied": False,
                    "error": result["error"]
                })
                continue
            
            # Skip files without changes
            if not result.get("has_changes", False):
                applied_results.append({
                    "file_path": result["file_path"],
                    "applied": False,
                    "message": "No changes to apply"
                })
                continue
            
            try:
                # Read original content
                with open(file_path, 'r', encoding='utf-8', errors='replace') as f:
                    original_content = f.read()
                    
                    
                if backup:
                    backup_path = file_path.with_suffix(file_path.suffix + '.bak')
                    with open(backup_path, 'w', encoding='utf-8') as f:
                        f.write(original_content)
                
                # Apply diff
                new_content, success = diff_manager.apply_diff(original_content, result["diff"])
                
                if not success:
                    # If diff application fails, regenerate the improved code
                    new_content = self._regenerate_improved_code(original_content, result["diff"])
                
                # Write the improved content
                with open(file_path, 'w', encoding='utf-8') as f:
                    f.write(new_content)
                
                applied_results.append({
                    "file_path": result["file_path"],
                    "applied": True,
                    "backup": str(backup_path) if backup else None,
                    "explanation": result.get("explanation", "")
                })
            except Exception as e:
                self._logger.error(f"Error applying changes to {file_path}: {str(e)}")
                applied_results.append({
                    "file_path": result["file_path"],
                    "applied": False,
                    "error": str(e)
                })
        
        return {
            "success": True,
            "project_dir": str(project_dir),
            "results": applied_results,
            "files_processed": len(applied_results),
            "files_changed": sum(1 for r in applied_results if r.get("applied", False))
        }
    
    def _get_language_from_extension(self, extension: str) -> Optional[str]:
        """
        Get programming language from file extension.
        
        Args:
            extension: File extension (with dot)
            
        Returns:
            Language name or None if unknown
        """
        # Map of extensions to languages
        extension_map = {
            '.py': 'Python',
            '.js': 'JavaScript',
            '.jsx': 'JavaScript (React)',
            '.ts': 'TypeScript',
            '.tsx': 'TypeScript (React)',
            '.html': 'HTML',
            '.css': 'CSS',
            '.java': 'Java',
            '.c': 'C',
            '.cpp': 'C++',
            '.h': 'C/C++ Header',
            '.rb': 'Ruby',
            '.go': 'Go',
            '.rs': 'Rust',
            '.php': 'PHP',
            '.swift': 'Swift',
            '.kt': 'Kotlin',
            '.md': 'Markdown',
            '.json': 'JSON',
            '.xml': 'XML',
            '.yaml': 'YAML',
            '.yml': 'YAML',
            '.toml': 'TOML',
            '.sh': 'Shell',
            '.bash': 'Bash',
            '.sql': 'SQL'
        }
        
        return extension_map.get(extension.lower())
    
    def _build_improvement_prompt(
        self, 
        feedback: str,
        original_code: str,
        language: Optional[str],
        file_path: Optional[str],
        context: Optional[Dict[str, Any]] = None
    ) -> str:
        """
        Build a prompt for code improvement.
        
        Args:
            feedback: User feedback
            original_code: Original code to improve
            language: Programming language
            file_path: Path to the file
            context: Additional context
            
        Returns:
            Prompt for the AI service
        """
        # Add language context
        language_str = f"Language: {language}" if language else "Language: Unknown"
        
        # Add file path context
        file_context = f"File: {file_path}" if file_path else ""
        
        # Add additional context if provided
        context_str = ""
        if context:
            context_str = "Additional context:\n"
            for key, value in context.items():
                context_str += f"- {key}: {value}\n"
        
        # Build the prompt
        prompt = f"""
You are an expert software developer helping to improve code based on user feedback.

{language_str}
{file_context}
{context_str}

User feedback:
{feedback}

Your task is to refine the code according to the feedback while preserving the original functionality.
Provide both the improved code and an explanation of the changes you made.

Original code:
{original_code}

Provide your response in this format:
1. First, the full improved code block
2. Then, a detailed explanation of the changes you made

Improved code:
// Your improved code here

Explanation:
// Your explanation here
"""
        
        return prompt
    
    def _extract_improved_code(
        self, 
        response: str, 
        original_code: str
    ) -> Tuple[str, str]:
        """
        Extract improved code and explanation from AI response.
        
        Args:
            response: AI response
            original_code: Original code (fallback)
            
        Returns:
            Tuple of (improved_code, explanation)
        """
        # Try to extract code block
        code_match = re.search(r'```(?:\w*\n)?(.*?)```', response, re.DOTALL)
        
        if code_match:
            code = code_match.group(1).strip()
        else:
            # Fallback: look for "Improved code:" section
            code_section_match = re.search(r'Improved code:\s*(.*?)(?:\n\n|$)', response, re.DOTALL)
            if code_section_match:
                code = code_section_match.group(1).strip()
            else:
                # No clear code section, use original
                code = original_code
        
        # Try to extract explanation
        explanation_match = re.search(r'Explanation:\s*(.*?)(?:\n\n|$)', response, re.DOTALL)
        
        if explanation_match:
            explanation = explanation_match.group(1).strip()
        else:
            # Fallback: anything after the code block
            if code_match:
                parts = response.split('```', 2)
                if len(parts) > 2:
                    explanation = parts[2].strip()
                else:
                    explanation = "No explanation provided."
            else:
                explanation = "No explanation provided."
        
        return code, explanation
    
    def _find_relevant_files(
        self, 
        project_dir: Path, 
        feedback: str
    ) -> List[Path]:
        """
        Find files relevant to the user feedback.
        
        Args:
            project_dir: Project directory
            feedback: User feedback
            
        Returns:
            List of relevant file paths
        """
        relevant_files = []
        
        # Extract potential file references from feedback
        file_mentions = set()
        
        # Look for explicit file references
        file_patterns = [
            r'file[s]?\s+(?:"|\')?([^"\'\s]+)(?:"|\')?',
            r'in\s+(?:"|\')?([^"\'\s]+)(?:"|\')?',
            r'(?:"|\')?([^"\'\s]+\.(?:py|js|java|html|css|cpp|h|go|rb))(?:"|\')?'
        ]
        
        for pattern in file_patterns:
            for match in re.finditer(pattern, feedback, re.IGNORECASE):
                file_mentions.add(match.group(1))
        
        # Check if mentioned files exist
        for mention in file_mentions:
            # Check for exact path
            file_path = project_dir / mention
            if file_path.exists() and file_path.is_file():
                relevant_files.append(file_path)
                continue
            
            # Check for glob pattern
            if '*' in mention or '?' in mention:
                matches = list(project_dir.glob(mention))
                for match in matches:
                    if match.is_file():
                        relevant_files.append(match)
                continue
            
            # Check for just the filename (could be in any directory)
            for root, _, files in os.walk(project_dir):
                if mention in files:
                    relevant_files.append(Path(root) / mention)
        
        # If no specific files mentioned, return all source code files
        if not relevant_files:
            for root, _, files in os.walk(project_dir):
                for file in files:
                    # Skip common non-source files and directories
                    if any(excluded in root for excluded in ['.git', 'node_modules', '__pycache__', '.venv']):
                        continue
                    
                    # Check if it's a source file
                    _, ext = os.path.splitext(file)
                    if ext.lower() in ['.py', '.js', '.jsx', '.ts', '.tsx', '.java', '.c', '.cpp', '.h', '.go', '.rb', '.php', '.swift']:
                        relevant_files.append(Path(root) / file)
        
        return relevant_files
    
    def _regenerate_improved_code(
        self, 
        original_content: str, 
        diff: str
    ) -> str:
        """
        Regenerate improved code from original and diff when apply_diff fails.
        
        This is a fallback method when the diff can't be applied cleanly.
        It uses simple heuristics to apply changes.
        
        Args:
            original_content: Original content
            diff: Unified diff string
            
        Returns:
            Regenerated improved content
        """
        # Simple heuristic: if diff shows additions, add them at the end
        # if diff shows deletions, try to find and remove them
        
        lines = diff.splitlines()
        adds = []
        removes = []
        
        for line in lines:
            if line.startswith('+') and not line.startswith('+++'):
                adds.append(line[1:])
            elif line.startswith('-') and not line.startswith('---'):
                removes.append(line[1:])
        
        # Start with original content
        result = original_content
        
        # Try to remove lines
        for remove in removes:
            result = result.replace(remove + '\n', '')
            result = result.replace(remove, '')
        
        # Add new lines at the end
        if adds:
            if not result.endswith('\n'):
                result += '\n'
            result += '\n'.join(adds)
        
        return result

# Global feedback manager instance
feedback_manager = FeedbackManager()
</file>

<file path="angela/safety/__init__.py">
"""
Safety system for Angela CLI.

This module provides a unified interface to the safety system components:
- Risk classification
- Operation validation
- User confirmation
- Command previews
"""

from angela.safety.classifier import classify_command_risk, analyze_command_impact
from angela.safety.validator import validate_command_safety, validate_operation, ValidationError
from angela.safety.confirmation import get_confirmation, requires_confirmation
from angela.safety.preview import generate_preview

from angela.utils.logging import get_logger

logger = get_logger(__name__)


async def check_command_safety(command: str, dry_run: bool = False) -> bool:
    """
    Check if a command is safe to execute and obtain user confirmation if needed.
    
    Args:
        command: The shell command to check.
        dry_run: Whether this is a dry run (show preview without executing).
        
    Returns:
        True if the command is safe and confirmed, False otherwise.
    """
    # Step 1: Validate basic safety constraints
    is_valid, error_message = validate_command_safety(command)
    if not is_valid:
        logger.warning(f"Command validation failed: {error_message}")
        return False
    
    # Step 2: Classify the risk level
    risk_level, risk_reason = classify_command_risk(command)
    
    # Step 3: Analyze the command impact
    impact = analyze_command_impact(command)
    
    # Step 4: Generate preview if possible
    preview = await generate_preview(command)
    
    # Step 5: Get user confirmation based on risk level
    confirmed = await get_confirmation(
        command=command,
        risk_level=risk_level,
        risk_reason=risk_reason,
        impact=impact,
        preview=preview,
        dry_run=dry_run
    )
    
    if not confirmed:
        logger.info(f"Command execution cancelled by user: {command}")
        return False
    
    return True


async def check_operation_safety(
    operation_type: str, 
    params: dict, 
    dry_run: bool = False
) -> bool:
    """
    Check if a high-level operation is safe to execute and obtain user confirmation if needed.
    
    Args:
        operation_type: The type of operation (e.g., 'create_file', 'delete_file').
        params: Parameters for the operation.
        dry_run: Whether this is a dry run (show preview without executing).
        
    Returns:
        True if the operation is safe and confirmed, False otherwise.
    """
    # Step 1: Validate the operation
    is_valid, error_message = validate_operation(operation_type, params)
    if not is_valid:
        logger.warning(f"Operation validation failed: {error_message}")
        return False
    
    # Step 2: Convert to a command if possible for risk analysis
    command = None
    if operation_type == 'execute_command':
        command = params.get('command', '')
    elif operation_type == 'create_file':
        command = f"touch {params.get('path', '')}"
    elif operation_type == 'write_file':
        command = f"echo '...' > {params.get('path', '')}"
    elif operation_type == 'delete_file':
        command = f"rm {params.get('path', '')}"
    elif operation_type == 'create_directory':
        command = f"mkdir -p {params.get('path', '')}"
    elif operation_type == 'delete_directory':
        command = f"rmdir {params.get('path', '')}"
    
    # If we have a command representation, use the command safety check
    if command:
        return await check_command_safety(command, dry_run)
    
    # For operations without a command representation, use a simplified approach
    # Determine risk level based on operation type
    if operation_type in ['delete_file', 'delete_directory']:
        risk_level = 3  # HIGH
        risk_reason = f"Deleting {operation_type.split('_')[1]}"
    elif operation_type in ['write_file', 'create_file', 'create_directory']:
        risk_level = 1  # LOW
        risk_reason = f"Creating {operation_type.split('_')[1]}"
    else:
        risk_level = 2  # MEDIUM
        risk_reason = f"Unknown operation type: {operation_type}"
    
    # Get user confirmation
    confirmed = await get_confirmation(
        command=f"{operation_type}: {params}",
        risk_level=risk_level,
        risk_reason=risk_reason,
        impact={"operations": [operation_type], "affected_files": [params.get('path', '')]},
        preview=None,
        dry_run=dry_run
    )
    
    if not confirmed:
        logger.info(f"Operation cancelled by user: {operation_type}")
        return False
    
    return True
</file>

<file path="angela/safety/adaptive_confirmation.py">
# angela/safety/adaptive_confirmation.py

import asyncio
from typing import Dict, Any, Optional, List, Tuple

from prompt_toolkit import PromptSession
from prompt_toolkit.shortcuts import input_dialog, message_dialog, yes_no_dialog
from prompt_toolkit.styles import Style
from rich.console import Console
from rich.panel import Panel
from rich.syntax import Syntax
from rich.text import Text
from rich.table import Table

from angela.constants import RISK_LEVELS
from angela.context.history import history_manager
from angela.context.preferences import preferences_manager
from angela.utils.logging import get_logger

logger = get_logger(__name__)

# Styles for different risk levels
CONFIRMATION_STYLES = Style.from_dict({
    'safe': '#2DA44E',        # Green
    'low': '#0969DA',         # Blue
    'medium': '#BF8700',      # Yellow/Orange
    'high': '#CF222E',        # Red
    'critical': '#820000',    # Dark Red
    'dialog': 'bg:#222222',
    'dialog.body': 'bg:#222222 #ffffff',
    'dialog.border': '#888888',
    'button': 'bg:#222222 #ffffff',
    'button.focused': 'bg:#0969DA #ffffff',
})

# Risk level names
RISK_LEVEL_NAMES = {v: k for k, v in RISK_LEVELS.items()}

# Console setup
console = Console()

async def get_adaptive_confirmation(
    command: str, 
    risk_level: int, 
    risk_reason: str,
    impact: Dict[str, Any],
    preview: Optional[str] = None,
    explanation: Optional[str] = None,
    natural_request: Optional[str] = None,
    dry_run: bool = False
) -> bool:
    """
    Get user confirmation for a command based on risk level and user history.
    
    Args:
        command: The command to be executed
        risk_level: The risk level of the command
        risk_reason: The reason for the risk classification
        impact: The impact analysis dictionary
        preview: Optional preview of command results
        explanation: AI explanation of the command
        natural_request: The original natural language request
        dry_run: Whether this is a dry run
        
    Returns:
        True if the user confirms, False otherwise
    """
    # If this is a dry run, skip confirmation
    if dry_run:
        await _show_dry_run_preview(command, risk_level, preview, explanation)
        return False
    
    # Check if auto-execution is enabled for this risk level and command
    if preferences_manager.should_auto_execute(risk_level, command):
        # Get command frequency and success rate
        frequency = history_manager.get_command_frequency(command)
        success_rate = history_manager.get_command_success_rate(command)
        
        # For frequently used commands with high success rate, auto-execute
        if frequency >= 5 and success_rate > 0.8:
            logger.info(f"Auto-executing command with high trust: {command}")
            await _show_auto_execution_notice(command, risk_level, preview)
            return True
    
    # For all other cases, get explicit confirmation
    risk_name = RISK_LEVEL_NAMES.get(risk_level, "UNKNOWN")
    risk_style = risk_name.lower() if risk_name.lower() in CONFIRMATION_STYLES.names else 'medium'
    
    # For high-risk operations, use a more detailed confirmation dialog
    if risk_level >= RISK_LEVELS["HIGH"]:
        return await _get_detailed_confirmation(command, risk_level, risk_reason, impact, preview, explanation)
    
    # For medium and lower risk operations, use a simpler confirmation
    return await _get_simple_confirmation(command, risk_level, risk_reason, preview, explanation)


async def _show_dry_run_preview(
    command: str, 
    risk_level: int, 
    preview: Optional[str],
    explanation: Optional[str]
) -> None:
    """Show a preview for dry run mode."""
    risk_name = RISK_LEVEL_NAMES.get(risk_level, "UNKNOWN")
    
    console.print("\n")
    console.print(Panel(
        Syntax(command, "bash", theme="monokai", word_wrap=True),
        title="[bold blue]DRY RUN PREVIEW[/bold blue]",
        subtitle=f"Risk Level: {risk_name}",
        border_style="blue",
        expand=False
    ))
    
    if explanation:
        console.print("[bold blue]Explanation:[/bold blue]")
        console.print(explanation)
    
    if preview:
        console.print(Panel(
            preview,
            title="Command Preview",
            border_style="blue",
            expand=False
        ))
    
    console.print("[blue]This is a dry run. No changes will be made.[/blue]")


async def _show_auto_execution_notice(
    command: str, 
    risk_level: int,
    preview: Optional[str]
) -> None:
    """Show a notice for auto-execution."""
    risk_name = RISK_LEVEL_NAMES.get(risk_level, "UNKNOWN")
    
    # Use a more subtle notification for auto-execution
    console.print("\n")
    console.print(Panel(
        Syntax(command, "bash", theme="monokai", word_wrap=True),
        title="Auto-Executing Command",
        border_style="green",
        expand=False
    ))
    
    # Only show preview if it's enabled in preferences
    if preview and preferences_manager.preferences.ui.show_command_preview:
        console.print(preview)
    
    # Brief pause to allow user to see what's happening
    await asyncio.sleep(0.5)


async def _get_simple_confirmation(
    command: str, 
    risk_level: int, 
    risk_reason: str,
    preview: Optional[str],
    explanation: Optional[str]
) -> bool:
    """Get a simple confirmation for medium/low risk operations."""
    risk_name = RISK_LEVEL_NAMES.get(risk_level, "UNKNOWN")
    risk_style = risk_name.lower() if risk_name.lower() in CONFIRMATION_STYLES.names else 'medium'
    
    # Display the command
    console.print("\n")
    console.print(Panel(
        Syntax(command, "bash", theme="monokai", word_wrap=True),
        title=f"Execute [{risk_name} Risk]",
        border_style=risk_style,
        expand=False
    ))
    
    # Display explanation if provided
    if explanation:
        console.print(explanation)
    
    # Display preview if available and enabled
    if preview and preferences_manager.preferences.ui.show_preview:
        console.print(Panel(
            preview,
            title="Preview",
            border_style=risk_style,
            expand=False
        ))
    
    # Use prompt_toolkit dialog for confirmation
    confirmed = yes_no_dialog(
        title=f'Execute {risk_name} Risk Command?',
        text=f'{command}\n\nReason: {risk_reason}',
        style=CONFIRMATION_STYLES
    ).run()
    
    return confirmed


async def _get_detailed_confirmation(
    command: str, 
    risk_level: int, 
    risk_reason: str,
    impact: Dict[str, Any],
    preview: Optional[str],
    explanation: Optional[str]
) -> bool:
    """Get a detailed confirmation for high/critical risk operations."""
    risk_name = RISK_LEVEL_NAMES.get(risk_level, "UNKNOWN")
    risk_style = risk_name.lower() if risk_name.lower() in CONFIRMATION_STYLES.names else 'high'
    
    # Format the command and impact information
    console.print("\n")
    console.print(Panel(
        Syntax(command, "bash", theme="monokai", word_wrap=True),
        title=f"[bold {risk_style}]HIGH RISK OPERATION[/bold {risk_style}]",
        border_style=risk_style,
        expand=False
    ))
    
    console.print(f"[bold {risk_style}]Risk Level:[/bold {risk_style}] {risk_name}")
    console.print(f"[bold {risk_style}]Reason:[/bold {risk_style}] {risk_reason}")
    
    # Display explanation if provided
    if explanation:
        console.print("[bold]Explanation:[/bold]")
        console.print(explanation)
    
    # Display impact analysis if enabled
    if preferences_manager.preferences.ui.show_impact_analysis:
        # Create a table for impact analysis
        table = Table(title="Impact Analysis", expand=True)
        table.add_column("Aspect", style="bold cyan")
        table.add_column("Details", style="white")
        
        # Add operations
        operations = ", ".join(impact.get("operations", ["unknown"]))
        table.add_row("Operations", operations)
        
        # Add warning for destructive operations
        if impact.get("destructive", False):
            table.add_row("⚠️ Warning", f"[bold {risk_style}]This operation may delete or overwrite files[/bold {risk_style}]")
        
        # Add affected files/directories
        affected_files = impact.get("affected_files", [])
        if affected_files:
            file_list = "\n".join(affected_files[:5])
            if len(affected_files) > 5:
                file_list += f"\n...and {len(affected_files) - 5} more"
            table.add_row("Affected Files", file_list)
        
        console.print(table)
    
    # Display preview if available and enabled
    if preview and preferences_manager.preferences.ui.show_command_preview:
        console.print(Panel(
            preview,
            title="Command Preview",
            border_style=risk_style,
            expand=False
        ))
    
    # For critical operations, use an even more prominent warning
    if risk_level == RISK_LEVELS["CRITICAL"]:
        console.print(Panel(
            "⚠️  [bold red]This is a CRITICAL risk operation[/bold red] ⚠️\n"
            "It may cause significant changes to your system or data loss.",
            border_style="red",
            expand=False
        ))
    
    # Use prompt_toolkit dialog for confirmation
    confirmed = yes_no_dialog(
        title=f'WARNING: Execute {risk_name} Risk Command?',
        text=f'{command}\n\nThis is a {risk_name} risk operation.\nReason: {risk_reason}\n\nAre you sure you want to proceed?',
        style=CONFIRMATION_STYLES
    ).run()
    
    # If confirmed for a high-risk operation, offer to add to trusted commands
    if confirmed and risk_level >= RISK_LEVELS["HIGH"]:
        add_to_trusted = yes_no_dialog(
            title='Add to Trusted Commands?',
            text=f'Would you like to auto-execute similar commands in the future?',
            style=CONFIRMATION_STYLES
        ).run()
        
        if add_to_trusted:
            preferences_manager.add_trusted_command(command)
    
    return confirmed


async def offer_command_learning(command: str) -> None:
    """
    After a successful execution, offer to add the command to trusted commands.
    
    Args:
        command: The command that was executed
    """
    # Check if the command should be offered for learning
    base_command = history_manager._extract_base_command(command)
    pattern = history_manager._patterns.get(base_command)
    
    # Only offer for commands used a few times but not yet trusted
    if pattern and 2 <= pattern.count <= 5 and command not in preferences_manager.preferences.trust.trusted_commands:
        add_to_trusted = yes_no_dialog(
            title='Add to Trusted Commands?',
            text=f'You\'ve used "{base_command}" {pattern.count} times. Would you like to auto-execute it in the future?',
            style=CONFIRMATION_STYLES
        ).run()
        
        if add_to_trusted:
            preferences_manager.add_trusted_command(command)
            console.print(f"Added [green]{base_command}[/green] to trusted commands.")
</file>

<file path="angela/safety/classifier.py">
"""
Command and operation risk classification system for Angela CLI.

This module is responsible for determining the risk level of commands 
and operations to ensure appropriate confirmation and safety measures.
"""
import re
import shlex
from typing import List, Dict, Tuple, Set, Optional

from angela.constants import RISK_LEVELS
from angela.utils.logging import get_logger

logger = get_logger(__name__)

# Define risk patterns for shell commands
RISK_PATTERNS = {
    # Critical risk - destructive operations
    RISK_LEVELS["CRITICAL"]: [
        # rm with recursive or force flags
        (r"^rm\s+.*((-r|-rf|--recursive|-f|--force)\b|--)", "File deletion with dangerous flags"),
        # Disk formatting
        (r"^(mkfs|fdisk|dd)\b", "Disk formatting/partitioning"), 
        # Systemwide configuration changes
        (r"^(sudo|pkexec|su)\s+", "Privileged operation"),
        # Direct writes to device files
        (r">\s*/dev/", "Direct write to device file"),
    ],
    
    # High risk - significant changes
    RISK_LEVELS["HIGH"]: [
        # Regular file deletion
        (r"^rm\s+", "File deletion"),
        # Moving files
        (r"^mv\s+", "File movement"),
        # Installing packages
        (r"^(apt(-get)?|yum|pacman|dnf|brew)\s+(install|remove|purge)\b", "Package management"),
        # Changing permissions
        (r"^chmod\s+", "Changing file permissions"),
        # Changing ownership
        (r"^chown\s+", "Changing file ownership"),
    ],
    
    # Medium risk - file modifications
    RISK_LEVELS["MEDIUM"]: [
        # Writing to files
        (r"(>|>>)\s*[\w\./-]+", "Writing to files"),
        # Editing files
        (r"^(nano|vim|vi|emacs|sed)\s+", "File editing"),
        # Creating symbolic links
        (r"^ln\s+(-s|--symbolic)\s+", "Creating symbolic links"),
        # Transferring files remotely
        (r"^(scp|rsync)\s+", "File transfer"),
    ],
    
    # Low risk - creating files/dirs without overwriting
    RISK_LEVELS["LOW"]: [
        # Making directories
        (r"^mkdir\s+", "Creating directory"),
        # Touching files
        (r"^touch\s+", "Creating/updating file timestamp"),
        # Copying files
        (r"^cp\s+", "Copying files"),
    ],
    
    # Safe - read-only operations
    RISK_LEVELS["SAFE"]: [
        # Listing files
        (r"^ls\s+", "Listing files"),
        # Reading files
        (r"^(cat|less|more|head|tail)\s+", "Reading file content"),
        # Finding files
        (r"^find\s+", "Finding files"),
        # Viewing disk usage
        (r"^du\s+", "Checking disk usage"),
        # Getting working directory
        (r"^pwd\s*$", "Printing working directory"),
        # Checking file status
        (r"^(stat|file)\s+", "Checking file information"),
    ],
}

# Special case patterns that override normal classification
OVERRIDE_PATTERNS = {
    # Force certain grep operations to be safe
    "SAFE": [
        r"^grep\s+(-r|--recursive)?\s+[\w\s]+\s+[\w\s\./-]+$",  # Basic grep with fixed strings
        r"^find\s+[\w\s\./-]+\s+-name\s+[\w\s\*\./-]+$",  # Basic find by name
    ],
    # Operations that should always be considered critical regardless of base command
    "CRITICAL": [
        r"[\s;|`]+rm\s+(-r|-f|--recursive|--force)\s+[~/]",  # rm commands affecting home or root
        r"[\s;|`]+dd\s+",  # dd embedded in a command chain
        r">/dev/null\s+2>&1",  # Redirecting errors (often hiding destructive operations)
    ],
}

def classify_command_risk(command: str) -> Tuple[int, str]:
    """
    Classify the risk level of a shell command.
    
    Args:
        command: The shell command to classify.
        
    Returns:
        A tuple of (risk_level, reason), where risk_level is an integer
        from the RISK_LEVELS constants and reason is a string explaining
        the classification.
    """
    if not command.strip():
        return RISK_LEVELS["SAFE"], "Empty command"
    
    # First check override patterns that would force a specific risk level
    for level_name, patterns in OVERRIDE_PATTERNS.items():
        for pattern in patterns:
            if re.search(pattern, command):
                level = RISK_LEVELS[level_name]
                return level, f"Matched override pattern for {level_name} risk"
    
    # Check standard risk patterns from highest to lowest risk
    for level, patterns in sorted(RISK_PATTERNS.items(), key=lambda x: x[0], reverse=True):
        for pattern, reason in patterns:
            if re.search(pattern, command.strip()):
                return level, reason
    
    # Default to medium risk if no pattern matches
    # It's safer to require confirmation for unknown commands
    return RISK_LEVELS["MEDIUM"], "Unrecognized command type"


def analyze_command_impact(command: str) -> Dict[str, any]:
    """
    Analyze the potential impact of a command.
    
    Args:
        command: The shell command to analyze.
        
    Returns:
        A dictionary containing impact analysis information, such as
        affected files, operations, etc.
    """
    impact = {
        "affected_files": set(),
        "affected_dirs": set(),
        "operations": [],
        "destructive": False,
        "creates_files": False,
        "modifies_files": False,
    }
    
    try:
        # Simple lexical analysis of the command
        tokens = shlex.split(command)
        if not tokens:
            return impact
        
        base_cmd = tokens[0]
        args = tokens[1:]
        
        # Extract potentially affected files and directories
        for arg in args:
            # Skip arguments that start with a dash (options)
            if arg.startswith('-'):
                continue
            
            # Skip redirection operators
            if arg in ['>', '>>', '<', '|']:
                continue
                
            # Assume it might be a file or directory path
            if '/' in arg or '.' in arg or not arg.startswith('-'):
                if base_cmd in ['rm', 'mv', 'rmdir']:
                    impact["destructive"] = True
                
                if base_cmd in ['mkdir']:
                    impact["affected_dirs"].add(arg)
                    impact["creates_files"] = True
                else:
                    impact["affected_files"].add(arg)
                
                if base_cmd in ['cp', 'mv', 'touch', 'mkdir', 'ln']:
                    impact["creates_files"] = True
                
                if base_cmd in ['vim', 'nano', 'sed', 'cp', 'mv']:
                    impact["modifies_files"] = True
        
        # Record the type of operation
        if base_cmd in ['ls', 'find', 'grep', 'cat', 'less', 'more', 'tail', 'head']:
            impact["operations"].append("read")
        elif base_cmd in ['rm', 'rmdir']:
            impact["operations"].append("delete")
        elif base_cmd in ['mv']:
            impact["operations"].append("move")
        elif base_cmd in ['cp']:
            impact["operations"].append("copy")
        elif base_cmd in ['touch', 'mkdir']:
            impact["operations"].append("create")
        elif base_cmd in ['chmod', 'chown']:
            impact["operations"].append("change_attributes")
        else:
            impact["operations"].append("unknown")
    
    except Exception as e:
        logger.exception(f"Error analyzing command impact for '{command}': {str(e)}")
    
    # Convert sets to lists for easier serialization
    impact["affected_files"] = list(impact["affected_files"])
    impact["affected_dirs"] = list(impact["affected_dirs"])
    
    return impact
</file>

<file path="angela/toolchain/ci_cd.py">
# angela/toolchain/ci_cd.py
"""
CI/CD configuration generation for Angela CLI.

This module provides functionality for generating CI/CD configurations
for common CI platforms.
"""
import os
from pathlib import Path
from typing import Dict, Any, List, Optional, Union
import yaml
import json

from angela.utils.logging import get_logger
from angela.context import context_manager

logger = get_logger(__name__)

class CiCdIntegration:
    """
    Integration for CI/CD platforms.
    """
    
    def __init__(self):
        """Initialize the CI/CD integration."""
        self._logger = logger
        
        # Supported CI/CD platforms
        self._supported_platforms = [
            "github_actions",
            "gitlab_ci",
            "jenkins",
            "travis",
            "circle_ci"
        ]
    
    async def detect_project_type(
        self, 
        path: Union[str, Path]
    ) -> Dict[str, Any]:
        """
        Detect the project type for CI/CD configuration.
        
        Args:
            path: Path to the project
            
        Returns:
            Dictionary with the detected project info
        """
        self._logger.info(f"Detecting project type in {path}")
        
        path_obj = Path(path)
        
        # Check if path exists
        if not path_obj.exists() or not path_obj.is_dir():
            return {
                "detected": False,
                "error": f"Path does not exist or is not a directory: {path}",
                "project_type": None
            }
        
        # Check for project type indicators
        project_type = None
        
        # Python indicators
        if (path_obj / "requirements.txt").exists() or (path_obj / "setup.py").exists() or (path_obj / "pyproject.toml").exists():
            project_type = "python"
        # Node.js indicators
        elif (path_obj / "package.json").exists():
            project_type = "node"
        # Go indicators
        elif (path_obj / "go.mod").exists():
            project_type = "go"
        # Rust indicators
        elif (path_obj / "Cargo.toml").exists():
            project_type = "rust"
        # Java indicators
        elif (path_obj / "pom.xml").exists():
            project_type = "java"
        elif (path_obj / "build.gradle").exists() or (path_obj / "build.gradle.kts").exists():
            project_type = "java"
        # Ruby indicators
        elif (path_obj / "Gemfile").exists():
            project_type = "ruby"
        
        if project_type:
            return {
                "detected": True,
                "project_type": project_type,
                "project_path": str(path_obj)
            }
        
        # Try from context
        context = context_manager.get_context_dict()
        if context.get("project_type"):
            return {
                "detected": True,
                "project_type": context["project_type"],
                "project_path": str(path_obj),
                "from_context": True
            }
        
        return {
            "detected": False,
            "error": "Could not detect project type",
            "project_type": None
        }
    
    async def generate_ci_configuration(
        self, 
        path: Union[str, Path],
        platform: str,
        project_type: Optional[str] = None,
        custom_settings: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Generate a CI/CD configuration file.
        
        Args:
            path: Path to the project
            platform: CI/CD platform to generate for
            project_type: Optional project type
            custom_settings: Optional custom settings
            
        Returns:
            Dictionary with the generation result
        """
        self._logger.info(f"Generating CI configuration for {platform}")
        
        path_obj = Path(path)
        
        # Check if platform is supported
        if platform not in self._supported_platforms:
            return {
                "success": False,
                "error": f"Unsupported CI/CD platform: {platform}",
                "platform": platform
            }
        
        # Detect project type if not provided
        if project_type is None:
            detection_result = await self.detect_project_type(path_obj)
            project_type = detection_result.get("project_type")
            
            if not project_type:
                return {
                    "success": False,
                    "error": f"Could not detect project type: {detection_result.get('error', 'Unknown error')}",
                    "platform": platform
                }
        
        # Generate configuration based on platform
        if platform == "github_actions":
            return await self._generate_github_actions(path_obj, project_type, custom_settings)
        elif platform == "gitlab_ci":
            return await self._generate_gitlab_ci(path_obj, project_type, custom_settings)
        elif platform == "jenkins":
            return await self._generate_jenkins(path_obj, project_type, custom_settings)
        elif platform == "travis":
            return await self._generate_travis(path_obj, project_type, custom_settings)
        elif platform == "circle_ci":
            return await self._generate_circle_ci(path_obj, project_type, custom_settings)
        
        return {
            "success": False,
            "error": f"Unsupported CI/CD platform: {platform}",
            "platform": platform
        }
    
    async def _generate_github_actions(
        self, 
        path: Path,
        project_type: str,
        custom_settings: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Generate GitHub Actions configuration.
        
        Args:
            path: Path to the project
            project_type: Project type
            custom_settings: Optional custom settings
            
        Returns:
            Dictionary with the generation result
        """
        self._logger.info(f"Generating GitHub Actions configuration for {project_type}")
        
        # Create .github/workflows directory
        workflows_dir = path / ".github" / "workflows"
        if not workflows_dir.exists():
            os.makedirs(workflows_dir, exist_ok=True)
        
        # Get configuration settings
        settings = {}
        if custom_settings:
            settings.update(custom_settings)
        
        # Set default settings based on project type
        if project_type == "python":
            workflow = {
                "name": "Python CI",
                "on": {
                    "push": {
                        "branches": ["main", "master"]
                    },
                    "pull_request": {
                        "branches": ["main", "master"]
                    }
                },
                "jobs": {
                    "build": {
                        "runs-on": "ubuntu-latest",
                        "strategy": {
                            "matrix": {
                                "python-version": ["3.8", "3.9", "3.10"]
                            }
                        },
                        "steps": [
                            {
                                "name": "Checkout code",
                                "uses": "actions/checkout@v3"
                            },
                            {
                                "name": "Set up Python ${{ matrix.python-version }}",
                                "uses": "actions/setup-python@v4",
                                "with": {
                                    "python-version": "${{ matrix.python-version }}"
                                }
                            },
                            {
                                "name": "Install dependencies",
                                "run": "python -m pip install --upgrade pip\npip install -r requirements.txt\npip install pytest pytest-cov flake8"
                            },
                            {
                                "name": "Lint with flake8",
                                "run": "flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics"
                            },
                            {
                                "name": "Test with pytest",
                                "run": "pytest --cov=. --cov-report=xml"
                            },
                            {
                                "name": "Upload coverage to Codecov",
                                "uses": "codecov/codecov-action@v3",
                                "with": {
                                    "file": "./coverage.xml",
                                    "fail_ci_if_error": "false"
                                }
                            }
                        ]
                    }
                }
            }
        elif project_type == "node":
            workflow = {
                "name": "Node.js CI",
                "on": {
                    "push": {
                        "branches": ["main", "master"]
                    },
                    "pull_request": {
                        "branches": ["main", "master"]
                    }
                },
                "jobs": {
                    "build": {
                        "runs-on": "ubuntu-latest",
                        "strategy": {
                            "matrix": {
                                "node-version": ["14.x", "16.x", "18.x"]
                            }
                        },
                        "steps": [
                            {
                                "name": "Checkout code",
                                "uses": "actions/checkout@v3"
                            },
                            {
                                "name": "Use Node.js ${{ matrix.node-version }}",
                                "uses": "actions/setup-node@v3",
                                "with": {
                                    "node-version": "${{ matrix.node-version }}",
                                    "cache": "npm"
                                }
                            },
                            {
                                "name": "Install dependencies",
                                "run": "npm ci"
                            },
                            {
                                "name": "Run linting",
                                "run": "npm run lint --if-present"
                            },
                            {
                                "name": "Build",
                                "run": "npm run build --if-present"
                            },
                            {
                                "name": "Test",
                                "run": "npm test"
                            }
                        ]
                    }
                }
            }
        elif project_type == "go":
            workflow = {
                "name": "Go CI",
                "on": {
                    "push": {
                        "branches": ["main", "master"]
                    },
                    "pull_request": {
                        "branches": ["main", "master"]
                    }
                },
                "jobs": {
                    "build": {
                        "runs-on": "ubuntu-latest",
                        "steps": [
                            {
                                "name": "Checkout code",
                                "uses": "actions/checkout@v3"
                            },
                            {
                                "name": "Set up Go",
                                "uses": "actions/setup-go@v3",
                                "with": {
                                    "go-version": "1.18"
                                }
                            },
                            {
                                "name": "Build",
                                "run": "go build -v ./..."
                            },
                            {
                                "name": "Test",
                                "run": "go test -v ./..."
                            }
                        ]
                    }
                }
            }
        # Add other project types as needed
        
        # Update with custom settings
        if settings:
            # Use recursive update function (not shown) or libraries
            # This is a simplified approach
            if "jobs" in settings and "build" in settings["jobs"] and "steps" in settings["jobs"]["build"]:
                # Append custom steps
                workflow["jobs"]["build"]["steps"].extend(settings["jobs"]["build"]["steps"])
        
        # Write the workflow file
        workflow_file = workflows_dir / f"{project_type}-ci.yml"
        try:
            with open(workflow_file, 'w') as f:
                yaml.dump(workflow, f, default_flow_style=False, sort_keys=False)
            
            return {
                "success": True,
                "platform": "github_actions",
                "project_type": project_type,
                "config_file": str(workflow_file)
            }
        except Exception as e:
            return {
                "success": False,
                "error": f"Failed to write GitHub Actions workflow: {str(e)}",
                "platform": "github_actions",
                "project_type": project_type
            }
    
    async def _generate_gitlab_ci(
        self, 
        path: Path,
        project_type: str,
        custom_settings: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Generate GitLab CI configuration.
        
        Args:
            path: Path to the project
            project_type: Project type
            custom_settings: Optional custom settings
            
        Returns:
            Dictionary with the generation result
        """
        self._logger.info(f"Generating GitLab CI configuration for {project_type}")
        
        # Get configuration settings
        settings = {}
        if custom_settings:
            settings.update(custom_settings)
        
        # Set default settings based on project type
        if project_type == "python":
            config = {
                "image": "python:3.9",
                "stages": ["test", "build", "deploy"],
                "before_script": [
                    "python -V",
                    "pip install -r requirements.txt"
                ],
                "test": {
                    "stage": "test",
                    "script": [
                        "pip install pytest pytest-cov",
                        "pytest --cov=. --cov-report=xml",
                    ],
                    "artifacts": {
                        "reports": {
                            "coverage_report": {
                                "coverage_format": "cobertura",
                                "path": "coverage.xml"
                            }
                        }
                    }
                },
                "lint": {
                    "stage": "test",
                    "script": [
                        "pip install flake8",
                        "flake8 ."
                    ]
                },
                "build": {
                    "stage": "build",
                    "script": [
                        "echo 'Building package'",
                        "pip install build",
                        "python -m build"
                    ],
                    "artifacts": {
                        "paths": ["dist/"]
                    }
                }
            }
        elif project_type == "node":
            config = {
                "image": "node:16",
                "stages": ["test", "build", "deploy"],
                "cache": {
                    "paths": ["node_modules/"]
                },
                "install_dependencies": {
                    "stage": "test",
                    "script": ["npm ci"]
                },
                "test": {
                    "stage": "test",
                    "script": ["npm test"]
                },
                "lint": {
                    "stage": "test",
                    "script": ["npm run lint"]
                },
                "build": {
                    "stage": "build",
                    "script": ["npm run build"],
                    "artifacts": {
                        "paths": ["dist/", "build/"]
                    }
                }
            }
        # Add other project types as needed
        
        # Update with custom settings
        if settings:
            # Use recursive update function or libraries
            # This is a simplified approach
            for key, value in settings.items():
                if isinstance(value, dict) and key in config and isinstance(config[key], dict):
                    config[key].update(value)
                else:
                    config[key] = value
        
        # Write the config file
        config_file = path / ".gitlab-ci.yml"
        try:
            with open(config_file, 'w') as f:
                yaml.dump(config, f, default_flow_style=False, sort_keys=False)
            
            return {
                "success": True,
                "platform": "gitlab_ci",
                "project_type": project_type,
                "config_file": str(config_file)
            }
        except Exception as e:
            return {
                "success": False,
                "error": f"Failed to write GitLab CI config: {str(e)}",
                "platform": "gitlab_ci",
                "project_type": project_type
            }
    
    async def _generate_jenkins(
        self, 
        path: Path,
        project_type: str,
        custom_settings: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Generate Jenkins configuration (Jenkinsfile).
        
        Args:
            path: Path to the project
            project_type: Project type
            custom_settings: Optional custom settings
            
        Returns:
            Dictionary with the generation result
        """
        self._logger.info(f"Generating Jenkins configuration for {project_type}")
        
        # Set Jenkinsfile content based on project type
        if project_type == "python":
            content = """
pipeline {
    agent {
        docker {
            image 'python:3.9'
        }
    }
    stages {
        stage('Build') {
            steps {
                sh 'python -m pip install --upgrade pip'
                sh 'pip install -r requirements.txt'
            }
        }
        stage('Test') {
            steps {
                sh 'pip install pytest pytest-cov'
                sh 'pytest --cov=. --cov-report=xml'
            }
            post {
                always {
                    junit 'pytest-results.xml'
                    cobertura coberturaReportFile: 'coverage.xml'
                }
            }
        }
        stage('Lint') {
            steps {
                sh 'pip install flake8'
                sh 'flake8 .'
            }
        }
        stage('Deploy') {
            when {
                branch 'main'
            }
            steps {
                echo 'Deploying to production...'
                // Add deployment steps here
            }
        }
    }
}
"""
        elif project_type == "node":
            content = """
pipeline {
    agent {
        docker {
            image 'node:16'
        }
    }
    stages {
        stage('Build') {
            steps {
                sh 'npm ci'
                sh 'npm run build --if-present'
            }
        }
        stage('Test') {
            steps {
                sh 'npm test'
            }
        }
        stage('Lint') {
            steps {
                sh 'npm run lint --if-present'
            }
        }
        stage('Deploy') {
            when {
                branch 'main'
            }
            steps {
                echo 'Deploying to production...'
                // Add deployment steps here
            }
        }
    }
}
"""
        # Add other project types as needed
        
        # Update with custom settings
        # For Jenkins, we'd need more sophisticated templating to properly merge
        # custom settings into the Jenkinsfile
        
        # Write the Jenkinsfile
        jenkinsfile_path = path / "Jenkinsfile"
        try:
            with open(jenkinsfile_path, 'w') as f:
                f.write(content.strip())
            
            return {
                "success": True,
                "platform": "jenkins",
                "project_type": project_type,
                "config_file": str(jenkinsfile_path)
            }
        except Exception as e:
            return {
                "success": False,
                "error": f"Failed to write Jenkinsfile: {str(e)}",
                "platform": "jenkins",
                "project_type": project_type
            }
    
    async def _generate_travis(
        self, 
        path: Path,
        project_type: str,
        custom_settings: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Generate Travis CI configuration.
        
        Args:
            path: Path to the project
            project_type: Project type
            custom_settings: Optional custom settings
            
        Returns:
            Dictionary with the generation result
        """
        self._logger.info(f"Generating Travis CI configuration for {project_type}")
        
        # Get configuration settings
        settings = {}
        if custom_settings:
            settings.update(custom_settings)
        
        # Set default settings based on project type
        if project_type == "python":
            config = {
                "language": "python",
                "python": ["3.8", "3.9", "3.10"],
                "install": [
                    "pip install -r requirements.txt",
                    "pip install pytest pytest-cov flake8"
                ],
                "script": [
                    "flake8 .",
                    "pytest --cov=."
                ],
                "after_success": [
                    "bash <(curl -s https://codecov.io/bash)"
                ]
            }
        elif project_type == "node":
            config = {
                "language": "node_js",
                "node_js": ["14", "16", "18"],
                "cache": "npm",
                "install": [
                    "npm ci"
                ],
                "script": [
                    "npm run lint --if-present",
                    "npm run build --if-present",
                    "npm test"
                ]
            }
        # Add other project types as needed
        
        # Update with custom settings
        if settings:
            for key, value in settings.items():
                if isinstance(value, list) and key in config and isinstance(config[key], list):
                    config[key].extend(value)
                else:
                    config[key] = value
        
        # Write the config file
        config_file = path / ".travis.yml"
        try:
            with open(config_file, 'w') as f:
                yaml.dump(config, f, default_flow_style=False, sort_keys=False)
            
            return {
                "success": True,
                "platform": "travis",
                "project_type": project_type,
                "config_file": str(config_file)
            }
        except Exception as e:
            return {
                "success": False,
                "error": f"Failed to write Travis CI config: {str(e)}",
                "platform": "travis",
                "project_type": project_type
            }
    
    async def _generate_circle_ci(
        self, 
        path: Path,
        project_type: str,
        custom_settings: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Generate CircleCI configuration.
        
        Args:
            path: Path to the project
            project_type: Project type
            custom_settings: Optional custom settings
            
        Returns:
            Dictionary with the generation result
        """
        self._logger.info(f"Generating CircleCI configuration for {project_type}")
        
        # Create .circleci directory
        circleci_dir = path / ".circleci"
        if not circleci_dir.exists():
            os.makedirs(circleci_dir, exist_ok=True)
        
        # Get configuration settings
        settings = {}
        if custom_settings:
            settings.update(custom_settings)
        
        # Set default settings based on project type
        if project_type == "python":
            config = {
                "version": 2.1,
                "orbs": {
                    "python": "circleci/python@1.5"
                },
                "jobs": {
                    "build-and-test": {
                        "docker": [
                            {"image": "cimg/python:3.9"}
                        ],
                        "steps": [
                            "checkout",
                            {
                                "python/install-packages": {
                                    "pkg-manager": "pip",
                                    "packages": [
                                        "pytest",
                                        "pytest-cov"
                                    ]
                                }
                            },
                            {
                                "run": {
                                    "name": "Install dependencies",
                                    "command": "pip install -r requirements.txt"
                                }
                            },
                            {
                                "run": {
                                    "name": "Run tests",
                                    "command": "pytest --cov=. --cov-report=xml"
                                }
                            },
                            {
                                "store_artifacts": {
                                    "path": "coverage.xml"
                                }
                            }
                        ]
                    }
                },
                "workflows": {
                    "main": {
                        "jobs": [
                            "build-and-test"
                        ]
                    }
                }
            }
        elif project_type == "node":
            config = {
                "version": 2.1,
                "orbs": {
                    "node": "circleci/node@5.0.0"
                },
                "jobs": {
                    "build-and-test": {
                        "docker": [
                            {"image": "cimg/node:16.14"}
                        ],
                        "steps": [
                            "checkout",
                            {
                                "node/install-packages": {
                                    "pkg-manager": "npm"
                                }
                            },
                            {
                                "run": {
                                    "name": "Run tests",
                                    "command": "npm test"
                                }
                            },
                            {
                                "run": {
                                    "name": "Run lint",
                                    "command": "npm run lint --if-present"
                                }
                            },
                            {
                                "run": {
                                    "name": "Build",
                                    "command": "npm run build --if-present"
                                }
                            }
                        ]
                    }
                },
                "workflows": {
                    "main": {
                        "jobs": [
                            "build-and-test"
                        ]
                    }
                }
            }
        # Add other project types as needed
        
        # Update with custom settings
        if settings:
            # This is a simplified approach; in a real implementation, 
            # we'd need more sophisticated merging
            if "jobs" in settings and "build-and-test" in settings["jobs"] and "steps" in settings["jobs"]["build-and-test"]:
                config["jobs"]["build-and-test"]["steps"].extend(settings["jobs"]["build-and-test"]["steps"])
        
        # Write the config file
        config_file = circleci_dir / "config.yml"
        try:
            with open(config_file, 'w') as f:
                yaml.dump(config, f, default_flow_style=False, sort_keys=False)
            
            return {
                "success": True,
                "platform": "circle_ci",
                "project_type": project_type,
                "config_file": str(config_file)
            }
        except Exception as e:
            return {
                "success": False,
                "error": f"Failed to write CircleCI config: {str(e)}",
                "platform": "circle_ci",
                "project_type": project_type
            }

# Global CI/CD integration instance
ci_cd_integration = CiCdIntegration()
</file>

<file path="angela/toolchain/git.py">
# angela/toolchain/git.py
"""
Enhanced Git integration for Angela CLI.

This module provides advanced Git functionality for the code generation lifecycle,
such as automatic repository initialization, commit management, and feature branch creation.
"""
import os
import asyncio
import subprocess
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple, Union
import re

from angela.utils.logging import get_logger
from angela.execution.engine import execution_engine

logger = get_logger(__name__)

class GitIntegration:
    """
    Enhanced Git integration for the code generation lifecycle.
    """
    
    def __init__(self):
        """Initialize the Git integration."""
        self._logger = logger
    
    async def init_repository(
        self, 
        path: Union[str, Path], 
        initial_branch: str = "main",
        gitignore_template: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Initialize a Git repository.
        
        Args:
            path: Path to initialize the repository in
            initial_branch: Name of the initial branch
            gitignore_template: Optional template for .gitignore (e.g., 'python', 'node')
            
        Returns:
            Dictionary with the operation result
        """
        self._logger.info(f"Initializing Git repository in {path}")
        
        path_obj = Path(path)
        
        # Check if path exists
        if not path_obj.exists():
            return {
                "success": False,
                "error": f"Path does not exist: {path}",
                "command": None,
                "stdout": "",
                "stderr": f"Path does not exist: {path}"
            }
        
        # Check if already a Git repository
        if (path_obj / ".git").exists():
            return {
                "success": True,
                "message": "Repository already initialized",
                "command": None,
                "stdout": "Repository already initialized",
                "stderr": ""
            }
        
        # Initialize the repository
        init_command = f"git init -b {initial_branch}"
        
        # Execute the command
        stdout, stderr, return_code = await execution_engine.execute_command(
            init_command,
            check_safety=True,
            working_dir=str(path_obj)
        )
        
        if return_code != 0:
            return {
                "success": False,
                "error": f"Failed to initialize repository: {stderr}",
                "command": init_command,
                "stdout": stdout,
                "stderr": stderr
            }
        
        # Create .gitignore if requested
        if gitignore_template:
            gitignore_result = await self._create_gitignore(path_obj, gitignore_template)
            if not gitignore_result["success"]:
                # Continue even if gitignore creation fails
                self._logger.warning(f"Failed to create .gitignore: {gitignore_result['error']}")
        
        return {
            "success": True,
            "message": "Repository initialized successfully",
            "command": init_command,
            "stdout": stdout,
            "stderr": stderr
        }
    
    async def stage_files(
        self, 
        path: Union[str, Path], 
        files: Optional[List[str]] = None
    ) -> Dict[str, Any]:
        """
        Stage files for a Git commit.
        
        Args:
            path: Path to the Git repository
            files: List of files to stage (all files if None)
            
        Returns:
            Dictionary with the operation result
        """
        self._logger.info(f"Staging files in {path}")
        
        path_obj = Path(path)
        
        # Check if path is a Git repository
        if not (path_obj / ".git").exists():
            return {
                "success": False,
                "error": f"Not a Git repository: {path}",
                "command": None,
                "stdout": "",
                "stderr": f"Not a Git repository: {path}"
            }
        
        # Build the git add command
        if files:
            # Quote file paths to handle spaces
            quoted_files = [f'"{f}"' for f in files]
            add_command = f"git add {' '.join(quoted_files)}"
        else:
            add_command = "git add ."
        
        # Execute the command
        stdout, stderr, return_code = await execution_engine.execute_command(
            add_command,
            check_safety=True,
            working_dir=str(path_obj)
        )
        
        if return_code != 0:
            return {
                "success": False,
                "error": f"Failed to stage files: {stderr}",
                "command": add_command,
                "stdout": stdout,
                "stderr": stderr
            }
        
        return {
            "success": True,
            "message": "Files staged successfully",
            "command": add_command,
            "stdout": stdout,
            "stderr": stderr
        }
    
    async def commit_changes(
        self, 
        path: Union[str, Path], 
        message: str,
        files: Optional[List[str]] = None,
        auto_stage: bool = True
    ) -> Dict[str, Any]:
        """
        Commit changes to a Git repository.
        
        Args:
            path: Path to the Git repository
            message: Commit message
            files: Optional list of files to commit (all staged files if None)
            auto_stage: Whether to automatically stage files before committing
            
        Returns:
            Dictionary with the operation result
        """
        self._logger.info(f"Committing changes in {path}")
        
        path_obj = Path(path)
        
        # Check if path is a Git repository
        if not (path_obj / ".git").exists():
            return {
                "success": False,
                "error": f"Not a Git repository: {path}",
                "command": None,
                "stdout": "",
                "stderr": f"Not a Git repository: {path}"
            }
        
        # Stage files if requested
        if auto_stage:
            stage_result = await self.stage_files(path_obj, files)
            if not stage_result["success"]:
                return stage_result
        
        # Build the git commit command
        commit_command = f'git commit -m "{message}"'
        
        # Add specific files if provided and not auto-staging
        if files and not auto_stage:
            # Quote file paths to handle spaces
            quoted_files = [f'"{f}"' for f in files]
            commit_command += f" {' '.join(quoted_files)}"
        
        # Execute the command
        stdout, stderr, return_code = await execution_engine.execute_command(
            commit_command,
            check_safety=True,
            working_dir=str(path_obj)
        )
        
        if return_code != 0:
            return {
                "success": False,
                "error": f"Failed to commit changes: {stderr}",
                "command": commit_command,
                "stdout": stdout,
                "stderr": stderr
            }
        
        return {
            "success": True,
            "message": "Changes committed successfully",
            "command": commit_command,
            "stdout": stdout,
            "stderr": stderr
        }
    
    async def create_branch(
        self, 
        path: Union[str, Path], 
        branch_name: str,
        checkout: bool = True,
        start_point: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Create a new Git branch.
        
        Args:
            path: Path to the Git repository
            branch_name: Name of the branch to create
            checkout: Whether to check out the new branch
            start_point: Optional starting point for the branch
            
        Returns:
            Dictionary with the operation result
        """
        self._logger.info(f"Creating branch {branch_name} in {path}")
        
        path_obj = Path(path)
        
        # Check if path is a Git repository
        if not (path_obj / ".git").exists():
            return {
                "success": False,
                "error": f"Not a Git repository: {path}",
                "command": None,
                "stdout": "",
                "stderr": f"Not a Git repository: {path}"
            }
        
        # Build the git branch command
        if checkout:
            branch_command = f"git checkout -b {branch_name}"
        else:
            branch_command = f"git branch {branch_name}"
        
        # Add start point if provided
        if start_point:
            branch_command += f" {start_point}"
        
        # Execute the command
        stdout, stderr, return_code = await execution_engine.execute_command(
            branch_command,
            check_safety=True,
            working_dir=str(path_obj)
        )
        
        if return_code != 0:
            return {
                "success": False,
                "error": f"Failed to create branch: {stderr}",
                "command": branch_command,
                "stdout": stdout,
                "stderr": stderr
            }
        
        return {
            "success": True,
            "message": f"Branch {branch_name} created successfully",
            "command": branch_command,
            "stdout": stdout,
            "stderr": stderr
        }
    
    async def get_repository_status(
        self, 
        path: Union[str, Path]
    ) -> Dict[str, Any]:
        """
        Get the status of a Git repository.
        
        Args:
            path: Path to the Git repository
            
        Returns:
            Dictionary with the repository status
        """
        path_obj = Path(path)
        
        # Check if path is a Git repository
        if not (path_obj / ".git").exists():
            return {
                "is_repo": False,
                "error": f"Not a Git repository: {path}",
                "command": None,
                "stdout": "",
                "stderr": f"Not a Git repository: {path}"
            }
        
        # Get current branch
        branch_command = "git branch --show-current"
        branch_stdout, branch_stderr, branch_code = await execution_engine.execute_command(
            branch_command,
            check_safety=True,
            working_dir=str(path_obj)
        )
        
        current_branch = branch_stdout.strip() if branch_code == 0 else "unknown"
        
        # Get status
        status_command = "git status --porcelain"
        status_stdout, status_stderr, status_code = await execution_engine.execute_command(
            status_command,
            check_safety=True,
            working_dir=str(path_obj)
        )
        
        if status_code != 0:
            return {
                "is_repo": True,
                "current_branch": current_branch,
                "error": f"Failed to get status: {status_stderr}",
                "command": status_command,
                "stdout": status_stdout,
                "stderr": status_stderr
            }
        
        # Parse status output
        status_lines = status_stdout.strip().split('\n') if status_stdout.strip() else []
        
        modified_files = []
        untracked_files = []
        staged_files = []
        
        for line in status_lines:
            if not line:
                continue
                
            status_code = line[:2]
            file_path = line[3:]
            
            if status_code.startswith('??'):
                untracked_files.append(file_path)
            elif status_code.startswith('M'):
                modified_files.append(file_path)
            elif status_code.startswith('A'):
                staged_files.append(file_path)
        
        return {
            "is_repo": True,
            "current_branch": current_branch,
            "modified_files": modified_files,
            "untracked_files": untracked_files,
            "staged_files": staged_files,
            "clean": len(status_lines) == 0,
            "command": status_command,
            "stdout": status_stdout,
            "stderr": status_stderr
        }
    
    async def _create_gitignore(
        self, 
        path: Union[str, Path], 
        template: str
    ) -> Dict[str, Any]:
        """
        Create a .gitignore file from a template.
        
        Args:
            path: Path to the Git repository
            template: Template to use (e.g., 'python', 'node')
            
        Returns:
            Dictionary with the operation result
        """
        self._logger.info(f"Creating .gitignore with template {template} in {path}")
        
        path_obj = Path(path)
        
        # Check if .gitignore already exists
        gitignore_path = path_obj / ".gitignore"
        if gitignore_path.exists():
            return {
                "success": True,
                "message": ".gitignore already exists",
                "path": str(gitignore_path),
                "modified": False
            }
        
        # Get template content
        if template == "python":
            content = """
# Byte-compiled / optimized / DLL files
__pycache__/
*.py[cod]
*$py.class

# Distribution / packaging
dist/
build/
*.egg-info/

# Virtual environments
venv/
env/
.env/
.venv/

# Unit test / coverage reports
htmlcov/
.tox/
.coverage
.coverage.*
.cache
coverage.xml
*.cover

# Local development settings
.env
.env.local

# IDE specific files
.idea/
.vscode/
*.swp
*.swo
"""
        elif template == "node":
            content = """
# Logs
logs
*.log
npm-debug.log*
yarn-debug.log*
yarn-error.log*

# Dependency directories
node_modules/
jspm_packages/

# Build output
dist/
build/

# Environment variables
.env
.env.local
.env.development
.env.test
.env.production

# IDE specific files
.idea/
.vscode/
*.swp
*.swo

# OS specific files
.DS_Store
Thumbs.db
"""
        else:
            # Generic gitignore
            content = """
# IDE specific files
.idea/
.vscode/
*.swp
*.swo

# OS specific files
.DS_Store
Thumbs.db

# Local development settings
.env
.env.local

# Logs
*.log
"""
        
        # Write the .gitignore file
        try:
            with open(gitignore_path, 'w') as f:
                f.write(content.strip())
            
            return {
                "success": True,
                "message": ".gitignore created successfully",
                "path": str(gitignore_path),
                "modified": True
            }
        except Exception as e:
            return {
                "success": False,
                "error": f"Failed to create .gitignore: {str(e)}",
                "path": str(gitignore_path),
                "modified": False
            }

# Global Git integration instance
git_integration = GitIntegration()
</file>

<file path="angela/toolchain/package_managers.py">
# angela/toolchain/package_managers.py
"""
Package manager integration for Angela CLI.

This module provides functionality for interacting with package managers
to install dependencies required by generated code.
"""
import os
import json
import asyncio
import subprocess
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple, Union
import re

from angela.utils.logging import get_logger
from angela.execution.engine import execution_engine
from angela.context import context_manager

logger = get_logger(__name__)

class PackageManagerIntegration:
    """
    Integration with package managers for dependency management.
    """
    
    def __init__(self):
        """Initialize the package manager integration."""
        self._logger = logger
        
        # Map of project types to package managers
        self._package_managers = {
            "python": ["pip", "pipenv", "poetry"],
            "node": ["npm", "yarn", "pnpm"],
            "ruby": ["gem", "bundler"],
            "php": ["composer"],
            "go": ["go"],
            "rust": ["cargo"],
            "java": ["maven", "gradle"]
        }
    
    async def detect_package_manager(
        self, 
        path: Union[str, Path],
        project_type: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Detect the package manager used in a project.
        
        Args:
            path: Path to the project
            project_type: Optional type of project
            
        Returns:
            Dictionary with the detected package manager info
        """
        self._logger.info(f"Detecting package manager in {path}")
        
        path_obj = Path(path)
        
        # Check if path exists
        if not path_obj.exists() or not path_obj.is_dir():
            return {
                "detected": False,
                "error": f"Path does not exist or is not a directory: {path}",
                "package_manager": None,
                "project_type": project_type
            }
        
        # Determine project type if not provided
        if project_type is None:
            # Try to detect from context
            context = context_manager.get_context_dict()
            if context.get("project_type"):
                project_type = context["project_type"]
            else:
                # Try to infer from files
                project_type = await self._infer_project_type(path_obj)
        
        # Files that indicate package managers
        package_manager_files = {
            "python": {
                "requirements.txt": "pip",
                "Pipfile": "pipenv",
                "pyproject.toml": "poetry"  # Could also be other tools
            },
            "node": {
                "package.json": "npm",  # Could also be yarn or pnpm
                "yarn.lock": "yarn",
                "pnpm-lock.yaml": "pnpm"
            },
            "ruby": {
                "Gemfile": "bundler"
            },
            "php": {
                "composer.json": "composer"
            },
            "go": {
                "go.mod": "go"
            },
            "rust": {
                "Cargo.toml": "cargo"
            },
            "java": {
                "pom.xml": "maven",
                "build.gradle": "gradle",
                "build.gradle.kts": "gradle"
            }
        }
        
        # Check for package manager files based on project type
        if project_type in package_manager_files:
            for file_name, manager in package_manager_files[project_type].items():
                if (path_obj / file_name).exists():
                    # For Python, check if poetry is actually used in pyproject.toml
                    if file_name == "pyproject.toml" and manager == "poetry":
                        # Check if [tool.poetry] section exists
                        try:
                            with open(path_obj / file_name, 'r') as f:
                                content = f.read()
                                if "[tool.poetry]" not in content:
                                    # Might be another tool, default to pip
                                    manager = "pip"
                        except Exception:
                            manager = "pip"
                    
                    # For Node.js, check if yarn or pnpm is used
                    if file_name == "package.json" and manager == "npm":
                        # If yarn.lock or pnpm-lock.yaml exists, use that instead
                        if (path_obj / "yarn.lock").exists():
                            manager = "yarn"
                        elif (path_obj / "pnpm-lock.yaml").exists():
                            manager = "pnpm"
                    
                    return {
                        "detected": True,
                        "package_manager": manager,
                        "project_type": project_type,
                        "indicator_file": file_name
                    }
        
        # If no specific package manager detected, use default for project type
        if project_type in self._package_managers:
            default_manager = self._package_managers[project_type][0]
            return {
                "detected": False,
                "package_manager": default_manager,
                "project_type": project_type,
                "indicator_file": None,
                "message": f"No package manager detected, defaulting to {default_manager}"
            }
        
        return {
            "detected": False,
            "error": f"Unable to detect package manager for project type: {project_type}",
            "package_manager": None,
            "project_type": project_type
        }
    
    async def install_dependencies(
        self, 
        path: Union[str, Path],
        dependencies: List[str],
        dev_dependencies: Optional[List[str]] = None,
        package_manager: Optional[str] = None,
        project_type: Optional[str] = None,
        update_dependency_file: bool = True,
        virtual_env: bool = False
    ) -> Dict[str, Any]:
        """
        Install dependencies using the appropriate package manager.
        
        Args:
            path: Path to the project
            dependencies: List of dependencies to install
            dev_dependencies: Optional list of development dependencies
            package_manager: Optional package manager to use
            project_type: Optional project type
            update_dependency_file: Whether to update dependency file
            virtual_env: Whether to use a virtual environment for Python
            
        Returns:
            Dictionary with the installation result
        """
        self._logger.info(f"Installing dependencies in {path}")
        
        path_obj = Path(path)
        
        # Check if path exists
        if not path_obj.exists() or not path_obj.is_dir():
            return {
                "success": False,
                "error": f"Path does not exist or is not a directory: {path}",
                "package_manager": package_manager,
                "project_type": project_type
            }
        
        # Detect package manager if not provided
        if package_manager is None or project_type is None:
            detection_result = await self.detect_package_manager(path_obj, project_type)
            package_manager = detection_result.get("package_manager")
            project_type = detection_result.get("project_type")
            
            if not package_manager:
                return {
                    "success": False,
                    "error": f"Unable to detect package manager: {detection_result.get('error', 'Unknown error')}",
                    "package_manager": None,
                    "project_type": project_type
                }
        
        # Install dependencies based on package manager
        if package_manager == "pip":
            return await self._install_pip_dependencies(
                path_obj, dependencies, dev_dependencies, update_dependency_file, virtual_env
            )
        elif package_manager == "npm":
            return await self._install_npm_dependencies(
                path_obj, dependencies, dev_dependencies, update_dependency_file
            )
        elif package_manager == "yarn":
            return await self._install_yarn_dependencies(
                path_obj, dependencies, dev_dependencies, update_dependency_file
            )
        elif package_manager == "poetry":
            return await self._install_poetry_dependencies(
                path_obj, dependencies, dev_dependencies, update_dependency_file
            )
        elif package_manager == "cargo":
            return await self._install_cargo_dependencies(
                path_obj, dependencies, dev_dependencies, update_dependency_file
            )
        # Add other package managers as needed
        
        return {
            "success": False,
            "error": f"Unsupported package manager: {package_manager}",
            "package_manager": package_manager,
            "project_type": project_type
        }
    
    async def _install_pip_dependencies(
        self, 
        path: Path,
        dependencies: List[str],
        dev_dependencies: Optional[List[str]] = None,
        update_dependency_file: bool = True,
        virtual_env: bool = False
    ) -> Dict[str, Any]:
        """
        Install Python dependencies using pip.
        
        Args:
            path: Path to the project
            dependencies: List of dependencies to install
            dev_dependencies: Optional list of development dependencies
            update_dependency_file: Whether to update requirements.txt
            virtual_env: Whether to use a virtual environment
            
        Returns:
            Dictionary with the installation result
        """
        self._logger.info(f"Installing Python dependencies with pip in {path}")
        
        results = {
            "success": True,
            "package_manager": "pip",
            "project_type": "python",
            "commands": [],
            "outputs": [],
            "errors": []
        }
        
        # Create virtual environment if requested
        if virtual_env and not (path / "venv").exists():
            venv_command = "python -m venv venv"
            venv_stdout, venv_stderr, venv_code = await execution_engine.execute_command(
                venv_command,
                check_safety=True,
                working_dir=str(path)
            )
            
            results["commands"].append(venv_command)
            results["outputs"].append(venv_stdout)
            
            if venv_code != 0:
                results["success"] = False
                results["errors"].append(f"Failed to create virtual environment: {venv_stderr}")
                return results
        
        # Determine pip command
        pip_cmd = "venv/bin/pip" if virtual_env and (path / "venv").exists() else "pip"
        
        # Install dependencies
        if dependencies:
            deps_str = " ".join(dependencies)
            install_command = f"{pip_cmd} install {deps_str}"
            
            install_stdout, install_stderr, install_code = await execution_engine.execute_command(
                install_command,
                check_safety=True,
                working_dir=str(path)
            )
            
            results["commands"].append(install_command)
            results["outputs"].append(install_stdout)
            
            if install_code != 0:
                results["success"] = False
                results["errors"].append(f"Failed to install dependencies: {install_stderr}")
                return results
        
        # Install dev dependencies
        if dev_dependencies:
            dev_deps_str = " ".join(dev_dependencies)
            dev_install_command = f"{pip_cmd} install {dev_deps_str}"
            
            dev_stdout, dev_stderr, dev_code = await execution_engine.execute_command(
                dev_install_command,
                check_safety=True,
                working_dir=str(path)
            )
            
            results["commands"].append(dev_install_command)
            results["outputs"].append(dev_stdout)
            
            if dev_code != 0:
                results["success"] = False
                results["errors"].append(f"Failed to install dev dependencies: {dev_stderr}")
                return results
        
        # Update requirements.txt if requested
        if update_dependency_file:
            # Check if requirements.txt already exists
            req_file = path / "requirements.txt"
            existing_deps = []
            
            if req_file.exists():
                try:
                    with open(req_file, 'r') as f:
                        existing_deps = [line.strip() for line in f if line.strip() and not line.strip().startswith('#')]
                except Exception as e:
                    results["errors"].append(f"Failed to read requirements.txt: {str(e)}")
            
            # Combine existing and new dependencies
            all_deps = list(set(existing_deps + dependencies))
            
            # Write back to requirements.txt
            try:
                with open(req_file, 'w') as f:
                    for dep in sorted(all_deps):
                        f.write(f"{dep}\n")
                
                results["updated_files"] = [str(req_file)]
            except Exception as e:
                results["errors"].append(f"Failed to update requirements.txt: {str(e)}")
        
        return results
    
    async def _install_npm_dependencies(
        self, 
        path: Path,
        dependencies: List[str],
        dev_dependencies: Optional[List[str]] = None,
        update_dependency_file: bool = True
    ) -> Dict[str, Any]:
        """
        Install Node.js dependencies using npm.
        
        Args:
            path: Path to the project
            dependencies: List of dependencies to install
            dev_dependencies: Optional list of development dependencies
            update_dependency_file: Whether to update package.json
            
        Returns:
            Dictionary with the installation result
        """
        self._logger.info(f"Installing Node.js dependencies with npm in {path}")
        
        results = {
            "success": True,
            "package_manager": "npm",
            "project_type": "node",
            "commands": [],
            "outputs": [],
            "errors": []
        }
        
        # Initialize npm project if package.json doesn't exist
        package_json = path / "package.json"
        if not package_json.exists() and update_dependency_file:
            init_command = "npm init -y"
            init_stdout, init_stderr, init_code = await execution_engine.execute_command(
                init_command,
                check_safety=True,
                working_dir=str(path)
            )
            
            results["commands"].append(init_command)
            results["outputs"].append(init_stdout)
            
            if init_code != 0:
                results["success"] = False
                results["errors"].append(f"Failed to initialize npm project: {init_stderr}")
                return results
        
        # Install dependencies
        if dependencies:
            deps_str = " ".join(dependencies)
            install_command = f"npm install --save {deps_str}"
            
            install_stdout, install_stderr, install_code = await execution_engine.execute_command(
                install_command,
                check_safety=True,
                working_dir=str(path)
            )
            
            results["commands"].append(install_command)
            results["outputs"].append(install_stdout)
            
            if install_code != 0:
                results["success"] = False
                results["errors"].append(f"Failed to install dependencies: {install_stderr}")
                return results
        
        # Install dev dependencies
        if dev_dependencies:
            dev_deps_str = " ".join(dev_dependencies)
            dev_install_command = f"npm install --save-dev {dev_deps_str}"
            
            dev_stdout, dev_stderr, dev_code = await execution_engine.execute_command(
                dev_install_command,
                check_safety=True,
                working_dir=str(path)
            )
            
            results["commands"].append(dev_install_command)
            results["outputs"].append(dev_stdout)
            
            if dev_code != 0:
                results["success"] = False
                results["errors"].append(f"Failed to install dev dependencies: {dev_stderr}")
                return results
        
        # Update package.json directly if using npm doesn't work
        if update_dependency_file and package_json.exists() and (dependencies or dev_dependencies):
            try:
                with open(package_json, 'r') as f:
                    package_data = json.load(f)
                
                # Make sure dependencies sections exist
                if dependencies and "dependencies" not in package_data:
                    package_data["dependencies"] = {}
                
                if dev_dependencies and "devDependencies" not in package_data:
                    package_data["devDependencies"] = {}
                
                # Update package.json
                with open(package_json, 'w') as f:
                    json.dump(package_data, f, indent=2)
                
                results["updated_files"] = [str(package_json)]
            except Exception as e:
                results["errors"].append(f"Failed to update package.json: {str(e)}")
        
        return results
    
    async def _install_yarn_dependencies(
        self, 
        path: Path,
        dependencies: List[str],
        dev_dependencies: Optional[List[str]] = None,
        update_dependency_file: bool = True
    ) -> Dict[str, Any]:
        """
        Install Node.js dependencies using yarn.
        
        Args:
            path: Path to the project
            dependencies: List of dependencies to install
            dev_dependencies: Optional list of development dependencies
            update_dependency_file: Whether to update package.json
            
        Returns:
            Dictionary with the installation result
        """
        self._logger.info(f"Installing Node.js dependencies with yarn in {path}")
        
        results = {
            "success": True,
            "package_manager": "yarn",
            "project_type": "node",
            "commands": [],
            "outputs": [],
            "errors": []
        }
        
        # Initialize yarn project if package.json doesn't exist
        package_json = path / "package.json"
        if not package_json.exists() and update_dependency_file:
            init_command = "yarn init -y"
            init_stdout, init_stderr, init_code = await execution_engine.execute_command(
                init_command,
                check_safety=True,
                working_dir=str(path)
            )
            
            results["commands"].append(init_command)
            results["outputs"].append(init_stdout)
            
            if init_code != 0:
                results["success"] = False
                results["errors"].append(f"Failed to initialize yarn project: {init_stderr}")
                return results
        
        # Install dependencies
        if dependencies:
            deps_str = " ".join(dependencies)
            install_command = f"yarn add {deps_str}"
            
            install_stdout, install_stderr, install_code = await execution_engine.execute_command(
                install_command,
                check_safety=True,
                working_dir=str(path)
            )
            
            results["commands"].append(install_command)
            results["outputs"].append(install_stdout)
            
            if install_code != 0:
                results["success"] 
                
                
            if install_code != 0:
                results["success"] = False
                results["errors"].append(f"Failed to install dependencies: {install_stderr}")
                return results
        
        # Install dev dependencies
        if dev_dependencies:
            dev_deps_str = " ".join(dev_dependencies)
            dev_install_command = f"yarn add --dev {dev_deps_str}"
            
            dev_stdout, dev_stderr, dev_code = await execution_engine.execute_command(
                dev_install_command,
                check_safety=True,
                working_dir=str(path)
            )
            
            results["commands"].append(dev_install_command)
            results["outputs"].append(dev_stdout)
            
            if dev_code != 0:
                results["success"] = False
                results["errors"].append(f"Failed to install dev dependencies: {dev_stderr}")
                return results
        
        return results
    
    async def _install_poetry_dependencies(
        self, 
        path: Path,
        dependencies: List[str],
        dev_dependencies: Optional[List[str]] = None,
        update_dependency_file: bool = True
    ) -> Dict[str, Any]:
        """
        Install Python dependencies using Poetry.
        
        Args:
            path: Path to the project
            dependencies: List of dependencies to install
            dev_dependencies: Optional list of development dependencies
            update_dependency_file: Whether to update pyproject.toml
            
        Returns:
            Dictionary with the installation result
        """
        self._logger.info(f"Installing Python dependencies with Poetry in {path}")
        
        results = {
            "success": True,
            "package_manager": "poetry",
            "project_type": "python",
            "commands": [],
            "outputs": [],
            "errors": []
        }
        
        # Initialize poetry project if pyproject.toml doesn't exist
        pyproject_toml = path / "pyproject.toml"
        if not pyproject_toml.exists() and update_dependency_file:
            init_command = "poetry init --no-interaction"
            init_stdout, init_stderr, init_code = await execution_engine.execute_command(
                init_command,
                check_safety=True,
                working_dir=str(path)
            )
            
            results["commands"].append(init_command)
            results["outputs"].append(init_stdout)
            
            if init_code != 0:
                results["success"] = False
                results["errors"].append(f"Failed to initialize Poetry project: {init_stderr}")
                return results
        
        # Install dependencies
        if dependencies:
            for dep in dependencies:
                install_command = f"poetry add {dep}"
                
                install_stdout, install_stderr, install_code = await execution_engine.execute_command(
                    install_command,
                    check_safety=True,
                    working_dir=str(path)
                )
                
                results["commands"].append(install_command)
                results["outputs"].append(install_stdout)
                
                if install_code != 0:
                    results["success"] = False
                    results["errors"].append(f"Failed to install dependency {dep}: {install_stderr}")
                    return results
        
        # Install dev dependencies
        if dev_dependencies:
            for dev_dep in dev_dependencies:
                dev_install_command = f"poetry add --dev {dev_dep}"
                
                dev_stdout, dev_stderr, dev_code = await execution_engine.execute_command(
                    dev_install_command,
                    check_safety=True,
                    working_dir=str(path)
                )
                
                results["commands"].append(dev_install_command)
                results["outputs"].append(dev_stdout)
                
                if dev_code != 0:
                    results["success"] = False
                    results["errors"].append(f"Failed to install dev dependency {dev_dep}: {dev_stderr}")
                    return results
        
        return results
    
    async def _install_cargo_dependencies(
        self, 
        path: Path,
        dependencies: List[str],
        dev_dependencies: Optional[List[str]] = None,
        update_dependency_file: bool = True
    ) -> Dict[str, Any]:
        """
        Install Rust dependencies using Cargo.
        
        Args:
            path: Path to the project
            dependencies: List of dependencies to install
            dev_dependencies: Optional list of development dependencies
            update_dependency_file: Whether to update Cargo.toml
            
        Returns:
            Dictionary with the installation result
        """
        self._logger.info(f"Installing Rust dependencies with Cargo in {path}")
        
        results = {
            "success": True,
            "package_manager": "cargo",
            "project_type": "rust",
            "commands": [],
            "outputs": [],
            "errors": []
        }
        
        # Check if this is a Cargo project
        cargo_toml = path / "Cargo.toml"
        if not cargo_toml.exists():
            if update_dependency_file:
                # Initialize a new Cargo project
                project_name = path.name.replace("-", "_").lower()
                init_command = f"cargo init --name {project_name}"
                
                init_stdout, init_stderr, init_code = await execution_engine.execute_command(
                    init_command,
                    check_safety=True,
                    working_dir=str(path)
                )
                
                results["commands"].append(init_command)
                results["outputs"].append(init_stdout)
                
                if init_code != 0:
                    results["success"] = False
                    results["errors"].append(f"Failed to initialize Cargo project: {init_stderr}")
                    return results
            else:
                results["success"] = False
                results["errors"].append("Not a Cargo project and update_dependency_file is False")
                return results
        
        # Add dependencies to Cargo.toml
        if (dependencies or dev_dependencies) and update_dependency_file:
            try:
                with open(cargo_toml, 'r') as f:
                    cargo_content = f.read()
                
                # Add [dependencies] section if it doesn't exist
                if dependencies and "[dependencies]" not in cargo_content:
                    cargo_content += "\n[dependencies]\n"
                
                # Add dependencies
                if dependencies:
                    for dep in dependencies:
                        # Check if dependency is already in the file
                        if dep not in cargo_content:
                            # Parse dependency name and version (if provided)
                            if "=" in dep:
                                dep_name, dep_version = dep.split("=", 1)
                                cargo_content += f'{dep_name.strip()} = {dep_version.strip()}\n'
                            else:
                                cargo_content += f'{dep.strip()} = "*"\n'
                
                # Add [dev-dependencies] section if it doesn't exist
                if dev_dependencies and "[dev-dependencies]" not in cargo_content:
                    cargo_content += "\n[dev-dependencies]\n"
                
                # Add dev dependencies
                if dev_dependencies:
                    for dep in dev_dependencies:
                        # Check if dependency is already in the file
                        if dep not in cargo_content:
                            # Parse dependency name and version (if provided)
                            if "=" in dep:
                                dep_name, dep_version = dep.split("=", 1)
                                cargo_content += f'{dep_name.strip()} = {dep_version.strip()}\n'
                            else:
                                cargo_content += f'{dep.strip()} = "*"\n'
                
                # Write back to Cargo.toml
                with open(cargo_toml, 'w') as f:
                    f.write(cargo_content)
                
                results["updated_files"] = [str(cargo_toml)]
            except Exception as e:
                results["errors"].append(f"Failed to update Cargo.toml: {str(e)}")
        
        # Run cargo build to install dependencies
        build_command = "cargo build"
        build_stdout, build_stderr, build_code = await execution_engine.execute_command(
            build_command,
            check_safety=True,
            working_dir=str(path)
        )
        
        results["commands"].append(build_command)
        results["outputs"].append(build_stdout)
        
        if build_code != 0:
            results["success"] = False
            results["errors"].append(f"Failed to build project: {build_stderr}")
            return results
        
        return results
    
    async def _infer_project_type(self, path: Path) -> Optional[str]:
        """
        Infer the project type from the files in the directory.
        
        Args:
            path: Path to the project
            
        Returns:
            Inferred project type, or None if unable to infer
        """
        # Check for key files that indicate project type
        if (path / "requirements.txt").exists() or (path / "setup.py").exists() or (path / "pyproject.toml").exists():
            return "python"
        elif (path / "package.json").exists():
            return "node"
        elif (path / "Gemfile").exists() or (path / "Gemfile.lock").exists():
            return "ruby"
        elif (path / "composer.json").exists():
            return "php"
        elif (path / "go.mod").exists():
            return "go"
        elif (path / "Cargo.toml").exists():
            return "rust"
        elif (path / "pom.xml").exists() or (path / "build.gradle").exists() or (path / "build.gradle.kts").exists():
            return "java"
        
        # Count file extensions to guess project type
        extensions = {}
        for file_path in path.glob("**/*"):
            if file_path.is_file():
                ext = file_path.suffix.lower()
                if ext:
                    extensions[ext] = extensions.get(ext, 0) + 1
        
        # Determine project type based on most common extension
        if extensions:
            py_exts = extensions.get(".py", 0)
            js_exts = extensions.get(".js", 0) + extensions.get(".jsx", 0) + extensions.get(".ts", 0) + extensions.get(".tsx", 0)
            rb_exts = extensions.get(".rb", 0)
            php_exts = extensions.get(".php", 0)
            go_exts = extensions.get(".go", 0)
            rs_exts = extensions.get(".rs", 0)
            java_exts = extensions.get(".java", 0)
            
            max_ext = max([
                ("python", py_exts),
                ("node", js_exts),
                ("ruby", rb_exts),
                ("php", php_exts),
                ("go", go_exts),
                ("rust", rs_exts),
                ("java", java_exts)
            ], key=lambda x: x[1])
            
            if max_ext[1] > 0:
                return max_ext[0]
        
        return None

# Global package manager integration instance
package_manager_integration = PackageManagerIntegration()
</file>

<file path="angela/utils/__init__.py">
"""
Utility functions for Angela CLI.
"""

from angela.utils.logging import setup_logging, get_logger
</file>

<file path="angela/utils/logging.py">
"""
Logging configuration for Angela CLI.
"""
import sys
from pathlib import Path

from loguru import logger

from angela.constants import LOG_DIR, LOG_FORMAT, LOG_ROTATION, LOG_RETENTION


def setup_logging(debug: bool = False) -> None:
    """
    Configure the application logging.
    
    Args:
        debug: Whether to enable debug logging.
    """
    # Ensure log directory exists
    LOG_DIR.mkdir(parents=True, exist_ok=True)
    
    # Remove default handlers
    logger.remove()
    
    # Add console handler with appropriate level
    log_level = "DEBUG" if debug else "INFO"
    logger.add(
        sys.stderr,
        format=LOG_FORMAT,
        level=log_level,
        diagnose=debug,  # Include variable values in traceback if debug is True
    )
    
    # Add file handler
    log_file = LOG_DIR / "angela.log"
    logger.add(
        log_file,
        format=LOG_FORMAT,
        level="INFO",
        rotation=LOG_ROTATION,
        retention=LOG_RETENTION,
        compression="zip",
    )
    
    logger.debug(f"Logging initialized. Log file: {log_file}")


def get_logger(name: str = "angela"):
    """
    Get a logger instance with the given name.
    
    Args:
        name: The name for the logger.
        
    Returns:
        A logger instance.
    """
    return logger.bind(name=name)
</file>

<file path="angela/workflows/__init__.py">
"""
Workflow management for Angela CLI.

This package handles creating, managing, and executing user-defined
workflows - reusable sequences of commands that can be invoked by name.
"""

from angela.workflows.manager import workflow_manager
</file>

<file path="angela/workflows/manager.py">
"""
Workflow management for Angela CLI.

This module handles user-defined workflows - reusable sequences
of commands that can be invoked by name.
"""
import os
import json
import shlex
from pathlib import Path
from typing import Dict, Any, List, Tuple, Optional
from datetime import datetime
from dataclasses import dataclass, field, asdict

from pydantic import BaseModel, Field

from angela.config import config_manager
from angela.intent.planner import TaskPlan, PlanStep
from angela.utils.logging import get_logger

logger = get_logger(__name__)

# File for storing workflows
WORKFLOWS_FILE = config_manager.CONFIG_DIR / "workflows.json"

class WorkflowStep(BaseModel):
    """Model for a step in a workflow."""
    command: str = Field(..., description="The command to execute")
    explanation: str = Field(..., description="Explanation of what the command does")
    optional: bool = Field(False, description="Whether this step is optional")
    requires_confirmation: bool = Field(False, description="Whether this step requires explicit confirmation")


class Workflow(BaseModel):
    """Model for a user-defined workflow."""
    name: str = Field(..., description="Unique name for the workflow")
    description: str = Field(..., description="Human-readable description")
    steps: List[WorkflowStep] = Field(..., description="Steps in the workflow")
    variables: Dict[str, str] = Field(default_factory=dict, description="Variable placeholders")
    created: datetime = Field(default_factory=datetime.now, description="When the workflow was created")
    modified: datetime = Field(default_factory=datetime.now, description="When the workflow was last modified")
    tags: List[str] = Field(default_factory=list, description="Tags for categorizing workflows")
    author: Optional[str] = Field(None, description="Author of the workflow")


class WorkflowManager:
    """
    Manager for user-defined workflows.
    
    This class handles:
    1. Defining new workflows from natural language descriptions
    2. Storing and retrieving workflows
    3. Executing workflows with parameter substitution
    4. Listing and searching available workflows
    """
    
    def __init__(self):
        """Initialize the workflow manager."""
        self._workflows: Dict[str, Workflow] = {}
        self._workflow_file = WORKFLOWS_FILE
        self._logger = logger
        self._load_workflows()
    
    def _load_workflows(self) -> None:
        """Load workflows from the storage file."""
        try:
            if self._workflow_file.exists():
                with open(self._workflow_file, "r") as f:
                    data = json.load(f)
                    
                for workflow_data in data:
                    try:
                        # Handle datetime serialization
                        if "created" in workflow_data:
                            workflow_data["created"] = datetime.fromisoformat(workflow_data["created"])
                        if "modified" in workflow_data:
                            workflow_data["modified"] = datetime.fromisoformat(workflow_data["modified"])
                            
                        workflow = Workflow(**workflow_data)
                        self._workflows[workflow.name] = workflow
                    except Exception as e:
                        self._logger.error(f"Error loading workflow: {str(e)}")
                
                self._logger.info(f"Loaded {len(self._workflows)} workflows")
            else:
                self._logger.info("No workflows file found, starting with empty workflows")
                self._save_workflows()  # Create the file
        except Exception as e:
            self._logger.error(f"Error loading workflows: {str(e)}")
    
    def _save_workflows(self) -> None:
        """Save workflows to the storage file."""
        try:
            # Ensure the directory exists
            self._workflow_file.parent.mkdir(parents=True, exist_ok=True)
            
            # Convert workflows to serializable dict
            data = []
            for workflow in self._workflows.values():
                workflow_dict = workflow.dict()
                # Handle datetime serialization
                workflow_dict["created"] = workflow_dict["created"].isoformat()
                workflow_dict["modified"] = workflow_dict["modified"].isoformat()
                data.append(workflow_dict)
            
            # Write to file
            with open(self._workflow_file, "w") as f:
                json.dump(data, f, indent=2)
                
            self._logger.info(f"Saved {len(self._workflows)} workflows")
        except Exception as e:
            self._logger.error(f"Error saving workflows: {str(e)}")
    
    async def define_workflow(
        self, 
        name: str, 
        description: str, 
        steps: List[Dict[str, Any]],
        variables: Optional[Dict[str, str]] = None,
        tags: Optional[List[str]] = None,
        author: Optional[str] = None
    ) -> Workflow:
        """
        Define a new workflow or update an existing one.
        
        Args:
            name: Unique name for the workflow
            description: Human-readable description
            steps: List of step dictionaries with commands and explanations
            variables: Optional variable placeholders
            tags: Optional tags for categorization
            author: Optional author name
            
        Returns:
            The created or updated Workflow
        """
        # Convert steps to WorkflowStep objects
        workflow_steps = []
        for step_data in steps:
            workflow_step = WorkflowStep(
                command=step_data["command"],
                explanation=step_data.get("explanation", ""),
                optional=step_data.get("optional", False),
                requires_confirmation=step_data.get("requires_confirmation", False)
            )
            workflow_steps.append(workflow_step)
        
        # Check if workflow already exists
        if name in self._workflows:
            # Update existing workflow
            workflow = self._workflows[name]
            workflow.description = description
            workflow.steps = workflow_steps
            workflow.variables = variables or {}
            workflow.modified = datetime.now()
            if tags:
                workflow.tags = tags
            if author:
                workflow.author = author
                
            self._logger.info(f"Updated workflow: {name}")
        else:
            # Create new workflow
            workflow = Workflow(
                name=name,
                description=description,
                steps=workflow_steps,
                variables=variables or {},
                tags=tags or [],
                author=author
            )
            self._workflows[name] = workflow
            self._logger.info(f"Created new workflow: {name}")
        
        # Save updated workflows
        self._save_workflows()
        
        return workflow
    
    async def define_workflow_from_natural_language(
        self, 
        name: str, 
        description: str, 
        natural_language: str,
        context: Dict[str, Any]
    ) -> Workflow:
        """
        Define a workflow from a natural language description.
        
        Args:
            name: Unique name for the workflow
            description: Human-readable description
            natural_language: Natural language description of the workflow steps
            context: Context information
            
        Returns:
            The created Workflow
        """
        # Import here to avoid circular imports
        from angela.intent.planner import task_planner
        from angela.ai.client import gemini_client, GeminiRequest
        
        self._logger.info(f"Creating workflow from natural language: {name}")
        
        # Generate a plan using the task planner
        try:
            plan = await task_planner.plan_task(natural_language, context)
            
            # Convert plan steps to workflow steps
            steps = []
            for plan_step in plan.steps:
                step = {
                    "command": plan_step.command,
                    "explanation": plan_step.explanation,
                    "optional": False,
                    "requires_confirmation": plan_step.estimated_risk >= 3  # High or Critical risk
                }
                steps.append(step)
                
            # Identify potential variables
            variables = await self._identify_variables(steps, natural_language)
            
            # Create the workflow
            workflow = await self.define_workflow(
                name=name,
                description=description,
                steps=steps,
                variables=variables,
                tags=["user-defined"]
            )
            
            return workflow
            
        except Exception as e:
            self._logger.exception(f"Error creating workflow from natural language: {str(e)}")
            # Create a placeholder workflow
            placeholder_workflow = await self.define_workflow(
                name=name,
                description=description,
                steps=[{
                    "command": f"echo 'Error creating workflow: {str(e)}'",
                    "explanation": "This is a placeholder for a workflow that could not be created",
                    "optional": False,
                    "requires_confirmation": False
                }],
                tags=["error", "placeholder"]
            )
            return placeholder_workflow
    
    async def _identify_variables(
        self, 
        steps: List[Dict[str, Any]], 
        natural_language: str
    ) -> Dict[str, str]:
        """
        Identify potential variables in workflow steps.
        
        Args:
            steps: The workflow steps
            natural_language: Original natural language description
            
        Returns:
            Dictionary of variable names and descriptions
        """
        # Extract all commands
        commands = [step["command"] for step in steps]
        
        # Build prompt for variable identification
        prompt = f"""
Identify potential variables in the following workflow commands:

Commands:
{json.dumps(commands, indent=2)}

Original description:
{natural_language}

Identify parameters or values that might change each time the workflow is run.
For each variable, provide:
1. A variable name (use format like $NAME or {{NAME}})
2. A description of what the variable represents

Format your response as JSON:
{{
  "variables": {{
    "$VARIABLE1": "Description of variable 1",
    "$VARIABLE2": "Description of variable 2",
    ...
  }}
}}
"""
        
        # Call AI service
        from angela.ai.client import gemini_client, GeminiRequest
        
        api_request = GeminiRequest(prompt=prompt, max_tokens=2000)
        response = await gemini_client.generate_text(api_request)
        
        # Parse the response
        try:
            # Extract JSON from the response
            import re
            json_match = re.search(r'```(?:json)?\s*(.*?)\s*```', response.text, re.DOTALL)
            if json_match:
                json_str = json_match.group(1)
            else:
                # Fallback to entire response
                json_str = response.text
                
            # Parse JSON
            result = json.loads(json_str)
            variables = result.get("variables", {})
            
            self._logger.info(f"Identified {len(variables)} variables")
            return variables
            
        except Exception as e:
            self._logger.error(f"Error identifying variables: {str(e)}")
            return {}
    
    def get_workflow(self, name: str) -> Optional[Workflow]:
        """
        Get a workflow by name.
        
        Args:
            name: Name of the workflow to retrieve
            
        Returns:
            The Workflow if found, None otherwise
        """
        return self._workflows.get(name)
    
    def list_workflows(self, tag: Optional[str] = None) -> List[Workflow]:
        """
        List all workflows, optionally filtered by tag.
        
        Args:
            tag: Optional tag to filter workflows
            
        Returns:
            List of matching Workflows
        """
        if tag:
            return [w for w in self._workflows.values() if tag in w.tags]
        else:
            return list(self._workflows.values())
    
    def search_workflows(self, query: str) -> List[Workflow]:
        """
        Search for workflows by name or description.
        
        Args:
            query: Search query
            
        Returns:
            List of matching Workflows
        """
        query_lower = query.lower()
        results = []
        
        for workflow in self._workflows.values():
            # Check name, description, and tags
            if (query_lower in workflow.name.lower() or 
                query_lower in workflow.description.lower() or
                any(query_lower in tag.lower() for tag in workflow.tags)):
                results.append(workflow)
                
        return results
    
    def delete_workflow(self, name: str) -> bool:
        """
        Delete a workflow by name.
        
        Args:
            name: Name of the workflow to delete
            
        Returns:
            True if deleted, False if not found
        """
        if name in self._workflows:
            del self._workflows[name]
            self._save_workflows()
            self._logger.info(f"Deleted workflow: {name}")
            return True
        
        return False
    
    async def execute_workflow(
        self, 
        workflow_name: str, 
        variables: Dict[str, Any],
        context: Dict[str, Any],
        dry_run: bool = False
    ) -> Dict[str, Any]:
        """
        Execute a workflow with variable substitution.
        
        Args:
            workflow_name: Name of the workflow to execute
            variables: Variable values for substitution
            context: Context information
            dry_run: Whether to simulate execution without making changes
            
        Returns:
            Dictionary with execution results
        """
        workflow = self.get_workflow(workflow_name)
        if not workflow:
            return {
                "success": False,
                "error": f"Workflow not found: {workflow_name}"
            }
        
        # Import here to avoid circular imports
        from angela.intent.planner import TaskPlan, PlanStep, task_planner
        
        # Convert workflow to a task plan
        plan_steps = []
        for i, step in enumerate(workflow.steps):
            # Apply variable substitution
            command = self._substitute_variables(step.command, variables)
            
            plan_step = PlanStep(
                command=command,
                explanation=step.explanation,
                dependencies=[i-1] if i > 0 else [],  # Simple linear dependencies
                estimated_risk=3 if step.requires_confirmation else 1  # Default risk levels
            )
            plan_steps.append(plan_step)
            
        plan = TaskPlan(
            goal=f"Execute workflow: {workflow.name}",
            steps=plan_steps,
            context=context
        )
        
        # Execute the plan
        results = await task_planner.execute_plan(plan, dry_run=dry_run)
        
        return {
            "workflow": workflow.name,
            "description": workflow.description,
            "steps": len(workflow.steps),
            "results": results,
            "success": all(result.get("success", False) for result in results),
            "dry_run": dry_run
        }
    
    def _substitute_variables(self, command: str, variables: Dict[str, Any]) -> str:
        """
        Substitute variables in a command.
        
        Args:
            command: The command template
            variables: Variable values for substitution
            
        Returns:
            Command with variables substituted
        """
        result = command
        
        # Handle ${VAR} and $VAR syntax
        for var_name, var_value in variables.items():
            # Remove leading $ if present
            clean_name = var_name[1:] if var_name.startswith('$') else var_name
            
            # Substitute ${VAR} syntax
            result = result.replace(f"${{{clean_name}}}", str(var_value))
            
            # Substitute $VAR syntax
            result = result.replace(f"${clean_name}", str(var_value))
        
        return result


# Global workflow manager instance
workflow_manager = WorkflowManager()
</file>

<file path="angela/workflows/sharing.py">
Let's also create an implementation plan for the workflow sharing and importing:

```python
# angela/workflows/sharing.py

import os
import sys
import json
import tempfile
import shutil
import zipfile
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple, Union
from datetime import datetime
import uuid
import hashlib

from pydantic import BaseModel, Field

from angela.workflows.manager import Workflow, WorkflowManager, workflow_manager
from angela.config import config_manager
from angela.utils.logging import get_logger

logger = get_logger(__name__)

# Constants
WORKFLOW_EXPORT_DIR = config_manager.CONFIG_DIR / "exported_workflows"
WORKFLOW_IMPORT_DIR = config_manager.CONFIG_DIR / "imported_workflows"

class WorkflowExportMetadata(BaseModel):
    """Metadata for an exported workflow package."""
    id: str = Field(..., description="Unique identifier for this workflow package")
    name: str = Field(..., description="Name of the workflow")
    version: str = Field("1.0.0", description="Version of the workflow")
    description: str = Field(..., description="Description of the workflow")
    author: Optional[str] = Field(None, description="Author of the workflow")
    created: str = Field(..., description="Creation timestamp")
    exported: str = Field(..., description="Export timestamp")
    checksum: str = Field(..., description="SHA-256 checksum of the workflow data")
    tags: List[str] = Field(default_factory=list, description="Tags for the workflow")
    dependencies: Dict[str, str] = Field(default_factory=dict, description="External dependencies")

class WorkflowSharingManager:
    """Manager for workflow sharing, importing, and exporting."""
    
    def __init__(self, workflow_manager: WorkflowManager):
        """
        Initialize the workflow sharing manager.
        
        Args:
            workflow_manager: The workflow manager instance
        """
        self._workflow_manager = workflow_manager
        self._logger = logger
        
        # Ensure directories exist
        WORKFLOW_EXPORT_DIR.mkdir(parents=True, exist_ok=True)
        WORKFLOW_IMPORT_DIR.mkdir(parents=True, exist_ok=True)
    
    async def export_workflow(
        self, 
        workflow_name: str,
        output_path: Optional[Path] = None,
        include_dependencies: bool = True
    ) -> Dict[str, Any]:
        """
        Export a workflow to a shareable package.
        
        Args:
            workflow_name: Name of the workflow to export
            output_path: Optional custom output path
            include_dependencies: Whether to include external dependencies
            
        Returns:
            Dictionary with export results
        """
        # Get the workflow
        workflow = self._workflow_manager.get_workflow(workflow_name)
        if not workflow:
            return {
                "success": False,
                "error": f"Workflow not found: {workflow_name}"
            }
        
        try:
            # Create a temporary directory for packaging
            with tempfile.TemporaryDirectory() as temp_dir:
                temp_path = Path(temp_dir)
                
                # Convert workflow to serializable dict
                workflow_dict = workflow.dict()
                # Handle datetime serialization
                workflow_dict["created"] = workflow_dict["created"].isoformat()
                workflow_dict["modified"] = workflow_dict["modified"].isoformat()
                
                # Create workflow data file
                workflow_data_path = temp_path / "workflow.json"
                with open(workflow_data_path, "w") as f:
                    json.dump(workflow_dict, f, indent=2)
                
                # Generate checksum
                checksum = self._generate_checksum(workflow_data_path)
                
                # Create metadata
                metadata = WorkflowExportMetadata(
                    id=str(uuid.uuid4()),
                    name=workflow.name,
                    description=workflow.description,
                    author=workflow.author,
                    created=workflow.created.isoformat(),
                    exported=datetime.now().isoformat(),
                    checksum=checksum,
                    tags=workflow.tags
                )
                
                # Detect dependencies if requested
                if include_dependencies:
                    dependencies = await self._detect_dependencies(workflow)
                    metadata.dependencies = dependencies
                
                # Write metadata
                metadata_path = temp_path / "metadata.json"
                with open(metadata_path, "w") as f:
                    json.dump(metadata.dict(), f, indent=2)
                
                # Create README with information
                readme_path = temp_path / "README.md"
                with open(readme_path, "w") as f:
                    f.write(f"# {workflow.name}\n\n")
                    f.write(f"{workflow.description}\n\n")
                    if workflow.author:
                        f.write(f"Author: {workflow.author}\n\n")
                    f.write(f"Created: {workflow.created.isoformat()}\n")
                    f.write(f"Exported: {datetime.now().isoformat()}\n\n")
                    f.write("## Steps\n\n")
                    for i, step in enumerate(workflow.steps, 1):
                        f.write(f"### Step {i}: {step.command}\n")
                        f.write(f"{step.explanation}\n\n")
                
                # Determine output path
                if not output_path:
                    safe_name = workflow.name.replace(" ", "_").lower()
                    output_path = WORKFLOW_EXPORT_DIR / f"{safe_name}.angela-workflow"
                
                # Create zip archive
                with zipfile.ZipFile(output_path, "w") as zip_file:
                    for file_path in [workflow_data_path, metadata_path, readme_path]:
                        zip_file.write(file_path, arcname=file_path.name)
                
                return {
                    "success": True,
                    "workflow": workflow.name,
                    "output_path": str(output_path),
                    "metadata": metadata.dict()
                }
                
        except Exception as e:
            self._logger.exception(f"Error exporting workflow {workflow_name}: {str(e)}")
            return {
                "success": False,
                "error": f"Export failed: {str(e)}"
            }
    
    async def import_workflow(
        self, 
        workflow_path: Union[str, Path],
        rename: Optional[str] = None,
        replace_existing: bool = False
    ) -> Dict[str, Any]:
        """
        Import a workflow from a package.
        
        Args:
            workflow_path: Path to the workflow package
            rename: Optional new name for the workflow
            replace_existing: Whether to replace existing workflow with same name
            
        Returns:
            Dictionary with import results
        """
        path_obj = Path(workflow_path)
        
        # Check if file exists
        if not path_obj.exists():
            return {
                "success": False,
                "error": f"File not found: {path_obj}"
            }
        
        try:
            # Create a temporary directory for extraction
            with tempfile.TemporaryDirectory() as temp_dir:
                temp_path = Path(temp_dir)
                
                # Extract the zip archive
                with zipfile.ZipFile(path_obj, "r") as zip_file:
                    zip_file.extractall(temp_path)
                
                # Check metadata
                metadata_path = temp_path / "metadata.json"
                if not metadata_path.exists():
                    return {
                        "success": False,
                        "error": "Invalid workflow package: missing metadata.json"
                    }
                
                # Load metadata
                with open(metadata_path, "r") as f:
                    metadata = WorkflowExportMetadata(**json.load(f))
                
                # Check workflow data
                workflow_data_path = temp_path / "workflow.json"
                if not workflow_data_path.exists():
                    return {
                        "success": False,
                        "error": "Invalid workflow package: missing workflow.json"
                    }
                
                # Verify checksum
                computed_checksum = self._generate_checksum(workflow_data_path)
                if computed_checksum != metadata.checksum:
                    return {
                        "success": False,
                        "error": "Checksum verification failed. The workflow package may be corrupted."
                    }
                
                # Load workflow data
                with open(workflow_data_path, "r") as f:
                    workflow_data = json.load(f)
                
                # Apply rename if provided
                if rename:
                    workflow_data["name"] = rename
                
                # Check if workflow already exists
                existing_workflow = self._workflow_manager.get_workflow(workflow_data["name"])
                if existing_workflow and not replace_existing:
                    return {
                        "success": False,
                        "error": f"Workflow '{workflow_data['name']}' already exists. Use replace_existing=True to replace it."
                    }
                
                # Import the workflow
                workflow = await self._workflow_manager.define_workflow_from_data(
                    workflow_data,
                    source=f"Imported from {path_obj.name}"
                )
                
                return {
                    "success": True,
                    "workflow": workflow.name,
                    "metadata": metadata.dict()
                }
                
        except Exception as e:
            self._logger.exception(f"Error importing workflow from {workflow_path}: {str(e)}")
            return {
                "success": False,
                "error": f"Import failed: {str(e)}"
            }
    
    def _generate_checksum(self, file_path: Path) -> str:
        """
        Generate SHA-256 checksum of a file.
        
        Args:
            file_path: Path to the file
            
        Returns:
            Hex digest of the checksum
        """
        sha256 = hashlib.sha256()
        with open(file_path, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                sha256.update(chunk)
        return sha256.hexdigest()
    
    async def _detect_dependencies(self, workflow: Workflow) -> Dict[str, str]:
        """
        Detect external dependencies of a workflow.
        
        Args:
            workflow: The workflow to analyze
            
        Returns:
            Dictionary of dependencies
        """
        dependencies = {}
        
        # Check for tool dependencies in commands
        for step in workflow.steps:
            command = step.command.split()[0] if step.command else ""
            
            # Common tools to check
            if command in ["python", "python3"]:
                # Check Python version
                result = await self._run_command("python --version")
                if result["success"]:
                    dependencies["python"] = result["stdout"].strip().replace("Python ", "")
            elif command in ["node", "npm"]:
                # Check Node.js/npm version
                result = await self._run_command("node --version")
                if result["success"]:
                    dependencies["node"] = result["stdout"].strip().replace("v", "")
            elif command == "docker":
                # Check Docker version
                result = await self._run_command("docker --version")
                if result["success"]:
                    dependencies["docker"] = result["stdout"].strip()
            # Add more tool checks as needed
        
        return dependencies
    
    async def _run_command(self, command: str) -> Dict[str, Any]:
        """
        Run a shell command and return its output.
        
        Args:
            command: The command to run
            
        Returns:
            Dictionary with command results
        """
        import asyncio
        
        try:
            process = await asyncio.create_subprocess_shell(
                command,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            
            stdout, stderr = await process.communicate()
            
            return {
                "command": command,
                "stdout": stdout.decode('utf-8', errors='replace').strip(),
                "stderr": stderr.decode('utf-8', errors='replace').strip(),
                "return_code": process.returncode,
                "success": process.returncode == 0
            }
        except Exception as e:
            self._logger.error(f"Error running command '{command}': {str(e)}")
            return {
                "command": command,
                "stdout": "",
                "stderr": str(e),
                "return_code": -1,
                "success": False
            }

# Global workflow sharing manager instance
workflow_sharing_manager = WorkflowSharingManager(workflow_manager)
</file>

<file path="angela/__init__.py">
"""
Angela-CLI: AI-powered command-line assistant integrated into your terminal shell.
"""

from angela.constants import APP_VERSION

__version__ = APP_VERSION
</file>

<file path="angela/__main__.py">
# angela/__main__.py
"""
Entry point for Angela CLI.
"""
from angela.cli import app

if __name__ == "__main__":
    app()
</file>

<file path="MD/Info.md">
# Angela-CLI: Comprehensive System Architecture Overview
STEPS -1-5
## Project Purpose and Vision

Angela-CLI is an advanced AI-powered command-line assistant that integrates directly into your terminal shell. It fundamentally transforms how you interact with the command line by allowing you to express complex intentions in natural language rather than memorizing exact command syntax.

The system aims to serve as an intelligent "copilot" for terminal operations with these core capabilities:
- Translating natural language requests into shell commands
- Breaking down complex tasks into executable steps
- Understanding and manipulating file content
- Learning from user interactions to improve over time
- Providing rich, contextual feedback and suggestions

## High-Level Architecture

Angela-CLI follows a modular architecture with clear separation of concerns:

```
angela-cli/
├── angela/          # Core package containing all functionality
├── scripts/         # Installation and utility scripts
├── shell/           # Shell integration files
├── tests/           # Test suite (mentioned in roadmap)
├── pyproject.toml   # Project configuration
├── setup.py         # Package installation
└── Makefile         # Build automation
```

## Core System Components

### 1. `/angela/` - Core Package

This is the heart of the system, containing all functional modules. Each subdirectory serves a specific purpose in the system's operation.

#### `/angela/ai/` - AI Integration Layer

Handles all interaction with the Gemini API and processes natural language:

- **`client.py`**: Manages communication with Google's Gemini API with error handling and retry logic
- **`prompts.py`**: Sophisticated prompt engineering that incorporates context into AI queries
- **`parser.py`**: Transforms AI responses into structured command suggestions
- **`analyzer.py`**: Analyzes command errors and generates fix suggestions
- **`confidence.py`**: Scores confidence in AI suggestions to determine when to seek clarification
- **`intent_analyzer.py`**: Enhanced NLU with tolerance for variations and misspellings
- **`content_analyzer.py`**: Analyzes and manipulates file content based on natural language requests
- **`file_integration.py`**: Bridges AI suggestions with actual file system operations

#### `/angela/context/` - Context Management System

Maintains awareness of the user's environment for more relevant suggestions:

- **`manager.py`**: Central orchestrator of all context information
- **`file_detector.py`**: Sophisticated file type and language detection
- **`history.py`**: Tracks command history and success patterns
- **`preferences.py`**: Manages user-specific settings and preferences
- **`session.py`**: Maintains conversational memory within and between sessions
- **`trust.py`**: Progressive trust system that adapts confirmation requirements based on history

#### `/angela/execution/` - Command Execution System

Safely executes commands with rich feedback:

- **`engine.py`**: Core execution engine for running shell commands
- **`adaptive_engine.py`**: Context-aware execution with dynamic behavior based on history
- **`filesystem.py`**: High-level file operations with safety checks and rollback capabilities
- **`rollback.py`**: Sophisticated undo functionality for operations

#### `/angela/intent/` - Intent Understanding System

Models and processes user intentions:

- **`models.py`**: Data models for structured representation of intent
- **`planner.py`**: Task planning for breaking complex goals into actionable steps

#### `/angela/safety/` - Safety System

Ensures operations are safe and appropriate:

- **`classifier.py`**: Risk classification for commands and operations
- **`validator.py`**: Validation against safety rules and constraints
- **`confirmation.py`**: User confirmation for potentially risky operations
- **`adaptive_confirmation.py`**: Context-aware confirmation that adapts based on history
- **`preview.py`**: Command preview generation to show expected outcomes

#### `/angela/shell/` - Shell Integration

Connects Angela with the terminal environment:

- **`formatter.py`**: Rich terminal formatting with async support and interactive elements
- **`angela.bash`**: Integration with Bash shell
- **`angela.zsh`**: Integration with Zsh shell

#### `/angela/cli/` - Command Line Interface

Provides the user-facing interface:

- **`main.py`**: Primary CLI entry point and command handling
- **`files.py`**: File operation commands with rich formatting
- **`workflows.py`**: Workflow management for defining and running command sequences

#### `/angela/workflows/` - Workflow Management

Handles user-defined sequences of commands (mentioned but implementation files may not be visible in the provided code)

#### `/angela/utils/` - Utilities

Support functionality:

- **`logging.py`**: Logging setup and configuration

### 2. Core Files and Their Functions

- **`orchestrator.py`**: Central coordinator that manages the flow from user request to execution
- **`config.py`**: Configuration management using TOML format
- **`constants.py`**: System-wide constants and settings
- **`__main__.py`**: Entry point when module is executed directly

### 3. `/scripts/` - Installation Scripts

- **`install.sh`**: Installs the package and shell integration
- **`uninstall.sh`**: Removes the package and shell integration

## System Data Flow

1. **Input Reception**: User input is received through the shell integration
2. **Intent Analysis**: The `intent_analyzer` determines what the user is trying to accomplish
3. **Context Gathering**: The `context_manager` gathers relevant information about the environment
4. **AI Processing**: The `gemini_client` generates command suggestions based on intent and context
5. **Risk Analysis**: The `safety` system assesses and classifies risk
6. **Confirmation**: The `adaptive_confirmation` system gets user approval when needed
7. **Execution**: The `adaptive_engine` executes the command with appropriate safeguards
8. **Feedback**: The `formatter` provides rich terminal feedback about the execution
9. **Learning**: The system updates history and patterns based on execution outcome

## Implementation Phases

According to the project documentation, implementation follows these phases:

1. **Basic Setup & Shell Hook** (Completed)
2. **Orchestration & Context** (Completed)
3. **Gemini API Integration** (Completed)
4. **Intelligent Interaction & Contextual Execution** (Completed)
   - Enhanced NLU with tolerance for variations
   - Rich terminal feedback with async streaming
   - Context-aware adaptive confirmation
   - Error analysis and fix suggestions
   - Enhanced file operations

5. **Autonomous Task Orchestration & Proactive Assistance** (In Progress)
   - High-level goal decomposition
   - Multi-step orchestration
   - Conversational context and session memory
   - AI-powered file content comprehension
   - User-defined workflows

6. **Enhanced Project Context** (Planned)
   - Project type inference
   - Dependency detection
   - File reference resolution
   - Activity tracking

7. **Developer Tool Integration** (Planned)
   - Git integration
   - Docker support
   - Advanced code generation
   - Multi-step workflow execution

## Key Technical Features

1. **Progressive Trust System**: Learns which commands are safe and reduces confirmation requirements over time

2. **Adaptive Execution**: Adjusts behavior based on command history and patterns

3. **Rich Error Analysis**: When commands fail, analyzes errors and suggests fixes

4. **Task Planning**: Breaks down complex goals into actionable steps with dependencies

5. **File Content Manipulation**: Can analyze and modify file content based on natural language instructions

6. **Session Memory**: Maintains context between commands for natural conversations

7. **Workflow Definition**: Allows creating and executing multi-step workflows

## Configuration and Customization

The system uses:
- TOML configuration files in `~/.config/angela/`
- Command history and patterns stored for learning
- User preferences for customizing behavior
- Workflow definitions for reusable command sequences

## Safety and Security Considerations

Angela-CLI prioritizes safety with:
- Risk classification for all operations
- Command validation against safety rules
- Adaptive confirmation based on risk level
- Command previews to show expected outcomes
- Rollback capabilities for undoing changes
- Backup creation before destructive operations

This system creates a seamless, intelligent interface to the terminal that feels like "having AGI in your terminal" - understanding your intentions and translating them into effective actions while constantly learning and adapting to your workflow.
--------------------
# STEPS 1-7 COMPLETE
....
</file>

<file path="MD/tree.md">
```bash
tree -I "__pycache__|.git|node_modules|.venv|target|dist|build"
```
</file>

<file path="scripts/install.sh">
#!/bin/bash
# Angela CLI Installation Script

set -e

# Determine script directory
SCRIPT_DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" &> /dev/null && pwd )"
ANGELA_ROOT="$(dirname "$SCRIPT_DIR")"

# ANSI color codes
GREEN='\033[0;32m'
BLUE='\033[0;34m'
RED='\033[0;31m'
YELLOW='\033[1;33m'
NC='\033[0m' # No Color

echo -e "${BLUE}╔════════════════════════════════════════╗${NC}"
echo -e "${BLUE}║        Angela CLI Installation         ║${NC}"
echo -e "${BLUE}╚════════════════════════════════════════╝${NC}"
echo

# Install Python package in development mode
echo -e "${YELLOW}Installing Angela CLI Python package...${NC}"
pip install -e "$ANGELA_ROOT"
echo -e "${GREEN}Python package installed successfully!${NC}"
echo

# Detect shell and perform appropriate installation
echo -e "${YELLOW}Detecting shell...${NC}"
DETECTED_SHELL="$(basename "$SHELL")"
echo -e "Detected shell: ${BLUE}$DETECTED_SHELL${NC}"

# Function to install Bash integration
install_bash() {
    echo -e "${YELLOW}Installing Bash integration...${NC}"
    BASH_RC="$HOME/.bashrc"
    
    # Check if integration is already installed
    if grep -q "# Angela CLI Integration" "$BASH_RC"; then
        echo -e "${YELLOW}Angela CLI integration already exists in $BASH_RC${NC}"
        echo -e "${YELLOW}Updating existing integration...${NC}"
        # Remove existing integration
        sed -i '/# Angela CLI Integration/,/# End Angela CLI Integration/d' "$BASH_RC"
    fi
    
    # Add integration to .bashrc
    echo "" >> "$BASH_RC"
    echo "# Angela CLI Integration" >> "$BASH_RC"
    echo "source \"$ANGELA_ROOT/shell/angela.bash\"" >> "$BASH_RC"
    echo "# End Angela CLI Integration" >> "$BASH_RC"
    
    echo -e "${GREEN}Bash integration installed successfully!${NC}"
    echo -e "${YELLOW}Please restart your terminal or run 'source ~/.bashrc' to apply changes.${NC}"
}

# Function to install Zsh integration
install_zsh() {
    echo -e "${YELLOW}Installing Zsh integration...${NC}"
    ZSH_RC="$HOME/.zshrc"
    
    # Check if integration is already installed
    if grep -q "# Angela CLI Integration" "$ZSH_RC"; then
        echo -e "${YELLOW}Angela CLI integration already exists in $ZSH_RC${NC}"
        echo -e "${YELLOW}Updating existing integration...${NC}"
        # Remove existing integration
        sed -i '/# Angela CLI Integration/,/# End Angela CLI Integration/d' "$ZSH_RC"
    fi
    
    # Add integration to .zshrc
    echo "" >> "$ZSH_RC"
    echo "# Angela CLI Integration" >> "$ZSH_RC"
    echo "source \"$ANGELA_ROOT/shell/angela.zsh\"" >> "$ZSH_RC"
    echo "# End Angela CLI Integration" >> "$ZSH_RC"
    
    echo -e "${GREEN}Zsh integration installed successfully!${NC}"
    echo -e "${YELLOW}Please restart your terminal or run 'source ~/.zshrc' to apply changes.${NC}"
}

# Install based on detected shell
case "$DETECTED_SHELL" in
    bash)
        install_bash
        ;;
    zsh)
        install_zsh
        ;;
    *)
        echo -e "${RED}Unsupported shell: $DETECTED_SHELL${NC}"
        echo -e "${YELLOW}Supported shells: bash, zsh${NC}"
        echo 
        echo -e "${YELLOW}Would you like to install for bash anyway? (y/n)${NC}"
        read -r INSTALL_BASH
        if [[ "$INSTALL_BASH" =~ ^[Yy]$ ]]; then
            install_bash
        else
            echo -e "${RED}Installation cancelled.${NC}"
            exit 1
        fi
        ;;
esac

# Initialize Angela CLI
echo -e "${YELLOW}Initializing Angela CLI...${NC}"
python -m angela init

echo
echo -e "${GREEN}╔════════════════════════════════════════╗${NC}"
echo -e "${GREEN}║    Angela CLI installed successfully!  ║${NC}"
echo -e "${GREEN}╚════════════════════════════════════════╝${NC}"
echo
echo -e "You can now use Angela CLI by typing: ${BLUE}angela <your request>${NC}"
echo
</file>

<file path="scripts/uninstall.sh">
#!/bin/bash
# Angela CLI Uninstallation Script

set -e

# ANSI color codes
GREEN='\033[0;32m'
BLUE='\033[0;34m'
RED='\033[0;31m'
YELLOW='\033[1;33m'
NC='\033[0m' # No Color

echo -e "${BLUE}╔════════════════════════════════════════╗${NC}"
echo -e "${BLUE}║        Angela CLI Uninstallation       ║${NC}"
echo -e "${BLUE}╚════════════════════════════════════════╝${NC}"
echo

# Confirm uninstallation
echo -e "${YELLOW}Are you sure you want to uninstall Angela CLI? (y/n)${NC}"
read -r CONFIRM
if [[ ! "$CONFIRM" =~ ^[Yy]$ ]]; then
    echo -e "${RED}Uninstallation cancelled.${NC}"
    exit 0
fi

# Function to remove Bash integration
remove_bash_integration() {
    echo -e "${YELLOW}Removing Bash integration...${NC}"
    BASH_RC="$HOME/.bashrc"
    
    if grep -q "# Angela CLI Integration" "$BASH_RC"; then
        # Remove integration from .bashrc
        sed -i '/# Angela CLI Integration/,/# End Angela CLI Integration/d' "$BASH_RC"
        echo -e "${GREEN}Bash integration removed successfully!${NC}"
    else
        echo -e "${YELLOW}No Angela CLI integration found in $BASH_RC${NC}"
    fi
}

# Function to remove Zsh integration
remove_zsh_integration() {
    echo -e "${YELLOW}Removing Zsh integration...${NC}"
    ZSH_RC="$HOME/.zshrc"
    
    if grep -q "# Angela CLI Integration" "$ZSH_RC"; then
        # Remove integration from .zshrc
        sed -i '/# Angela CLI Integration/,/# End Angela CLI Integration/d' "$ZSH_RC"
        echo -e "${GREEN}Zsh integration removed successfully!${NC}"
    else
        echo -e "${YELLOW}No Angela CLI integration found in $ZSH_RC${NC}"
    fi
}

# Remove both integrations to ensure complete uninstallation
remove_bash_integration
remove_zsh_integration

# Remove configuration
echo -e "${YELLOW}Would you like to remove Angela CLI configuration? (y/n)${NC}"
read -r REMOVE_CONFIG
if [[ "$REMOVE_CONFIG" =~ ^[Yy]$ ]]; then
    CONFIG_DIR="$HOME/.config/angela"
    if [ -d "$CONFIG_DIR" ]; then
        rm -rf "$CONFIG_DIR"
        echo -e "${GREEN}Configuration removed successfully!${NC}"
    else
        echo -e "${YELLOW}No configuration directory found.${NC}"
    fi
fi

# Uninstall Python package
echo -e "${YELLOW}Would you like to uninstall the Angela CLI Python package? (y/n)${NC}"
read -r UNINSTALL_PACKAGE
if [[ "$UNINSTALL_PACKAGE" =~ ^[Yy]$ ]]; then
    echo -e "${YELLOW}Uninstalling Angela CLI Python package...${NC}"
    pip uninstall -y angela-cli
    echo -e "${GREEN}Python package uninstalled successfully!${NC}"
fi

echo
echo -e "${GREEN}╔════════════════════════════════════════╗${NC}"
echo -e "${GREEN}║   Angela CLI uninstalled successfully! ║${NC}"
echo -e "${GREEN}╚════════════════════════════════════════╝${NC}"
echo
echo -e "${YELLOW}Please restart your terminal for changes to take effect.${NC}"
echo
</file>

<file path=".env.example">
# Angela CLI Environment Variables

# Google Gemini API Key
# Get your API key from https://ai.google.dev/
GEMINI_API_KEY=your_gemini_api_key_here

# Debug mode (true/false)
DEBUG=false
</file>

<file path=".gitignore">
.env
</file>

<file path="Makefile">
.PHONY: install uninstall dev-setup clean test lint format

# Installation
install:
	@echo "Installing Angela CLI..."
	bash scripts/install.sh

uninstall:
	@echo "Uninstalling Angela CLI..."
	bash scripts/uninstall.sh

# Development
dev-setup:
	@echo "Setting up development environment..."
	pip install -e ".[dev]"
	@echo "Development environment set up successfully!"

# Testing
test:
	@echo "Running tests..."
	pytest

# Linting and formatting
lint:
	@echo "Running linters..."
	flake8 angela tests
	mypy angela tests

format:
	@echo "Formatting code..."
	black angela tests
	isort angela tests

# Cleaning
clean:
	@echo "Cleaning up..."
	rm -rf build/
	rm -rf dist/
	rm -rf *.egg-info
	rm -rf .pytest_cache
	rm -rf .mypy_cache
	find . -type d -name __pycache__ -exec rm -rf {} +
	find . -type f -name "*.pyc" -delete
	@echo "Cleanup complete!"

# Help
help:
	@echo "Angela CLI Makefile"
	@echo ""
	@echo "Usage:"
	@echo "  make install      Install Angela CLI"
	@echo "  make uninstall    Uninstall Angela CLI"
	@echo "  make dev-setup    Set up development environment"
	@echo "  make test         Run tests"
	@echo "  make lint         Run linters"
	@echo "  make format       Format code"
	@echo "  make clean        Clean up build artifacts"
	@echo "  make help         Show this help message"

# Default target
.DEFAULT_GOAL := help
</file>

<file path="pytest.ini">
[pytest]
asyncio_mode = strict
asyncio_default_fixture_loop_scope = function
</file>

<file path="requirements.txt">
# Core dependencies
typer>=0.9.0
rich>=13.4.2
pydantic>=2.0.0
click>=8.1.3

# Shell integration
pexpect>=4.8.0
prompt_toolkit>=3.0.38

# Configuration
python-dotenv>=1.0.0
tomli>=2.0.1; python_version < "3.11"

# API communication
aiohttp>=3.8.5
google-generativeai>=0.1.0

# Testing
pytest>=7.3.1
pytest-asyncio>=0.21.0

# Utilities
loguru>=0.7.0
</file>

<file path="setup.py">
from setuptools import setup

# Use pyproject.toml for project configuration
if __name__ == "__main__":
    setup()
</file>

<file path="angela/cli/generation.py">
# angela/cli/generation.py
"""
CLI interface for code generation features in Angela CLI.
"""
import os
import asyncio
from pathlib import Path
from typing import List, Optional, Tuple, Dict, Any
import typer
from rich.console import Console
from rich.panel import Panel
from rich.syntax import Syntax
from rich.table import Table
from rich import print as rich_print

from angela.utils.logging import get_logger
from angela.generation.engine import code_generation_engine
from angela.toolchain.git import git_integration
from angela.toolchain.package_managers import package_manager_integration
from angela.toolchain.test_frameworks import test_framework_integration
from angela.toolchain.ci_cd import ci_cd_integration
from angela.review.diff_manager import diff_manager
from angela.review.feedback import feedback_manager
from angela.context import context_manager

app = typer.Typer(help="Code generation commands")
console = Console()
logger = get_logger(__name__)

@app.command("create-project")
def create_project(
    description: str = typer.Argument(..., help="Description of the project to generate"),
    output_dir: str = typer.Option(".", help="Directory where the project should be generated"),
    project_type: Optional[str] = typer.Option(None, help="Project type (python, node, etc.)"),
    git_init: bool = typer.Option(True, help="Initialize Git repository"),
    install_deps: bool = typer.Option(False, help="Install dependencies"),
    generate_tests: bool = typer.Option(False, help="Generate test files"),
    ci_platform: Optional[str] = typer.Option(None, help="Generate CI configuration (github, gitlab, etc.)"),
    dry_run: bool = typer.Option(False, help="Preview without creating files")
):
    """
    Generate a complete project from a description.
    """
    console.print(Panel(
        f"[bold green]Generating project from description:[/bold green]\n{description}",
        title="Project Generation",
        expand=False
    ))
    
    try:
        # Get current context
        context = context_manager.get_context_dict()
        
        # Generate project plan
        project_plan = asyncio.run(code_generation_engine.generate_project(
            description=description,
            output_dir=output_dir,
            project_type=project_type,
            context=context
        ))
        
        # Display project plan
        console.print("\n[bold blue]Project Plan:[/bold blue]")
        console.print(f"Name: [bold]{project_plan.name}[/bold]")
        console.print(f"Type: [bold]{project_plan.project_type}[/bold]")
        console.print(f"Files: [bold]{len(project_plan.files)}[/bold]")
        
        # Show the list of files
        table = Table(title="Files to Generate")
        table.add_column("Path", style="cyan")
        table.add_column("Purpose", style="green")
        
        for file in project_plan.files:
            table.add_row(file.path, file.purpose)
        
        console.print(table)
        
        # Create the files if not dry run
        if not dry_run:
            console.print("\n[bold]Creating project files...[/bold]")
            
            with console.status("[bold green]Creating files...[/bold green]"):
                result = asyncio.run(code_generation_engine.create_project_files(project_plan))
            
            console.print(f"[green]Created {result['file_count']} files[/green]")
            
            # Initialize Git repository if requested
            if git_init:
                console.print("\n[bold]Initializing Git repository...[/bold]")
                
                with console.status("[bold green]Initializing Git...[/bold green]"):
                    git_result = asyncio.run(git_integration.init_repository(
                        path=output_dir,
                        initial_branch="main",
                        gitignore_template=project_plan.project_type
                    ))
                
                if git_result["success"]:
                    console.print("[green]Git repository initialized successfully[/green]")
                else:
                    console.print(f"[yellow]Failed to initialize Git repository: {git_result.get('error', 'Unknown error')}[/yellow]")
            
            # Install dependencies if requested
            if install_deps:
                console.print("\n[bold]Installing dependencies...[/bold]")
                
                with console.status("[bold green]Installing dependencies...[/bold green]"):
                    deps_result = asyncio.run(package_manager_integration.install_dependencies(
                        path=output_dir,
                        dependencies=project_plan.dependencies.get("runtime", []),
                        dev_dependencies=project_plan.dependencies.get("development", []),
                        project_type=project_plan.project_type
                    ))
                
                if deps_result["success"]:
                    console.print("[green]Dependencies installed successfully[/green]")
                else:
                    console.print(f"[yellow]Failed to install dependencies: {deps_result.get('error', 'Unknown error')}[/yellow]")
            
            # Generate test files if requested
            if generate_tests:
                console.print("\n[bold]Generating test files...[/bold]")
                
                with console.status("[bold green]Generating tests...[/bold green]"):
                    test_result = asyncio.run(test_framework_integration.generate_test_files(
                        src_files=project_plan.files,
                        project_type=project_plan.project_type,
                        root_dir=output_dir
                    ))
                
                if test_result["success"]:
                    console.print(f"[green]Generated {test_result['file_count']} test files[/green]")
                else:
                    console.print(f"[yellow]Failed to generate test files: {test_result.get('error', 'Unknown error')}[/yellow]")
            
            # Generate CI/CD configuration if requested
            if ci_platform:
                console.print("\n[bold]Generating CI/CD configuration...[/bold]")
                
                with console.status(f"[bold green]Generating {ci_platform} configuration...[/bold green]"):
                    ci_result = asyncio.run(ci_cd_integration.generate_ci_configuration(
                        path=output_dir,
                        platform=ci_platform,
                        project_type=project_plan.project_type
                    ))
                
                if ci_result["success"]:
                    console.print(f"[green]Generated {ci_platform} configuration successfully[/green]")
                else:
                    console.print(f"[yellow]Failed to generate CI/CD configuration: {ci_result.get('error', 'Unknown error')}[/yellow]")
            
            # Create initial commit if Git was initialized
            if git_init:
                console.print("\n[bold]Creating initial commit...[/bold]")
                
                with console.status("[bold green]Creating commit...[/bold green]"):
                    commit_result = asyncio.run(git_integration.commit_changes(
                        path=output_dir,
                        message="Initial project generation",
                        auto_stage=True
                    ))
                
                if commit_result["success"]:
                    console.print("[green]Created initial commit successfully[/green]")
                else:
                    console.print(f"[yellow]Failed to create initial commit: {commit_result.get('error', 'Unknown error')}[/yellow]")
            
            console.print(f"\n[bold green]Project generated successfully in: {output_dir}[/bold green]")
        else:
            console.print("\n[bold yellow]Dry run - no files were created[/bold yellow]")
    
    except Exception as e:
        logger.exception("Error generating project")
        console.print(f"[bold red]Error generating project:[/bold red] {str(e)}")

@app.command("add-feature")
def add_feature(
    description: str = typer.Argument(..., help="Description of the feature to add"),
    project_dir: str = typer.Option(".", help="Project directory"),
    branch: Optional[str] = typer.Option(None, help="Create a feature branch"),
    generate_tests: bool = typer.Option(False, help="Generate test files"),
    install_deps: bool = typer.Option(False, help="Install new dependencies"),
    dry_run: bool = typer.Option(False, help="Preview without creating files"),
    auto_commit: bool = typer.Option(False, help="Commit changes automatically")
):
    """
    Add a new feature to an existing project.
    """
    console.print(Panel(
        f"[bold green]Adding feature to project:[/bold green]\n{description}",
        title="Feature Addition",
        expand=False
    ))
    
    try:
        # Get current context
        context = context_manager.get_context_dict()
        context = asyncio.run(context_enhancer.enrich_context(context))
        
        # Check if project directory exists
        project_path = Path(project_dir)
        if not project_path.exists() or not project_path.is_dir():
            console.print(f"[bold red]Project directory does not exist: {project_dir}[/bold red]")
            return
        
        # Create a feature branch if requested
        if branch:
            console.print(f"\n[bold]Creating feature branch: {branch}[/bold]")
            
            if not dry_run:
                with console.status("[bold green]Creating branch...[/bold green]"):
                    branch_result = asyncio.run(git_integration.create_branch(
                        path=project_dir,
                        branch_name=branch,
                        checkout=True
                    ))
                
                if branch_result["success"]:
                    console.print(f"[green]Created and checked out branch: {branch}[/green]")
                else:
                    console.print(f"[yellow]Failed to create branch: {branch_result.get('error', 'Unknown error')}[/yellow]")
        
        # Get project info
        with console.status("[bold green]Analyzing project...[/bold green]"):
            # Detect project type
            project_type_result = asyncio.run(ci_cd_integration.detect_project_type(project_dir))
            project_type = project_type_result.get("project_type")
            
            if not project_type:
                console.print("[yellow]Could not detect project type. Proceeding anyway...[/yellow]")
            else:
                console.print(f"[green]Detected project type: {project_type}[/green]")
        
        # Generate feature implementation
        console.print("\n[bold]Generating feature implementation...[/bold]")
        
        with console.status("[bold green]Generating feature implementation...[/bold green]"):
            # Use the new feature addition method
            result = asyncio.run(code_generation_engine.add_feature_to_project(
                description=description,
                project_dir=project_dir,
                context=context
            ))
        
        if result["success"]:
            # Display results
            console.print(f"[green]Successfully added feature to project![/green]")
            
            if result.get("new_files"):
                console.print("\n[bold]Created Files:[/bold]")
                for file_path in result["new_files"]:
                    console.print(f"  ✅ {file_path}")
            
            if result.get("modified_files"):
                console.print("\n[bold]Modified Files:[/bold]")
                for file_path in result["modified_files"]:
                    console.print(f"  ✏️ {file_path}")
            
            # Generate tests if requested
            if generate_tests and not dry_run:
                console.print("\n[bold]Generating tests for new feature...[/bold]")
                
                with console.status("[bold green]Generating tests...[/bold green]"):
                    # Create a list of new files with CodeFile objects
                    src_files = []
                    for file_path in result.get("new_files", []):
                        try:
                            with open(file_path, 'r', encoding='utf-8', errors='replace') as f:
                                content = f.read()
                                
                            rel_path = str(Path(file_path).relative_to(project_dir))
                            src_files.append(CodeFile(
                                path=rel_path,
                                content=content,
                                purpose=f"New feature: {description}",
                                dependencies=[],
                                language=project_type
                            ))
                        except Exception as e:
                            console.print(f"[yellow]Error reading file {file_path}: {str(e)}[/yellow]")
                    
                    if src_files:
                        test_result = asyncio.run(test_framework_integration.generate_test_files(
                            src_files=src_files,
                            project_type=project_type,
                            root_dir=project_dir
                        ))
                        
                        if test_result["success"]:
                            console.print(f"[green]Generated {test_result['file_count']} test files[/green]")
                        else:
                            console.print(f"[yellow]Failed to generate test files: {test_result.get('error', 'Unknown error')}[/yellow]")
                    else:
                        console.print("[yellow]No source files available for test generation[/yellow]")
            
            # Install dependencies if requested
            if install_deps and not dry_run:
                console.print("\n[bold]Installing dependencies...[/bold]")
                
                with console.status("[bold green]Extracting and installing dependencies...[/bold green]"):
                    # Extract dependencies from the feature files
                    dependencies = await code_generation_engine._extract_dependencies_from_feature(
                        feature_files={
                            "new_files": [{"path": path, "content": open(path, 'r', encoding='utf-8', errors='replace').read()} 
                                          for path in result.get("new_files", []) if Path(path).exists()],
                            "modified_files": [{"path": path, 
                                               "original_content": "", # We don't need original content for dependency extraction
                                               "modified_content": open(path, 'r', encoding='utf-8', errors='replace').read()} 
                                              for path in result.get("modified_files", []) if Path(path).exists()]
                        },
                        project_type=project_type
                    )
                    
                    if not dependencies["runtime"] and not dependencies["development"]:
                        console.print("[yellow]No new dependencies detected in the feature.[/yellow]")
                    else:
                        # Install the detected dependencies
                        runtime_deps = dependencies.get("runtime", [])
                        dev_deps = dependencies.get("development", [])
                        
                        if runtime_deps:
                            console.print(f"[bold]Runtime dependencies to install:[/bold] {', '.join(runtime_deps)}")
                            install_result = await package_manager_integration.install_dependencies(
                                path=project_dir,
                                dependencies=runtime_deps,
                                project_type=project_type
                            )
                            
                            if install_result["success"]:
                                console.print(f"[green]Successfully installed runtime dependencies[/green]")
                            else:
                                console.print(f"[yellow]Failed to install runtime dependencies: {install_result.get('error', 'Unknown error')}[/yellow]")
                        
                        if dev_deps:
                            console.print(f"[bold]Development dependencies to install:[/bold] {', '.join(dev_deps)}")
                            dev_install_result = await package_manager_integration.install_dependencies(
                                path=project_dir,
                                dependencies=[],
                                dev_dependencies=dev_deps,
                                project_type=project_type
                            )
                            
                            if dev_install_result["success"]:
                                console.print(f"[green]Successfully installed development dependencies[/green]")
                            else:
                                console.print(f"[yellow]Failed to install development dependencies: {dev_install_result.get('error', 'Unknown error')}[/yellow]")
    
    except Exception as e:
        logger.exception("Error adding feature")
        console.print(f"[bold red]Error adding feature:[/bold red] {str(e)}")

@app.command("refine-code")
def refine_code(
    feedback: str = typer.Argument(..., help="Feedback for code improvement"),
    file_path: str = typer.Argument(..., help="Path to the file to refine"),
    apply: bool = typer.Option(False, help="Apply the changes"),
    backup: bool = typer.Option(True, help="Create backup before applying changes")
):
    """
    Refine code based on feedback.
    """
    console.print(Panel(
        f"[bold green]Refining code based on feedback:[/bold green]\n{feedback}",
        title="Code Refinement",
        expand=False
    ))
    
    try:
        # Get current context
        context = context_manager.get_context_dict()
        
        # Check if file exists
        file = Path(file_path)
        if not file.exists() or not file.is_file():
            console.print(f"[bold red]File does not exist: {file_path}[/bold red]")
            return
        
        # Read file content
        with open(file, 'r', encoding='utf-8', errors='replace') as f:
            original_code = f.read()
        
        # Process feedback
        console.print("\n[bold]Processing feedback...[/bold]")
        
        with console.status("[bold green]Generating improvements...[/bold green]"):
            result = asyncio.run(feedback_manager.process_feedback(
                feedback=feedback,
                original_code=original_code,
                file_path=str(file),
                context=context
            ))
        
        # Display diff
        console.print("\n[bold blue]Code changes:[/bold blue]")
        
        syntax = Syntax(
            result["diff"],
            "diff",
            theme="monokai",
            line_numbers=True,
            word_wrap=True
        )
        console.print(syntax)
        
        # Display explanation
        console.print("\n[bold blue]Explanation:[/bold blue]")
        console.print(result["explanation"])
        
        # Apply changes if requested
        if apply:
            console.print("\n[bold]Applying changes...[/bold]")
            
            refinements = {
                "project_dir": str(file.parent),
                "results": [
                    {
                        "file_path": file.name,
                        "has_changes": original_code != result["improved_code"],
                        "diff": result["diff"],
                        "explanation": result["explanation"]
                    }
                ]
            }
            
            with console.status("[bold green]Applying changes...[/bold green]"):
                apply_result = asyncio.run(feedback_manager.apply_refinements(
                    refinements=refinements,
                    backup=backup
                ))
            
            if apply_result["files_changed"] > 0:
                console.print("[green]Changes applied successfully[/green]")
                if backup:
                    backup_file = apply_result["results"][0].get("backup")
                    if backup_file:
                        console.print(f"[blue]Backup created: {backup_file}[/blue]")
            else:
                console.print("[yellow]No changes were applied[/yellow]")
        else:
            console.print("\n[bold yellow]Changes were not applied. Use --apply to apply changes.[/bold yellow]")
    
    except Exception as e:
        logger.exception("Error refining code")
        console.print(f"[bold red]Error refining code:[/bold red] {str(e)}")

@app.command("refine-project")
def refine_project(
    feedback: str = typer.Argument(..., help="Feedback for project improvement"),
    project_dir: str = typer.Option(".", help="Project directory"),
    focus: Optional[List[str]] = typer.Option(None, help="Files to focus on (glob patterns supported)"),
    apply: bool = typer.Option(False, help="Apply the changes"),
    backup: bool = typer.Option(True, help="Create backup before applying changes")
):
    """
    Refine an entire project based on feedback.
    """
    console.print(Panel(
        f"[bold green]Refining project based on feedback:[/bold green]\n{feedback}",
        title="Project Refinement",
        expand=False
    ))
    
    try:
        # Get current context
        context = context_manager.get_context_dict()
        
        # Check if project directory exists
        project_path = Path(project_dir)
        if not project_path.exists() or not project_path.is_dir():
            console.print(f"[bold red]Project directory does not exist: {project_dir}[/bold red]")
            return
        
        # Process feedback
        console.print("\n[bold]Processing feedback...[/bold]")
        
        with console.status("[bold green]Analyzing project and generating improvements...[/bold green]"):
            result = asyncio.run(feedback_manager.refine_project(
                project_dir=project_path,
                feedback=feedback,
                focus_files=focus,
                context=context
            ))
        
        # Display results
        console.print(f"\n[bold blue]Files analyzed: {len(result['results'])}[/bold blue]")
        
        # Create a table to show the changes
        table = Table(title="Refinement Results")
        table.add_column("File", style="cyan")
        table.add_column("Status", style="green")
        table.add_column("Changes", style="yellow")
        
        for file_result in result["results"]:
            file_path = file_result["file_path"]
            
            if "error" in file_result:
                status = "[red]Error[/red]"
                changes = file_result["error"]
            elif not file_result.get("has_changes", False):
                status = "[blue]No changes[/blue]"
                changes = "No changes needed"
            else:
                status = "[green]Changes pending[/green]"
                diff_lines = file_result["diff"].splitlines()
                additions = sum(1 for line in diff_lines if line.startswith('+') and not line.startswith('+++'))
                deletions = sum(1 for line in diff_lines if line.startswith('-') and not line.startswith('---'))
                changes = f"+{additions} -{deletions}"
            
            table.add_row(file_path, status, changes)
        
        console.print(table)
        
        # Show details for files with changes
        changed_files = [r for r in result["results"] if r.get("has_changes", False)]
        if changed_files:
            console.print(f"\n[bold blue]Files with changes ({len(changed_files)}):[/bold blue]")
            
            for file_result in changed_files:
                console.print(f"\n[bold cyan]File: {file_result['file_path']}[/bold cyan]")
                
                # Display diff
                syntax = Syntax(
                    file_result["diff"],
                    "diff",
                    theme="monokai",
                    line_numbers=True,
                    word_wrap=True
                )
                console.print(syntax)
                
                # Display explanation
                if "explanation" in file_result:
                    console.print(f"[italic]{file_result['explanation']}[/italic]")
        
        # Apply changes if requested
        if apply:
            console.print("\n[bold]Applying changes...[/bold]")
            
            with console.status("[bold green]Applying changes...[/bold green]"):
                apply_result = asyncio.run(feedback_manager.apply_refinements(
                    refinements=result,
                    backup=backup
                ))
            
            if apply_result["files_changed"] > 0:
                console.print(f"[green]Changes applied to {apply_result['files_changed']} files[/green]")
                if backup:
                    console.print("[blue]Backups created for modified files[/blue]")
            else:
                console.print("[yellow]No changes were applied[/yellow]")
        else:
            console.print("\n[bold yellow]Changes were not applied. Use --apply to apply changes.[/bold yellow]")
    
    except Exception as e:
        logger.exception("Error refining project")
        console.print(f"[bold red]Error refining project:[/bold red] {str(e)}")

@app.command("generate-ci")
def generate_ci(
    platform: str = typer.Argument(..., help="CI platform (github_actions, gitlab_ci, jenkins, travis, circle_ci)"),
    project_dir: str = typer.Option(".", help="Project directory"),
    project_type: Optional[str] = typer.Option(None, help="Project type (python, node, etc.)")
):
    """
    Generate CI/CD configuration for a project.
    """
    console.print(Panel(
        f"[bold green]Generating CI/CD configuration for platform:[/bold green] {platform}",
        title="CI/CD Configuration",
        expand=False
    ))
    
    try:
        # Check if project directory exists
        project_path = Path(project_dir)
        if not project_path.exists() or not project_path.is_dir():
            console.print(f"[bold red]Project directory does not exist: {project_dir}[/bold red]")
            return
        
        # Detect project type if not provided
        if not project_type:
            console.print("\n[bold]Detecting project type...[/bold]")
            
            with console.status("[bold green]Analyzing project...[/bold green]"):
                detection_result = asyncio.run(ci_cd_integration.detect_project_type(project_dir))
                project_type = detection_result.get("project_type")
            
            if project_type:
                console.print(f"[green]Detected project type: {project_type}[/green]")
            else:
                console.print("[red]Could not detect project type[/red]")
                return
        
        # Generate CI configuration
        console.print(f"\n[bold]Generating {platform} configuration...[/bold]")
        
        with console.status(f"[bold green]Generating configuration...[/bold green]"):
            result = asyncio.run(ci_cd_integration.generate_ci_configuration(
                path=project_dir,
                platform=platform,
                project_type=project_type
            ))
        
        if result["success"]:
            console.print(f"[green]Generated {platform} configuration successfully[/green]")
            console.print(f"Configuration file: [bold]{result['config_file']}[/bold]")
        else:
            console.print(f"[red]Failed to generate configuration: {result.get('error', 'Unknown error')}[/red]")
    
    except Exception as e:
        logger.exception("Error generating CI/CD configuration")
        console.print(f"[bold red]Error generating CI/CD configuration:[/bold red] {str(e)}")

@app.command("generate-tests")
def generate_tests(
    project_dir: str = typer.Option(".", help="Project directory"),
    test_framework: Optional[str] = typer.Option(None, help="Test framework to use"),
    focus: Optional[List[str]] = typer.Option(None, help="Files to focus on (glob patterns supported)")
):
    """
    Generate test files for a project.
    """
    console.print(Panel(
        "[bold green]Generating test files for project[/bold green]",
        title="Test Generation",
        expand=False
    ))
    
    try:
        # Check if project directory exists
        project_path = Path(project_dir)
        if not project_path.exists() or not project_path.is_dir():
            console.print(f"[bold red]Project directory does not exist: {project_dir}[/bold red]")
            return
        
        # Detect test framework if not provided
        if not test_framework:
            console.print("\n[bold]Detecting test framework...[/bold]")
            
            with console.status("[bold green]Analyzing project...[/bold green]"):
                detection_result = asyncio.run(test_framework_integration.detect_test_framework(project_dir))
                test_framework = detection_result.get("test_framework")
            
            if test_framework:
                console.print(f"[green]Detected test framework: {test_framework}[/green]")
            else:
                console.print("[yellow]Could not detect test framework. Using default for project type...[/yellow]")
        
        # Find source files
        console.print("\n[bold]Finding source files...[/bold]")
        
        src_files = []
        with console.status("[bold green]Scanning project...[/bold green]"):
            # Get project type
            project_type_result = asyncio.run(ci_cd_integration.detect_project_type(project_dir))
            project_type = project_type_result.get("project_type")
            
            # Map of project types to file extensions
            extensions = {
                "python": [".py"],
                "node": [".js", ".jsx", ".ts", ".tsx"],
                "java": [".java"],
                "go": [".go"],
                "rust": [".rs"],
                "ruby": [".rb"]
            }
            
            # Get relevant file extensions
            file_exts = extensions.get(project_type, [".py", ".js", ".java", ".go", ".rs", ".rb"])
            
            # Find all source files
            for root, _, files in os.walk(project_path):
                # Skip common test directories and non-source directories
                if any(excluded in root for excluded in ["test", "tests", "__pycache__", "node_modules", ".git"]):
                    continue
                
                for file in files:
                    _, ext = os.path.splitext(file)
                    if ext in file_exts:
                        # If focus is specified, check if file matches any pattern
                        if focus:
                            file_path = Path(root) / file
                            rel_path = file_path.relative_to(project_path)
                            
                            matched = False
                            for pattern in focus:
                                if Path(pattern).name == file or rel_path.match(pattern):
                                    matched = True
                                    break
                            
                            if not matched:
                                continue
                        
                        # Read file content
                        try:
                            with open(Path(root) / file, 'r', encoding='utf-8', errors='replace') as f:
                                content = f.read()
                            
                            # Create CodeFile object
                            from angela.generation.engine import CodeFile
                            file_path = Path(root) / file
                            rel_path = file_path.relative_to(project_path)
                            
                            src_files.append(CodeFile(
                                path=str(rel_path),
                                content=content,
                                purpose=f"Source file: {file}",
                                dependencies=[],
                                language=project_type
                            ))
                        except Exception as e:
                            console.print(f"[yellow]Error reading file {file}: {str(e)}[/yellow]")
        
        console.print(f"[green]Found {len(src_files)} source files[/green]")
        
        # Generate test files
        console.print("\n[bold]Generating test files...[/bold]")
        
        with console.status("[bold green]Generating tests...[/bold green]"):
            result = asyncio.run(test_framework_integration.generate_test_files(
                src_files=src_files,
                test_framework=test_framework,
                project_type=project_type,
                root_dir=project_dir
            ))
        
        if result["success"]:
            console.print(f"[green]Generated {result['file_count']} test files[/green]")
            
            # Show generated files
            if result["generated_files"]:
                console.print("\n[bold blue]Generated test files:[/bold blue]")
                for file in result["generated_files"]:
                    console.print(f"- {file}")
        else:
            console.print(f"[red]Failed to generate test files: {result.get('error', 'Unknown error')}[/red]")
    
    except Exception as e:
        logger.exception("Error generating test files")
        console.print(f"[bold red]Error generating test files:[/bold red] {str(e)}")

if __name__ == "__main__":
    app()
</file>

<file path="angela/cli/workflows.py">
"""
Workflow management commands for Angela CLI.

This module provides CLI commands for creating, running, and managing workflows.
"""
import sys
import asyncio
from pathlib import Path
from typing import List, Optional, Dict, Any

import typer
from rich.console import Console
from rich.table import Table
from rich.panel import Panel
from rich.syntax import Syntax
from rich.prompt import Prompt, Confirm

from angela.workflows.manager import workflow_manager
from angela.context import context_manager
from angela.shell.formatter import terminal_formatter
from angela.utils.logging import get_logger

logger = get_logger(__name__)
console = Console()

# Create the workflow commands app
app = typer.Typer(help="Manage Angela workflows")


@app.command("list")
def list_workflows(
    tag: Optional[str] = typer.Option(
        None, "--tag", "-t", help="Filter workflows by tag"
    ),
    verbose: bool = typer.Option(
        False, "--verbose", "-v", help="Show detailed information"
    ),
):
    """List available workflows."""
    try:
        # Get workflows
        workflows = workflow_manager.list_workflows(tag)
        
        if not workflows:
            if tag:
                console.print(f"No workflows found with tag: {tag}")
            else:
                console.print("No workflows defined yet. Use 'angela workflows create' to define one.")
            return
        
        # Create table for workflows
        table = Table(title="Available Workflows")
        table.add_column("Name", style="cyan")
        table.add_column("Description", style="white")
        table.add_column("Steps", style="magenta", justify="right")
        
        if verbose:
            table.add_column("Tags", style="blue")
            table.add_column("Created", style="green")
        
        # Add workflows to table
        for workflow in workflows:
            if verbose:
                tags = ", ".join(workflow.tags) if workflow.tags else ""
                created = workflow.created.strftime("%Y-%m-%d %H:%M")
                table.add_row(
                    workflow.name,
                    workflow.description,
                    str(len(workflow.steps)),
                    tags,
                    created
                )
            else:
                table.add_row(
                    workflow.name,
                    workflow.description,
                    str(len(workflow.steps))
                )
        
        # Display the table
        console.print(table)
        
    except Exception as e:
        logger.exception(f"Error listing workflows: {str(e)}")
        console.print(f"[bold red]Error:[/bold red] {str(e)}")
        sys.exit(1)


# angela/cli/workflows.py
# Add new commands to the existing workflows app

@app.command("export")
def export_workflow(
    name: str = typer.Argument(
        ..., help="Name of the workflow to export"
    ),
    output: Optional[Path] = typer.Option(
        None, "--output", "-o", help="Output path for the exported workflow"
    ),
):
    """Export a workflow to a shareable package."""
    try:
        # Get the workflow sharing manager
        from angela.workflows.sharing import workflow_sharing_manager
        
        # Export the workflow
        result = asyncio.run(workflow_sharing_manager.export_workflow(
            workflow_name=name,
            output_path=output
        ))
        
        if result["success"]:
            console.print(f"[bold green]Workflow '{name}' exported successfully![/bold green]")
            console.print(f"Output path: {result['output_path']}")
        else:
            console.print(f"[bold red]Error:[/bold red] {result.get('error', 'Unknown error')}")
            sys.exit(1)
        
    except Exception as e:
        logger.exception(f"Error exporting workflow: {str(e)}")
        console.print(f"[bold red]Error:[/bold red] {str(e)}")
        sys.exit(1)


@app.command("import")
def import_workflow(
    path: Path = typer.Argument(
        ..., help="Path to the workflow package to import"
    ),
    rename: Optional[str] = typer.Option(
        None, "--rename", "-r", help="New name for the imported workflow"
    ),
    replace: bool = typer.Option(
        False, "--replace", help="Replace existing workflow with same name"
    ),
):
    """Import a workflow from a package."""
    try:
        # Get the workflow sharing manager
        from angela.workflows.sharing import workflow_sharing_manager
        
        # Import the workflow
        result = asyncio.run(workflow_sharing_manager.import_workflow(
            workflow_path=path,
            rename=rename,
            replace_existing=replace
        ))
        
        if result["success"]:
            console.print(f"[bold green]Workflow imported successfully as '{result['workflow']}'![/bold green]")
        else:
            console.print(f"[bold red]Error:[/bold red] {result.get('error', 'Unknown error')}")
            sys.exit(1)
        
    except Exception as e:
        logger.exception(f"Error importing workflow: {str(e)}")
        console.print(f"[bold red]Error:[/bold red] {str(e)}")
        sys.exit(1)



@app.command("create")
def create_workflow(
    name: str = typer.Argument(..., help="Name for the workflow"),
    description: Optional[str] = typer.Option(
        None, "--description", "-d", help="Description of the workflow"
    ),
    from_file: Optional[Path] = typer.Option(
        None, "--file", "-f", help="Load workflow definition from a file"
    ),
):
    """Create a new workflow."""
    try:
        # Get description if not provided
        if not description:
            description = Prompt.ask("Enter workflow description")
        
        steps = []
        
        if from_file:
            # Load workflow definition from file
            if not from_file.exists():
                console.print(f"[bold red]Error:[/bold red] File not found: {from_file}")
                sys.exit(1)
            
            # Read the file
            with open(from_file, "r") as f:
                workflow_text = f.read()
            
            # Get context
            context = context_manager.get_context_dict()
            
            # Define workflow from file content
            workflow = asyncio.run(workflow_manager.define_workflow_from_natural_language(
                name=name,
                description=description,
                natural_language=workflow_text,
                context=context
            ))
        else:
            # Interactive workflow creation
            console.print("Enter the steps for your workflow. Each step should be a shell command.")
            console.print("Press [bold cyan]Enter[/bold cyan] on an empty line when finished.")
            
            step_num = 1
            while True:
                command = Prompt.ask(f"Step {step_num} command", default="")
                if not command:
                    break
                
                explanation = Prompt.ask(f"Step {step_num} explanation", default="")
                optional = Confirm.ask(f"Is step {step_num} optional?", default=False)
                requires_confirmation = Confirm.ask(f"Does step {step_num} require confirmation?", default=False)
                
                steps.append({
                    "command": command,
                    "explanation": explanation,
                    "optional": optional,
                    "requires_confirmation": requires_confirmation
                })
                
                step_num += 1
            
            # Need at least one step
            if not steps:
                console.print("[bold red]Error:[/bold red] Workflow must have at least one step.")
                sys.exit(1)
            
            # Define the workflow
            workflow = asyncio.run(workflow_manager.define_workflow(
                name=name,
                description=description,
                steps=steps
            ))
        
        # Display the created workflow
        await_func(terminal_formatter.display_workflow(workflow))
        
        console.print(f"[bold green]Workflow '{name}' created successfully![/bold green]")
        console.print(f"Run it with: [bold cyan]angela workflows run {name}[/bold cyan]")
        
    except Exception as e:
        logger.exception(f"Error creating workflow: {str(e)}")
        console.print(f"[bold red]Error:[/bold red] {str(e)}")
        sys.exit(1)


@app.command("run")
def run_workflow(
    name: str = typer.Argument(..., help="Name of the workflow to run"),
    variable: List[str] = typer.Option(
        [], "--var", "-v", help="Variable value in format NAME=VALUE"
    ),
    dry_run: bool = typer.Option(
        False, "--dry-run", help="Show what would happen without executing"
    ),
):
    """Run a workflow."""
    try:
        # Check if workflow exists
        workflow = workflow_manager.get_workflow(name)
        if not workflow:
            console.print(f"[bold red]Error:[/bold red] Workflow '{name}' not found.")
            
            # Show available workflows
            available = workflow_manager.list_workflows()
            if available:
                console.print("\nAvailable workflows:")
                for w in available:
                    console.print(f"  - {w.name}")
            else:
                console.print("\nNo workflows defined yet. Use 'angela workflows create' to define one.")
                
            sys.exit(1)
        
        # Parse variables
        variables = {}
        for var in variable:
            if "=" in var:
                key, value = var.split("=", 1)
                variables[key] = value
            else:
                console.print(f"[bold yellow]Warning:[/bold yellow] Ignoring invalid variable format: {var}")
        
        # Get context
        context = context_manager.get_context_dict()
        
        # Display the workflow
        await_func(terminal_formatter.display_workflow(workflow, variables))
        
        # Confirm execution
        if not dry_run:
            if not Confirm.ask("Execute this workflow?", default=True):
                console.print("Workflow execution cancelled.")
                return
        
        # Execute the workflow
        result = asyncio.run(workflow_manager.execute_workflow(
            workflow_name=name,
            variables=variables,
            context=context,
            dry_run=dry_run
        ))
        
        # Display results
        if result["success"]:
            status = "[bold green]Workflow executed successfully![/bold green]"
            if dry_run:
                status = "[bold blue]Dry run completed successfully![/bold blue]"
            console.print(status)
        else:
            console.print("[bold red]Workflow execution failed.[/bold red]")
            
            if result.get("error"):
                console.print(f"Error: {result['error']}")
        
    except Exception as e:
        logger.exception(f"Error running workflow: {str(e)}")
        console.print(f"[bold red]Error:[/bold red] {str(e)}")
        sys.exit(1)


@app.command("delete")
def delete_workflow(
    name: str = typer.Argument(..., help="Name of the workflow to delete"),
    force: bool = typer.Option(
        False, "--force", "-f", help="Delete without confirmation"
    ),
):
    """Delete a workflow."""
    try:
        # Check if workflow exists
        workflow = workflow_manager.get_workflow(name)
        if not workflow:
            console.print(f"[bold red]Error:[/bold red] Workflow '{name}' not found.")
            sys.exit(1)
        
        # Confirm deletion
        if not force:
            if not Confirm.ask(f"Are you sure you want to delete workflow '{name}'?", default=False):
                console.print("Deletion cancelled.")
                return
        
        # Delete the workflow
        workflow_manager.delete_workflow(name)
        
        console.print(f"[bold green]Workflow '{name}' deleted successfully.[/bold green]")
        
    except Exception as e:
        logger.exception(f"Error deleting workflow: {str(e)}")
        console.print(f"[bold red]Error:[/bold red] {str(e)}")
        sys.exit(1)


@app.command("show")
def show_workflow(
    name: str = typer.Argument(..., help="Name of the workflow to show"),
    verbose: bool = typer.Option(
        False, "--verbose", "-v", help="Show detailed information"
    ),
):
    """Show details of a workflow."""
    try:
        # Get the workflow
        workflow = workflow_manager.get_workflow(name)
        if not workflow:
            console.print(f"[bold red]Error:[/bold red] Workflow '{name}' not found.")
            sys.exit(1)
        
        # Display the workflow
        await_func(terminal_formatter.display_workflow(workflow))
        
        # Show additional details if verbose
        if verbose:
            console.print("\n[bold]Additional Details:[/bold]")
            console.print(f"Created: {workflow.created.strftime('%Y-%m-%d %H:%M:%S')}")
            console.print(f"Last Modified: {workflow.modified.strftime('%Y-%m-%d %H:%M:%S')}")
            
            if workflow.tags:
                console.print(f"Tags: {', '.join(workflow.tags)}")
            
            if workflow.author:
                console.print(f"Author: {workflow.author}")
            
            if workflow.variables:
                console.print("\n[bold]Variables:[/bold]")
                for var_name, var_desc in workflow.variables.items():
                    console.print(f"  {var_name}: {var_desc}")
        
    except Exception as e:
        logger.exception(f"Error showing workflow: {str(e)}")
        console.print(f"[bold red]Error:[/bold red] {str(e)}")
        sys.exit(1)


def await_func(coro):
    """Helper to run a coroutine in a synchronous context."""
    return asyncio.run(coro)
</file>

<file path="angela/context/file_detector.py">
"""
File type detection for Angela CLI.

This module provides functionality to detect file types and languages
to enhance context awareness for operations.
"""
import re
import os
import mimetypes
from pathlib import Path
from typing import Dict, Any, Optional, List, Tuple

from angela.utils.logging import get_logger

logger = get_logger(__name__)

# Initialize mimetypes
mimetypes.init()

# Map of file extensions to programming languages
LANGUAGE_EXTENSIONS = {
    # Web
    '.html': 'HTML',
    '.htm': 'HTML',
    '.css': 'CSS',
    '.js': 'JavaScript',
    '.jsx': 'JavaScript (React)',
    '.ts': 'TypeScript',
    '.tsx': 'TypeScript (React)',
    
    # Python
    '.py': 'Python',
    '.pyi': 'Python Interface',
    '.pyx': 'Cython',
    '.ipynb': 'Jupyter Notebook',
    
    # Ruby
    '.rb': 'Ruby',
    '.erb': 'Ruby (ERB)',
    '.rake': 'Ruby (Rake)',
    
    # Java/JVM
    '.java': 'Java',
    '.kt': 'Kotlin',
    '.groovy': 'Groovy',
    '.scala': 'Scala',
    
    # C/C++
    '.c': 'C',
    '.h': 'C Header',
    '.cpp': 'C++',
    '.cc': 'C++',
    '.hpp': 'C++ Header',
    
    # C#
    '.cs': 'C#',
    
    # Go
    '.go': 'Go',
    
    # Rust
    '.rs': 'Rust',
    
    # Swift
    '.swift': 'Swift',
    
    # PHP
    '.php': 'PHP',
    
    # Shell
    '.sh': 'Shell (Bash)',
    '.bash': 'Bash',
    '.zsh': 'Zsh',
    '.fish': 'Fish',
    
    # Configuration
    '.json': 'JSON',
    '.yaml': 'YAML',
    '.yml': 'YAML',
    '.toml': 'TOML',
    '.ini': 'INI',
    '.cfg': 'Config',
    '.conf': 'Config',
    
    # Markup
    '.md': 'Markdown',
    '.rst': 'reStructuredText',
    '.xml': 'XML',
    '.svg': 'SVG',
    
    # Data
    '.csv': 'CSV',
    '.tsv': 'TSV',
    '.txt': 'Text',
    '.log': 'Log',
    
    # Documents
    '.pdf': 'PDF',
    '.doc': 'MS Word',
    '.docx': 'MS Word',
    '.xls': 'MS Excel',
    '.xlsx': 'MS Excel',
    '.ppt': 'MS PowerPoint',
    '.pptx': 'MS PowerPoint',
    
    # Images
    '.jpg': 'JPEG Image',
    '.jpeg': 'JPEG Image',
    '.png': 'PNG Image',
    '.gif': 'GIF Image',
    '.bmp': 'BMP Image',
    '.webp': 'WebP Image',
    
    # Audio
    '.mp3': 'MP3 Audio',
    '.wav': 'WAV Audio',
    '.ogg': 'OGG Audio',
    '.flac': 'FLAC Audio',
    
    # Video
    '.mp4': 'MP4 Video',
    '.avi': 'AVI Video',
    '.mkv': 'MKV Video',
    '.mov': 'MOV Video',
    
    # Archives
    '.zip': 'ZIP Archive',
    '.tar': 'TAR Archive',
    '.gz': 'GZIP Archive',
    '.bz2': 'BZIP2 Archive',
    '.xz': 'XZ Archive',
    '.7z': '7-Zip Archive',
    '.rar': 'RAR Archive',
    
    # Executables
    '.exe': 'Windows Executable',
    '.dll': 'Windows Library',
    '.so': 'Shared Object',
    '.dylib': 'macOS Library',
    
    # Other
    '.sql': 'SQL',
    '.db': 'Database',
    '.sqlite': 'SQLite Database',
}

# Mapping of file names to types
FILENAME_MAPPING = {
    'Dockerfile': 'Docker',
    'docker-compose.yml': 'Docker Compose',
    'docker-compose.yaml': 'Docker Compose',
    '.dockerignore': 'Docker',
    'Makefile': 'Makefile',
    'CMakeLists.txt': 'CMake',
    'package.json': 'Node.js',
    'package-lock.json': 'Node.js',
    'yarn.lock': 'Yarn',
    'requirements.txt': 'Python',
    'setup.py': 'Python',
    'pyproject.toml': 'Python',
    'Pipfile': 'Python (Pipenv)',
    'Pipfile.lock': 'Python (Pipenv)',
    'Gemfile': 'Ruby',
    'Gemfile.lock': 'Ruby',
    'build.gradle': 'Gradle',
    'build.gradle.kts': 'Gradle (Kotlin)',
    'pom.xml': 'Maven',
    'Cargo.toml': 'Rust',
    'Cargo.lock': 'Rust',
    '.gitignore': 'Git',
    '.gitattributes': 'Git',
    '.gitlab-ci.yml': 'GitLab CI',
    '.travis.yml': 'Travis CI',
    'Jenkinsfile': 'Jenkins',
    '.editorconfig': 'EditorConfig',
    '.eslintrc': 'ESLint',
    '.eslintrc.js': 'ESLint',
    '.eslintrc.json': 'ESLint',
    '.prettierrc': 'Prettier',
    '.prettierrc.js': 'Prettier',
    '.prettierrc.json': 'Prettier',
    'tsconfig.json': 'TypeScript',
    'tslint.json': 'TSLint',
    '.babelrc': 'Babel',
    'babel.config.js': 'Babel',
    'webpack.config.js': 'Webpack',
    'rollup.config.js': 'Rollup',
    'vite.config.js': 'Vite',
    'jest.config.js': 'Jest',
    '.env': 'Environment Variables',
    '.env.example': 'Environment Variables',
    'README.md': 'Documentation',
    'LICENSE': 'License',
    'CHANGELOG.md': 'Changelog',
    'CONTRIBUTING.md': 'Documentation',
    'CODE_OF_CONDUCT.md': 'Documentation',
}

# Language-specific shebang patterns
SHEBANG_PATTERNS = [
    (r'^#!/bin/bash', 'Bash'),
    (r'^#!/usr/bin/env\s+bash', 'Bash'),
    (r'^#!/bin/sh', 'Shell'),
    (r'^#!/usr/bin/env\s+sh', 'Shell'),
    (r'^#!/usr/bin/python', 'Python'),
    (r'^#!/usr/bin/env\s+python', 'Python'),
    (r'^#!/usr/bin/node', 'JavaScript'),
    (r'^#!/usr/bin/env\s+node', 'JavaScript'),
    (r'^#!/usr/bin/ruby', 'Ruby'),
    (r'^#!/usr/bin/env\s+ruby', 'Ruby'),
    (r'^#!/usr/bin/perl', 'Perl'),
    (r'^#!/usr/bin/env\s+perl', 'Perl'),
    (r'^#!/usr/bin/php', 'PHP'),
    (r'^#!/usr/bin/env\s+php', 'PHP'),
]

# Mapping of MIME type prefixes to general types
MIME_TYPE_MAPPING = {
    'image/': 'image',
    'audio/': 'audio',
    'video/': 'video',
    'text/': 'text',
    'application/pdf': 'document',
    'application/msword': 'document',
    'application/vnd.openxmlformats-officedocument': 'document',
    'application/zip': 'archive',
    'application/x-tar': 'archive',
    'application/x-gzip': 'archive',
    'application/x-bzip2': 'archive',
    'application/x-xz': 'archive',
    'application/x-7z-compressed': 'archive',
    'application/x-rar-compressed': 'archive',
}


# In angela/context/file_detector.py - update the detect_file_type function

def detect_file_type(path: Path) -> Dict[str, Any]:
    """
    Detect the type of a file based on extension, content, and other heuristics.
    
    Args:
        path: The path to the file.
        
    Returns:
        A dictionary with file type information.
    """
    result = {
        'type': 'unknown',
        'language': None,
        'mime_type': None,
        'binary': False,
        'encoding': None,
    }
    
    try:
        if not path.exists():
            return result
        
        # Check if it's a directory
        if path.is_dir():
            result['type'] = 'directory'
            return result
        
        # Get file name and extension
        name = path.name
        extension = path.suffix.lower()
        
        # Check if it's a known file by name
        if name in FILENAME_MAPPING:
            result['type'] = FILENAME_MAPPING[name]
            
        # Get MIME type
        mime_type, encoding = mimetypes.guess_type(str(path))
        if mime_type:
            result['mime_type'] = mime_type
            result['encoding'] = encoding
            
            # Get general type from MIME
            main_type = mime_type.split('/')[0]
            result['type'] = main_type
        
        # Detect language based on extension
        if extension in LANGUAGE_EXTENSIONS:
            result['language'] = LANGUAGE_EXTENSIONS[extension]
            result['type'] = 'source_code'  # Set type to source_code when a language is detected
        
        # Special case for known project files
        if name == "requirements.txt":
            result['type'] = "Python"  # Force correct type for requirements.txt
            
        # For text files without a clear type, check for shebangs
        if extension in ['.txt', ''] or not result['language']:
            try:
                # Read the first line of the file
                with open(path, 'r', errors='ignore') as f:
                    first_line = f.readline().strip()
                
                # Check for shebang patterns
                for pattern, language in SHEBANG_PATTERNS:
                    if re.match(pattern, first_line):
                        result['language'] = language
                        result['type'] = 'source_code'  # Set the type for scripting files with shebangs
                        break
            except UnicodeDecodeError:
                # File is likely binary
                result['binary'] = True
                result['type'] = 'binary'
        
        # Check if the file is binary
        if not result['binary'] and not result['type'] == 'directory':
            try:
                with open(path, 'rb') as f:
                    chunk = f.read(4096)
                    # Check for null bytes (common in binary files)
                    if b'\0' in chunk:
                        result['binary'] = True
                        if not result['type'] or result['type'] == 'unknown':
                            result['type'] = 'binary'
            except IOError:
                pass
        
        return result
    
    except Exception as e:
        logger.exception(f"Error detecting file type for {path}: {str(e)}")
        return result


def get_content_preview(path: Path, max_lines: int = 10, max_chars: int = 1000) -> Optional[str]:
    """
    Get a preview of a file's content.
    
    Args:
        path: The path to the file.
        max_lines: Maximum number of lines to preview.
        max_chars: Maximum number of characters to preview.
        
    Returns:
        A string with the file preview, or None if the file is not readable.
    """
    try:
        if not path.exists() or not path.is_file():
            return None
        
        # Check file type
        file_info = detect_file_type(path)
        if file_info['binary']:
            return "[Binary file]"
        
        # Read the file content
        with open(path, 'r', errors='replace') as f:
            lines = []
            total_chars = 0
            
            for i, line in enumerate(f):
                if i >= max_lines:
                    lines.append("...")
                    break
                
                if total_chars + len(line) > max_chars:
                    # Truncate the line if it would exceed max_chars
                    available_chars = max_chars - total_chars
                    if available_chars > 3:
                        lines.append(line[:available_chars - 3] + "...")
                    break
                
                lines.append(line.rstrip('\n'))
                total_chars += len(line)
        
        return '\n'.join(lines)
    
    except Exception as e:
        logger.exception(f"Error getting content preview for {path}: {str(e)}")
        return None
</file>

<file path="angela/context/manager.py">
"""
Context management for Angela CLI.
"""
import os
from pathlib import Path
from typing import Optional, Dict, Any, List, Set

from angela.constants import PROJECT_MARKERS
from angela.utils.logging import get_logger
from angela.context.file_detector import detect_file_type, get_content_preview

logger = get_logger(__name__)


class ContextManager:
    """
    Manages context information about the current environment.
    
    The context includes:
    - Current working directory
    - Project root (if detected)
    - Project type (if detected)
    - File details for current or specified path
    """
    
    def __init__(self):
        self._cwd: Path = Path.cwd()
        self._project_root: Optional[Path] = None
        self._project_type: Optional[str] = None
        self._current_file: Optional[Path] = None
        self._file_cache: Dict[str, Dict[str, Any]] = {}
        
        # Initialize context
        self.refresh_context()
    
    def refresh_context(self) -> None:
        """Refresh all context information."""
        self._update_cwd()
        self._detect_project_root()
        logger.debug(f"Context refreshed: cwd={self._cwd}, project_root={self._project_root}")
    
    def _update_cwd(self) -> None:
        """Update the current working directory."""
        self._cwd = Path.cwd()
    
    def _detect_project_root(self) -> None:
        """
        Detect the project root by looking for marker files.
        
        Traverses up from the current directory until a marker is found or
        the filesystem root is reached.
        """
        self._project_root = None
        self._project_type = None
        
        # Start from current directory
        current_dir = self._cwd
        
        # Walk up the directory tree
        while current_dir != current_dir.parent:  # Stop at filesystem root
            # Check for project markers
            for marker in PROJECT_MARKERS:
                marker_path = current_dir / marker
                if marker_path.exists():
                    self._project_root = current_dir
                    self._project_type = self._determine_project_type(marker)
                    logger.debug(f"Project detected: {self._project_type} at {self._project_root}")
                    return
            
            # Move up to parent directory
            current_dir = current_dir.parent
    
    def _determine_project_type(self, marker: str) -> str:
        """
        Determine the project type based on the marker file.
        
        Args:
            marker: The marker file that was found.
            
        Returns:
            A string representing the project type.
        """
        marker_to_type = {
            ".git": "git",
            "package.json": "node",
            "requirements.txt": "python",
            "Cargo.toml": "rust",
            "pom.xml": "maven",
            "build.gradle": "gradle",
            "Dockerfile": "docker",
            "docker-compose.yml": "docker-compose",
            "CMakeLists.txt": "cmake",
            "Makefile": "make",
        }
        
        return marker_to_type.get(marker, "unknown")
    
    def set_current_file(self, file_path: Path) -> None:
        """
        Set the current file being worked on.
        
        Args:
            file_path: The path to the current file.
        """
        self._current_file = file_path
    
    def get_file_info(self, path: Optional[Path] = None) -> Dict[str, Any]:
        """
        Get information about a file or the current file.
        
        Args:
            path: The path to get information about, or None to use the current file.
            
        Returns:
            A dictionary with file information, or an empty dict if no file is available.
        """
        file_path = path or self._current_file
        if not file_path:
            return {}
        
        # Check if we have cached information
        cache_key = str(file_path)
        if cache_key in self._file_cache:
            return self._file_cache[cache_key]
        
        # If file doesn't exist, return minimal info
        if not file_path.exists():
            return {
                "path": str(file_path),
                "exists": False,
                "name": file_path.name,
                "extension": file_path.suffix,
            }
        
        # Get basic file info
        stat = file_path.stat()
        
        # Get detailed file type info
        type_info = detect_file_type(file_path)
        
        # Create the result
        result = {
            "path": str(file_path),
            "exists": True,
            "name": file_path.name,
            "extension": file_path.suffix,
            "size": stat.st_size,
            "modified": stat.st_mtime,
            "is_dir": file_path.is_dir(),
            "type": type_info["type"],
            "language": type_info["language"],
            "mime_type": type_info["mime_type"],
            "binary": type_info["binary"],
        }
        
        # Cache the result
        self._file_cache[cache_key] = result
        
        return result
    
    def get_directory_contents(self, path: Optional[Path] = None, include_hidden: bool = False) -> List[Dict[str, Any]]:
        """
        Get information about the contents of a directory.
        
        Args:
            path: The directory path to examine, or None to use the current directory.
            include_hidden: Whether to include hidden files (starting with .).
            
        Returns:
            A list of dictionaries with information about each item in the directory.
        """
        dir_path = path or self._cwd
        if not dir_path.is_dir():
            return []
        
        result = []
        
        try:
            for item in dir_path.iterdir():
                # Skip hidden files unless requested
                if not include_hidden and item.name.startswith('.'):
                    continue
                
                # Get information about this item
                item_info = self.get_file_info(item)
                result.append(item_info)
            
            # Sort by directories first, then by name
            result.sort(key=lambda x: (not x["is_dir"], x["name"].lower()))
            
            return result
        
        except Exception as e:
            logger.exception(f"Error getting directory contents for {dir_path}: {str(e)}")
            return []
    
    def get_file_preview(self, path: Optional[Path] = None, max_lines: int = 10) -> Optional[str]:
        """
        Get a preview of a file's contents.
        
        Args:
            path: The file path to preview, or None to use the current file.
            max_lines: Maximum number of lines to preview.
            
        Returns:
            A string with a preview of the file's contents, or None if not available.
        """
        file_path = path or self._current_file
        if not file_path or not file_path.is_file():
            return None
        
        return get_content_preview(file_path, max_lines=max_lines)
    
    def find_files(
        self, 
        pattern: str, 
        base_dir: Optional[Path] = None, 
        max_depth: int = 10,
        include_hidden: bool = False
    ) -> List[Path]:
        """
        Find files matching a pattern.
        
        Args:
            pattern: The glob pattern to match.
            base_dir: The directory to start from, or None to use the current directory.
            max_depth: Maximum directory depth to search.
            include_hidden: Whether to include hidden files (starting with .).
            
        Returns:
            A list of paths matching the pattern.
        """
        start_dir = base_dir or self._cwd
        if not start_dir.is_dir():
            return []
        
        result = []
        
        try:
            # Use Path.glob for pattern matching
            for path in start_dir.glob(pattern):
                # Skip hidden files unless requested
                if not include_hidden and any(part.startswith('.') for part in path.parts):
                    continue
                
                result.append(path)
            
            return result
        
        except Exception as e:
            logger.exception(f"Error finding files with pattern {pattern}: {str(e)}")
            return []
    
    @property
    def cwd(self) -> Path:
        """Get the current working directory."""
        return self._cwd
    
    @property
    def project_root(self) -> Optional[Path]:
        """Get the detected project root."""
        return self._project_root
    
    @property
    def project_type(self) -> Optional[str]:
        """Get the detected project type."""
        return self._project_type
    
    @property
    def is_in_project(self) -> bool:
        """Check if the current directory is within a project."""
        return self._project_root is not None
    
    @property
    def relative_path(self) -> Optional[Path]:
        """Get the path relative to the project root."""
        if not self._project_root:
            return None
        
        return self._cwd.relative_to(self._project_root)
    
    @property
    def current_file(self) -> Optional[Path]:
        """Get the current file being worked on."""
        return self._current_file
    
    def get_context_dict(self) -> Dict[str, Any]:
        """
        Get a dictionary representation of the current context.
        
        Returns:
            A dictionary with context information.
        """
        context = {
            "cwd": str(self._cwd),
            "project_root": str(self._project_root) if self._project_root else None,
            "project_type": self._project_type,
            "is_in_project": self.is_in_project,
            "relative_path": str(self.relative_path) if self.relative_path else None,
        }
        
        # Add information about the current file if available
        if self._current_file:
            context["current_file"] = self.get_file_info(self._current_file)
        
        return context


# Global context manager instance
context_manager = ContextManager()
</file>

<file path="angela/execution/engine.py">
"""
Engine for safely executing commands.

This module provides the core functionality for command execution with 
safety checks, dry-run capabilities, and execution tracking.
"""
import asyncio
import shlex
import subprocess
from typing import Dict, Any, List, Tuple, Optional

from angela.utils.logging import get_logger
from angela.intent.models import ActionPlan

logger = get_logger(__name__)

class ExecutionEngine:
    """Engine for safely executing commands."""
    
    def __init__(self):
        """Initialize the execution engine."""
        self._logger = logger
    
    async def execute_command(
        self, 
        command: str,
        check_safety: bool = True,
        dry_run: bool = False
    ) -> Tuple[str, str, int]:
        """
        Execute a shell command and return its output.
        
        Args:
            command: The shell command to execute.
            check_safety: Whether to perform safety checks before execution.
            dry_run: Whether to simulate the command without actual execution.
            
        Returns:
            A tuple of (stdout, stderr, return_code).
        """
        self._logger.info(f"Preparing to execute command: {command}")
        
        # If safety checks are requested, perform them
        if check_safety:
            # Import here to avoid circular imports
            from angela.safety import check_command_safety
            
            # Check if the command is safe to execute
            is_safe = await check_command_safety(command, dry_run)
            if not is_safe:
                self._logger.warning(f"Command execution cancelled due to safety concerns: {command}")
                return "", "Command execution cancelled due to safety concerns", 1
            
            # For dry runs, return simulated results
            if dry_run:
                self._logger.info(f"DRY RUN: Would execute command: {command}")
                return f"[DRY RUN] Would execute: {command}", "", 0
        
        # Execute the command
        try:
            # Split the command properly using shlex
            args = shlex.split(command)
            
            # Execute the command and capture output
            process = await asyncio.create_subprocess_exec(
                *args,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE,
            )
            
            # Wait for the command to complete
            stdout_bytes, stderr_bytes = await process.communicate()
            stdout = stdout_bytes.decode('utf-8', errors='replace')
            stderr = stderr_bytes.decode('utf-8', errors='replace')
            
            self._logger.debug(f"Command completed with return code: {process.returncode}")
            self._logger.debug(f"stdout: {stdout[:100]}{'...' if len(stdout) > 100 else ''}")
            if stderr:
                self._logger.debug(f"stderr: {stderr}")
            
            # Record the operation for potential rollback
            if not dry_run and process.returncode == 0:
                from angela.execution.rollback import rollback_manager
                await rollback_manager.record_operation(
                    operation_type="execute_command",
                    params={"command": command},
                    backup_path=None  # Commands don't have direct file backups
                )
            
            return stdout, stderr, process.returncode
        
        except Exception as e:
            self._logger.exception(f"Error executing command '{command}': {str(e)}")
            return "", str(e), -1
    
    async def execute_plan(
        self, 
        plan: ActionPlan,
        check_safety: bool = True,
        dry_run: bool = False
    ) -> List[Dict[str, Any]]:
        """
        Execute an action plan and return the results.
        
        Args:
            plan: The action plan to execute.
            check_safety: Whether to perform safety checks before execution.
            dry_run: Whether to simulate the commands without actual execution.
            
        Returns:
            A list of execution results, one for each command in the plan.
        """
        results = []
        
        for i, command in enumerate(plan.commands):
            explanation = plan.explanations[i] if i < len(plan.explanations) else ""
            
            # Execute the command with safety checks if requested
            stdout, stderr, return_code = await self.execute_command(
                command, 
                check_safety=check_safety,
                dry_run=dry_run
            )
            
            # Record the result
            result = {
                "command": command,
                "explanation": explanation,
                "stdout": stdout,
                "stderr": stderr,
                "return_code": return_code,
                "success": return_code == 0,
                "dry_run": dry_run
            }
            
            results.append(result)
            
            # If a command fails, stop executing the plan
            if return_code != 0 and not dry_run:
                self._logger.warning(f"Stopping plan execution due to command failure: {command}")
                break
        
        return results
    
    async def dry_run_command(self, command: str) -> Tuple[str, str, int]:
        """
        Perform a dry run of a command without executing it.
        
        Args:
            command: The shell command to simulate.
            
        Returns:
            A tuple of (simulated_stdout, simulated_stderr, return_code).
        """
        # Import here to avoid circular imports
        from angela.safety.preview import generate_preview
        
        preview = await generate_preview(command)
        
        if preview:
            return preview, "", 0
        else:
            return f"[DRY RUN] Would execute: {command}", "", 0

# Global execution engine instance
execution_engine = ExecutionEngine()
</file>

<file path="angela/safety/confirmation.py">
"""
User confirmation interface for potentially risky operations.

This module handles presenting command previews and obtaining user confirmation
based on the risk level of operations.
"""
import sys
from typing import Dict, Any, Optional, List, Tuple

from rich.console import Console
from rich.panel import Panel
from rich.syntax import Syntax
from rich.prompt import Confirm
from rich.table import Table
from rich.text import Text

from angela.constants import RISK_LEVELS, DEFAULT_CONFIRMATION_REQUIREMENTS
from angela.config import config_manager
from angela.utils.logging import get_logger

logger = get_logger(__name__)
console = Console()

# Risk level color mapping
RISK_COLORS = {
    RISK_LEVELS["SAFE"]: "green",
    RISK_LEVELS["LOW"]: "blue", 
    RISK_LEVELS["MEDIUM"]: "yellow",
    RISK_LEVELS["HIGH"]: "bright_red",  
    RISK_LEVELS["CRITICAL"]: "red",
}

# Risk level names for display
RISK_LEVEL_NAMES = {v: k for k, v in RISK_LEVELS.items()}


def requires_confirmation(risk_level: int) -> bool:
    """
    Determine if a risk level requires confirmation based on configuration.
    
    Args:
        risk_level: The risk level to check.
        
    Returns:
        True if confirmation is required, False otherwise.
    """
    # If confirm_all_actions is set, always confirm
    if config_manager.config.user.confirm_all_actions:
        return True
    
    # Otherwise, check the default requirements
    return DEFAULT_CONFIRMATION_REQUIREMENTS.get(risk_level, True)


def format_impact_analysis(impact: Dict[str, Any]) -> Table:
    """
    Format the command impact analysis into a rich Table.
    
    Args:
        impact: The impact analysis dictionary.
        
    Returns:
        A rich Table object with the formatted impact analysis.
    """
    table = Table(title="Impact Analysis", expand=True)
    
    table.add_column("Aspect", style="bold cyan")
    table.add_column("Details", style="white")
    
    # Add operation types
    operations = ", ".join(impact.get("operations", ["unknown"]))
    table.add_row("Operations", operations)
    
    # Add destructive warning if applicable
    if impact.get("destructive", False):
        table.add_row("⚠️ Warning", "[bold red]This operation may delete or overwrite files[/bold red]")
    
    # Add file creation info
    if impact.get("creates_files", False):
        table.add_row("Creates Files", "Yes")
    
    # Add file modification info
    if impact.get("modifies_files", False):
        table.add_row("Modifies Files", "Yes")
    
    # Add affected files
    affected_files = impact.get("affected_files", [])
    if affected_files:
        file_list = "\n".join(affected_files[:5])
        if len(affected_files) > 5:
            file_list += f"\n...and {len(affected_files) - 5} more"
        table.add_row("Affected Files", file_list)
    
    # Add affected directories
    affected_dirs = impact.get("affected_dirs", [])
    if affected_dirs:
        dir_list = "\n".join(affected_dirs[:5])
        if len(affected_dirs) > 5:
            dir_list += f"\n...and {len(affected_dirs) - 5} more"
        table.add_row("Affected Directories", dir_list)
    
    return table


async def get_confirmation(
    command: str, 
    risk_level: int, 
    risk_reason: str,
    impact: Dict[str, Any],
    preview: Optional[str] = None,
    dry_run: bool = False
) -> bool:
    """
    Get user confirmation for a command based on its risk level.
    
    Args:
        command: The command to be executed.
        risk_level: The risk level of the command.
        risk_reason: The reason for the risk classification.
        impact: The impact analysis dictionary.
        preview: Optional preview of command results.
        dry_run: Whether this is a dry run.
        
    Returns:
        True if the user confirms, False otherwise.
    """
    # If the risk doesn't require confirmation, return True
    if not requires_confirmation(risk_level) and not dry_run:
        return True
    
    # Get the risk level name and color
    risk_name = RISK_LEVEL_NAMES.get(risk_level, "UNKNOWN")
    risk_color = RISK_COLORS.get(risk_level, "yellow")
    
    # Create panel title based on risk
    if dry_run:
        title = Text("DRY RUN", style=f"bold {risk_color}")
    else:
        title = Text(f"Confirm {risk_name} Risk Operation", style=f"bold {risk_color}")
    
    # Display the command with syntax highlighting
    console.print("\n")
    console.print(Panel(
        Syntax(command, "bash", theme="monokai", word_wrap=True),
        title="Command",
        border_style=risk_color,
        expand=False
    ))
    
    # Display the risk information
    console.print(f"[bold {risk_color}]Risk Level:[/bold {risk_color}] {risk_name}")
    console.print(f"[bold {risk_color}]Reason:[/bold {risk_color}] {risk_reason}")
    
    # Display impact analysis
    console.print(format_impact_analysis(impact))
    
    # Display preview if available
    if preview:
        console.print(Panel(
            preview,
            title="Command Preview",
            border_style=risk_color,
            expand=False
        ))
    
    # For critical operations, use a more prominent warning
    if risk_level == RISK_LEVELS["CRITICAL"]:
        console.print(Panel(
            "⚠️  [bold red]This is a CRITICAL risk operation[/bold red] ⚠️\n"
            "It may cause significant changes to your system or data loss.",
            border_style="red",
            expand=False
        ))
    
    # For dry run, just show the information without asking for confirmation
    if dry_run:
        console.print(Panel(
            "[bold blue]This is a dry run.[/bold blue] No changes will be made.",
            border_style="blue",
            expand=False
        ))
        return False
    
    # Ask for confirmation
    return Confirm.ask("Do you want to proceed?", default=False)
</file>

<file path="angela/safety/preview.py">
"""
Preview generator for command execution.

This module generates previews of what commands will do before they are executed,
helping users make informed decisions about risky operations.
"""
import os
import re
import shlex
import glob
import tempfile
from pathlib import Path
from typing import Dict, Any, Optional, List, Tuple

from angela.execution.engine import execution_engine
from angela.utils.logging import get_logger

logger = get_logger(__name__)

async def preview_mkdir(command: str, tokens: List[str]) -> str:
    """Generate a preview for mkdir command."""
    # Parse flags and paths
    paths = []
    recursive = '-p' in tokens or '--parents' in tokens
    
    for arg in tokens[1:]:
        if not arg.startswith('-'):
            paths.append(arg)
    
    result = []
    for path in paths:
        path_obj = Path(path)
        if path_obj.exists():
            result.append(f"⚠️ Path already exists: {path}")
        elif path_obj.parent.exists() or recursive:
            result.append(f"✓ Will create directory: {path}")
        else:
            result.append(f"❌ Parent directory does not exist: {path.parent}")
    
    if not result:
        return "No directories specified to create."
    
    return "\n".join(result)


async def preview_touch(command: str, tokens: List[str]) -> str:
    """Generate a preview for touch command."""
    # Parse flags and paths
    paths = []
    
    for arg in tokens[1:]:
        if not arg.startswith('-'):
            paths.append(arg)
    
    result = []
    for path in paths:
        path_obj = Path(path)
        if path_obj.exists():
            result.append(f"Will update timestamp: {path}")
        elif path_obj.parent.exists():
            result.append(f"Will create empty file: {path}")
        else:
            result.append(f"❌ Parent directory does not exist: {path_obj.parent}")
    
    if not result:
        return "No files specified to touch."
    
    return "\n".join(result)


async def preview_rm(command: str, tokens: List[str]) -> str:
    """Generate a preview for rm command."""
    # Parse flags and paths
    paths = []
    recursive = '-r' in tokens or '--recursive' in tokens or '-rf' in tokens
    force = '-f' in tokens or '--force' in tokens or '-rf' in tokens
    
    for arg in tokens[1:]:
        if not arg.startswith('-'):
            paths.append(arg)
    
    # Expand any glob patterns
    expanded_paths = []
    for path in paths:
        if '*' in path or '?' in path or '[' in path:
            # Use glob to expand wildcards
            expanded = glob.glob(path)
            if expanded:
                expanded_paths.extend(expanded)
            else:
                expanded_paths.append(f"{path} (no matches)")
        else:
            expanded_paths.append(path)
    
    result = []
    for path in expanded_paths:
        path_obj = Path(path)
        if not path_obj.exists():
            if force:
                continue  # With force flag, non-existent files are silently ignored
            else:
                result.append(f"❌ Not found: {path}")
        elif path_obj.is_dir() and not recursive:
            result.append(f"❌ Cannot remove directory without -r flag: {path}")
        elif path_obj.is_dir():
            file_count = sum(1 for _ in path_obj.glob('**/*'))
            result.append(f"⚠️ Will remove directory containing {file_count} files: {path}")
        else:
            result.append(f"Will remove file: {path}")
    
    if not result:
        return "No files specified to remove or all paths are invalid."
    
    return "\n".join(result)


async def preview_cp(command: str, tokens: List[str]) -> str:
    """Generate a preview for cp command."""
    # This is a simplified preview that doesn't handle all cp options
    
    # Need at least 3 tokens: cp source dest
    if len(tokens) < 3:
        return "Invalid cp command: missing source or destination"
    
    # Last argument is the destination
    destination = tokens[-1]
    # All arguments except the command and destination are sources
    sources = [arg for arg in tokens[1:-1] if not arg.startswith('-')]
    
    recursive = '-r' in tokens or '--recursive' in tokens
    
    result = []
    for source in sources:
        source_path = Path(source)
        
        if not source_path.exists():
            result.append(f"❌ Source does not exist: {source}")
            continue
        
        if source_path.is_dir() and not recursive:
            result.append(f"❌ Cannot copy directory without -r flag: {source}")
            continue
        
        # Determine the destination path
        dest_path = Path(destination)
        if len(sources) > 1 or dest_path.is_dir():
            # Multiple sources or destination is a directory
            if not dest_path.exists():
                if dest_path.name.endswith('/'):  # Explicitly specified as directory
                    result.append(f"Will create directory: {destination}")
                else:
                    result.append(f"Will copy {source} to {destination}")
            else:
                if dest_path.is_dir():
                    result.append(f"Will copy {source} to {destination}/{source_path.name}")
                else:
                    result.append(f"⚠️ Cannot copy multiple sources to a single file: {destination}")
        else:
            # Single source to destination
            if dest_path.exists() and dest_path.is_file():
                result.append(f"⚠️ Will overwrite: {destination}")
            else:
                result.append(f"Will copy {source} to {destination}")
    
    if not result:
        return "No files specified to copy."
    
    return "\n".join(result)


async def preview_mv(command: str, tokens: List[str]) -> str:
    """Generate a preview for mv command."""
    # This is a simplified preview that doesn't handle all mv options
    
    # Need at least 3 tokens: mv source dest
    if len(tokens) < 3:
        return "Invalid mv command: missing source or destination"
    
    # Last argument is the destination
    destination = tokens[-1]
    # All arguments except the command and destination are sources
    sources = [arg for arg in tokens[1:-1] if not arg.startswith('-')]
    
    result = []
    for source in sources:
        source_path = Path(source)
        
        if not source_path.exists():
            result.append(f"❌ Source does not exist: {source}")
            continue
        
        # Determine the destination path
        dest_path = Path(destination)
        if len(sources) > 1 or dest_path.is_dir():
            # Multiple sources or destination is a directory
            if not dest_path.exists():
                if dest_path.name.endswith('/'):  # Explicitly specified as directory
                    result.append(f"Will create directory: {destination}")
                else:
                    result.append(f"Will move {source} to {destination}")
            else:
                if dest_path.is_dir():
                    result.append(f"Will move {source} to {destination}/{source_path.name}")
                else:
                    result.append(f"⚠️ Cannot move multiple sources to a single file: {destination}")
        else:
            # Single source to destination
            if dest_path.exists() and dest_path.is_file():
                result.append(f"⚠️ Will overwrite: {destination}")
            else:
                result.append(f"Will move {source} to {destination}")
    
    if not result:
        return "No files specified to move."
    
    return "\n".join(result)


async def preview_ls(command: str, tokens: List[str]) -> str:
    """Generate a preview for ls command."""
    # Extract the paths from the command
    paths = []
    for arg in tokens[1:]:
        if not arg.startswith('-'):
            paths.append(arg)
    
    # If no paths specified, use current directory
    if not paths:
        paths = ['.']
    
    result = []
    for path in paths:
        try:
            path_obj = Path(path)
            if not path_obj.exists():
                result.append(f"❌ Path does not exist: {path}")
                continue
            
            if path_obj.is_dir():
                # Just count files rather than listing them all
                file_count = sum(1 for _ in path_obj.iterdir())
                result.append(f"Will list directory: {path} (contains {file_count} entries)")
            else:
                result.append(f"Will show file information: {path}")
        except Exception as e:
            result.append(f"Error analyzing {path}: {str(e)}")
    
    return "\n".join(result)


async def preview_cat(command: str, tokens: List[str]) -> str:
    """Generate a preview for cat command."""
    # Extract the paths from the command
    paths = []
    for arg in tokens[1:]:
        if not arg.startswith('-'):
            paths.append(arg)
    
    if not paths:
        return "No files specified to display."
    
    result = []
    for path in paths:
        path_obj = Path(path)
        if not path_obj.exists():
            result.append(f"❌ File does not exist: {path}")
        elif path_obj.is_dir():
            result.append(f"❌ Cannot display directory content: {path}")
        else:
            # Get file size
            size = path_obj.stat().st_size
            size_str = f"{size} bytes"
            if size > 1024:
                size_str = f"{size/1024:.1f} KB"
            if size > 1024 * 1024:
                size_str = f"{size/(1024*1024):.1f} MB"
            
            # Try to determine if it's a text file
            try:
                with open(path_obj, 'rb') as f:
                    is_text = True
                    for block in iter(lambda: f.read(1024), b''):
                        if b'\0' in block:
                            is_text = False
                            break
                
                if is_text:
                    # Count lines
                    with open(path_obj, 'r', errors='replace') as f:
                        line_count = sum(1 for _ in f)
                    result.append(f"Will display text file: {path} ({size_str}, {line_count} lines)")
                else:
                    result.append(f"⚠️ Will display binary file: {path} ({size_str})")
            except Exception as e:
                result.append(f"Error analyzing {path}: {str(e)}")
    
    return "\n".join(result)


async def preview_grep(command: str, tokens: List[str]) -> str:
    """Generate a preview for grep command."""
    # This is a simplified preview that doesn't handle all grep options
    
    # Need at least 3 tokens: grep pattern file
    if len(tokens) < 3:
        return "Invalid grep command: missing pattern or file"
    
    pattern = None
    files = []
    recursive = '-r' in tokens or '--recursive' in tokens
    
    # Simple parsing to extract pattern and files
    pattern_found = False
    for arg in tokens[1:]:
        if arg.startswith('-'):
            continue
        
        if not pattern_found:
            pattern = arg
            pattern_found = True
        else:
            files.append(arg)
    
    if not pattern:
        return "No pattern specified for grep."
    
    if not files:
        if recursive:
            files = ['.']
        else:
            return "No files specified for grep."
    
    result = []
    for file_path in files:
        path_obj = Path(file_path)
        if not path_obj.exists():
            result.append(f"❌ Path does not exist: {file_path}")
        elif path_obj.is_dir() and not recursive:
            result.append(f"❌ Cannot grep directory without -r flag: {file_path}")
        elif path_obj.is_dir() and recursive:
            # Count files in directory
            file_count = sum(1 for _ in path_obj.glob('**/*') if Path(_).is_file())
            result.append(f"Will search for '{pattern}' in directory: {file_path} "
                         f"(contains {file_count} files)")
        else:
            # Try to count occurrences in file
            try:
                with open(path_obj, 'r', errors='replace') as f:
                    content = f.read()
                    count = len(re.findall(pattern, content))
                    result.append(f"Will search for '{pattern}' in {file_path} "
                                 f"(potentially {count} matches)")
            except Exception as e:
                result.append(f"Will search in {file_path}, but preview failed: {str(e)}")
    
    return "\n".join(result)


async def preview_find(command: str, tokens: List[str]) -> str:
    """Generate a preview for find command."""
    # Extract directories to search from the command
    # This is a simple implementation that doesn't handle all find options
    
    dirs = []
    name_pattern = None
    type_filter = None
    
    # Find the directories (arguments before the first option)
    for i, arg in enumerate(tokens[1:], 1):
        if arg.startswith('-'):
            break
        dirs.append(arg)
    
    # If no directories specified, use current directory
    if not dirs:
        dirs = ['.']
    
    # Try to extract name pattern if present
    for i, arg in enumerate(tokens):
        if arg == '-name' and i + 1 < len(tokens):
            name_pattern = tokens[i + 1]
        elif arg == '-type' and i + 1 < len(tokens):
            type_filter = tokens[i + 1]
    
    result = []
    for directory in dirs:
        dir_path = Path(directory)
        if not dir_path.exists():
            result.append(f"❌ Directory does not exist: {directory}")
            continue
        
        if not dir_path.is_dir():
            result.append(f"❌ Not a directory: {directory}")
            continue
        
        # Count files and directories in the search path
        file_count = sum(1 for _ in dir_path.glob('**/*') if Path(_).is_file())
        dir_count = sum(1 for _ in dir_path.glob('**/*') if Path(_).is_dir())
        
        search_desc = f"Will search in: {directory} ({file_count} files, {dir_count} directories)"
        
        if name_pattern:
            search_desc += f"\nLooking for files matching: {name_pattern}"
        
        if type_filter:
            type_desc = {'f': 'files', 'd': 'directories', 'l': 'symbolic links'}.get(type_filter, type_filter)
            search_desc += f"\nFiltering by type: {type_desc}"
        
        result.append(search_desc)
    
    return "\n".join(result)

# Commands that can be simulated with more specific previews
PREVIEWABLE_COMMANDS = {
    'mkdir': preview_mkdir,
    'touch': preview_touch,
    'rm': preview_rm,
    'cp': preview_cp,
    'mv': preview_mv,
    'ls': preview_ls,
    'cat': preview_cat,
    'grep': preview_grep,
    'find': preview_find,
}

async def generate_preview(command: str) -> Optional[str]:
    """
    Generate a preview of what a command will do.
    
    Args:
        command: The shell command to preview.
        
    Returns:
        A string containing the preview, or None if preview is not available.
    """
    try:
        # Parse the command
        tokens = shlex.split(command)
        if not tokens:
            return None
        
        base_cmd = tokens[0]
        
        # Check if we have a specific preview function for this command
        if base_cmd in PREVIEWABLE_COMMANDS:
            return await PREVIEWABLE_COMMANDS[base_cmd](command, tokens)
        
        # For other commands, try to use --dry-run or similar flags if available
        return await generic_preview(command)
    
    except Exception as e:
        logger.exception(f"Error generating preview for '{command}': {str(e)}")
        return f"Preview generation failed: {str(e)}"


async def generic_preview(command: str) -> Optional[str]:
    """
    Generate a generic preview for commands without specific implementations.
    Attempts to use --dry-run flags when available.
    
    Args:
        command: The shell command to preview.
        
    Returns:
        A string containing the preview, or None if preview is not available.
    """
    # List of commands that support --dry-run or similar
    dry_run_commands = {
        'rsync': '--dry-run',
        'apt': '--dry-run',
        'apt-get': '--dry-run',
        'dnf': '--dry-run',
        'yum': '--dry-run',
        'pacman': '--print',
    }
    
    tokens = shlex.split(command)
    base_cmd = tokens[0]
    
    if base_cmd in dry_run_commands:
        # Add the dry run flag
        dry_run_flag = dry_run_commands[base_cmd]
        
        # Check if the flag is already in the command
        if dry_run_flag not in command:
            modified_command = f"{command} {dry_run_flag}"
        else:
            modified_command = command
        
        # Execute the command with the dry run flag
        stdout, stderr, return_code = await execution_engine.execute_command(modified_command)
        
        if return_code == 0:
            return f"Dry run output:\n{stdout}"
        else:
            return f"Dry run failed with error:\n{stderr}"
    
    # For commands without dry run support, return a generic message
    return "Preview not available for this command type. Use --dry-run to simulate."
</file>

<file path="angela/safety/validator.py">
"""
Safety validation for operations.

This module validates operations against safety policies and constraints
before they are executed.
"""
import os
import re
import shlex
from pathlib import Path
from typing import Dict, Any, Optional, List, Tuple

from angela.constants import RISK_LEVELS
from angela.utils.logging import get_logger

logger = get_logger(__name__)

# Define dangerous patterns that should be blocked
DANGEROUS_PATTERNS = [
    # Remove critical system directories
    (r"rm\s+(-r|-f|--recursive|--force)\s+(/|/boot|/etc|/bin|/sbin|/lib|/usr|/var|~)",
     "Removing critical system directories is not allowed"),
    
    # Format disk operations
    (r"(mkfs|fdisk|dd|shred)\s+.*(/dev/sd[a-z]|/dev/nvme[0-9])",
     "Disk formatting operations are not allowed"),
    
    # Critical system commands
    (r"(shutdown|reboot|halt|poweroff|init\s+0|init\s+6)",
     "System power commands are not allowed"),
    
    # Chmod 777 recursively
    (r"chmod\s+(-R|--recursive)\s+777",
     "Setting recursive 777 permissions is not allowed"),
    
    # Network disruption
    (r"(ifconfig|ip)\s+.*down",
     "Network interface disabling is not allowed"),
    
    # Dangerous redirects
    (r">\s*(/etc/passwd|/etc/shadow|/etc/sudoers)",
     "Writing directly to critical system files is not allowed"),
    
    # Hidden command execution
    (r";\s*rm\s+",
     "Hidden deletion commands are not allowed"),
    
    # Web download + execute
    (r"(curl|wget).*\|\s*(bash|sh)",
     "Downloading and executing scripts is not allowed"),
    
    # Disk full attack
    (r"(dd|fallocate)\s+.*if=/dev/zero",
     "Creating large files that may fill disk space is not allowed"),
    
    # Dangerous shell loops
    (r"for\s+.*\s+in\s+.*;.*rm\s+",
     "Shell loops with file deletion are not allowed"),
]

# Define patterns that would require root/sudo access
ROOT_PATTERNS = [
    r"^sudo\s+",
    r"^pkexec\s+",
    r"^su\s+(-|--|-c|\w+)\s+",
    r"(chmod|chown|chgrp)\s+.*(/usr/|/etc/|/bin/|/sbin/|/lib/|/var/)",
    r"(touch|rm|mv|cp)\s+.*(/usr/|/etc/|/bin/|/sbin/|/lib/|/var/)",
    r">\s*(/usr/|/etc/|/bin/|/sbin/|/lib/|/var/)",
]

class ValidationError(Exception):
    """Exception raised when a command fails validation."""
    pass


def validate_command_safety(command: str) -> Tuple[bool, Optional[str]]:
    """
    Validate a command against safety rules.
    
    Args:
        command: The shell command to validate.
        
    Returns:
        A tuple of (is_valid, error_message). If is_valid is False,
        error_message will contain the reason.
    """
    if not command.strip():
        return True, None
    
    # Check against dangerous patterns
    for pattern, message in DANGEROUS_PATTERNS:
        if re.search(pattern, command):
            logger.warning(f"Command '{command}' blocked: {message}")
            return False, message
    
    # Check permission requirements
    if not is_superuser() and requires_superuser(command):
        logger.warning(f"Command '{command}' requires superuser privileges")
        return False, "This command requires superuser privileges, which Angela CLI doesn't have."
    
    return True, None


def requires_superuser(command: str) -> bool:
    """
    Check if a command requires superuser privileges.
    
    Args:
        command: The shell command to check.
        
    Returns:
        True if the command requires superuser privileges, False otherwise.
    """
    for pattern in ROOT_PATTERNS:
        if re.search(pattern, command):
            return True
    
    return False


def is_superuser() -> bool:
    """
    Check if the current process has superuser privileges.
    
    Returns:
        True if running as superuser, False otherwise.
    """
    return os.geteuid() == 0 if hasattr(os, 'geteuid') else False


def check_file_permission(path: Path, require_write: bool = False) -> Tuple[bool, Optional[str]]:
    """
    Check if a file has the required permissions.
    
    Args:
        path: The path to check.
        require_write: Whether write permission is required.
        
    Returns:
        A tuple of (has_permission, error_message). If has_permission is False,
        error_message will contain the reason.
    """
    try:
        if not path.exists():
            # If the file doesn't exist, check if the parent directory is writable
            if require_write:
                parent = path.parent
                if not parent.exists():
                    return False, f"Parent directory {parent} does not exist"
                if not os.access(parent, os.W_OK):
                    return False, f"No write permission for directory {parent}"
            return True, None
        
        if not os.access(path, os.R_OK):
            return False, f"No read permission for {path}"
        
        if require_write and not os.access(path, os.W_OK):
            return False, f"No write permission for {path}"
        
        return True, None
    
    except Exception as e:
        logger.exception(f"Error checking permissions for {path}: {str(e)}")
        return False, f"Permission check failed: {str(e)}"


def validate_operation(operation_type: str, params: Dict[str, Any]) -> Tuple[bool, Optional[str]]:
    """
    Validate a high-level operation against safety rules.
    
    Args:
        operation_type: The type of operation (e.g., 'create_file', 'delete_file').
        params: Parameters for the operation.
        
    Returns:
        A tuple of (is_valid, error_message). If is_valid is False,
        error_message will contain the reason.
    """
    try:
        if operation_type == 'create_file':
            path = Path(params.get('path', ''))
            return check_file_permission(path, require_write=True)
        
        elif operation_type == 'write_file':
            path = Path(params.get('path', ''))
            return check_file_permission(path, require_write=True)
        
        elif operation_type == 'read_file':  # Add this handler
            path = Path(params.get('path', ''))
            return check_file_permission(path, require_write=False)
            
        elif operation_type == 'delete_file':
            path = Path(params.get('path', ''))
            # Check if this is a system file
            system_dirs = ['/bin', '/sbin', '/lib', '/usr', '/etc', '/var']
            if any(str(path).startswith(dir) for dir in system_dirs):
                return False, f"Deleting system files is not allowed: {path}"
            
            return check_file_permission(path, require_write=True)
        
        elif operation_type == 'create_directory':
            path = Path(params.get('path', ''))
            if path.exists():
                return False, f"Path already exists: {path}"
            return check_file_permission(path.parent, require_write=True)
        
        elif operation_type == 'delete_directory':
            path = Path(params.get('path', ''))
            # Check if this is a system directory
            system_dirs = ['/bin', '/sbin', '/lib', '/usr', '/etc', '/var']
            if any(str(path).startswith(dir) for dir in system_dirs):
                return False, f"Deleting system directories is not allowed: {path}"
            
            return check_file_permission(path, require_write=True)
        
        elif operation_type == 'copy_file':  # Add this handler
            source = Path(params.get('source', ''))
            destination = Path(params.get('destination', ''))
            
            # Check if source exists
            if not source.exists():
                return False, f"Source file does not exist: {source}"
                
            # Check destination permissions
            return check_file_permission(destination.parent, require_write=True)
            
        elif operation_type == 'move_file':  # Add this handler
            source = Path(params.get('source', ''))
            destination = Path(params.get('destination', ''))
            
            # Check if source exists
            if not source.exists():
                return False, f"Source file does not exist: {source}"
                
            # Check permissions for both source and destination
            source_ok, source_err = check_file_permission(source, require_write=True)
            if not source_ok:
                return False, source_err
                
            return check_file_permission(destination.parent, require_write=True)
            
        elif operation_type == 'execute_command':
            command = params.get('command', '')
            return validate_command_safety(command)
        
        # Unknown operation type
        logger.warning(f"Unknown operation type: {operation_type}")
        return False, f"Unknown operation type: {operation_type}"
    
    except Exception as e:
        logger.exception(f"Error validating operation {operation_type}: {str(e)}")
        return False, f"Validation failed: {str(e)}"
</file>

<file path="angela/shell/angela.zsh">
#!/bin/zsh
# Angela CLI Zsh Integration

# Function to handle Angela CLI requests
angela() {
    # Check if arguments were provided
    if [ $# -eq 0 ]; then
        # No arguments, show help
        python -m angela --help
    else
        # Capture the current working directory
        local current_dir=$(pwd)
        
        # Process the request
        python -m angela request "$@"
        
        # Note: In future phases, we'll add support for command execution,
        # directory changing, etc. For now, this is just a simple pass-through.
    fi
}

# Enable command completion for angela function
# This will be implemented in a future phase
</file>

<file path="angela/shell/formatter.py">
"""
Rich terminal formatting for Angela CLI.

This module provides enhanced terminal output formatting with
support for async operations and interactive elements.
"""
import asyncio
import sys
from typing import Optional, List, Dict, Any, Callable, Awaitable, Tuple
from enum import Enum
from pathlib import Path

from rich.console import Console
from rich.panel import Panel
from rich.syntax import Syntax
from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn, TimeElapsedColumn
from rich.table import Table
from rich.live import Live
from rich.text import Text
from rich.layout import Layout
from rich.tree import Tree
from rich import box

from angela.utils.logging import get_logger

logger = get_logger(__name__)

class OutputType(Enum):
    """Types of command output."""
    STDOUT = "stdout"
    STDERR = "stderr"
    INFO = "info"
    SUCCESS = "success"
    WARNING = "warning"
    ERROR = "error"
    PROGRESS = "progress"

class TerminalFormatter:
    """
    Rich terminal formatter with support for asynchronous output
    and interactive elements.
    """
    
    def __init__(self):
        """Initialize the terminal formatter."""
        self._console = Console()
        self._logger = logger
    
    def print_command(self, command: str, title: Optional[str] = None) -> None:
        """
        Display a command with syntax highlighting.
        
        Args:
            command: The command to display
            title: Optional title for the panel
        """
        title = title or "Command"
        syntax = Syntax(command, "bash", theme="monokai", word_wrap=True)
        self._console.print(Panel(syntax, title=title, expand=False))
    
    def print_output(
        self, 
        output: str, 
        output_type: OutputType = OutputType.STDOUT,
        title: Optional[str] = None
    ) -> None:
        """
        Display command output with appropriate formatting.
        
        Args:
            output: The output text
            output_type: Type of output
            title: Optional title for the panel
        """
        if not output:
            return
            
        # Set styling based on output type
        if output_type == OutputType.STDERR or output_type == OutputType.ERROR:
            style = "bold red"
            title = title or "Error"
            border_style = "red"
        elif output_type == OutputType.WARNING:
            style = "yellow"
            title = title or "Warning"
            border_style = "yellow"
        elif output_type == OutputType.SUCCESS:
            style = "green"
            title = title or "Success"
            border_style = "green"
        elif output_type == OutputType.INFO:
            style = "blue"
            title = title or "Info"
            border_style = "blue"
        else:  # Default for STDOUT
            style = "white"
            title = title or "Output"
            border_style = "white"
        
        # Create panel with output
        panel = Panel(output, title=title, border_style=border_style, expand=False)
        self._console.print(panel)
    
    def print_error_analysis(self, analysis: Dict[str, Any]) -> None:
        """
        Display error analysis with fix suggestions.
        
        Args:
            analysis: The error analysis dictionary
        """
        # Create a table for the error analysis
        table = Table(title="Error Analysis", expand=False)
        table.add_column("Aspect", style="bold cyan")
        table.add_column("Details", style="white")
        
        # Add error summary
        table.add_row("Error", Text(analysis.get("error_summary", "Unknown error"), style="bold red"))
        
        # Add possible cause
        table.add_row("Possible Cause", analysis.get("possible_cause", "Unknown"))
        
        # Add command issues
        if analysis.get("command_issues"):
            issues = "\n".join(f"• {issue}" for issue in analysis["command_issues"])
            table.add_row("Command Issues", issues)
        
        # Add file issues
        if analysis.get("file_issues"):
            file_issues = []
            for issue in analysis["file_issues"]:
                path = issue.get("path", "unknown")
                if "suggestion" in issue:
                    file_issues.append(f"• {path}: {issue['suggestion']}")
                if "similar_files" in issue:
                    similar = ", ".join(issue["similar_files"])
                    file_issues.append(f"  Did you mean: {similar}?")
            
            if file_issues:
                table.add_row("File Issues", "\n".join(file_issues))
        
        # Display the table
        self._console.print(table)
        
        # Display fix suggestions if available
        if analysis.get("fix_suggestions"):
            suggestions = analysis["fix_suggestions"]
            if suggestions:
                self._console.print(Panel(
                    "\n".join(f"• {suggestion}" for suggestion in suggestions),
                    title="Fix Suggestions",
                    border_style="green",
                    expand=False
                ))
    
    async def stream_output(
        self,
        command: str,
        show_spinner: bool = True,
        show_output: bool = True,
        callback: Optional[Callable[[str, OutputType], Awaitable[None]]] = None
    ) -> Tuple[str, str, int]:
        """
        Stream command output asynchronously with rich formatting.
        
        Args:
            command: The command to execute
            show_spinner: Whether to show a spinner
            show_output: Whether to display output
            callback: Optional callback for when output is received
            
        Returns:
            Tuple of (stdout, stderr, return_code)
        """
        # Import here to avoid circular imports
        from angela.execution.engine import execution_engine
        
        stdout_chunks = []
        stderr_chunks = []
        return_code = None
        
        # Set up progress display if requested
        if show_spinner:
            progress = Progress(
                SpinnerColumn(),
                TextColumn("[bold blue]Executing command...[/bold blue]"),
                TimeElapsedColumn(),
                console=self._console
            )
        else:
            progress = None
        
        try:
            # Start progress if requested
            if progress:
                progress.start()
                task = progress.add_task("Executing", total=None)
            
            # Create the process
            proc = await asyncio.create_subprocess_shell(
                command,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            
            # Set up tasks to read output
            async def read_stream(stream, output_type: OutputType):
                while True:
                    line = await stream.readline()
                    if not line:
                        break
                        
                    try:
                        line_str = line.decode('utf-8', errors='replace')
                        
                        # Store the output
                        if output_type == OutputType.STDOUT:
                            stdout_chunks.append(line_str)
                        else:
                            stderr_chunks.append(line_str)
                        
                        # Display if requested
                        if show_output:
                            if output_type == OutputType.STDOUT:
                                self._console.print(line_str, end="")
                            else:
                                self._console.print(f"[bold red]{line_str}[/bold red]", end="")
                        
                        # Call callback if provided
                        if callback:
                            await callback(line_str, output_type)
                            
                    except Exception as e:
                        self._logger.error(f"Error processing output: {str(e)}")
            
            # Create tasks for stdout and stderr
            stdout_task = asyncio.create_task(read_stream(proc.stdout, OutputType.STDOUT))
            stderr_task = asyncio.create_task(read_stream(proc.stderr, OutputType.STDERR))
            
            # Wait for the process to complete
            return_code = await proc.wait()
            
            # Wait for the streams to complete
            await stdout_task
            await stderr_task
            
            # Update progress
            if progress:
                progress.update(task, completed=True)
        
        finally:
            # Clean up progress
            if progress:
                progress.stop()
        
        # Return the collected output
        return "".join(stdout_chunks), "".join(stderr_chunks), return_code
    
    def create_table(
        self, 
        title: str, 
        columns: List[Tuple[str, Optional[str]]]
    ) -> Table:
        """
        Create a rich table.
        
        Args:
            title: The table title
            columns: List of (column_name, style) tuples
            
        Returns:
            A Rich Table object
        """
        table = Table(title=title, expand=False)
        
        for name, style in columns:
            table.add_column(name, style=style)
            
        return table
    
    async def display_task_plan(self, plan: Any) -> None:
        """
        Display a task plan with rich interactive visualization.
        
        Args:
            plan: The task plan to display
        """
        # Create a table for the plan steps
        table = Table(title=f"Plan for: {plan.goal}", box=box.ROUNDED)
        table.add_column("#", style="cyan", no_wrap=True)
        table.add_column("Command", style="green")
        table.add_column("Explanation", style="white")
        table.add_column("Risk", style="yellow", no_wrap=True)
        table.add_column("Dependencies", style="magenta", no_wrap=True)
        
        # Risk level names
        risk_names = ["SAFE", "LOW", "MEDIUM", "HIGH", "CRITICAL"]
        risk_styles = ["green", "blue", "yellow", "red", "bold red"]
        
        # Add steps to the table
        for i, step in enumerate(plan.steps):
            risk_idx = step.estimated_risk if 0 <= step.estimated_risk < len(risk_names) else 0
            risk_name = risk_names[risk_idx]
            risk_style = risk_styles[risk_idx]
            
            # Format dependencies
            deps = ", ".join([str(d+1) for d in step.dependencies]) if step.dependencies else "None"
            
            table.add_row(
                str(i + 1),
                Syntax(step.command, "bash", theme="monokai", word_wrap=True).markup,
                step.explanation,
                f"[{risk_style}]{risk_name}[/{risk_style}]",
                deps
            )
        
        # Display the table
        self._console.print("\n")
        self._console.print(Panel(
            "I've created a plan to accomplish your goal. Here are the steps I'll take:",
            title="Task Plan",
            border_style="blue"
        ))
        self._console.print(table)
        
        # Create a dependency visualization if there are non-trivial dependencies
        has_dependencies = any(step.dependencies for step in plan.steps)
        if has_dependencies:
            await self._display_dependency_graph(plan)
    
    async def _display_dependency_graph(self, plan: Any) -> None:
        """
        Display a visual representation of the dependency graph.
        
        Args:
            plan: The task plan with dependencies
        """
        # Create a dependency tree
        tree = Tree("Execution Flow", guide_style="bold blue")
        
        # Track processed steps
        processed = set()
        
        # Add steps with no dependencies first (roots)
        roots = []
        for i, step in enumerate(plan.steps):
            if not step.dependencies:
                roots.append(i)
                node = tree.add(f"Step {i+1}: {step.command[:30]}..." if len(step.command) > 30 else step.command)
                processed.add(i)
                
                # Add children recursively
                self._add_dependency_children(node, i, plan, processed)
        
        # Check if any steps were not processed (in case of circular dependencies)
        if len(processed) < len(plan.steps):
            for i, step in enumerate(plan.steps):
                if i not in processed:
                    node = tree.add(f"Step {i+1}: {step.command[:30]}..." if len(step.command) > 30 else step.command)
                    processed.add(i)
                    
                    # Add children recursively
                    self._add_dependency_children(node, i, plan, processed)
        
        # Display the tree
        self._console.print("\n[bold blue]Dependency Graph:[/bold blue]")
        self._console.print(tree)
    
    def _add_dependency_children(
        self, 
        parent_node: Any, 
        step_idx: int, 
        plan: Any, 
        processed: Set[int]
    ) -> None:
        """
        Recursively add children to a dependency node.
        
        Args:
            parent_node: The parent tree node
            step_idx: The index of the current step
            plan: The task plan
            processed: Set of already processed steps
        """
        # Find steps that depend on this one
        for i, step in enumerate(plan.steps):
            if step_idx in step.dependencies and i not in processed:
                node = parent_node.add(f"Step {i+1}: {step.command[:30]}..." 
                                       if len(step.command) > 30 else step.command)
                processed.add(i)
                
                # Recurse
                self._add_dependency_children(node, i, plan, processed)
    
    async def display_multi_step_execution(
        self, 
        plan: Any, 
        results: List[Dict[str, Any]]
    ) -> None:
        """
        Display the results of a multi-step execution.
        
        Args:
            plan: The task plan that was executed
            results: The execution results for each step
        """
        # Create a table for the execution results
        table = Table(title="Execution Results", box=box.ROUNDED)
        table.add_column("#", style="cyan", no_wrap=True)
        table.add_column("Command", style="green")
        table.add_column("Status", style="white", no_wrap=True)
        table.add_column("Output", style="white")
        
        # Add results to the table
        for i, result in enumerate(results):
            # Get the command
            command = result.get("command", plan.steps[i].command if i < len(plan.steps) else "Unknown")
            
            # Get status
            status = "[green]Success[/green]" if result.get("success", False) else "[red]Failed[/red]"
            
            # Get output (combine stdout and stderr)
            stdout = result.get("stdout", "").strip()
            stderr = result.get("stderr", "").strip()
            
            # Truncate output if too long
            output = stdout
            if stderr:
                if output:
                    output += "\n"
                output += f"[red]{stderr}[/red]"
            
            if len(output) > 100:
                output = output[:97] + "..."
            
            table.add_row(
                str(i + 1),
                Syntax(command, "bash", theme="monokai", word_wrap=True).markup,
                status,
                output
            )
        
        # Display the table
        self._console.print("\n")
        self._console.print(Panel(
            "Execution results for your multi-step task:",
            title="Multi-Step Execution",
            border_style="blue"
        ))
        self._console.print(table)
        
        # Display summary
        success_count = sum(1 for r in results if r.get("success", False))
        total_count = len(results)
        
        if success_count == total_count:
            self._console.print(f"[bold green]All {total_count} steps completed successfully![/bold green]")
        else:
            self._console.print(f"[bold yellow]{success_count} of {total_count} steps completed successfully[/bold yellow]")
            
            # Show which steps failed
            failed_steps = [i+1 for i, r in enumerate(results) if not r.get("success", False)]
            if failed_steps:
                self._console.print(f"[bold red]Failed steps: {', '.join(map(str, failed_steps))}[/bold red]")
    
    async def display_workflow(self, workflow: Any, variables: Dict[str, Any] = None) -> None:
        """
        Display a workflow with rich formatting.
        
        Args:
            workflow: The workflow to display
            variables: Optional variables for the workflow
        """
        # Create a table for the workflow steps
        table = Table(title=f"Workflow: {workflow.name}", box=box.ROUNDED)
        table.add_column("#", style="cyan", no_wrap=True)
        table.add_column("Command", style="green")
        table.add_column("Explanation", style="white")
        table.add_column("Options", style="yellow")
        
        # Add steps to the table
        for i, step in enumerate(workflow.steps):
            # Apply variable substitution if variables provided
            command = step.command
            if variables:
                for var_name, var_value in variables.items():
                    # Remove leading $ if present
                    clean_name = var_name[1:] if var_name.startswith('$') else var_name
                    
                    # Substitute ${VAR} syntax
                    command = command.replace(f"${{{clean_name}}}", str(var_value))
                    
                    # Substitute $VAR syntax
                    command = command.replace(f"${clean_name}", str(var_value))
            
            options = []
            if step.optional:
                options.append("Optional")
            if step.requires_confirmation:
                options.append("Requires Confirmation")
            
            table.add_row(
                str(i + 1),
                Syntax(command, "bash", theme="monokai", word_wrap=True).markup,
                step.explanation,
                ", ".join(options) if options else ""
            )
        
        # Display the table
        self._console.print("\n")
        self._console.print(Panel(
            workflow.description,
            title=f"Workflow: {workflow.name}",
            border_style="blue"
        ))
        self._console.print(table)
        
        # Display variables if provided
        if variables:
            var_table = Table(title="Variables", box=box.SIMPLE)
            var_table.add_column("Name", style="cyan")
            var_table.add_column("Value", style="green")
            
            for var_name, var_value in variables.items():
                var_table.add_row(var_name, str(var_value))
            
            self._console.print(var_table)
    
    async def display_file_analysis(self, analysis: Dict[str, Any]) -> None:
        """
        Display file content analysis results.
        
        Args:
            analysis: The analysis results
        """
        self._console.print("\n")
        self._console.print(Panel(
            f"Analysis of {analysis.get('path', 'file')}",
            title="File Analysis",
            border_style="blue"
        ))
        
        # Display language and type info
        file_type = analysis.get("type", "unknown")
        language = analysis.get("language")
        
        if language:
            self._console.print(f"[bold]File type:[/bold] {file_type} ({language})")
        else:
            self._console.print(f"[bold]File type:[/bold] {file_type}")
        
        # Display the analysis text
        self._console.print("\n[bold]Analysis:[/bold]")
        self._console.print(analysis.get("analysis", "No analysis available"))
    
    async def display_file_manipulation(self, manipulation: Dict[str, Any]) -> None:
        """
        Display file manipulation results with diff.
        
        Args:
            manipulation: The manipulation results
        """
        self._console.print("\n")
        self._console.print(Panel(
            f"Changes to {manipulation.get('path', 'file')}",
            title="File Manipulation",
            border_style="blue"
        ))
        
        # Display the instruction
        self._console.print(f"[bold]Instruction:[/bold] {manipulation.get('instruction', 'Unknown')}")
        
        # Display the diff
        self._console.print("\n[bold]Changes:[/bold]")
        syntax = Syntax(manipulation.get("diff", "No changes"), "diff", theme="monokai")
        self._console.print(syntax)
        
        # Show whether changes were applied
        if manipulation.get("changes_applied", False):
            self._console.print("[bold green]Changes have been applied to the file.[/bold green]")
        elif manipulation.get("dry_run", False):
            self._console.print("[bold blue]Dry run: Changes were not applied to the file.[/bold blue]")
        else:
            self._console.print("[bold yellow]Changes were not applied to the file.[/bold yellow]")
    
    async def display_file_search_results(self, search_results: Dict[str, Any]) -> None:
        """
        Display file search results.
        
        Args:
            search_results: The search results
        """
        self._console.print("\n")
        self._console.print(Panel(
            f"Search results in {search_results.get('path', 'file')}",
            title="File Search",
            border_style="blue"
        ))
        
        # Display the query
        self._console.print(f"[bold]Query:[/bold] {search_results.get('query', 'Unknown')}")
        
        # Display match count
        match_count = search_results.get("match_count", 0)
        self._console.print(f"[bold]Found {match_count} matches[/bold]")
        
        # Display matches
        if match_count > 0:
            matches = search_results.get("matches", [])
            
            for i, match in enumerate(matches, 1):
                self._console.print(f"\n[bold cyan]Match #{i}[/bold cyan] (Lines {match.get('line_start', '?')}-{match.get('line_end', '?')})")
                
                # Display the content with context
                syntax = Syntax(match.get("content", ""), search_results.get("language", "text"), theme="monokai", line_numbers=True)
                self._console.print(syntax)
                
                # Display explanation
                if "explanation" in match:
                    self._console.print(f"[italic]{match['explanation']}[/italic]")
    
    def print_suggestion(self, suggestion: Dict[str, Any], with_confidence: bool = True) -> None:
        """
        Print a command suggestion with rich formatting.
        
        Args:
            suggestion: The command suggestion
            with_confidence: Whether to show confidence score
        """
        self._console.print("\n")
        
        # Extract suggestion components
        command = suggestion.get("command", "")
        explanation = suggestion.get("explanation", "")
        confidence = suggestion.get("confidence", 0.0)
        
        # Display the command
        self.print_command(command)
        
        # Show confidence if requested
        if with_confidence:
            confidence_color = "green" if confidence > 0.8 else "yellow" if confidence > 0.6 else "red"
            self._console.print(f"[bold]Confidence:[/bold] [{confidence_color}]{confidence:.2f}[/{confidence_color}]")
        
        # Show explanation
        self._console.print("\n[bold]Explanation:[/bold]")
        self._console.print(explanation)
    
    def print_proactive_suggestion(self, suggestion: str, source: str = "AI") -> None:
        """
        Print a proactive suggestion.
        
        Args:
            suggestion: The suggestion text
            source: The source of the suggestion
        """
        self._console.print("\n")
        self._console.print(Panel(
            suggestion,
            title=f"Suggestion from {source}",
            border_style="green",
            expand=False
        ))

# Global formatter instance
terminal_formatter = TerminalFormatter()
</file>

<file path="angela/cli.py">
# angela/cli.py
"""
Command-line interface for Angela CLI.
"""
iimport sys
import asyncio
from typing import List

import typer
from rich.console import Console
from rich.panel import Panel
from rich.text import Text
from rich.syntax import Syntax
from rich import print as rich_print

from angela import __version__
from angela.config import config_manager
from angela.orchestrator import orchestrator
from angela.utils.logging import setup_logging, get_logger

# Keep existing app definition and version_callback

@app.command()
def request(
    request_text: List[str] = typer.Argument(
        ..., help="The natural language request for Angela."
    ),
    execute: bool = typer.Option(
        False, "--execute", "-e", help="Execute the suggested command."
    ),
):
    """Send a natural language request to Angela."""
    # Combine all arguments into a single request string
    full_request = " ".join(request_text)
    
    try:
        # Process the request
        result = asyncio.run(orchestrator.process_request(full_request, execute))
        
        # Display the response
        panel_title = Text("Angela", style="bold green")
        
        if "suggestion" in result:
            suggestion = result["suggestion"]
            
            # Build panel content with command suggestion
            panel_content = Text()
            panel_content.append("I suggest using this command:\n\n", style="bold")
            
            # Add the command with syntax highlighting
            command_syntax = Syntax(suggestion.command, "bash", theme="monokai", word_wrap=True)
            console.print(Panel(command_syntax, title="Command", expand=False))
            
            # Show explanation
            console.print("\n[bold]Explanation:[/bold]")
            console.print(suggestion.explanation)
            
            # Show execution results if executed
            if "execution" in result:
                execution = result["execution"]
                console.print("\n[bold]Command Output:[/bold]")
                
                if execution["success"]:
                    if execution["stdout"].strip():
                        output_panel = Panel(
                            execution["stdout"], 
                            title="Output", 
                            expand=False,
                            border_style="green"
                        )
                        console.print(output_panel)
                    else:
                        console.print("[green]Command executed successfully with no output.[/green]")
                else:
                    console.print("[bold red]Command failed[/bold red]")
                    if execution["stderr"].strip():
                        error_panel = Panel(
                            execution["stderr"], 
                            title="Error", 
                            expand=False,
                            border_style="red"
                        )
                        console.print(error_panel)
            
        else:
            # Fall back to simple response if no suggestion
            panel_content = Text(result.get("response", "I couldn't process that request."))
            console.print(Panel(panel_content, title=panel_title, expand=False))
        
        # In debug mode, show context information
        if config_manager.config.debug:
            context_text = "\n".join([f"{k}: {v}" for k, v in result["context"].items()])
            rich_print("[bold blue]Context:[/bold blue]")
            rich_print(context_text)
            
    except Exception as e:
        logger.exception("Error processing request")
        console.print(f"[bold red]Error:[/bold red] {str(e)}")
        if config_manager.config.debug:
            import traceback
            console.print(traceback.format_exc())
        sys.exit(1)


@app.command()
def init():
    """
    Initialize Angela CLI with configuration.
    """
    console.print("Initializing Angela CLI...")
    
    # Check if API key is already set
    if config_manager.config.api.gemini_api_key:
        console.print("[green]API key already configured.[/green]")
    else:
        console.print("Google Gemini API key is required for Angela to function.")
        api_key = typer.prompt("Enter your Gemini API key", hide_input=True)
        config_manager.config.api.gemini_api_key = api_key
    
    # Save the configuration
    config_manager.save_config()
    
    console.print("[green]Configuration saved successfully![/green]")
    console.print("\nTo install shell integration, run the installation script:")
    console.print("[blue]  bash scripts/install.sh[/blue]")


if __name__ == "__main__":
    app()
</file>

<file path="angela/config.py">
"""
Configuration management for Angela CLI.
Uses TOML format for configuration files.
"""
import os
import json # Keep json import for potential debugging or other uses if needed
from pathlib import Path
from typing import Dict, Any, Optional
import sys

# --- TOML Library Handling ---

# Reader (tomllib for >= 3.11, tomli for < 3.11)
if sys.version_info >= (3, 11):
    import tomllib
    _TOML_LOAD_AVAILABLE = True
    _TOML_READ_ERROR_TYPE = tomllib.TOMLDecodeError
else:
    try:
        import tomli as tomllib # Alias tomli as tomllib
        _TOML_LOAD_AVAILABLE = True
        _TOML_READ_ERROR_TYPE = tomllib.TOMLDecodeError
    except ImportError:
        tomllib = None
        _TOML_LOAD_AVAILABLE = False
        _TOML_READ_ERROR_TYPE = Exception # Fallback

# Writer (tomli-w)
try:
    import tomli_w
    _TOML_WRITE_AVAILABLE = True
except ImportError:
    tomli_w = None
    _TOML_WRITE_AVAILABLE = False

# --- Pydantic and Environment Handling ---
from pydantic import BaseModel, Field
from dotenv import load_dotenv

from angela.constants import CONFIG_DIR, CONFIG_FILE


# --- Configuration Models ---

class ApiConfig(BaseModel):
    """API configuration settings."""
    gemini_api_key: Optional[str] = Field(None, description="Google Gemini API Key")


class UserConfig(BaseModel):
    """User-specific configuration settings."""
    default_project_root: Optional[Path] = Field(None, description="Default project root directory")
    confirm_all_actions: bool = Field(False, description="Whether to confirm all actions regardless of risk level")


class AppConfig(BaseModel):
    """Application configuration settings."""
    api: ApiConfig = Field(default_factory=ApiConfig, description="API configuration")
    user: UserConfig = Field(default_factory=UserConfig, description="User configuration")
    debug: bool = Field(False, description="Enable debug mode")


# --- Configuration Manager ---

class ConfigManager:
    """Manages the configuration for the Angela CLI application using TOML."""

    def __init__(self):
        """Initializes the ConfigManager with default settings."""
        self._config: AppConfig = AppConfig()
        self._load_environment()
        self._ensure_config_dir()
        # Note: Loading from file happens via the global instance later

    def _load_environment(self) -> None:
        """Loads API keys from environment variables and .env file."""
        load_dotenv() # Load .env file if present
        gemini_api_key = os.getenv("GEMINI_API_KEY")
        if gemini_api_key:
            self._config.api.gemini_api_key = gemini_api_key
            # Add other environment variable loadings here if needed

    def _ensure_config_dir(self) -> None:
        """Ensures the application's configuration directory exists."""
        try:
            CONFIG_DIR.mkdir(parents=True, exist_ok=True)
        except OSError as e:
            print(f"Error creating configuration directory {CONFIG_DIR}: {e}")
            # Depending on severity, might want to raise or exit here

    def load_config(self) -> None:
        """Loads configuration from the TOML config file."""
        if not CONFIG_FILE.exists():
            print(f"Configuration file not found at '{CONFIG_FILE}'. Saving default configuration.")
            self.save_config() # Save default TOML config
            return

        if not _TOML_LOAD_AVAILABLE:
            print(f"Warning: Cannot load TOML config file '{CONFIG_FILE}'.")
            if sys.version_info < (3, 11):
                print("       Reason: 'tomli' package is not installed for this Python version.")
                print("       To fix, ensure 'tomli; python_version < \"3.11\"' is in your dependencies.")
            else:
                 print("       Reason: Could not import the built-in 'tomllib' module.") # Should be unlikely
            print("       Using default configuration and environment variables.")
            return

        try:
            print(f"Loading configuration from: {CONFIG_FILE}") # Debugging info
            with open(CONFIG_FILE, "rb") as f: # TOML requires binary read mode
                config_data = tomllib.load(f)

            # Update configuration with loaded data, using Pydantic validation
            if "api" in config_data and isinstance(config_data["api"], dict):
                self._config.api = ApiConfig(**config_data["api"])

            if "user" in config_data and isinstance(config_data["user"], dict):
                 # Pydantic will handle Path conversion from string during validation
                 self._config.user = UserConfig(**config_data["user"])

            if "debug" in config_data:
                # Explicitly check type for robustness
                if isinstance(config_data["debug"], bool):
                     self._config.debug = config_data["debug"]
                else:
                     print(f"Warning: Invalid type for 'debug' in {CONFIG_FILE}. Expected boolean, got {type(config_data['debug'])}. Ignoring.")

        except _TOML_READ_ERROR_TYPE as e:
             print(f"Error decoding TOML configuration file ({CONFIG_FILE}): {e}")
             print("       Please check the file syntax. Using default configuration and environment variables.")
             print("       Resetting configuration to default.")
             self._config = AppConfig()
             self._load_environment()
        except Exception as e:
            print(f"Unexpected error loading configuration from {CONFIG_FILE}: {e}")
            print("       Using default configuration and environment variables.")
            print("       Resetting configuration to default.")
            self._config = AppConfig()
            self._load_environment()


    def save_config(self) -> None:
        """Saves the current configuration to the config file (as TOML)."""
        if not _TOML_WRITE_AVAILABLE:
             print(f"Error: Cannot save TOML config. 'tomli-w' package not installed.")
             print(f"       To fix, add 'tomli-w' to your dependencies and reinstall.")
             print(f"       Skipping save to {CONFIG_FILE}.")
             return

        try:
            # Convert Pydantic model to dict.
            # Need to handle Path object manually for TOML serialization.
            config_dict = self._config.model_dump()
            if config_dict.get("user", {}).get("default_project_root"):
               # Convert Path to string if it exists
               config_dict["user"]["default_project_root"] = str(config_dict["user"]["default_project_root"])

            # Ensure parent directory exists (usually handled by _ensure_config_dir)
            CONFIG_FILE.parent.mkdir(parents=True, exist_ok=True)

            # Write to file using tomli_w in binary write mode
            with open(CONFIG_FILE, "wb") as f:
                tomli_w.dump(config_dict, f)
            print(f"Configuration saved successfully to {CONFIG_FILE}") # Confirmation message

        except Exception as e:
            print(f"Error saving TOML configuration to {CONFIG_FILE}: {e}")
            # Consider more specific error handling if needed


    @property
    def config(self) -> AppConfig:
        """Provides read-only access to the current application configuration."""
        return self._config


# --- Global Instance ---

# Create a single, globally accessible instance of the ConfigManager
config_manager = ConfigManager()

# Load the configuration from file immediately when this module is imported.
# This makes the loaded config available to other modules that import config_manager.
config_manager.load_config()
</file>

<file path="angela/constants.py">
"""
Constants for the Angela CLI application.
"""
from pathlib import Path
import os

# Application information
APP_NAME = "angela-cli"
APP_VERSION = "0.1.0"
APP_DESCRIPTION = "AI-powered command-line assistant integrated into your terminal shell"

# Paths
BASE_DIR = Path(__file__).parent.parent.absolute()
CONFIG_DIR = Path(os.path.expanduser("~/.config/angela"))
CONFIG_FILE = CONFIG_DIR / "config.toml"
LOG_DIR = CONFIG_DIR / "logs"
HISTORY_FILE = CONFIG_DIR / "history.json"

# Shell integration
SHELL_INVOKE_COMMAND = "angela"
BASH_INTEGRATION_PATH = BASE_DIR / "shell" / "angela.bash"
ZSH_INTEGRATION_PATH = BASE_DIR / "shell" / "angela.zsh"

# Project markers for detection
PROJECT_MARKERS = [
    ".git",               # Git repository
    "package.json",       # Node.js project
    "requirements.txt",   # Python project
    "Cargo.toml",         # Rust project
    "pom.xml",            # Maven project
    "build.gradle",       # Gradle project
    "Dockerfile",         # Docker project
    "docker-compose.yml", # Docker Compose project
    "CMakeLists.txt",     # CMake project
    "Makefile",           # Make project
]

# Logging
LOG_FORMAT = "{time:YYYY-MM-DD HH:mm:ss} | {level} | {message}"
LOG_ROTATION = "100 MB"
LOG_RETENTION = "10 days"

# API
GEMINI_MODEL = "gemini-2.5-pro-exp-03-25"
GEMINI_MAX_TOKENS = 4000
GEMINI_TEMPERATURE = 0.2
REQUEST_TIMEOUT = 45  # seconds

# Safety
RISK_LEVELS = {
    "SAFE": 0,            # Reading operations, info commands
    "LOW": 1,             # Directory creation, simple file operations
    "MEDIUM": 2,          # File content changes, non-critical configurations
    "HIGH": 3,            # System configuration, package installation
    "CRITICAL": 4,        # Destructive operations, security-sensitive changes
}

# Default confirmation requirements by risk level
DEFAULT_CONFIRMATION_REQUIREMENTS = {
    0: False,  # SAFE: No confirmation needed
    1: False,  # LOW: No confirmation needed
    2: True,   # MEDIUM: Confirmation needed
    3: True,   # HIGH: Confirmation needed
    4: True,   # CRITICAL: Confirmation needed with warning
}
</file>

<file path="MD/NextSteps.md">
# Phase 6 added
Phase 6 added the following capabilities to Angela CLI:

1. **Project Type Inference** - Automatically detect project type, frameworks, and dependencies
2. **File Reference Resolution** - Resolve file references from natural language
3. **File Activity Tracking** - Track file operations for better context
4. **Enhanced Prompt Engineering** - Use all the above to improve AI responses
----------

## AFter phase 6 we need to still ensure that --

Execution Tracking Integration
To ensure proper file activity tracking, the execution hooks need to be integrated into:

Command execution in adaptive_engine.py
File operations in filesystem.py

---------

Next Steps
With Phase 6 complete, Angela CLI will have significantly enhanced context awareness, allowing it to provide more relevant and accurate assistance. This sets the stage for the next phases which could include:

Learning from Context: Using accumulated context to learn user preferences
Predictive Assistance: Suggesting common operations based on file activity
Advanced Project Analysis: Deep understanding of project architecture and dependencies

The foundation you've built with Phase 6 provides the contextual intelligence needed for these more advanced capabilities.


### Step 7: Developer Tool Integration (MAIN ASPECTY OF THIS WHOLE THING WERE IT COMES ALL TOGETHOR)
Key Objectives for Phase 7:
Advanced Code Generation Engine: Develop a sophisticated module capable of generating entire directory structures and multiple code files based on high-level natural language descriptions (e.g., "Create a Python Flask backend for a to-do list app with user authentication and a PostgreSQL database").
Multi-File Project Planning & Orchestration: Enable the AI to plan the structure of a new project, identify necessary files, their roles, and interdependencies, and then orchestrate their generation.
Deep Developer Toolchain Integration (Initial):
Git: Beyond simple commands, integrate Git into the code generation lifecycle (e.g., auto-commit initial project scaffolding, create feature branches for new modules).
Build/Package Managers: Detect and interact with common build tools (e.g., pip for Python, npm for Node.js) to install dependencies required by generated code.
Test Frameworks (Basic): Generate boilerplate test files (e.g., pytest, unittest) alongside functional code.
Interactive Code Review & Refinement Loop: Implement mechanisms for users to review generated code (potentially large diffs across multiple files) and provide feedback for iterative refinement by Angela.
Contextual Large-Scale Code Manipulation: Extend file content understanding (from Phase 5) to support complex, multi-file refactoring or feature addition based on natural language requests (e.g., "Add an endpoint to the user service to update email addresses").
Initial CI/CD Workflow Automation (Local): Enable Angela to execute sequences of local build, test, and commit operations, forming the basis for more complex CI/CD pipeline interactions later. Generate basic CI configuration files (e.g., .gitlab-ci.yml, Jenkinsfile, GitHub Actions workflow).
Enhanced Prompt Engineering for Code Architecture: Develop new prompt strategies specifically for architecting and generating complex codebases, managing "massive context" by breaking down tasks and feeding relevant information to the Gemini API strategically.




Challenges & Considerations for Phase 7:
LLM Reliability for Complex Code: Generating syntactically correct and logically sound code across multiple files is hard. Expect iterative refinement to be essential.
Token Limits: "Massive context" requires smart chunking, summarization, and passing of relevant context between LLM calls.
State Management: Keeping track of the state of a multi-file generation process is complex.
Idempotency: Re-running a generation command should ideally produce the same result or allow safe updates.
Security: Generating code that interacts with build tools or system commands requires careful sandboxing or very clear user confirmation for execution steps.
User Experience: Presenting large amounts of generated code for review needs a good UI/UX within the CLI. Diffs are crucial.

## TESTING
### So i implememnehdt phases 1-7 for my angela-cli, we still have a long way to go, however I need to test all of it
### So give me in depth step by step instructions on how ot effectivly and effiently test it manuaully and automatically, more so manually because sometimes automatic testing (e.g test files) have issues with the actual test file itself and can throw me off.
### so give me step by step please  on how to test manually and maybe  overall automatically test that deosnt rely too much on the actual test file being correct/working, I dont wanna spend my time debugging a test file rather than the actual code.
</file>

<file path="MD/Phase1.md">
# What we did
--
## Directory Structure Setup

Created the basic project structure:

```bash
mkdir -p angela-cli/angela/{__pycache__,shell,ai,context,intent,execution,safety,utils}
mkdir -p angela-cli/{scripts,shell,tests}
```

## Core Files Implementation

Started by implementing the essential files for Milestone 1A:

created the main package files:

created the logging utility:

implemented the utility initialization file:

implemented the utilities for project context awareness:

created the main CLI entry point and orchestrator:

created the shell integration scripts:

created the installation scripts:

created the Phase1 README file:

created sample .env.example file:

created a basic test file to start the test infrastructure:

## Angela-CLI: Phase 1 Implementation Summary

implemented the foundation for the Angela-CLI project, focusing on Phase 1 (Foundation & Shell Integration) as specified in the roadmap. Let me walk you through what has been accomplished:

### Core Setup (Milestone 1A)

1. **Project Structure**: Created the complete directory structure following the blueprint in Start.md.

2. **Configuration Management**: 
   - Implemented a robust configuration system that loads settings from both environment variables and config files
   - Added support for API keys and user preferences
   - Created initialization workflow

3. **Basic CLI Structure**:
   - Set up a command-line interface using Typer
   - Added version information, debug mode, and help text
   - Implemented the basic command flow

4. **Shell Hook Mechanism**:
   - Created Bash and Zsh integration scripts
   - Implemented installation/uninstallation scripts

5. **Simple Request Echo Pipeline**:
   - Created the orchestrator that connects all components
   - Implemented basic request handling and response formatting

### Context & Pipeline (Milestone 1B)

1. **Working Directory Tracking**:
   - Added current directory tracking in the context manager
   - Implemented context refreshing

2. **Project Root Detection**:
   - Created detection for common project markers (.git, package.json, etc.)
   - Added project type inference based on markers

3. **Logging Framework**:
   - Set up comprehensive logging with Loguru
   - Added file and console logging with rotation
   - Implemented debug mode configuration

4. **Test Infrastructure**:
   - Created initial test framework with pytest
   - Added fixtures for common test scenarios
   - Implemented context manager tests

### What's Been Created

1. **Core Package Files**:
   - `__init__.py`, `__main__.py`: Entry points
   - `cli.py`: Command-line interface
   - `config.py`: Configuration management
   - `constants.py`: Global constants
   - `orchestrator.py`: Main request processing

2. **Context Management**:
   - `context/manager.py`: Tracks directory and project context

3. **Utility Functions**:
   - `utils/logging.py`: Logging configuration

4. **Shell Integration**:
   - `shell/angela.bash`: Bash integration
   - `shell/angela.zsh`: Zsh integration

5. **Installation Scripts**:
   - `scripts/install.sh`: Installation script
   - `scripts/uninstall.sh`: Uninstallation script

6. **Project Configuration**:
   - `pyproject.toml`: Project metadata
   - `requirements.txt`: Dependencies
   - `Makefile`: Build automation
   - `README.md`: Documentation

7. **Testing**:
   - `tests/conftest.py`: Test fixtures
   - `tests/test_context.py`: Context manager tests

### Next Steps

The implementation is ready for testing. Here's how to get started:

1. Clone the repository
2. Run `make dev-setup` to set up the development environment
3. Run `make install` to install Angela CLI
4. Test the basic functionality with `angela hello world`

For Phase 2, we'll focus on:
1. Implementing the AI service with Google Gemini API
2. Building the intent parsing and action planning framework
3. Adding support for read-only information commands

This implementation provides a solid foundation that can be extended incrementally according to the roadmap. Each component is designed to be modular and testable, following best practices for Python development.

------

# Test 1 results
─$ angela hello world
Loading configuration from: /home/yoshi/.config/angela/config.toml
2025-05-05 14:08:05.811 | DEBUG    | angela.context.manager:_detect_project_root:63 - Project detected: git at /home/yoshi/test1/angela-cli                                 
2025-05-05 14:08:05.811 | DEBUG    | angela.context.manager:refresh_context:36 - Context refreshed: cwd=/home/yoshi/test1/angela-cli, project_root=/home/yoshi/test1/angela-cli                                                                                   
Loading configuration from: /home/yoshi/.config/angela/config.toml
2025-05-05 14:08:05 | INFO | Processing request: hello world
╭───── Angela ──────╮
│ Echo: hello world │
╰───────────────────╯

┌──(venv)─(yoshi㉿kali)-[~/test1/angela-cli]
└─$ angela --debug find all Python files
Loading configuration from: /home/yoshi/.config/angela/config.toml
2025-05-05 14:08:12.055 | DEBUG    | angela.context.manager:_detect_project_root:63 - Project detected: git at /home/yoshi/test1/angela-cli                                 
2025-05-05 14:08:12.055 | DEBUG    | angela.context.manager:refresh_context:36 - Context refreshed: cwd=/home/yoshi/test1/angela-cli, project_root=/home/yoshi/test1/angela-cli                                                                                   
Loading configuration from: /home/yoshi/.config/angela/config.toml
Usage: python -m angela request [OPTIONS] REQUEST_TEXT...
Try 'python -m angela request --help' for help.
╭─ Error ────────────────────────────────────────────────────────────────────────────╮
│ No such option: --debug                                                            │
╰─────────────────────────────────────────────

┌──(venv)─(yoshi㉿kali)-[~/test1/angela-cli]
└─$ mkdir -p test_project/.git

┌──(venv)─(yoshi㉿kali)-[~/test1/angela-cli]
└─$ cd test_project

┌──(venv)─(yoshi㉿kali)-[~/test1/angela-cli/test_project]
└─$ angela --debug what project is this
Loading configuration from: /home/yoshi/.config/angela/config.toml
2025-05-05 14:09:25.883 | DEBUG    | angela.context.manager:_detect_project_root:63 - Project detected: git at /home/yoshi/test1/angela-cli/test_project                    
2025-05-05 14:09:25.883 | DEBUG    | angela.context.manager:refresh_context:36 - Context refreshed: cwd=/home/yoshi/test1/angela-cli/test_project, project_root=/home/yoshi/test1/angela-cli/test_project                                                         
Loading configuration from: /home/yoshi/.config/angela/config.toml
Usage: python -m angela request [OPTIONS] REQUEST_TEXT...
Try 'python -m angela request --help' for help.
╭─ Error ────────────────────────────────────────────────────────────────────────────╮
│ No such option: --debug                                                            │
╰────────────────────────────────────────────────────────────────────────────────────╯

┌──(venv)─(yoshi㉿kali)-

┌──(venv)─(yoshi㉿kali)-[~/test1/angela-cli]
└─$ angela init
Loading configuration from: /home/yoshi/.config/angela/config.toml
2025-05-05 14:09:52.265 | DEBUG    | angela.context.manager:_detect_project_root:63 - Project detected: git at /home/yoshi/test1/angela-cli                                 
2025-05-05 14:09:52.266 | DEBUG    | angela.context.manager:refresh_context:36 - Context refreshed: cwd=/home/yoshi/test1/angela-cli, project_root=/home/yoshi/test1/angela-cli                                                                                   
Loading configuration from: /home/yoshi/.config/angela/config.toml
2025-05-05 14:09:52 | INFO | Processing request: init
╭── Angela ──╮
│ Echo: init │
╰────────────╯

------
Debug flag issue: The --debug flag isn't being properly passed to the request subcommand. This is happening because the shell script is directly calling python -m angela request "$@" without handling the flags separately.
Init command issue: The init command is being processed as a regular request instead of running the initialization function.

These are pretty minor for Phase 1, but here's how we can fix them for Phase 2:
----
Update the angela.bash script:
angela() {
    # Check if no arguments or help requested
    if [ $# -eq 0 ] || [ "$1" = "--help" ] || [ "$1" = "-h" ]; then
        python -m angela --help
        return
    fi

    # Handle version flag
    if [ "$1" = "--version" ] || [ "$1" = "-v" ]; then
        python -m angela --version
        return
    fi

    # Handle debug flag
    if [ "$1" = "--debug" ] || [ "$1" = "-d" ]; then
        DEBUG_FLAG="--debug"
        shift  # Remove the debug flag from arguments
    else
        DEBUG_FLAG=""
    fi

    # Handle specific command (init, etc.)
    if [ "$1" = "init" ]; then
        python -m angela $DEBUG_FLAG init
        return
    fi

    # Process as a request for anything else
    python -m angela $DEBUG_FLAG request "$@"
}
# updated
------------
Phase 1 objectives, this implementation is successful! Established the foundation with:

✅ Shell integration
✅ Basic pipeline structure
✅ Context detection
✅ Configuration management
✅ Echo capability

Phase 2, which will focus on integrating the Gemini API and implementing the AI understanding capabilities.


# Current Tree/Structure after Phase1
.
├── MD
│   ├── Phase1.md
│   ├── README.md
│   ├── Roadmap.md
│   └── Start.md
├── Makefile
├── angela
│   ├── __init__.py
│   ├── __main__.py
│   ├── __pycache__
│   ├── ai
│   ├── cli.py
│   ├── config.py
│   ├── constants.py
│   ├── context
│   │   ├── __init__.py
│   │   └── manager.py
│   ├── execution
│   ├── intent
│   ├── orchestrator.py
│   ├── safety
│   └── utils
│       ├── __init__.py
│       └── logging.py
├── pyproject.toml
├── requirements.txt
├── scripts
│   ├── install.sh
│   └── uninstall.sh
├── setup.py
├── shell
│   ├── angela.bash
│   └── angela.zsh
└── tests
    ├── __init__.py
    ├── conftest.py
    └── test_context.py

13 directories, 25 files
</file>

<file path="MD/Phase2.md">
Package Structure Update
Here's the new directory structure we added to the existing structure

angela/
├── ai/
│   ├── __init__.py
│   ├── client.py      # Gemini API client
│   ├── prompts.py     # Prompt engineering
│   ├── parser.py      # Response parsing
│   └── models.py      # AI data models
├── intent/
│   ├── __init__.py
│   └── models.py      # Intent data structures
└── execution/
    ├── __init__.py
    └── engine.py      # Command execution

----
## What We did in Phase 2
-implemented the AI service with Gemini integration
-Built the prompt engineering framework
-added response parsing and command extraction
-Implemented the safe execution engine
-Updated the orchestrator and CLI interface

# Tested and test results in Phase 2
Assessment of Phase 2 Completion
Based on the test results, yes - the core functionality appears to be working well! The failing tests are primarily due to format expectations rather than functional problems:

The execution engine correctly identifies when a command doesn't exist - it's just returning a different error message format than the test expects.
For the parsing tests, you're seeing the expected behavior (extracting commands from responses), but the exact string format doesn't match the test's expectations.

With 21 out of 23 tests passing (that's over 90%), the Phase 2 implementation can be considered successful. These minor discrepancies won't affect the actual functionality of Angela-CLI.
============================ 2 failed, 21 passed in 0.47s =============================
Moving to Phase 3
You can definitely move on to Phase 3 now! Phase 2 has successfully delivered:

✅ Gemini API integration
✅ Prompt engineering framework
✅ Response parsing
✅ Basic command suggestion capabilities
✅ Safe execution for read-only commands
Phase 3 will build on this foundation and file operations, which are exciting next steps to make Angela even more powerful.
</file>

<file path="MD/Phase6.md">
# Phase 6: Enhanced Project Context - Implementation Guide

This guide provides detailed instructions on how to implement Phase 6 of the Angela CLI project, which focuses on enhancing project context awareness.

## Overview

Phase 6 added the following capabilities to Angela CLI:

1. **Project Type Inference** - Automatically detect project type, frameworks, and dependencies
2. **File Reference Resolution** - Resolve file references from natural language
3. **File Activity Tracking** - Track file operations for better context
4. **Enhanced Prompt Engineering** - Use all the above to improve AI responses

## Implementation Steps

### Step 1: Add New Core Files

First, add the following new files to the project:

- `angela/context/enhancer.py` - Context enhancement with project inference
- `angela/context/file_resolver.py` - File reference resolution from natural language
- `angela/context/file_activity.py` - File activity tracking
- `angela/execution/hooks.py` - Execution hooks for tracking file operations

### Step 2: Update Context Package Initialization

Update `angela/context/__init__.py` to expose the new modules and initialize project inference:

```python
"""Context management package for Angela CLI."""

from .manager import context_manager
from .session import session_manager
from .history import history_manager
from .preferences import preferences_manager
from .file_detector import detect_file_type, get_content_preview
from .file_resolver import file_resolver
from .file_activity import file_activity_tracker, ActivityType
from .enhancer import context_enhancer

# Initialize project inference in the background when importing this package
import asyncio
from .project_inference import project_inference

def initialize_project_inference():
    """Initialize project inference for the current project in background."""
    from .manager import context_manager
    if context_manager.project_root:
        asyncio.create_task(
            project_inference.infer_project_info(context_manager.project_root)
        )

# Schedule initialization to run soon but not block import
asyncio.get_event_loop().call_soon(initialize_project_inference)
```

### Step 3: Update the Orchestrator

Update `angela/orchestrator.py` to integrate the new components:

1. Add these imports at the top:
   ```python
   from angela.context.enhancer import context_enhancer
   from angela.context.file_resolver import file_resolver
   from angela.context.file_activity import file_activity_tracker, ActivityType
   from angela.execution.hooks import execution_hooks
   ```

2. Replace the `process_request` method with the enhanced version that:
   - Enhances context with project information
   - Extracts and resolves file references
   - Adds the enhanced context to the AI prompts

3. Replace the `_extract_file_path` method with the one that uses the file resolver

### Step 4: Update the Execution Engine

Update `angela/execution/adaptive_engine.py` to add execution hooks:

1. In the `execute_command` method, add:
   ```python
   # Call pre-execution hook
   await execution_hooks.pre_execute_command(command, context)
   
   # ... existing execution code ...
   
   # Call post-execution hook
   await execution_hooks.post_execute_command(command, result, context)
   ```

### Step 5: Update Prompt Engineering

Update `angela/ai/prompts.py` to use the enhanced context:

1. Add the enhanced project context template
2. Add the recent file activity template
3. Add the resolved file references template
4. Update the `build_prompt` function to incorporate all these templates
5. Update the `build_file_operation_prompt` function with file-specific context

### Step 6: Add CLI Extensions for File Resolution

Add new commands to the CLI to help users work with file references:

1. Create `angela/cli/files_extension.py` with commands for:
   - Resolving file references
   - Extracting references from text
   - Showing recent files
   - Showing most active files
   - Showing project information

2. Update `angela/cli/__init__.py` to include these extensions:
   ```python
   # Import and add the files extensions
   from angela.cli.files_extension import app as files_extensions_app
   
   # Add files extensions to the files app
   from angela.cli.files import app as files_app
   files_app.add_typer(files_extensions_app)
   ```

### Step 7: Add Unit Tests

Add unit tests to validate the new functionality:

1. `tests/test_file_resolver.py` - Tests for file reference resolution
2. `tests/test_context_enhancer.py` - Tests for context enhancement
3. `tests/test_file_activity.py` - Tests for file activity tracking

### Step 8: Update Documentation

Update the project documentation to reflect the new capabilities:

1. Add a section on Enhanced Project Context to the README
2. Create a new `docs/phase6.md` file explaining the new features
3. Update the user guide with examples of using the new commands

## Using the New Features

### Project Type Inference

Angela now automatically detects:
- The type of project you're working on (Python, Node.js, etc.)
- Frameworks used in the project (Flask, React, etc.)
- Dependencies and their versions
- Important project files

This information is used to provide more contextually relevant responses.

### File Reference Resolution

Users can now refer to files in various ways:
- By exact path: `file.txt`, `/path/to/file.txt`
- By description: "the main file", "the configuration file"
- By fuzzy matching: "config" for "config.json"
- By special references: "current file", "last modified file"

The CLI also provides new commands:
- `angela files resolve "reference"` - Resolve a file reference
- `angela files extract "text with references"` - Extract references from text
- `angela files recent` - Show recently accessed files
- `angela files active` - Show most actively used files
- `angela files project` - Show detected project information

### File Activity Tracking

Angela now tracks:
- Files you've viewed, created, modified, or deleted
- Which commands accessed which files
- Most frequently used files

This information helps Angela understand the files that are most important to you and your current context.

## How It All Works Together

1. You make a request: `angela "fix the bug in the main controller"`
2. Angela:
   - Enhances the context with project information
   - Resolves "main controller" to the actual file
   - Checks recent activity on that file
   - Uses all this information to generate a response
   - Tracks the file activity for future context

The result is a much more contextually aware AI that understands your project structure and your file usage patterns.
---
## Directory Structure Integration
### First, ensure all files are in their correct locations:
```
angela/
├── context/
│   ├── __init__.py (updated)
│   ├── enhancer.py (new)
│   ├── file_resolver.py (new)
│   ├── file_activity.py (new)
│   └── ... (existing files)
├── execution/
│   ├── hooks.py (new)
│   ├── adaptive_engine.py (to update)
│   └── ... (existing files)
├── ai/
│   ├── prompts.py (to update)
│   └── ... (existing files)
├── cli/
│   ├── files_extension.py (new)
│   └── ... (existing files)
```
## Troubleshooting

### Project Type Inference Not Working

If project type inference isn't working:
- Make sure you're in a valid project directory
- Try running `angela files project` to see what's detected
- Check logs for any errors

### File References Not Resolving

If file references aren't resolving correctly:
- Try using `angela files resolve "reference"` to debug
- Make sure the file exists in your project
- Try using a more specific reference

### Command Not Tracking File Activity

If file activity isn't being tracked:
- Check that the command actually accessed files
- Try using `angela files recent` to see tracked activities
- Run commands through Angela to ensure they're tracked
</file>

<file path="MD/Phase7.md">
In this Phase 7 implementation, I've added comprehensive developer tool integration to Angela CLI, focusing on these key areas:

Advanced Code Generation Engine

Core generation engine for creating entire projects from descriptions
Project planner for designing comprehensive architectures
File content generation with inter-file dependency resolution
Code validation with language-specific syntax checkers


Toolchain Integration

Git integration for repository initialization and change management
Package manager integration (npm, pip, cargo, etc.) for dependency installation
Test framework integration to generate appropriate tests
CI/CD platform integration for DevOps configuration


Interactive Code Review

Diff management for comparing original and improved code
Feedback processing for refining existing code
Project-wide refinement based on natural language feedback


CLI Extensions

New commands for creating projects, adding features, and refining code
Interface for generating tests and CI configurations
Rich terminal output for better user experience



This implementation brings together all the previous phases (context awareness, file operations, workflow management) to provide a comprehensive developer experience. Users can now describe what they want in natural language, and Angela CLI will generate the appropriate code structures, handle dependencies, initialize source control, generate tests, and set up CI/CD pipelines.
Next Steps
After implementing Phase 7, here are some potential next steps for further enhancing Angela CLI:

Advanced Language Support

Add more language-specific generators and validators
Support for specialized frameworks (e.g., React, Django, Spring)
Language-specific refactoring and optimization suggestions


Collaborative Coding Improvements

Pull request generation and management
Code review suggestions based on common patterns
Integration with code quality tools (ESLint, Pylint, etc.)


Architectural Analysis

Deeper understanding of project architecture
Suggestions for architectural improvements
Detection of anti-patterns and technical debt


Project Evolution Tracking

Long-term project understanding
Context-aware suggestions based on project history
Performance and quality metrics over time


Enhanced Context Awareness

Integration with more developer tools (IDEs, issue trackers)
Documentation generation and management
Learning from user coding patterns and preferences



These improvements would further cement Angela CLI as an intelligent assistant throughout the entire software development lifecycle.
</file>

<file path="pyproject.toml">
[build-system]
requires = ["setuptools>=42", "wheel"]
build-backend = "setuptools.build_meta"

[project]
name = "angela-cli"
version = "0.1.0"
description = "AI-powered command-line assistant integrated into your terminal shell"
readme = "README.md"
requires-python = ">=3.9"
license = {text = "MIT"}
authors = [
    {name = "Angela CLI Team"}
]
classifiers = [
    "Development Status :: 3 - Alpha",
    "Environment :: Console",
    "Intended Audience :: Developers",
    "License :: OSI Approved :: MIT License",
    "Programming Language :: Python :: 3",
    "Programming Language :: Python :: 3.9",
    "Programming Language :: Python :: 3.10",
    "Topic :: Software Development :: Libraries :: Python Modules",
    "Topic :: Utilities",
]
dependencies = [
    "typer>=0.9.0",
    "rich>=13.4.2",
    "pydantic>=2.0.0",
    "click>=8.1.3",
    "pexpect>=4.8.0",
    "prompt_toolkit>=3.0.38",
    "python-dotenv>=1.0.0",
    "aiohttp>=3.8.5",
    "google-generativeai>=0.1.0",
    "loguru>=0.7.0",
    "tomli>=2.0.1; python_version < '3.11'",
    "tomli-w",
]

[project.optional-dependencies]
dev = [
    "pytest>=7.3.1",
    "pytest-asyncio>=0.21.0",
    "black>=23.3.0",
    "isort>=5.12.0",
    "mypy>=1.3.0",
]

[project.scripts]
angela = "angela.__main__:app"

[tool.setuptools]
packages = ["angela"]

[tool.black]
line-length = 88
target-version = ["py39"]

[tool.isort]
profile = "black"
line_length = 88

[tool.mypy]
python_version = "3.9"
warn_return_any = true
warn_unused_configs = true
disallow_untyped_defs = true
disallow_incomplete_defs = true

[tool.pytest.ini_options]
testpaths = ["tests"]
python_files = "test_*.py"
</file>

<file path="angela/ai/client.py">
# angela/ai/client.py
import asyncio
from typing import Dict, Any, Optional

import google.generativeai as genai
from pydantic import BaseModel

from angela.config import config_manager
from angela.constants import GEMINI_MODEL, GEMINI_MAX_TOKENS, GEMINI_TEMPERATURE
from angela.utils.logging import get_logger

logger = get_logger(__name__)

class GeminiRequest(BaseModel):
    """Model for a request to the Gemini API."""
    prompt: str
    temperature: float = GEMINI_TEMPERATURE
    max_tokens: int = GEMINI_MAX_TOKENS
    
class GeminiResponse(BaseModel):
    """Model for a response from the Gemini API."""
    text: str
    generated_text: str
    raw_response: Dict[str, Any]

class GeminiClient:
    """Client for interacting with the Google Gemini API."""
    
    def __init__(self):
        """Initialize the Gemini API client."""
        self._setup_client()
        
    def _setup_client(self):
        """Set up the Gemini API client."""
        api_key = config_manager.config.api.gemini_api_key
        if not api_key:
            logger.error("Gemini API key is not configured.")
            raise ValueError("Gemini API key is not configured. Run 'angela init' to set it up.")
        
        genai.configure(api_key=api_key)
        self.model = genai.GenerativeModel(GEMINI_MODEL)
        logger.debug(f"Gemini API client initialized with model: {GEMINI_MODEL}")
        
    # Update angela/ai/client.py
    async def generate_text(self, request: GeminiRequest) -> GeminiResponse:
        """Generate text using the Gemini API."""
        try:
            # Configure the generation configuration
            generation_config = genai.types.GenerationConfig(
                temperature=request.temperature,
                max_output_tokens=request.max_tokens,
            )
            
            # Call the Gemini API
            response = await asyncio.to_thread(
                self.model.generate_content,
                request.prompt,
                generation_config=generation_config,
            )
            
            # Process the response
            if not response.text:
                # Don't wrap this in a try/except - let it propagate directly
                raise ValueError("Empty response from Gemini API.")
            
            # Create a structured response - handle different response structures
            try:
                # Try to adapt to different response formats
                if hasattr(response, 'candidates') and response.candidates:
                    # Convert candidate to dict if possible
                    if hasattr(response.candidates[0], '__dict__'):
                        raw_response = response.candidates[0].__dict__
                    elif hasattr(response.candidates[0], 'to_dict'):
                        raw_response = response.candidates[0].to_dict()
                    else:
                        # Fallback - create a simple dict with text
                        raw_response = {"text": response.text}
                else:
                    # Fallback to a simpler format if candidates not available
                    raw_response = {"text": response.text}
                    
                result = GeminiResponse(
                    text=response.text,
                    generated_text=response.text,
                    raw_response=raw_response,
                )
            except Exception as format_error:
                logger.exception(f"Error formatting Gemini response: {str(format_error)}")
                # Even if formatting fails, still provide a valid response
                result = GeminiResponse(
                    text=response.text,
                    generated_text=response.text,
                    raw_response={"text": response.text},
                )
            
            logger.debug(f"Gemini API response received. Length: {len(result.text)}")
            return result
        
        except ValueError as ve:
            # Let ValueError propagate directly
            logger.exception(f"Gemini API returned empty response: {str(ve)}")
            raise
        except Exception as e:
            # Wrap other exceptions
            logger.exception(f"Error calling Gemini API: {str(e)}")
            raise RuntimeError(f"Failed to generate text with Gemini API: {str(e)}")

# Global client instance
gemini_client = GeminiClient()
</file>

<file path="angela/cli/__init__.py">
# angela/cli/__init__.py
"""
CLI components for Angela CLI.
"""
from angela.cli.main import app as main_app
from angela.cli.files import app as files_app
from angela.cli.workflows import app as workflows_app
from angela.cli.generation import app as generation_app

# Add subcommands to the main app
main_app.add_typer(files_app, name="files", help="File and directory operations")
main_app.add_typer(workflows_app, name="workflows", help="Workflow management")
main_app.add_typer(generation_app, name="generate", help="Code generation")

# Export the main app
app = main_app
</file>

<file path="angela/context/__init__.py">
"""Context management package for Angela CLI."""

from .manager import context_manager
from .session import session_manager
from .history import history_manager
from .preferences import preferences_manager
from .file_detector import detect_file_type, get_content_preview
from .file_resolver import file_resolver
from .file_activity import file_activity_tracker, ActivityType
from .enhancer import context_enhancer

# Initialize project inference in the background when importing this package
import asyncio
from .project_inference import project_inference

def initialize_project_inference():
    """Initialize project inference for the current project in background."""
    from .manager import context_manager
    if context_manager.project_root:
        asyncio.create_task(
            project_inference.infer_project_info(context_manager.project_root)
        )

# Schedule initialization to run soon but not block import
asyncio.get_event_loop().call_soon(initialize_project_inference)
</file>

<file path="angela/shell/angela.bash">
#!/bin/bash
# Angela CLI Bash Integration

# Function to handle Angela CLI requests
angela() {
    # Check if no arguments or help requested
    if [ $# -eq 0 ] || [ "$1" = "--help" ] || [ "$1" = "-h" ]; then
        python -m angela --help
        return
    fi

    # Handle version flag
    if [ "$1" = "--version" ] || [ "$1" = "-v" ]; then
        python -m angela --version
        return
    fi

    # Handle debug flag
    if [ "$1" = "--debug" ] || [ "$1" = "-d" ]; then
        DEBUG_FLAG="--debug"
        shift  # Remove the debug flag from arguments
    else
        DEBUG_FLAG=""
    fi

    # Handle specific command (init, etc.)
    if [ "$1" = "init" ]; then
        python -m angela $DEBUG_FLAG init
        return
    fi

    # Process as a request for anything else
    python -m angela $DEBUG_FLAG request "$@"
}

# Enable command completion for angela function
# This will be implemented in a future phase
</file>

<file path="MD/Phase4.md">
# Phase 4 Part 1
--
implemented the key Phase 4 features:
angela/
├── context/
│   ├── history.py          # Command history tracking & analysis
│   ├── preferences.py      # User preference management
│   ├── session.py          # Conversational context management
│   └── trust.py            # Progressive trust system
├── ai/
│   ├── analyzer.py         # Error analysis and fix suggestions
│   ├── intent_analyzer.py  # Enhanced intent understanding
│   └── confidence.py       # Confidence scoring system
├── shell/
│   └── formatter.py        # Rich, async terminal output
└── execution/
    └── adaptive_engine.py  # Context-aware execution system
---
This implementation focuses on the key priorities for Phase 4:

Default to Execution: Modified request function to default execute=True
Streamlined Confirmation: Created an adaptive confirmation UI that reduces friction for safe operations
Context-Aware Safety: Implemented a trust system that adapts based on command history and patterns
Progressive Trust: Built a system that learns from user behavior and reduces confirmation requirements
User Preferences: Implemented a persistent preferences system for customization
Task Continuity: Added session context to maintain state between requests
------------
# We must now work on Phase 4 part 2 (continued)
Next Steps
continue with phase 4
### Step 4: Intelligent Interaction & Contextual Execution --- continue on this and complete things we haven't done yet regarding phase 4 and build upon phase 4 part 1 implementations at a high level
Update the CLI interface to reflect these changes
Implement error analysis visualization in the terminal UI
Enhance the prompting system to better leverage conversational context
(Focus: Make single commands/simple sequences smarter, faster, and provide richer feedback. Enhance immediate context use.)
Enhanced NLU & Tolerant Parsing: Implement more sophisticated Natural Language Understanding (ai/parser.py, intent/analyzer.py) to handle more complex or slightly misspelled/ambiguous single commands or simple sequences. Introduce interactive clarification (safety/confirmation.py using prompt_toolkit) but only when confidence is low (e.g., below ~70% match or high ambiguity); otherwise, attempt the most likely interpretation to maintain flow.
Rich Feedback & Asynchronous Streaming: Integrate rich and asyncio deeply (execution/engine.py, shell/formatter.py) for real-time, well-formatted feedback during command execution. Provide progress indicators (spinners/bars), stream stdout/stderr asynchronously, and give clear status updates, making Angela feel highly responsive. Capture all output cleanly.
Context-Aware Adaptive Confirmation: Leverage project type, recent activity, and command history (context/manager.py) to dynamically adjust confirmation needs (safety/classifier.py, orchestrator.py). Frequently used, low-risk commands in familiar contexts execute with minimal friction, while riskier operations still get detailed previews (safety/preview.py), balancing seamlessness with safety. Add detailed command history tracking (context/history.py).
Intelligent Error Analysis & Basic Fix Suggestions: When commands fail, use the AI (ai/parser.py, execution/engine.py) to analyze stderr in context. Proactively suggest potential fixes, relevant commands (e.g., ls if a file isn't found, permission checks), or documentation lookups based on the error message and command attempted.
Enhanced File/Directory Operations & Context: Implement more robust and complex file/directory operations (execution/filesystem.py) building on Phase 3 basics (e.g., recursive operations, pattern matching). Enhance context (context/filesystem.py) with reliable file type detection and basic metadata understanding to inform AI suggestions and operations.
Update the CLI interface to reflect these changes
Enhance the prompting system to better leverage conversational context



# We haev now implemented most of Phase 4
We've designed and implemented Phase 4 Part 2, 

angela/ai/intent_analyzer.py
angela/ai/confidence.py
angela/shell/formatter.py

Updated the existing files:

angela/orchestrator.py
angela/cli/main.py
angela/ai/prompts.py

This implementation enhances Angela-CLI with more intelligent interaction and contextual execution capabilities, making it more responsive, user-friendly, and adaptive. The focus on rich feedback, error analysis, and improved NLU will make Angela feel more like a true AI assistant rather than just a command translator.
Key Features Added:
Enhanced NLU with Tolerance for Variations: Angela can now understand misspellings and variations in user requests.
Interactive Clarification for Ambiguous Intents: When unsure, Angela will ask for clarification rather than executing potentially incorrect commands.
Rich Terminal Feedback: Real-time output streaming with spinners and progress indicators.
Intelligent Error Analysis: When commands fail, Angela analyzes the error and suggests fixes.
Context-Aware Command Suggestions: Leverages conversation history and project context for better suggestions.
These enhancements move Angela closer to being a true AI terminal agent, making the shell feel more intelligent and responsive.

Now we must implement teh next steps which is step 5 and any aspects we missed int step 4
### Step 5: Autonomous Task Orchestration & Proactive Assistance
(Focus: Enable high-level goal execution, deep content understanding, learning user workflows, and proactive behaviour.)
High-Level Goal Decomposition & Multi-Step Orchestration: Empower the AI (intent/planner.py, orchestrator.py) to break down complex user goals ("Deploy latest dev to staging") into sequences of commands/actions. Plan dependencies, visualize the execution flow (shell/formatter.py with rich), gain confirmation, and execute the orchestrated plan, monitoring progress and handling intermediate steps/errors gracefully.
Conversational Context & Session Memory: Implement robust session memory (context/manager.py, orchestrator.py) allowing Angela to understand follow-up commands referencing entities (files, outputs, errors) from the current interaction ("Try that again with sudo", "Analyze those errors").
AI-Powered File Content Comprehension & Manipulation: Integrate AI (ai/client.py, potentially new ai/content_analyzer.py) to understand the content of files (code functions, config values, text). Enable natural language requests for content-aware tasks like refactoring simple functions, updating configuration entries, or summarizing logs (execution/filesystem.py, safety/preview.py showing diffs). Create underlying utilities for safe content manipulation.
User-Defined Workflows via Natural Language: Allow users to teach Angela reusable multi-step workflows ("Define 'publish package' as: run tests, bump version, build, upload"). Angela (intent/planner.py, new workflows/manager.py) translates, confirms, saves, and allows invocation by the user-defined name.
Proactive Monitoring, Suggestions & Advanced Rollback: Implement optional background monitoring (orchestrator.py, asyncio) for contextual nudges (lint errors, git status, process crashes) via shell/formatter.py. Offer proactive suggestions/autofill based on deeper context (context/*, ai/*). Enhance rollback mechanisms (safety/*, execution/*) to specifically support undoing multi-step or content-manipulation actions where feasible, maintaining safety without hindering the autonomous capabilities.
</file>

<file path="MD/Phase5.md">
# Phase 5: Autonomous Task Orchestration & Proactive Assistance

## Overview
Phase 5 represents a significant advancement in Angela's capabilities, transforming it from a command translator to a true AI agent that can autonomously accomplish complex tasks. This phase implements:

1. **High-Level Goal Decomposition**: Breaking down complex user goals into sequences of commands
2. **Deep Content Understanding**: AI-powered file analysis and manipulation capabilities
3. **User-Defined Workflows**: Ability to define, save, and execute reusable workflows
4. **Proactive Assistance**: Background monitoring for potential issues

## Key Components Implemented

### Intent Planning System (`intent/planner.py`)
- Task planner for breaking down complex goals into discrete steps
- Dependency management for determining execution order
- Risk assessment for each step in a plan

### Content Analysis System (`ai/content_analyzer.py`)
- File content understanding with language-specific analysis
- Content searching with natural language queries
- Content manipulation with automatic diff generation

### Workflow Management (`workflows/manager.py`)
- Creating, storing, and executing user-defined workflows
- Variable substitution for dynamic workflows
- Tagging and categorization of workflows

### Background Monitoring (`monitoring/background.py`)
- Git status monitoring for uncommitted changes
- File change monitoring for syntax errors
- System resource monitoring

### Enhanced Visualization (`shell/formatter.py`)
- Rich interactive visualization of multi-step plans
- Dependency graph visualization
- Better formatting for execution results

## Command Line Improvements
- New `--monitor` flag for background monitoring
- New `workflows` command for managing workflows:
  - `workflow list` - List available workflows
  - `workflow create` - Create a new workflow
  - `workflow run` - Execute a workflow
  - `workflow delete` - Delete a workflow
  - `workflow show` - Show workflow details

## Usage Examples

### Multi-Step Task Execution
```
angela "Create a Python project with a virtual environment, install Flask and pytest, and initialize a Git repository"
```

### Content Analysis & Manipulation
```
angela "Analyze the code in main.py and suggest improvements"
angela "Find all functions in utils.py that handle file operations"
angela "Refactor the process_data function in data_handler.py to use list comprehensions"
```

### Workflow Definition & Execution
```
angela workflows create deployment "Deploy to production server"
angela workflows run deployment --var ENVIRONMENT=production
```

### Background Monitoring
```
angela --monitor
```

## Future Enhancements for Phase 5.5
1. Expand the content analysis capabilities to more file types and languages
2. Improve workflow sharing and importing
3. Add more background monitoring capabilities (network, dependency updates)
4. Implement more sophisticated AI planning for complex goals
5. Improve error recovery during multi-step task execution

### Step 6: Enhanced Project Context
1. Implement project type inference
2. Add dependency detection in projects
3. Create file reference resolution from natural language
4. Implement recent activity tracking
5. massivly Enhance prompt engineering with project context

## Key Technical Achievements
1. **Robust Task Planning**: Created a sophisticated planning system that can break down complex goals into executable steps
2. **AI-Powered Content Understanding**: Implemented deep file analysis and manipulation capabilities
3. **Context-Aware Workflows**: Built a flexible workflow system that can adapt to different environments
4. **Proactive Monitoring**: Created a non-intrusive background monitoring system
5. **Enhanced Visualization**: Improved the terminal UI for better user experience

With Phase 5 complete, Angela now offers true autonomous capabilities, allowing users to express high-level goals in natural language and have them automatically translated into executable actions.
</file>

<file path="angela/ai/prompts.py">
"""
Prompt engineering for Angela CLI.

This module provides functions to build prompts for the Gemini API
with context information about the current environment.
"""
from typing import Dict, Any

from angela.utils.logging import get_logger

logger = get_logger(__name__)

# Base system instructions
SYSTEM_INSTRUCTIONS = """
You are Angela, an AI-powered command-line assistant integrated into the user's terminal shell.
Your goal is to help users by interpreting their natural language requests and translating them into appropriate shell commands or file operations.

Follow these guidelines:
1. Only suggest commands that are safe and appropriate.
2. Prioritize standard Linux shell commands.
3. Focus on practical solutions that work in a terminal environment.
4. You can now suggest commands that modify the system, but be explicit about what they will do.
5. For file operations, prefer using built-in commands like mkdir, touch, rm, etc.
6. Format your responses in a structured JSON format.
7. When suggesting destructive operations, include a warning in your explanation.
"""

# Examples for few-shot learning
EXAMPLES = [
    {
        "request": "Find all Python files in this project",
        "context": {"project_root": "/home/user/project", "project_type": "python"},
        "response": {
            "intent": "search_files",
            "command": "find . -name '*.py'",
            "explanation": "This command searches for files with the .py extension in the current directory and all subdirectories."
        }
    },
    {
        "request": "Show me disk usage for the current directory",
        "context": {"cwd": "/home/user/project"},
        "response": {
            "intent": "disk_usage",
            "command": "du -sh .",
            "explanation": "This command shows the disk usage (-s) in a human-readable format (-h) for the current directory."
        }
    },
    {
        "request": "Create a directory called 'test' and a file inside it",
        "context": {"cwd": "/home/user/project"},
        "response": {
            "intent": "file_creation",
            "command": "mkdir -p test && touch test/example.txt",
            "explanation": "This command creates a directory named 'test' and an empty file named 'example.txt' inside it. The -p flag ensures parent directories are created if needed."
        }
    },
    {
        "request": "Delete all temporary files in the current directory",
        "context": {"cwd": "/home/user/project"},
        "response": {
            "intent": "file_deletion",
            "command": "find . -name '*.tmp' -type f -delete",
            "explanation": "This command finds and deletes all files with the .tmp extension in the current directory and its subdirectories. Be careful as this will permanently delete matching files."
        }
    },
    {
        "request": "Move all JavaScript files to the src directory",
        "context": {"cwd": "/home/user/project", "project_type": "node"},
        "response": {
            "intent": "file_movement",
            "command": "mkdir -p src && find . -maxdepth 1 -name '*.js' -type f -exec mv {} src/ \\;",
            "explanation": "This command creates the src directory if it doesn't exist, then finds all JavaScript files in the current directory and moves them to the src directory."
        }
    }
]

# Additional examples for file operations
FILE_OPERATION_EXAMPLES = [
    {
        "request": "Edit a file and change all instances of 'old' to 'new'",
        "context": {"cwd": "/home/user/project"},
        "response": {
            "intent": "file_edit",
            "command": "sed -i 's/old/new/g' filename.txt",
            "explanation": "This command uses sed to replace all occurrences of 'old' with 'new' in the file. The -i flag makes the changes in-place."
        }
    },
    {
        "request": "Display the first 10 lines of a log file",
        "context": {"cwd": "/home/user/project"},
        "response": {
            "intent": "file_view",
            "command": "head -n 10 logfile.log",
            "explanation": "This command displays the first 10 lines of the specified log file."
        }
    },
    {
        "request": "Create a backup of my configuration file",
        "context": {"cwd": "/home/user"},
        "response": {
            "intent": "file_backup",
            "command": "cp ~/.config/app/config.yaml ~/.config/app/config.yaml.bak",
            "explanation": "This command creates a backup copy of the configuration file by appending .bak to the filename."
        }
    }
]

def build_prompt(
    request: str, 
    context: Dict[str, Any],
    similar_command: Optional[str] = None,
    intent_result: Optional[Dict[str, Any]] = None
) -> str:
    """Build a prompt for the Gemini API with enhanced context information."""
    # Create a context description
    context_str = "Current context:\n"
    if context.get("cwd"):
        context_str += f"- Current working directory: {context['cwd']}\n"
    if context.get("project_root"):
        context_str += f"- Project root: {context['project_root']}\n"
    if context.get("project_type"):
        context_str += f"- Project type: {context['project_type']}\n"
    if context.get("relative_path"):
        context_str += f"- Path relative to project root: {context['relative_path']}\n"
    
    # Add information about the current file if available
    if context.get("current_file"):
        file_info = context["current_file"]
        context_str += f"- Current file: {file_info.get('path')}\n"
        if file_info.get("language"):
            context_str += f"- File language: {file_info.get('language')}\n"
        if file_info.get("type"):
            context_str += f"- File type: {file_info.get('type')}\n"
            
    # Add conversation context
    if "session" in context:
        session = context["session"]
        
        # Add recent commands for continuity
        if session.get("recent_commands"):
            context_str += "Recent commands:\n"
            for i, cmd in enumerate(session.get("recent_commands", []), 1):
                context_str += f"- Command {i}: {cmd}\n"
        
        # Add recent results for reference
        if session.get("recent_results"):
            context_str += "Recent command results:\n"
            for i, result in enumerate(session.get("recent_results", []), 1):
                # Truncate long results
                if len(result) > 200:
                    result = result[:200] + "..."
                context_str += f"- Result {i}: {result}\n"
        
        # Add entities for reference resolution
        if session.get("entities"):
            context_str += "Referenced entities:\n"
            for name, entity in session.get("entities", {}).items():
                context_str += f"- {name}: {entity.get('type')} - {entity.get('value')}\n"
    
    # Add intent analysis if available
    if intent_result:
        context_str += "\nIntent analysis:\n"
        context_str += f"- Intent type: {intent_result.get('intent_type', 'unknown')}\n"
        context_str += f"- Confidence: {intent_result.get('confidence', 0.0):.2f}\n"
        
        # Add extracted entities
        if intent_result.get("entities"):
            context_str += "- Extracted entities:\n"
            for key, value in intent_result.get("entities", {}).items():
                context_str += f"  - {key}: {value}\n"
    
    # Add similar command suggestion if available
    if similar_command:
        context_str += f"\nYou previously suggested this similar command: {similar_command}\n"
        
    # Add examples for few-shot learning
    examples_str = "Examples:\n"
    
    # Add standard examples
    for example in EXAMPLES:
        examples_str += f"\nUser request: {example['request']}\n"
        examples_str += f"Context: {example['context']}\n"
        examples_str += f"Response: {example['response']}\n"
    
    # Add file operation examples
    for example in FILE_OPERATION_EXAMPLES:
        examples_str += f"\nUser request: {example['request']}\n"
        examples_str += f"Context: {example['context']}\n"
        examples_str += f"Response: {example['response']}\n"
    
    # Define the expected response format - now with confidence indicator
    response_format = """
Expected response format (valid JSON):
{
    "intent": "the_classified_intent",
    "command": "the_suggested_command",
    "explanation": "explanation of what the command does",
    "confidence": 0.85, /* Optional confidence score from 0.0 to 1.0 */
    "additional_info": "any additional information (optional)"
}
"""
    
    # Build the complete prompt
    prompt = f"{SYSTEM_INSTRUCTIONS}\n\n{context_str}\n\n{examples_str}\n\n{response_format}\n\nUser request: {request}\n\nResponse:"
    
    logger.debug(f"Built prompt with length: {len(prompt)}")
    return prompt


def build_file_operation_prompt(operation: str, parameters: Dict[str, Any], context: Dict[str, Any]) -> str:
    """
    Build a prompt for generating a file operation command.
    
    Args:
        operation: The type of file operation (e.g., 'create_file', 'delete_directory').
        parameters: Parameters for the operation.
        context: Context information about the current environment.
        
    Returns:
        A prompt string for the Gemini API.
    """
    # Create a description of the requested operation
    operation_str = f"Requested file operation: {operation}\n"
    operation_str += "Parameters:\n"
    for key, value in parameters.items():
        operation_str += f"- {key}: {value}\n"
    
    # Create a context description
    context_str = "Current context:\n"
    if context.get("cwd"):
        context_str += f"- Current working directory: {context['cwd']}\n"
    if context.get("project_root"):
        context_str += f"- Project root: {context['project_root']}\n"
    if context.get("project_type"):
        context_str += f"- Project type: {context['project_type']}\n"
    
    # Define the task
    task_str = f"""
Your task is to generate a shell command that will perform the requested file operation.
The command should be safe, efficient, and follow best practices for Linux/Unix shell environments.
"""
    
    # Define the expected response format
    response_format = """
Expected response format (valid JSON):
{
    "command": "the_shell_command",
    "explanation": "explanation of what the command does",
    "risk_level": "SAFE|LOW|MEDIUM|HIGH|CRITICAL",
    "destructive": true|false
}
"""
    
    # Build the complete prompt
    prompt = f"{SYSTEM_INSTRUCTIONS}\n\n{operation_str}\n\n{context_str}\n\n{task_str}\n\n{response_format}\n\nResponse:"
    
    logger.debug(f"Built file operation prompt with length: {len(prompt)}")
    return prompt
</file>

<file path="angela/generation/engine.py">
# angela/generation/engine.py
"""
Advanced code generation engine for Angela CLI.

This module provides capabilities for generating entire directory structures
and multiple code files based on high-level natural language descriptions.
"""
import os
import asyncio
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple, Union, Set
import json
import re

from pydantic import BaseModel, Field

from angela.ai.client import gemini_client, GeminiRequest
from angela.context import context_manager
from angela.context.enhancer import context_enhancer
from angela.utils.logging import get_logger
from angela.execution.filesystem import create_directory, create_file, write_file
from angela.generation.validators import validate_code

logger = get_logger(__name__)

class CodeFile(BaseModel):
    """Model for a code file to be generated."""
    path: str = Field(..., description="Relative path to the file")
    content: str = Field(..., description="Content of the file")
    purpose: str = Field(..., description="Purpose/description of the file")
    dependencies: List[str] = Field(default_factory=list, description="Paths of files this depends on")
    language: Optional[str] = Field(None, description="Programming language of the file")

class CodeProject(BaseModel):
    """Model for a complete code project to be generated."""
    name: str = Field(..., description="Name of the project")
    description: str = Field(..., description="Description of the project")
    root_dir: str = Field(..., description="Root directory for the project")
    files: List[CodeFile] = Field(..., description="List of files to generate")
    dependencies: Dict[str, List[str]] = Field(default_factory=dict, description="External dependencies")
    project_type: str = Field(..., description="Type of project (e.g., python, node)")
    structure_explanation: str = Field(..., description="Explanation of the project structure")

class CodeGenerationEngine:
    """
    Advanced code generation engine that can create entire projects
    based on natural language descriptions.
    """
    
    def __init__(self):
        """Initialize the code generation engine."""
        self._logger = logger
    
    async def generate_project(
        self, 
        description: str, 
        output_dir: Optional[str] = None,
        project_type: Optional[str] = None,
        context: Optional[Dict[str, Any]] = None
    ) -> CodeProject:
        """
        Generate a complete project from a description.
        
        Args:
            description: Natural language description of the project
            output_dir: Directory where the project should be generated (defaults to cwd)
            project_type: Optional type of project to generate (auto-detected if None)
            context: Additional context information
            
        Returns:
            CodeProject object representing the generated project
        """
        self._logger.info(f"Generating project from description: {description}")
        
        # Get current context if not provided
        if context is None:
            context = context_manager.get_context_dict()
            context = await context_enhancer.enrich_context(context)
        
        # Determine output directory
        if output_dir is None:
            output_dir = context.get("cwd", os.getcwd())
        
        # Create project plan
        project_plan = await self._create_project_plan(description, output_dir, project_type, context)
        self._logger.info(f"Created project plan with {len(project_plan.files)} files")
        
        # Validate the project plan
        is_valid, validation_errors = await self._validate_project_plan(project_plan)
        
        if not is_valid:
            self._logger.error(f"Project plan validation failed: {validation_errors}")
            # Try to fix validation errors
            project_plan = await self._fix_validation_errors(project_plan, validation_errors)
        
        return project_plan
    
    async def create_project_files(
        self, 
        project: CodeProject,
        dry_run: bool = False
    ) -> Dict[str, Any]:
        """
        Create the actual files for a project.
        
        Args:
            project: CodeProject to generate
            dry_run: Whether to simulate file creation without making changes
            
        Returns:
            Dictionary with creation results
        """
        self._logger.info(f"Creating project files for: {project.name}")
        
        # Create the root directory if it doesn't exist
        root_path = Path(project.root_dir)
        if not root_path.exists() and not dry_run:
            await create_directory(root_path, parents=True)
        
        # Create files in dependency order
        created_files = []
        file_errors = []
        dependency_graph = self._build_dependency_graph(project.files)
        
        # Process files in dependency order
        for file in self._get_ordered_files(project.files, dependency_graph):
            file_path = root_path / file.path
            
            # Create parent directories if needed
            if not dry_run:
                await create_directory(file_path.parent, parents=True)
            
            # Write file content
            try:
                if not dry_run:
                    await write_file(file_path, file.content)
                created_files.append(str(file_path))
                self._logger.debug(f"Created file: {file_path}")
            except Exception as e:
                self._logger.error(f"Error creating file {file_path}: {str(e)}")
                file_errors.append({"path": str(file_path), "error": str(e)})
        
        return {
            "project_name": project.name,
            "root_dir": str(root_path),
            "created_files": created_files,
            "file_errors": file_errors,
            "file_count": len(created_files),
            "success": len(file_errors) == 0,
            "dry_run": dry_run
        }
    
    async def _create_project_plan(
        self, 
        description: str, 
        output_dir: str,
        project_type: Optional[str],
        context: Dict[str, Any]
    ) -> CodeProject:
        """
        Create a plan for a project based on the description.
        
        Args:
            description: Natural language description of the project
            output_dir: Directory where the project should be generated
            project_type: Optional type of project to generate
            context: Additional context information
            
        Returns:
            CodeProject object with the plan
        """
        # Determine project type if not specified
        if project_type is None:
            project_type = await self._infer_project_type(description, context)
            self._logger.debug(f"Inferred project type: {project_type}")
        
        # Build prompt for project planning
        prompt = self._build_project_planning_prompt(description, project_type, context)
        
        # Call AI service to generate project plan
        api_request = GeminiRequest(
            prompt=prompt,
            max_tokens=8000,  # Large token limit for complex project plans
            temperature=0.2   # Low temperature for more deterministic output
        )
        
        self._logger.debug("Sending project planning request to AI service")
        response = await gemini_client.generate_text(api_request)
        
        # Parse the response to extract the project plan
        project_plan = await self._parse_project_plan(response.text, output_dir, project_type)
        
        # Generate detailed content for each file
        project_plan = await self._generate_file_contents(project_plan, context)
        
        return project_plan
    
    async def _infer_project_type(
        self, 
        description: str, 
        context: Dict[str, Any]
    ) -> str:
        """
        Infer the project type from the description.
        
        Args:
            description: Natural language description of the project
            context: Additional context information
            
        Returns:
            String indicating the project type
        """
        # Check for explicit mentions of languages/frameworks
        tech_indicators = {
            "python": ["python", "flask", "django", "fastapi", "sqlalchemy", "pytest"],
            "node": ["node", "javascript", "express", "react", "vue", "angular", "npm"],
            "java": ["java", "spring", "maven", "gradle", "junit"],
            "go": ["go", "golang", "gin", "echo"],
            "ruby": ["ruby", "rails", "sinatra", "rspec"],
            "rust": ["rust", "cargo", "actix", "rocket"],
        }
        
        # Lowercase description for easier matching
        description_lower = description.lower()
        
        # Count mentions of each technology
        tech_counts = {}
        for tech, indicators in tech_indicators.items():
            count = sum(indicator in description_lower for indicator in indicators)
            if count > 0:
                tech_counts[tech] = count
        
        # If we found clear indicators, return the most mentioned
        if tech_counts:
            return max(tech_counts.items(), key=lambda x: x[1])[0]
        
        # No clear indicators, use AI to infer
        prompt = f"""
Determine the most suitable programming language/framework for this project:

"{description}"

Return only the project type as a single word, using one of these options:
python, node, java, go, ruby, rust, or other.
"""
        
        api_request = GeminiRequest(prompt=prompt, max_tokens=10)
        response = await gemini_client.generate_text(api_request)
        
        # Extract the project type from the response
        project_type = response.text.strip().lower()
        
        # Default to python if we couldn't determine
        if project_type not in {"python", "node", "java", "go", "ruby", "rust"}:
            return "python"
        
        return project_type
    
    def _build_project_planning_prompt(
        self, 
        description: str,
        project_type: str,
        context: Dict[str, Any]
    ) -> str:
        """
        Build a prompt for project planning.
        
        Args:
            description: Natural language description of the project
            project_type: Type of project to generate
            context: Additional context information
            
        Returns:
            Prompt string for the AI service
        """
        prompt = f"""
You are an expert software architect tasked with planning a {project_type} project based on this description:

"{description}"

First, analyze the requirements and identify:
1. Core components needed
2. Data models and their relationships
3. Key functionality to be implemented
4. External dependencies required

Then, create a detailed project structure plan in JSON format, including:

```json
{{
  "name": "project_name",
  "description": "brief project description",
  "project_type": "{project_type}",
  "dependencies": {{
    "runtime": ["list", "of", "dependencies"],
    "development": ["test", "frameworks", "etc"]
  }},
  "files": [
    {{
      "path": "relative/path/to/file.ext",
      "purpose": "description of the file's purpose",
      "dependencies": ["other/files/this/depends/on"],
      "language": "programming language"
    }}
  ],
  "structure_explanation": "explanation of why this structure was chosen"
}}
Focus on creating a well-structured, maintainable project following best practices for {project_type} projects. Include appropriate configuration files, tests, documentation, and proper project organization.
The project should be modular, follow SOLID principles, and be easy to extend.
"""
    return prompt

async def _parse_project_plan(
    self, 
    response: str, 
    output_dir: str,
    project_type: str
) -> CodeProject:
    """
    Parse the AI response to extract the project plan.
    
    Args:
        response: AI response text
        output_dir: Directory where the project should be generated
        project_type: Type of project to generate
        
    Returns:
        CodeProject object with the plan
    """
    try:
        # Look for JSON block in the response
        json_match = re.search(r'```(?:json)?\s*(.*?)\s*```', response, re.DOTALL)
        if json_match:
            json_str = json_match.group(1)
        else:
            # Try to find JSON without code blocks
            json_match = re.search(r'({.*})', response, re.DOTALL)
            if json_match:
                json_str = json_match.group(1)
            else:
                # Assume the entire response is JSON
                json_str = response
        
        # Parse the JSON
        plan_data = json.loads(json_str)
        
        # Create CodeFile objects
        files = []
        for file_data in plan_data.get("files", []):
            files.append(CodeFile(
                path=file_data["path"],
                content="",  # Content will be generated later
                purpose=file_data["purpose"],
                dependencies=file_data.get("dependencies", []),
                language=file_data.get("language")
            ))
        
        # Create CodeProject object
        project = CodeProject(
            name=plan_data.get("name", f"new_{project_type}_project"),
            description=plan_data.get("description", "Generated project"),
            root_dir=output_dir,
            files=files,
            dependencies=plan_data.get("dependencies", {}),
            project_type=project_type,
            structure_explanation=plan_data.get("structure_explanation", "")
        )
        
        return project
        
    except Exception as e:
        self._logger.exception(f"Error parsing project plan: {str(e)}")
        
        # Create a minimal fallback project
        fallback_file = CodeFile(
            path="main.py" if project_type == "python" else "index.js",
            content="",
            purpose="Main entry point",
            dependencies=[],
            language=project_type
        )
        
        return CodeProject(
            name=f"new_{project_type}_project",
            description="Generated project (fallback)",
            root_dir=output_dir,
            files=[fallback_file],
            dependencies={},
            project_type=project_type,
            structure_explanation="Fallback project structure due to parsing error."
        )

async def _generate_file_contents(
    self, 
    project: CodeProject,
    context: Dict[str, Any]
) -> CodeProject:
    """
    Generate content for each file in the project.
    
    Args:
        project: CodeProject with file information
        context: Additional context information
        
    Returns:
        Updated CodeProject with file contents
    """
    self._logger.info(f"Generating content for {len(project.files)} files")
    
    # Process files in dependency order
    dependency_graph = self._build_dependency_graph(project.files)
    
    # Prepare batches of files to generate (to avoid too many concurrent requests)
    batches = self._create_file_batches(project.files, dependency_graph)
    
    # Generate content for each batch
    for batch_idx, batch in enumerate(batches):
        self._logger.debug(f"Processing batch {batch_idx+1}/{len(batches)} with {len(batch)} files")
        
        # Generate content for each file in the batch concurrently
        tasks = []
        for file in batch:
            # Load previous file contents for dependencies
            dependencies_content = {}
            for dep_path in file.dependencies:
                # Find the dependent file
                for dep_file in project.files:
                    if dep_file.path == dep_path and dep_file.content:
                        dependencies_content[dep_path] = dep_file.content
            
            task = self._generate_file_content(
                file, 
                project, 
                dependencies_content,
                context
            )
            tasks.append(task)
        
        # Wait for all tasks in this batch to complete
        results = await asyncio.gather(*tasks)
        
        # Update file contents
        for file, content in zip(batch, results):
            file.content = content
    
    return project

async def _generate_file_content(
    self, 
    file: CodeFile, 
    project: CodeProject,
    dependencies_content: Dict[str, str],
    context: Dict[str, Any]
) -> str:
    """
    Generate content for a single file.
    
    Args:
        file: CodeFile to generate content for
        project: Parent CodeProject
        dependencies_content: Content of files this depends on
        context: Additional context information
        
    Returns:
        Generated file content
    """
    self._logger.debug(f"Generating content for file: {file.path}")
    
    # Build prompt for file content generation
    prompt = self._build_file_content_prompt(file, project, dependencies_content)
    
    # Call AI service to generate file content
    api_request = GeminiRequest(
        prompt=prompt,
        max_tokens=4000,
        temperature=0.2
    )
    
    response = await gemini_client.generate_text(api_request)
    
    # Extract code from the response
    content = self._extract_code_from_response(response.text, file.path)
    
    # Validate the generated code
    is_valid, validation_message = validate_code(content, file.path)
    
    # If validation failed, try once more with the error message
    if not is_valid:
        self._logger.warning(f"Validation failed for {file.path}: {validation_message}")
        
        # Build a new prompt with the validation error
        fix_prompt = f"""
The code you generated has an issue that needs to be fixed:
{validation_message}
Here is the original code:
{content}
Please provide the corrected code for file '{file.path}'.
Only respond with the corrected code, nothing else.
"""
        # Call AI service to fix the code
        fix_request = GeminiRequest(
            prompt=fix_prompt,
            max_tokens=4000,
            temperature=0.1
        )
        
        fix_response = await gemini_client.generate_text(fix_request)
        
        # Extract fixed code
        fixed_content = self._extract_code_from_response(fix_response.text, file.path)
        
        # Validate again
        is_valid, _ = validate_code(fixed_content, file.path)
        if is_valid:
            content = fixed_content
    
    return content

def _build_file_content_prompt(
    self, 
    file: CodeFile, 
    project: CodeProject,
    dependencies_content: Dict[str, str]
) -> str:
    """
    Build a prompt for generating file content.
    
    Args:
        file: CodeFile to generate content for
        project: Parent CodeProject
        dependencies_content: Content of files this depends on
        
    Returns:
        Prompt string for the AI service
    """
    # Add language context based on file extension
    language_hints = ""
    if file.language:
        language_hints = f"The file should be written in {file.language}."
    
    # Add dependencies context
    dependencies_context = ""
    if dependencies_content:
        dependencies_context = "This file depends on the following files:\n\n"
        
        for dep_path, content in dependencies_content.items():
            # Limit content size to avoid token limits
            if len(content) > 1000:
                content = content[:1000] + "\n... (truncated)"
            
            dependencies_context += f"File: {dep_path}\n```\n{content}\n```\n\n"
    
    prompt = f"""
You are an expert software developer working on a {project.project_type} project named "{project.name}".
{project.description}
You need to create the file "{file.path}" with the following purpose:
{file.purpose}
{language_hints}
The project has the following overall structure:
{project.structure_explanation}
{dependencies_context}
Generate only the code for this file. The code should be well-structured, properly formatted, and follow best practices for its language. Include appropriate comments and documentation.
Only return the file content, nothing else.
"""
    return prompt

def _extract_code_from_response(self, response: str, file_path: str) -> str:
    """
    Extract code from the AI response.
    
    Args:
        response: AI response text
        file_path: Path of the file being generated
        
    Returns:
        Extracted code content
    """
    # Try to extract code from markdown code blocks
    code_match = re.search(r'```(?:\w+)?\s*(.*?)\s*```', response, re.DOTALL)
    if code_match:
        return code_match.group(1)
    
    # No code block found, use the entire response
    return response.strip()

def _build_dependency_graph(self, files: List[CodeFile]) -> Dict[str, Set[str]]:
    """
    Build a dependency graph for the files.
    
    Args:
        files: List of files to process
        
    Returns:
        Dictionary mapping file paths to sets of dependent file paths
    """
    # Map file paths to indices
    path_to_index = {file.path: i for i, file in enumerate(files)}
    
    # Initialize the graph
    graph = {}
    for file in files:
        graph[file.path] = set()
        for dep_path in file.dependencies:
            if dep_path in path_to_index:
                graph[file.path].add(dep_path)
    
    return graph

def _get_ordered_files(
    self, 
    files: List[CodeFile], 
    graph: Dict[str, Set[str]]
) -> List[CodeFile]:
    """
    Get files in dependency order (topological sort).
    
    Args:
        files: List of files to order
        graph: Dependency graph
        
    Returns:
        Ordered list of files
    """
    # Map file paths to objects
    path_to_file = {file.path: file for file in files}
    
    # Keep track of visited and ordered nodes
    visited = set()
    ordered = []
    
    def visit(path):
        """DFS visit function for topological sort."""
        if path in visited:
            return
        
        visited.add(path)
        
        # Visit dependencies first
        for dep_path in graph.get(path, set()):
            visit(dep_path)
        
        # Add to ordered list
        if path in path_to_file:
            ordered.append(path_to_file[path])
    
    # Visit all nodes
    for file in files:
        visit(file.path)
    
    return ordered

def _create_file_batches(
    self, 
    files: List[CodeFile], 
    graph: Dict[str, Set[str]]
) -> List[List[CodeFile]]:
    """
    Create batches of files that can be generated concurrently.
    
    Args:
        files: List of files to batch
        graph: Dependency graph
        
    Returns:
        List of file batches
    """
    # Get files in dependency order
    ordered_files = self._get_ordered_files(files, graph)
    
    # Group files by their dependency level
    levels = {}
    path_to_level = {}
    
    # Compute the dependency level for each file
    for file in ordered_files:
        # The level is 1 + the maximum level of dependencies
        max_dep_level = 0
        for dep_path in file.dependencies:
            if dep_path in path_to_level:
                max_dep_level = max(max_dep_level, path_to_level[dep_path])
        
        level = max_dep_level + 1
        path_to_level[file.path] = level
        
        if level not in levels:
            levels[level] = []
        
        levels[level].append(file)
    
    # Create batches from levels
    batches = []
    for level in sorted(levels.keys()):
        batches.append(levels[level])
    
    return batches

async def _validate_project_plan(
    self, 
    project: CodeProject
) -> Tuple[bool, List[str]]:
    """
    Validate a project plan for consistency.
    
    Args:
        project: CodeProject to validate
        
    Returns:
        Tuple of (is_valid, list_of_errors)
    """
    errors = []
    
    # Check for duplicate file paths
    paths = [file.path for file in project.files]
    if len(paths) != len(set(paths)):
        duplicate_paths = [path for path in paths if paths.count(path) > 1]
        errors.append(f"Duplicate file paths: {set(duplicate_paths)}")
    
    # Check for circular dependencies
    try:
        graph = self._build_dependency_graph(project.files)
        self._get_ordered_files(project.files, graph)
    except Exception as e:
        errors.append(f"Circular dependencies detected: {str(e)}")
    
    # Check for missing dependencies
    path_set = set(paths)
    for file in project.files:
        for dep_path in file.dependencies:
            if dep_path not in path_set:
                errors.append(f"File {file.path} depends on non-existent file {dep_path}")
    
    return len(errors) == 0, errors

async def _fix_validation_errors(
    self, 
    project: CodeProject, 
    errors: List[str]
) -> CodeProject:
    """
    Try to fix validation errors in a project plan.
    
    Args:
        project: The project plan to fix
        errors: List of validation errors
        
    Returns:
        Fixed CodeProject
    """
    self._logger.info(f"Attempting to fix {len(errors)} validation errors")
    
    # Handle duplicate paths
    if any("Duplicate file paths" in error for error in errors):
        # Create a map of paths to files
        path_to_files = {}
        for file in project.files:
            if file.path not in path_to_files:
                path_to_files[file.path] = []
            path_to_files[file.path].append(file)
        
        # Keep only the first instance of each duplicate
        new_files = []
        for path, files in path_to_files.items():
            new_files.append(files[0])
        
        project.files = new_files
    
    # Handle circular dependencies
    if any("Circular dependencies" in error for error in errors):
        # Remove dependencies that create cycles
        graph = {}
        for file in project.files:
            graph[file.path] = set(file.dependencies)
        
        # Find and break cycles
        visited = set()
        path = []
        
        def find_cycles(node):
            if node in path:
                # Cycle detected, break it
                cycle_start = path.index(node)
                cycle = path[cycle_start:] + [node]
                
                # Remove the last dependency in the cycle
                source = cycle[-2]
                target = cycle[-1]
                for file in project.files:
                    if file.path == source:
                        if target in file.dependencies:
                            file.dependencies.remove(target)
                            self._logger.debug(f"Removed dependency from {source} to {target} to break cycle")
                
                return True
            
            if node in visited:
                return False
            
            visited.add(node)
            path.append(node)
            
            for neighbor in graph.get(node, set()):
                if find_cycles(neighbor):
                    return True
            
            path.pop()
            return False
        
        # Find and fix all cycles
        for node in list(graph.keys()):
            while find_cycles(node):
                visited = set()
                path = []
        
    # Handle missing dependencies
    if any("depends on non-existent file" in error for error in errors):
        # Remove dependencies that don't exist
        valid_paths = {file.path for file in project.files}
        for file in project.files:
            file.dependencies = [dep for dep in file.dependencies if dep in valid_paths]
    
    return project




async def add_feature_to_project(
    self, 
    description: str, 
    project_dir: Union[str, Path],
    context: Optional[Dict[str, Any]] = None
) -> Dict[str, Any]:
    """
    Add a new feature to an existing project.
    
    Args:
        description: Natural language description of the feature to add
        project_dir: Path to the project directory
        context: Additional context information
        
    Returns:
        Dictionary with information about the added feature
    """
    self._logger.info(f"Adding feature to project: {description}")
    
    # Get context if not provided
    if context is None:
        context = context_manager.get_context_dict()
        context = await context_enhancer.enrich_context(context)
    
    # Convert to Path object
    project_path = Path(project_dir)
    
    # Step 1: Analyze existing project structure
    project_analysis = await self._analyze_existing_project(project_path, context)
    project_type = project_analysis.get("project_type")
    
    if not project_type:
        self._logger.error("Could not determine project type")
        return {
            "success": False,
            "error": "Could not determine project type",
            "project_dir": str(project_path)
        }
    
    # Step 2: Generate feature plan based on description and existing project
    feature_plan = await self._generate_feature_plan(description, project_analysis, context)
    
    # Step 3: Generate file contents for new/modified files
    feature_files = await self._generate_feature_files(feature_plan, project_analysis, context)
    
    # Step 4: Apply changes to the project
    result = await self._apply_feature_changes(feature_files, project_path)
    
    return {
        "success": result.get("success", False),
        "description": description,
        "project_type": project_type,
        "new_files": result.get("created_files", []),
        "modified_files": result.get("modified_files", []),
        "errors": result.get("errors", []),
        "project_dir": str(project_path)
    }

async def _analyze_existing_project(
    self, 
    project_path: Path, 
    context: Dict[str, Any]
) -> Dict[str, Any]:
    """
    Analyze an existing project structure.
    
    Args:
        project_path: Path to the project directory
        context: Context information
        
    Returns:
        Dictionary with project analysis information
    """
    self._logger.info(f"Analyzing existing project structure at {project_path}")
    
    # Get project information from context if available
    project_info = {}
    
    if "enhanced_project" in context:
        project_info = {
            "project_type": context["enhanced_project"].get("type", "unknown"),
            "frameworks": context["enhanced_project"].get("frameworks", {}),
            "dependencies": context["enhanced_project"].get("dependencies", {})
        }
    
    # If project type is unknown or not in context, detect it
    if project_info.get("project_type") == "unknown" or not project_info:
        # Import here to avoid circular imports
        from angela.toolchain.ci_cd import ci_cd_integration
        detection_result = await ci_cd_integration.detect_project_type(project_path)
        project_info["project_type"] = detection_result.get("project_type")
    
    # Get file structure
    files = []
    for root, _, filenames in os.walk(project_path):
        for filename in filenames:
            # Skip common directories to ignore
            if any(ignored in root for ignored in [".git", "__pycache__", "node_modules", "venv"]):
                continue
                
            file_path = Path(root) / filename
            rel_path = file_path.relative_to(project_path)
            
            # Get basic file info
            file_info = {
                "path": str(rel_path),
                "full_path": str(file_path),
                "type": None,
                "language": None,
                "content": None
            }
            
            # Try to determine file type and language
            try:
                from angela.context.file_detector import detect_file_type
                type_info = detect_file_type(file_path)
                file_info["type"] = type_info.get("type")
                file_info["language"] = type_info.get("language")
                
                # Read content for source code files (limit to prevent memory issues)
                if type_info.get("type") == "source_code" and file_path.stat().st_size < 100000:
                    with open(file_path, 'r', encoding='utf-8', errors='replace') as f:
                        file_info["content"] = f.read()
            except Exception as e:
                self._logger.debug(f"Error analyzing file {file_path}: {str(e)}")
            
            files.append(file_info)
    
    # Add files to project info
    project_info["files"] = files
    project_info["main_files"] = []
    
    # Try to identify important files based on project type
    if project_info.get("project_type") == "python":
        # Look for main Python files
        for file_info in files:
            if file_info["path"].endswith(".py"):
                if any(name in file_info["path"].lower() for name in ["main", "app", "index", "server"]):
                    project_info["main_files"].append(file_info["path"])
    elif project_info.get("project_type") == "node":
        # Look for main JavaScript/TypeScript files
        for file_info in files:
            if file_info["path"].endswith((".js", ".ts", ".jsx", ".tsx")):
                if any(name in file_info["path"].lower() for name in ["main", "app", "index", "server"]):
                    project_info["main_files"].append(file_info["path"])
    
    return project_info

async def _generate_feature_plan(
    self, 
    description: str,
    project_analysis: Dict[str, Any],
    context: Dict[str, Any]
) -> Dict[str, Any]:
    """
    Generate a plan for adding a feature to the project.
    
    Args:
        description: Feature description
        project_analysis: Analysis of the existing project
        context: Context information
        
    Returns:
        Dictionary with the feature plan
    """
    self._logger.info(f"Generating feature plan for: {description}")
    
    project_type = project_analysis.get("project_type", "unknown")
    
    # Build a prompt for the AI to generate a feature plan
    prompt = f"""
You are an expert software developer tasked with planning how to add a new feature to an existing {project_type} project.

Feature description: "{description}"

Based on the existing project structure, determine:
1. What new files need to be created
2. What existing files need to be modified
3. How the new feature integrates with the existing codebase

Project Information:
- Project Type: {project_type}
- Main Files: {project_analysis.get('main_files', [])}
- Frameworks: {project_analysis.get('frameworks', {})}

Project Structure:
"""
    
    # Add information about existing files
    files_by_type = {}
    for file_info in project_analysis.get("files", []):
        file_type = file_info.get("type", "unknown")
        if file_type not in files_by_type:
            files_by_type[file_type] = []
        files_by_type[file_type].append(file_info["path"])
    
    for file_type, files in files_by_type.items():
        prompt += f"\n{file_type.upper()} FILES:\n"
        for file_path in files[:10]:  # Limit to 10 files per type to avoid token limits
            prompt += f"- {file_path}\n"
        if len(files) > 10:
            prompt += f"- ... and {len(files) - 10} more {file_type} files\n"
    
    # Add content of main files to give context
    prompt += "\nMain File Contents:\n"
    for file_path in project_analysis.get("main_files", [])[:3]:  # Limit to 3 main files
        for file_info in project_analysis.get("files", []):
            if file_info["path"] == file_path and file_info.get("content"):
                content = file_info["content"]
                if len(content) > 1000:  # Limit content size
                    content = content[:1000] + "\n... (truncated)"
                prompt += f"\nFile: {file_path}\n```\n{content}\n```\n"
    
    prompt += """
Provide your response as a JSON object with this structure:
```json
{
  "new_files": [
    {
      "path": "relative/path/to/file.ext",
      "purpose": "description of the file's purpose",
      "content_template": "template for file content with {{placeholders}}",
      "language": "programming language"
    }
  ],
  "modified_files": [
    {
      "path": "relative/path/to/existing/file.ext",
      "purpose": "description of the modifications",
      "modifications": [
        {
          "type": "add_import",
          "content": "import statement to add",
          "line": 0
        },
        {
          "type": "add_function",
          "content": "function to add",
          "after": "existing function or pattern"
        },
        {
          "type": "replace",
          "search": "code to search for",
          "replace": "replacement code"
        }
      ]
    }
  ],
  "integration_points": [
    "Description of how the feature integrates with existing code"
  ]
}
```
Focus on creating a clean, maintainable implementation that follows the project's existing patterns and best practices.
"""
    
    # Call AI service
    api_request = GeminiRequest(
        prompt=prompt,
        max_tokens=8000,
        temperature=0.2
    )
    
    self._logger.debug("Sending feature plan request to AI service")
    response = await gemini_client.generate_text(api_request)
    
    # Parse the response
    plan = await self._parse_feature_plan(response.text)
    
    return plan

async def _parse_feature_plan(self, response: str) -> Dict[str, Any]:
    """
    Parse the AI response to extract the feature plan.
    
    Args:
        response: AI response text
        
    Returns:
        Dictionary with the feature plan
    """
    # Look for JSON block in the response
    try:
        json_match = re.search(r'```(?:json)?\s*(.*?)\s*```', response, re.DOTALL)
        if json_match:
            json_str = json_match.group(1)
        else:
            # Try to find JSON without code blocks
            json_match = re.search(r'({.*})', response, re.DOTALL)
            if json_match:
                json_str = json_match.group(1)
            else:
                # Assume the entire response is JSON
                json_str = response
        
        # Parse the JSON
        plan_data = json.loads(json_str)
        
        return plan_data
    
    except Exception as e:
        self._logger.exception(f"Error parsing feature plan: {str(e)}")
        
        # Return a minimal fallback plan
        return {
            "new_files": [],
            "modified_files": [],
            "integration_points": [
                "Unable to parse AI response. Consider providing more specific feature description."
            ]
        }

async def _generate_feature_files(
    self, 
    feature_plan: Dict[str, Any],
    project_analysis: Dict[str, Any],
    context: Dict[str, Any]
) -> Dict[str, Any]:
    """
    Generate content for new files and modifications for existing files.
    
    Args:
        feature_plan: Feature plan from _generate_feature_plan
        project_analysis: Analysis of the existing project
        context: Context information
        
    Returns:
        Dictionary with file contents and modifications
    """
    self._logger.info("Generating feature file contents")
    
    feature_files = {
        "new_files": [],
        "modified_files": []
    }
    
    # Process new files
    for file_info in feature_plan.get("new_files", []):
        # Generate content for the new file
        content = await self._generate_new_file_content(
            file_info,
            project_analysis,
            feature_plan,
            context
        )
        
        feature_files["new_files"].append({
            "path": file_info["path"],
            "content": content,
            "purpose": file_info.get("purpose", "")
        })
    
    # Process modified files
    for file_info in feature_plan.get("modified_files", []):
        # Get original content
        original_content = self._get_file_content(file_info["path"], project_analysis)
        
        if original_content is None:
            self._logger.warning(f"Could not find content for file: {file_info['path']}")
            continue
        
        # Apply modifications
        modified_content = await self._apply_file_modifications(
            original_content,
            file_info.get("modifications", []),
            file_info,
            project_analysis,
            feature_plan,
            context
        )
        
        feature_files["modified_files"].append({
            "path": file_info["path"],
            "original_content": original_content,
            "modified_content": modified_content,
            "purpose": file_info.get("purpose", "")
        })
    
    return feature_files

def _get_file_content(self, file_path: str, project_analysis: Dict[str, Any]) -> Optional[str]:
    """
    Get the content of a file from the project analysis.
    
    Args:
        file_path: Path to the file
        project_analysis: Analysis of the existing project
        
    Returns:
        File content or None if not found
    """
    for file_info in project_analysis.get("files", []):
        if file_info["path"] == file_path:
            return file_info.get("content")
    return None

async def _generate_new_file_content(
    self, 
    file_info: Dict[str, Any],
    project_analysis: Dict[str, Any],
    feature_plan: Dict[str, Any],
    context: Dict[str, Any]
) -> str:
    """
    Generate content for a new file.
    
    Args:
        file_info: Information about the new file
        project_analysis: Analysis of the existing project
        feature_plan: Overall feature plan
        context: Context information
        
    Returns:
        Generated file content
    """
    self._logger.debug(f"Generating content for new file: {file_info['path']}")
    
    # Get template if provided
    template = file_info.get("content_template", "")
    
    # If template has placeholders, we should fill them in
    # This is simplified; in a real implementation you would have more context
    if template and "{{" in template:
        # Process template with placeholders
        # This is just a simple example
        template = template.replace("{{project_type}}", project_analysis.get("project_type", ""))
    
    # If template is not provided or is minimal, generate content with AI
    if len(template.strip()) < 50:  # Arbitrary threshold
        # Build prompt for file generation
        prompt = f"""
Generate the content for a new file in a {project_analysis.get('project_type', 'unknown')} project.

File path: {file_info['path']}
File purpose: {file_info.get('purpose', 'Unknown')}

This file is part of a new feature described as:
{feature_plan.get('integration_points', ['Unknown'])[0] if feature_plan.get('integration_points') else 'Unknown'}

The project already has files like:
"""
        # Add a few relevant existing files for context
        file_extension = Path(file_info['path']).suffix
        for existing_file in project_analysis.get("files", [])[:5]:
            if existing_file.get("path", "").endswith(file_extension):
                prompt += f"- {existing_file['path']}\n"
        
        # Add content of a similar file for style reference
        similar_files = [f for f in project_analysis.get("files", []) 
                        if f.get("path", "").endswith(file_extension) and f.get("content")]
        
        if similar_files:
            similar_file = similar_files[0]
            content = similar_file.get("content", "")
            if len(content) > 1000:  # Limit content size
                content = content[:1000] + "\n... (truncated)"
            prompt += f"\nReference file ({similar_file['path']}) for style consistency:\n"
            prompt += f"```\n{content}\n```\n"
        
        prompt += "\nGenerate the complete content for the new file, following the project's style and conventions."
        
        # Call AI service
        api_request = GeminiRequest(
            prompt=prompt,
            max_tokens=4000,
            temperature=0.2
        )
        
        response = await gemini_client.generate_text(api_request)
        
        # Extract code from the response
        content = self._extract_code_from_response(response.text, file_info['path'])
        return content
    
    return template

async def _apply_file_modifications(
    self, 
    original_content: str,
    modifications: List[Dict[str, Any]],
    file_info: Dict[str, Any],
    project_analysis: Dict[str, Any],
    feature_plan: Dict[str, Any],
    context: Dict[str, Any]
) -> str:
    """
    Apply modifications to an existing file.
    
    Args:
        original_content: Original file content
        modifications: List of modifications to apply
        file_info: Information about the file
        project_analysis: Analysis of the existing project
        feature_plan: Overall feature plan
        context: Context information
        
    Returns:
        Modified file content
    """
    self._logger.debug(f"Applying modifications to file: {file_info['path']}")
    
    modified_content = original_content
    
    # Apply each modification in sequence
    for mod in modifications:
        mod_type = mod.get("type", "")
        
        if mod_type == "add_import":
            # Add import statement at the top
            import_stmt = mod.get("content", "")
            if import_stmt:
                # Find where imports end
                lines = modified_content.splitlines()
                import_end_line = 0
                
                # Look for existing imports
                for i, line in enumerate(lines):
                    if line.strip().startswith(("import ", "from ")):
                        import_end_line = i + 1
                
                # Insert import at the right position
                lines.insert(import_end_line, import_stmt)
                modified_content = "\n".join(lines)
        
        elif mod_type == "add_function":
            # Add function/method to the file
            function_content = mod.get("content", "")
            after_pattern = mod.get("after", "")
            
            if function_content:
                if after_pattern and after_pattern in modified_content:
                    # Insert after specific pattern
                    parts = modified_content.split(after_pattern, 1)
                    modified_content = parts[0] + after_pattern + "\n\n" + function_content + "\n" + parts[1]
                else:
                    # Append to the end of the file
                    if not modified_content.endswith("\n"):
                        modified_content += "\n"
                    modified_content += "\n" + function_content + "\n"
        
        elif mod_type == "replace":
            # Replace text in the file
            search_text = mod.get("search", "")
            replace_text = mod.get("replace", "")
            
            if search_text and replace_text:
                modified_content = modified_content.replace(search_text, replace_text)
    
    # If no modifications were applied successfully or instructions were unclear,
    # use AI to apply the modifications
    if modified_content == original_content:
        modified_content = await self._generate_file_modifications_with_ai(
            original_content,
            file_info,
            project_analysis,
            feature_plan,
            context
        )
    
    return modified_content

async def _generate_file_modifications_with_ai(
    self, 
    original_content: str,
    file_info: Dict[str, Any],
    project_analysis: Dict[str, Any],
    feature_plan: Dict[str, Any],
    context: Dict[str, Any]
) -> str:
    """
    Use AI to generate modifications for a file when structured modifications fail.
    
    Args:
        original_content: Original file content
        file_info: Information about the file
        project_analysis: Analysis of the existing project
        feature_plan: Overall feature plan
        context: Context information
        
    Returns:
        Modified file content
    """
    self._logger.debug(f"Using AI to generate modifications for: {file_info['path']}")
    
    # Build prompt for generating modifications
    prompt = f"""
Modify the content of an existing file in a {project_analysis.get('project_type', 'unknown')} project to implement a new feature.

File path: {file_info['path']}
Modification purpose: {file_info.get('purpose', 'Unknown')}

This modification is part of a new feature described as:
{feature_plan.get('integration_points', ['Unknown'])[0] if feature_plan.get('integration_points') else 'Unknown'}

Original file content:
```
{original_content}
```

Your task is to modify this file to implement the specified feature. Return the complete modified content.
Follow the project's existing coding style and patterns.
"""
    
    # Call AI service
    api_request = GeminiRequest(
        prompt=prompt,
        max_tokens=8000,
        temperature=0.2
    )
    
    response = await gemini_client.generate_text(api_request)
    
    # Extract code from the response
    modified_content = self._extract_code_from_response(response.text, file_info['path'])
    return modified_content

async def _apply_feature_changes(
    self, 
    feature_files: Dict[str, Any],
    project_path: Path
) -> Dict[str, Any]:
    """
    Apply the generated feature changes to the project.
    
    Args:
        feature_files: Generated file contents and modifications
        project_path: Path to the project directory
        
    Returns:
        Dictionary with application results
    """
    self._logger.info("Applying feature changes to project")
    
    result = {
        "success": True,
        "created_files": [],
        "modified_files": [],
        "errors": []
    }
    
    # Create new files
    for file_info in feature_files.get("new_files", []):
        file_path = project_path / file_info["path"]
        
        try:
            # Create parent directories if needed
            await create_directory(file_path.parent, parents=True)
            
            # Write file content
            await write_file(file_path, file_info["content"])
            
            result["created_files"].append(str(file_path))
            self._logger.debug(f"Created new file: {file_path}")
        except Exception as e:
            self._logger.error(f"Error creating file {file_path}: {str(e)}")
            result["errors"].append({"path": str(file_path), "error": str(e)})
            result["success"] = False
    
    # Modify existing files
    for file_info in feature_files.get("modified_files", []):
        file_path = project_path / file_info["path"]
        
        try:
            # Check if file exists
            if not file_path.exists():
                raise FileNotFoundError(f"File not found: {file_path}")
            
            # Write modified content
            await write_file(file_path, file_info["modified_content"])
            
            result["modified_files"].append(str(file_path))
            self._logger.debug(f"Modified file: {file_path}")
        except Exception as e:
            self._logger.error(f"Error modifying file {file_path}: {str(e)}")
            result["errors"].append({"path": str(file_path), "error": str(e)})
            result["success"] = False
    
    return result


async def _extract_dependencies_from_feature(
    self, 
    feature_files: Dict[str, Any],
    project_type: str
) -> Dict[str, List[str]]:
    """
    Extract dependencies from newly added or modified feature files.
    
    Args:
        feature_files: Dictionary with new and modified files
        project_type: Type of the project (python, node, etc.)
        
    Returns:
        Dictionary with runtime and development dependencies
    """
    self._logger.info("Extracting dependencies from feature files")
    
    dependencies = {
        "runtime": [],
        "development": []
    }
    
    # Process based on project type
    if project_type == "python":
        # Look for Python import statements in new files
        for file_info in feature_files.get("new_files", []):
            if file_info["path"].endswith(".py"):
                content = file_info["content"]
                
                # Extract import statements using regex
                import_lines = re.findall(r'^(?:import|from)\s+([^\s.]+)(?:\.|.*?import)', content, re.MULTILINE)
                
                # Filter out standard library modules
                for module in import_lines:
                    if module not in sys.modules or (hasattr(sys.modules[module], '__file__') and 
                                                    sys.modules[module].__file__ and 
                                                    'site-packages' in sys.modules[module].__file__):
                        if module not in dependencies["runtime"] and module not in ["__future__", "typing", "os", "sys", "re", "json", "time", "datetime"]:
                            dependencies["runtime"].append(module)
        
        # Also check modified files for new imports
        for file_info in feature_files.get("modified_files", []):
            if file_info["path"].endswith(".py"):
                original_content = file_info["original_content"]
                modified_content = file_info["modified_content"]
                
                # Extract imports from original and modified content
                original_imports = set(re.findall(r'^(?:import|from)\s+([^\s.]+)(?:\.|.*?import)', original_content, re.MULTILINE))
                modified_imports = set(re.findall(r'^(?:import|from)\s+([^\s.]+)(?:\.|.*?import)', modified_content, re.MULTILINE))
                
                # Find new imports
                new_imports = modified_imports - original_imports
                
                # Add to dependencies list
                for module in new_imports:
                    if module not in dependencies["runtime"] and module not in ["__future__", "typing", "os", "sys", "re", "json", "time", "datetime"]:
                        dependencies["runtime"].append(module)
                        
    elif project_type == "node":
        # Look for import/require statements in JavaScript/TypeScript files
        js_extensions = [".js", ".jsx", ".ts", ".tsx"]
        
        for file_info in feature_files.get("new_files", []):
            if any(file_info["path"].endswith(ext) for ext in js_extensions):
                content = file_info["content"]
                
                # Look for ES6 imports
                es6_imports = re.findall(r'import\s+.*?from\s+[\'"]([^\'".][^\'"]*)[\'"]\s*;?', content)
                
                # Look for require statements
                require_imports = re.findall(r'(?:const|let|var)\s+.*?=\s+require\([\'"]([^\'".][^\'"]*)[\'"]\)\s*;?', content)
                
                # Combine all found imports
                all_imports = es6_imports + require_imports
                
                # Filter out relative imports
                for module in all_imports:
                    if not module.startswith(".") and module not in dependencies["runtime"]:
                        dependencies["runtime"].append(module)
        
        # Also check modified files
        for file_info in feature_files.get("modified_files", []):
            if any(file_info["path"].endswith(ext) for ext in js_extensions):
                original_content = file_info["original_content"]
                modified_content = file_info["modified_content"]
                
                # Extract imports from original content
                original_es6 = set(re.findall(r'import\s+.*?from\s+[\'"]([^\'".][^\'"]*)[\'"]\s*;?', original_content))
                original_require = set(re.findall(r'(?:const|let|var)\s+.*?=\s+require\([\'"]([^\'".][^\'"]*)[\'"]\)\s*;?', original_content))
                original_imports = original_es6.union(original_require)
                
                # Extract imports from modified content
                modified_es6 = set(re.findall(r'import\s+.*?from\s+[\'"]([^\'".][^\'"]*)[\'"]\s*;?', modified_content))
                modified_require = set(re.findall(r'(?:const|let|var)\s+.*?=\s+require\([\'"]([^\'".][^\'"]*)[\'"]\)\s*;?', modified_content))
                modified_imports = modified_es6.union(modified_require)
                
                # Find new imports
                new_imports = modified_imports - original_imports
                
                # Add to dependencies list
                for module in new_imports:
                    if not module.startswith(".") and module not in dependencies["runtime"]:
                        dependencies["runtime"].append(module)
    
    # Add common testing libraries based on project type
    if project_type == "python":
        dependencies["development"] = ["pytest", "pytest-cov"]
    elif project_type == "node":
        dependencies["development"] = ["jest", "eslint"]
    
    return dependencies



code_generation_engine = CodeGenerationEngine()
</file>

<file path="MD/context.md">
# I couldnt give you all teh files i have currently in repository so i haev thsi file to provide you some conetxxt regaridng teh files you may not be able to directly see. Aswell as a directopry tree/structure/hiererechy

## heres a breif description of each .py file you cannot directly see / see the code in them
---
**File: `angela/workflows/sharing.py`**
`angela/workflows/sharing.py` facilitates sharing workflows by enabling their export into packaged `.angela-workflow` zip files and subsequent import into the Angela system. It defines a `WorkflowExportMetadata` Pydantic model to structure package information, including ID, name, version, author, timestamps, and a SHA-256 checksum generated by `hashlib` for data integrity. The `WorkflowSharingManager` class orchestrates the export process by packaging `workflow.json` (workflow data), `metadata.json`, and a `README.md` into a zip archive, and the import process by extracting, validating the checksum, and adding the workflow via `WorkflowManager`. Export functionality can optionally detect external dependencies like Python or Node.js versions by asynchronously running shell commands (e.g., `python --version`) and includes this in the metadata. The module robustly utilizes `tempfile` for temporary directories, `datetime` for timestamps, `uuid` for unique package IDs, and `pathlib` for filesystem path manipulations.
---
**File: `angela/workflows/manager.py` (inferred from context)**
The file `angela/workflows/manager.py` provides the core `WorkflowManager` class for defining, storing, retrieving, and executing reusable command sequences, known as workflows, within the Angela CLI. It utilizes Pydantic models, `WorkflowStep` (detailing command, explanation, optionality, confirmation needs) and `Workflow` (encompassing name, description, steps, variables, timestamps, tags, author), to structure workflow data. Workflows are persisted in a `workflows.json` file within the application's configuration directory (`CONFIG_DIR`), with the manager handling loading at startup and saving upon modification, including `datetime` to ISO format serialization. A key feature is the `define_workflow_from_natural_language` async method, which leverages an AI (Gemini client) and a `TaskPlanner` to convert user descriptions into structured workflow steps and identify potential variables. Workflow execution involves substituting provided variable values into command templates, converting the workflow into a `TaskPlan`, and then running this plan via the `task_planner`, supporting both regular execution and dry runs.
---
**File: `angela/ai/content_analyzer_extensions.py`**
The `angela/ai/content_analyzer_extensions.py` file introduces `EnhancedContentAnalyzer`, a class that extends the base `ContentAnalyzer` to provide specialized analysis for a wider range of file types and programming languages. It employs a `LANGUAGE_HANDLERS` dictionary to route file analysis requests to language-specific async methods, such as `_analyze_typescript` or `_analyze_json`, based on detected file information from `_get_file_info`. For instance, `_analyze_typescript` uses custom AI prompts for TypeScript code analysis via `_get_ai_analysis` (calling Gemini) and regex patterns (in `_extract_typescript_types`) to identify types and interfaces. Similarly, `_analyze_json` validates JSON content using `json.loads`, infers a basic schema through `_infer_json_schema` by recursively examining data types, and leverages AI for a human-readable structural analysis. If no specific handler is found for a language, the `EnhancedContentAnalyzer` falls back to the `analyze_content` method of its parent class, ensuring general analysis capabilities remain.
---
**File: `angela/ai/parser.py`**
The `angela/ai/parser.py` file is responsible for parsing responses from an AI model, aiming to convert potentially unstructured text into a structured `CommandSuggestion` object. It defines the `CommandSuggestion` Pydantic model, which specifies the expected fields: `intent`, `command`, `explanation`, and optional `additional_info`, ensuring data integrity and providing typed access. The core `parse_ai_response` function intelligently attempts to locate and extract JSON data from the AI's output, specifically looking for JSON within markdown code blocks (e.g., ```json ... ```) or assuming the entire response is JSON. After extracting the JSON string, it uses `json.loads` for parsing and then Pydantic for validation against the `CommandSuggestion` model, logging success or errors. In cases of `json.JSONDecodeError` or Pydantic `ValidationError`, a fallback mechanism employs regular expressions (`re.search`) to try and salvage at least the `command` string, enhancing the parser's resilience to malformed AI responses.
---
**File: `angela/ai/confidence.py`**
`angela/ai/confidence.py` introduces the `ConfidenceScorer` class, designed to evaluate and assign a numerical confidence score (ranging from 0.0 to 1.0) to AI-generated command suggestions. The primary method, `score_command_confidence`, aggregates scores from multiple weighted heuristics: `_check_history` (queries `history_manager` for command frequency and success rate), `_check_complexity` (compares token counts of request and command), `_check_entities` (basic check for entity type matches like files/dirs), and `_check_command_flags` (looks for unusual/conflicting flags). Each heuristic function, like `_check_complexity` which compares request and command token lengths, contributes a value that is then weighted into the final score. The entity check (`_check_entities`) uses simple string matching for terms like "file" or "directory" and `re.findall` for path patterns. The module ensures the final confidence score remains within the 0.0 to 1.0 range and logs the breakdown of contributing factors for debuggability.
---
**File: `angela/config.py` (inferred from content)**
This configuration management file, `angela/config.py`, establishes a `ConfigManager` class to handle application settings using TOML for file-based configuration and environment variables for sensitive data like API keys. It employs Pydantic models (`ApiConfig`, `UserConfig`, `AppConfig`) to define a clear structure for settings such as Gemini API keys, default project root (`Path`), `confirm_all_actions` (bool), and `debug` mode, ensuring type validation. The manager dynamically selects TOML parsing libraries (`tomllib` for Python 3.11+ or `tomli` for older versions, and `tomli-w` for writing), loading from a predefined `CONFIG_FILE` path and `.env` files via `python-dotenv`. `load_config` reads the TOML file, populates the Pydantic models, and gracefully handles potential `TOMLDecodeError` or missing files by falling back to defaults or saving a new default configuration. The `save_config` method serializes the current `AppConfig` model (converting `Path` objects to strings for TOML compatibility) back to the configuration file, while a global `config_manager` instance ensures the loaded configuration is immediately available upon module import.
---
**File: `angela/cli/files.py` (inferred from content)**
This file, `angela/cli/files.py`, defines a Typer-based command-line interface application for managing file and directory operations within the Angela CLI, offering an enhanced user experience through the `rich` library. It implements common filesystem commands such as `ls` (with detailed table views via `rich.Table` and color-coding), `mkdir`, `rmdir`, `touch`, `cat` (with syntax highlighting using `rich.Syntax` based on `context_manager.get_file_info`), `rm`, `cp`, `mv`, and `write` (with interactive content input via `rich.Prompt`). These commands primarily delegate their core logic to async functions from `angela.execution.filesystem` (e.g., `create_directory`, `read_file`) and use `angela.context.context_manager` for path resolution and file metadata. Advanced commands include `find` for pattern-based file searching via `context_manager.find_files` and `info` for displaying detailed file/directory information using `rich.Panel` and content previews. A crucial feature is the `rollback` command, which interacts with `angela.execution.rollback.rollback_manager` to list recent operations and allow users to undo them if backups are available, confirming actions with `rich.Confirm`.
---
**File: `angela/cli/workflows.py`**
This file, `angela/cli/workflows.py`, implements a Typer-based command-line interface for managing Angela workflows, allowing users to list, create, run, delete, show, export, and import these reusable command sequences. It relies on `angela.workflows.manager.workflow_manager` for core operations like defining workflows (interactively via `rich.Prompt` or from natural language in a file via `create`), executing them with variable substitution (`run`), and managing their lifecycle (`list`, `delete`, `show`). The `export` and `import` commands interface with `angela.workflows.sharing.workflow_sharing_manager` to package workflows into shareable `.angela-workflow` archives and to integrate received packages into the system, handling potential renames or replacements. User interaction is enhanced using the `rich` library, providing formatted tables for listing workflows (`rich.Table`), detailed panels for showing workflow steps via `terminal_formatter.display_workflow`, and interactive prompts/confirmations (`rich.Prompt`, `rich.Confirm`). The `run` command supports dry-run mode for previewing execution and accepts variables via command-line options (`--var NAME=VALUE`), which are then passed to the `workflow_manager` for execution within the current `context_manager` context.
---
**File: `angela/context/history.py`**
`angela/context/history.py` implements the `HistoryManager` class to record and analyze user command execution history, persisting data in `command_history.json` (for `CommandRecord`s) and `command_patterns.json` (for `CommandPattern`s) within the `CONFIG_DIR`. It uses a `CommandRecord` class to store detailed information about each command execution, including the command string, natural language request, success status, ISO-formatted timestamp, output, error, and risk level. The `CommandPattern` class tracks metrics like execution count, success rate, and last used timestamp for base commands (e.g., "git commit", extracted by `_extract_base_command`), which are updated via `_update_patterns` if `auto_learn_patterns` preference is enabled. `HistoryManager` loads history and patterns upon initialization, trims history based on `preferences_manager.preferences.context.max_history_items`, and provides methods to retrieve recent commands, query command frequency/success rates, and search for similar past commands using Jaccard similarity on tokenized natural requests. This historical data is crucial for features like adaptive confirmation scoring, command suggestions, and potentially for AI fine-tuning or error recovery within the Angela CLI.
---
**File: `angela/execution/rollback.py`**
This file, `angela/execution/rollback.py`, implements a `RollbackManager` to provide undo functionality for file and directory operations performed by the Angela CLI, storing its state in `operation_history.json`. It defines an `OperationRecord` class to store details of each recorded action, including its type (e.g., "create_file", "delete_directory"), parameters, ISO-formatted timestamp, and the path to any created backup within the `BACKUP_DIR`. The manager loads this history on initialization and saves updates when `record_operation` is called after a filesystem action. The `rollback_operation` async method is central to the undo logic: based on the `operation_type` and `backup_path` from the selected `OperationRecord`, it reverses the original action, such as deleting a created file or restoring a deleted file/directory from its backup using `shutil` functions like `copy2` or `copytree`. Users can view recent undoable operations (formatted by `_get_operation_description`) via `get_recent_operations`, and upon successful rollback, the manager truncates the history to reflect the undone state.
---
**File: `angela/monitoring/network_monitor.py`**
`angela/monitoring/network_monitor.py` defines the `NetworkMonitor` class, responsible for proactively overseeing various network-related aspects such as local service availability, project dependency updates, and general internet connectivity. It utilizes `asyncio` to run concurrent monitoring tasks: `_monitor_local_services` periodically checks common service ports (e.g., 8000, 5432 using `socket.connect_ex` and `aiohttp` for HTTP checks), tailored by project type from `context_manager`. The `_monitor_dependency_updates` task checks for new versions of Python packages (via `pip list --outdated --format=json`) and Node.js packages (via `npm outdated --json`) relevant to the current project by running these as shell commands using the `_run_command` async helper. General network health is assessed by `_monitor_network_connectivity`, which attempts to resolve well-known domains using `asyncio.get_event_loop().getaddrinfo`. When issues or updates are detected, and a suggestion cooldown period managed by `_can_show_suggestion` and `_last_suggestion_time` has passed, the monitor uses `terminal_formatter.print_proactive_suggestion` to inform the user, enhancing Angela's proactive assistance capabilities.
---
**File: `angela/safety/adaptive_confirmation.py`**
`angela/safety/adaptive_confirmation.py` implements an intelligent user confirmation system that adapts its prompting behavior based on command risk, execution history, and user preferences. It leverages `prompt_toolkit`'s `yes_no_dialog` for interactive user consent and `rich` library components (`Console`, `Panel`, `Syntax`, `Table`) for presenting detailed, styled information, including command syntax, risk levels (using `CONFIRMATION_STYLES`), impact analysis, and previews. The core async function `get_adaptive_confirmation` determines whether to auto-execute a command by consulting `preferences_manager` (for trusted commands) and `history_manager` (for frequency/success rate), or to proceed with explicit user prompting. It tailors the confirmation UI: `_get_detailed_confirmation` is used for high/critical risk operations showing comprehensive details and impact tables, while `_get_simple_confirmation` provides a more concise prompt for lower risks; dry runs (`_show_dry_run_preview`) bypass confirmation entirely. After a successful execution of certain commands, `offer_command_learning` may prompt the user to add the command to their trusted list, further personalizing the safety behavior.
---
**File: `angela/safety/classifier.py`**
`angela/safety/classifier.py` is responsible for assessing the potential danger of shell commands by classifying their risk level and analyzing their likely impact on the system. The `classify_command_risk` function determines a command's risk by matching it against a predefined dictionary of regular expressions, `RISK_PATTERNS` (mapping risk levels to regexes and reasons, e.g., `rm -rf` as CRITICAL) and `OVERRIDE_PATTERNS` (for special cases), categorizing it from "SAFE" to "CRITICAL" as defined in `angela.constants.RISK_LEVELS`. These patterns are designed to identify operations like file deletion, package management, privileged execution (`sudo`), or disk formatting, assigning appropriate risk scores. The `analyze_command_impact` function performs a lexical analysis of the command using `shlex.split` to tokenize it, then heuristically identifies potential operations (e.g., delete, create, read), affected files/directories, and flags if the command is destructive or modifies/creates files. This classification and impact analysis data is then consumed by other safety modules to determine the appropriate level of user confirmation and the details to display.
---
**File: `angela/safety/confirmation.py`**
`angela/safety/confirmation.py` manages the user interaction phase of the safety workflow, presenting detailed information about a command and prompting for explicit user approval before execution. The `requires_confirmation` function determines if a prompt is necessary by checking the command's risk level against `DEFAULT_CONFIRMATION_REQUIREMENTS` and global user settings like `config_manager.config.user.confirm_all_actions`. The `get_confirmation` async function orchestrates the display using the `rich` library, showing the command with `Syntax` highlighting, its `risk_level` (colored using `RISK_COLORS` and named via `RISK_LEVEL_NAMES`), the `risk_reason`, an impact analysis table generated by `format_impact_analysis`, and any available `preview`. It utilizes `rich.Panel` for structuring information clearly and `rich.Confirm.ask` to get a boolean (yes/no) response from the user regarding whether to proceed with the command. For "CRITICAL" risk operations or if `dry_run` is true, additional specific warnings or informational messages are displayed to ensure the user is fully aware of the context and potential consequences.
---
**File: `angela/safety/preview.py`**
`angela/safety/preview.py` is dedicated to generating predictive textual summaries of what shell commands are expected to do, without actually executing them, to aid user decision-making during safety checks. It features a main `generate_preview` async function that dispatches to command-specific async handlers (e.g., `preview_mkdir`, `preview_rm`, `preview_ls`) listed in the `PREVIEWABLE_COMMANDS` dictionary, based on the parsed command (tokenized using `shlex.split`). These specific preview functions, like `preview_rm` or `preview_cp`, analyze command arguments, check file/directory existence and types using `pathlib.Path`, expand glob patterns using `glob.glob`, and then construct a human-readable summary of intended actions, including warnings for overwrites or non-existent paths. For example, `preview_cat` estimates file size and line count for text files or warns about displaying binary content, while `preview_find` describes the search scope and criteria. If no specialized previewer exists for a command, `generic_preview` attempts to execute it with common dry-run flags (e.g., `--dry-run`, `--print`) using `execution_engine.execute_command`, capturing its output as the preview.
---
**File: `angela/safety/validator.py`**
`angela/safety/validator.py` enforces safety policies by validating commands and operations against a set of predefined rules and system constraints before they can proceed to execution. The `validate_command_safety` function scrutinizes raw shell commands, checking them against `DANGEROUS_PATTERNS` (a list of regexes for forbidden actions like `rm -rf /` or `mkfs /dev/sda`) and ensuring necessary superuser privileges (checked via `is_superuser` which uses `os.geteuid`, and `ROOT_PATTERNS` regex list) are met if the current user is not root. The `validate_operation` function handles abstracted operations (e.g., 'create_file', 'delete_directory'), leveraging `check_file_permission` (which employs `os.access`) to verify read/write permissions for specified `Path` objects and checking against deletion of system directories. It defines a custom `ValidationError` exception, although it is not explicitly raised in the provided snippet, it's implied for severe validation failures not handled by returning a boolean and message. This module acts as a critical first line of defense, aiming to prevent overtly harmful or permission-violating operations from even being considered for execution by the Angela CLI.
---
**File: `angela/execution/error_recovery.py`:**
The `angela/execution/error_recovery.py` file implements the `ErrorRecoveryManager` class, a sophisticated system designed to intelligently handle and attempt recovery from errors encountered during multi-step command executions within the Angela CLI. Its primary entry point, the async `handle_error` method, orchestrates a sequence involving error information extraction (command, stderr), detailed analysis via `_analyze_error` which uses an external `error_analyzer` and internal regex patterns from `_get_common_error_patterns`, and generation of potential `RecoveryStrategy` enums (like `RETRY`, `MODIFY_COMMAND`, `PREPARE_ENV`) through `_generate_recovery_strategies`. Strategy generation is multi-faceted: `_generate_recovery_strategies` first attempts to derive actions by parsing fix suggestions from the error analysis using regex in `_parse_fix_suggestion` and `_create_strategy_from_pattern_fix`, and if these heuristic methods yield insufficient results, it queries the `gemini_client` via `_generate_ai_recovery_strategies` with a structured JSON-expecting prompt for AI-driven solutions. These strategies, which are standard Python dictionaries adhering to a defined structure including type, command, description, and confidence, are then sorted by their confidence score, always including fallback options like simple retry or skipping the problematic step.
Automatic recovery, determined by `_can_auto_recover`, is attempted for high-confidence strategies or those with a proven success record tracked in the internal `_recovery_history` dictionary (which logs successful strategy applications like `{ "type:command": {"success_count": N}}`). The `_execute_recovery_strategy` async method performs the actual recovery by dispatching actions based on the strategy type, often involving command execution via the `angela.execution.engine.execution_engine`, where safety checks might be selectively skipped for retries or enforced for newly generated commands. Notably, strategies such as `PREPARE_ENV` can execute a preparatory command (e.g., `mkdir` for a missing directory, or `apt-get install` for a missing package) and, if successful and indicated by a `retry_original` flag within the strategy dictionary, will subsequently re-attempt the original failed command. If automatic recovery isn't viable or fails, `_guided_recovery` presents the generated strategies to the user via `terminal_formatter` for display and uses `prompt_toolkit.input_dialog` to capture their choice, subsequently executing the selected strategy. The system thus combines heuristic rule-based error matching, AI-powered suggestion generation, a learning mechanism from past successful recoveries, and user-guided intervention to maximize the chances of successful task completion.
----------
**Summary for `BackgroundMonitor.py`:**
This Python module implements an asynchronous `BackgroundMonitor` class for the Angela CLI, designed to proactively assist users by observing system state and activities through concurrent, restartable `asyncio` tasks. It actively monitors Git repository status by periodically running `git status -s`, analyzing changes like modified, untracked, or deleted files, and then generating contextual suggestions for commits or additions if significant activity is detected, subject to a cooldown period and repetition avoidance. The monitor also tracks file modifications within the project, identified by `_find_source_files`, and upon detecting changes, it invokes language-specific checkers such as `python -m py_compile` and `flake8` for Python or `node --check` and `eslint` for JavaScript to report syntax errors and linting issues. System resource monitoring is partially implemented, focusing on disk usage by executing platform-specific commands (`wmic` or `df`) via an asynchronous `_run_command` utility, and it alerts the user if usage exceeds a threshold and has significantly increased. All suggestions are managed to prevent repetition using a set of seen suggestions and are displayed via `terminal_formatter`, with overall monitoring controlled by `start_monitoring` and `stop_monitoring` methods that manage the lifecycle of the underlying `asyncio` tasks.
**Summary for `ProjectInference.py`**
The `ProjectInference` Python module provides advanced, asynchronous capabilities to analyze a given project directory, aiming to deduce its primary type, technological stack, dependencies, and structural characteristics using file system inspection and pattern matching. It determines the project type (e.g., Python, Node, Java) by scoring matches against predefined `PROJECT_SIGNATURES` that include characteristic files (like `requirements.txt` or `package.json`), directories (e.g., `venv`, `node_modules`), and common file extensions, capable of identifying mixed-type projects by combining high-scoring candidates. Framework detection is achieved by cross-referencing project files and contents against `FRAMEWORK_SIGNATURES` (e.g., identifying Django via `manage.py` or React via `package.json` dependencies) and by directly parsing dependency lists from files like `requirements.txt` or `package.json` using dedicated analyzer methods. The module meticulously extracts project dependencies by parsing specific manifest files corresponding to the detected project type, such as `requirements.txt`, `setup.py`, and `pyproject.toml` for Python projects, or `package.json` for Node.js projects, detailing dependency names, version specifiers, and source files. It identifies and lists important project files, including signature files that confirmed the project type, common documentation like READMEs and LICENSEs, and attempts to locate potential entry point files (e.g., `main.py`, `index.js`) based on conventional naming within the project structure. Project structure analysis involves counting files by extension, identifying prominent subdirectories (ignoring common ones like `.git` or `node_modules`), and generating a hierarchical, depth-limited tree representation of the directory layout using the recursive `_generate_directory_structure` method. All inferred information, including project root, type, detected files, frameworks, dependencies, and structural details, is compiled into a comprehensive dictionary, with results cached per project root path to optimize subsequent analyses of the same project.
**`angela/cli/file_extensions.py`**
This module extends Angela's command-line interface (CLI) with advanced file-related functionalities, operating within the broader "Angela" application by interfacing with its context management, file resolution, and activity tracking systems. Its primary purpose is to empower users with sophisticated commands for resolving ambiguous file references, extracting file paths from text, viewing recently or frequently accessed files, and inspecting detailed project information. Technically, it leverages `typer` for command-line argument parsing and command definition, and `rich` for creating user-friendly, formatted console output like tables and panels, while also using `asyncio` to run asynchronous operations provided by other Angela modules. Key commands include `resolve` which uses `file_resolver.resolve_reference` and logs views with `file_activity_tracker`, and `extract` which processes text to find and display file references. The `recent` and `active` commands query the `file_activity_tracker` to present historical file interaction data in structured tables. Finally, the `project` command utilizes `context_manager` and `context_enhancer` to gather and present a rich summary of the current project's attributes, dependencies, and structure.
**`angela/cli/files.py`**
This file defines core file and directory manipulation commands for the Angela CLI, offering an enhanced, context-aware alternative to standard shell utilities by integrating with Angela's `context_manager` and `rollback_manager`. It aims to provide a comprehensive suite of operations such as listing (`ls`), creating (`mkdir`, `touch`), deleting (`rmdir`, `rm`), copying (`cp`), moving (`mv`), reading (`cat`), and writing (`write`) files, all augmented with rich console output and Angela's operational oversight. The module heavily relies on `typer` for its command structure and `rich` for visually appealing outputs, including tables, syntax-highlighted file previews, and interactive prompts; asynchronous filesystem operations are imported from `angela.execution.filesystem`. Commands like `ls` offer detailed views with file types and sizes, while `cat` provides syntax highlighting based on language detection from `context_manager`. Operations such as `mkdir` or `rm` include safety features like `--dry-run` and interact with `rollback_manager` to allow for undoing actions. Furthermore, it includes a `find` command for pattern-based file searching and an `info` command to display detailed metadata and previews for specified files or directories.
**`angela/context/history.py`**
The `angela/context/history.py` module provides the `HistoryManager` class, a crucial component for logging and analyzing user command interactions within the Angela ecosystem, supporting features like command suggestions and usage pattern identification. Its purpose is to persistently record details of each executed command—including the raw command, natural language query, success status, output, and risk level—and to derive actionable insights from this historical data. Technical implementation involves `CommandRecord` and `CommandPattern` data classes, with history and learned patterns stored in JSON files (`command_history.json`, `command_patterns.json`) managed by `config_manager` and influenced by `preferences_manager`. The `add_command` method logs new entries and triggers `_update_patterns` which uses `_extract_base_command` to intelligently group commands (e.g., "git commit") and update their frequency and success rates. Retrieval methods like `get_recent_commands`, `get_command_frequency`, and `get_command_success_rate` allow access to this data. Advanced analysis functions include `search_similar_command` using Jaccard similarity on natural language requests and `get_common_command_contexts` to identify typical command sequences.
**`angela/context/file_activity.py`**
This module, `angela/context/file_activity.py`, implements the `FileActivityTracker` to monitor and log interactions with files and directories within the Angela environment, providing a basis for features like "recently used files" and project activity analysis. Its core purpose is to capture various file events such as creations, modifications, deletions, and views, associating them with metadata like timestamps, the triggering command, and custom details for later retrieval and analysis. The system uses an `ActivityType` enum to classify events and a `FileActivity` class to structure logged data, which is maintained in an in-memory list with a configurable maximum size and integrated with `session_manager` to enrich Angela's session context. The `track_activity` method and its specialized wrappers (e.g., `track_file_creation`) are central to logging, appending new `FileActivity` objects and updating the session. Users can query this log via methods like `get_recent_activities`, which returns time-sorted activities filterable by type, and `get_most_active_files`, which aggregates data to highlight frequently accessed files. The `FileActivity` class supports dictionary conversion for serialization, although the primary activity list is currently managed in memory.
---
**`angela/context/preferences.py`**
This module provides a `PreferencesManager` class to handle user-configurable settings for the Angela application, influencing behavior related to command trust, UI presentation, and context management. It utilizes Pydantic `BaseModel`s (`TrustPreferences`, `UIPreferences`, `ContextPreferences`, nested under `UserPreferences`) to define a structured, validated schema for all preferences, which are persistently stored and retrieved from a `preferences.json` file located via `config_manager`. The `PreferencesManager` loads these settings upon initialization, creates a default file if one doesn't exist, and provides methods to update and save any changes back to the JSON file. Key functionalities include the `should_auto_execute` method, which determines if a command requires user confirmation based on its risk level (from `angela.constants.RISK_LEVELS`) and the user's `TrustPreferences`, including lists of explicitly trusted or untrusted commands. Users can dynamically modify these trusted/untrusted command lists through dedicated methods like `add_trusted_command`. A global instance, `preferences_manager`, makes these settings readily accessible throughout the Angela application, allowing other components to adapt their behavior accordingly.
---
**1. `angela/ai/analyzer.py` (ErrorAnalyzer)**
This module implements an `ErrorAnalyzer` class within Angela's AI capabilities, tasked with diagnosing command-line execution errors to offer users insightful feedback and potential solutions. It operates by matching error messages against a predefined list of `ERROR_PATTERNS` (regex, explanations, suggestions), analyzing command syntax with `shlex`, and checking for issues related to file references like non-existence or permission problems. The analyzer also consults the `history_manager` to find previously successful fixes for similar errors, enabling a degree of learning. Key methods like `_extract_key_error` simplify verbose errors, while `_check_file_references` intelligently suggests corrections for mistyped paths by looking for similar filenames in the parent directory. The `_analyze_command_structure` method identifies common syntactic pitfalls, such as missing arguments or malformed flags. Finally, `generate_fix_suggestions` consolidates all findings into a de-duplicated list of actionable advice for the user.
-----
**`angela/ai/content_analyzer.py` (ContentAnalyzer)**
The `angela/ai/content_analyzer.py` module provides the `ContentAnalyzer` class, empowering Angela with AI-driven functionalities to understand, summarize, manipulate, and search within file contents based on natural language instructions. It interfaces with an AI service (`gemini_client`), constructing detailed, asynchronous prompts that incorporate file content (truncated for token limits), detected file type (via `detect_file_type`), and the user's specific request. Core asynchronous methods include `analyze_content` for deep understanding, `summarize_content` for brevity, `manipulate_content` which attempts to apply changes and generates a `difflib` diff, and `search_content` for natural language querying of file text. Private methods like `_build_analysis_prompt` tailor AI requests for different file types and tasks, while `_extract_modified_content` and `_parse_search_results` robustly handle parsing the AI's textual output, often looking for markdown code blocks or specific line number patterns. This system allows Angela to offer intelligent assistance directly related to the substance of user files.
----
**`angela/ai/intent_analyzer.py` (IntentAnalyzer)**
This module defines the `IntentAnalyzer` for Angela's natural language understanding, designed to interpret user requests, identify the core intent (e.g., "file_search", "git_operation"), extract relevant entities, and manage ambiguity, potentially through interactive clarification. It employs request normalization by correcting common misspellings defined in `SPELLING_VARIATIONS` and then uses `difflib.SequenceMatcher` for fuzzy matching against predefined `INTENT_PATTERNS` to determine the most likely intent and a confidence score, outputting an `IntentAnalysisResult` Pydantic model. Entity extraction, performed by `_extract_entities`, uses intent-specific regular expressions to capture parameters like file paths or search terms from the normalized input. If ambiguity arises (e.g., multiple intents with close scores), the `get_interactive_disambiguation` method can leverage `prompt_toolkit.radiolist_dialog` to ask the user for clarification, refining the analysis. This system aims for robust interpretation of user commands, accommodating variations and offering a mechanism for resolving uncertainty.
----
**`angela/execution/hooks.py` (ExecutionHooks)**
The `angela/execution/hooks.py` module introduces the `ExecutionHooks` class, which integrates into Angela's command execution pipeline to automatically track file activities and enrich contextual understanding without explicit logging at every interaction point. It provides asynchronous `pre_execute_command` and `post_execute_command` methods that analyze command strings and their output for file paths using string parsing and regex, subsequently logging interactions like views, creations, or modifications with the `file_activity_tracker` using appropriate `ActivityType` enums. Specific common commands (e.g., `cat`, `rm`, `cp`, `echo >`) are explicitly handled in `post_execute_command` to infer the correct activity type, resolving paths against the current working directory from the provided context. Furthermore, `post_execute_file_operation` is tailored for Angela's internal filesystem calls, ensuring actions like `create_file` or `write_file` are accurately logged, thus building a comprehensive history of file interactions.
----
**`angela/context/file_detector.py`**
This module is crucial for Angela's contextual awareness, providing the `detect_file_type` function to identify file characteristics such as general type (e.g., image, text, source_code), specific programming language, MIME type, and binary status. It employs a multi-layered detection strategy, using Python's `mimetypes` library, extensive predefined dictionaries like `LANGUAGE_EXTENSIONS` (e.g., `.py` to Python) and `FILENAME_MAPPING` (e.g., `Dockerfile` to Docker), and checking for common `SHEBANG_PATTERNS` in the first line of text files. A binary check is performed by searching for null bytes in an initial chunk of the file, and the module also offers a `get_content_preview` function to display the beginning of text files. This detailed file information is then used by other Angela components for tasks like syntax highlighting, forming appropriate AI prompts, or deciding on safe file handling procedures.
----
**`angela/intent/planner.py` (TaskPlanner)**
The `angela/intent/planner.py` module implements a `TaskPlanner` that uses an AI model (`gemini_client`) to break down high-level natural language user goals into a sequence of executable shell commands. It defines `PlanStep` and `TaskPlan` Pydantic models, where each step includes the command, an explanation, dependencies on previous steps, and an estimated risk level. The core `_build_planning_prompt` method constructs a detailed request for the AI, providing the goal, current environmental context (like CWD files via `context_manager`), and a JSON schema for the expected output. `_parse_plan_response` then processes the AI's output, converting it into a `TaskPlan`, with a fallback mechanism for parsing errors. This `TaskPlan` can then be converted to a simpler `ActionPlan` and executed via the `execution_engine`, enabling Angela to tackle complex, multi-step objectives.
---
**`angela/intent/advanced_planner.py` (AdvancedTaskPlanner)**
This module introduces the `AdvancedTaskPlanner`, an enhanced version of the task planner designed to handle more complex user goals by creating sophisticated execution plans that can include branching, conditional logic, and various step types. It defines `PlanStepType` (Enum), `AdvancedPlanStep` (supporting commands, code, file operations, API calls, decisions, loops), and `AdvancedTaskPlan` Pydantic models, allowing for a graph-like structure with unique step IDs and explicit entry points. The planner first determines the goal's complexity; for simple goals, it may delegate to the basic `task_planner`, while for complex ones, `_build_advanced_planning_prompt` instructs the `gemini_client` AI to generate a JSON plan with these advanced features. The `execute_plan` method then interprets this `AdvancedTaskPlan`, managing a queue of pending steps, respecting dependencies, and dispatching execution based on step type, with placeholder or simplified implementations for non-command steps and basic retry logic.
-------
# Current Project Tree/Structure
```bash
.
├── MD
│   ├── Info.md
│   ├── NextSteps.md
│   ├── Phase1.md
│   ├── Phase2.md
│   ├── Phase3.md
│   ├── Phase4.md
│   ├── Phase5.md
│   ├── Phase6.md
│   ├── Phase7.md
│   ├── context.md
│   └── tree.md
├── Makefile
├── README.md
├── angela
│   ├── __init__.py
│   ├── __main__.py
│   ├── ai
│   │   ├── analyzer.py
│   │   ├── client.py
│   │   ├── confidence.py
│   │   ├── content_analyzer.py
│   │   ├── content_analyzer_extensions.py
│   │   ├── file_integration.py
│   │   ├── intent_analyzer.py
│   │   ├── parser.py
│   │   ├── prompts.py
│   │   └── prompts_update.py
│   ├── cli
│   │   ├── __init__.py
│   │   ├── files.py
│   │   ├── files_extensions.py
│   │   ├── generation.py
│   │   ├── main.py
│   │   └── workflows.py
│   ├── cli.py
│   ├── config.py
│   ├── constants.py
│   ├── context
│   │   ├── __init__.py
│   │   ├── enhancer.py
│   │   ├── file_activity.py
│   │   ├── file_detector.py
│   │   ├── file_resolver.py
│   │   ├── history.py
│   │   ├── manager.py
│   │   ├── preferences.py
│   │   ├── project_inference.py
│   │   └── session.py
│   ├── execution
│   │   ├── adaptive_engine.py
│   │   ├── engine.py
│   │   ├── error_recovery.py
│   │   ├── filesystem.py
│   │   ├── hooks.py
│   │   └── rollback.py
│   ├── generation
│   │   ├── architecture.py
│   │   ├── documentation.py
│   │   ├── engine.py
│   │   ├── frameworks.py
│   │   ├── planner.py
│   │   └── validators.py
│   ├── integrations
│   │   ├── integrations5.py
│   │   └── integrations6.py
│   ├── intent
│   │   ├── advanced_planner.py
│   │   ├── models.py
│   │   └── planner.py
│   ├── monitoring
│   │   ├── __init__.py
│   │   ├── background.py
│   │   └── network_monitor.py
│   ├── orchestrator.py
│   ├── review
│   │   ├── diff_manager.py
│   │   └── feedback.py
│   ├── safety
│   │   ├── __init__.py
│   │   ├── adaptive_confirmation.py
│   │   ├── classifier.py
│   │   ├── confirmation.py
│   │   ├── preview.py
│   │   └── validator.py
│   ├── shell
│   │   ├── angela.bash
│   │   ├── angela.zsh
│   │   └── formatter.py
│   ├── toolchain
│   │   ├── ci_cd.py
│   │   ├── git.py
│   │   └── package_managers.py
│   ├── utils
│   │   ├── __init__.py
│   │   └── logging.py
│   └── workflows
│       ├── __init__.py
│       ├── manager.py
│       └── sharing.py
├── pyproject.toml
├── pytest.ini
├── requirements.txt
├── scripts
│   ├── install.sh
│   └── uninstall.sh
├── setup.py
├── test_frameworks.py
└── tests
    ├── __init__.py
    ├── conftest.py
    ├── test_ai_client.py
    ├── test_context.py
    ├── test_context_enhancer.py
    ├── test_execution.py
    ├── test_file_activity.py
    ├── test_file_detector.py
    ├── test_file_resolver.py
    ├── test_filesystem.py
    ├── test_integration.py
    ├── test_orchestration.py
    ├── test_prompt_building.py
    ├── test_response_parsing.py
    └── test_safety.py

19 directories, 106 files

```
</file>

<file path="MD/Phase3.md">
# Angela-CLI: Phase 3 Implementation

## What We've Accomplished

In Phase 3, we've successfully implemented the Safety System and File Operations components of Angela-CLI. This phase represents a significant advancement in the project's capabilities, allowing it to safely manipulate files and directories based on user requests.

### Milestone 3A: Safety System

✅ **Risk Classification System**
- Created a comprehensive risk classifier that categorizes commands based on their potential impact (SAFE, LOW, MEDIUM, HIGH, CRITICAL)
- Implemented command impact analysis to identify affected files and directories
- Developed pattern matching for potentially dangerous command detection

✅ **Confirmation Interface with Previews**
- Built an interactive confirmation interface that scales with risk level
- Added color-coded risk indicators and detailed explanations
- Implemented command previews to show what operations will happen before execution

✅ **Command Impact Analysis**
- Created a system to analyze the effects of commands on files and directories
- Implemented detection of file creation, modification, and deletion operations
- Added support for analyzing complex commands with multiple operations

✅ **Permission Model Implementation**
- Implemented validation against system directory modifications
- Added checks for proper file and directory permissions
- Created safeguards against operations requiring root privileges

✅ **Dry-Run Capability**
- Added a dry-run mode that simulates command execution without actual changes
- Implemented detailed preview of what would happen during command execution
- Built support for native dry-run flags in tools that support them (like rsync)

### Milestone 3B: File Operations

✅ **Directory Operations**
- Implemented creation, deletion, and manipulation of directories
- Added support for recursive operations with proper safety checks
- Created enhanced directory listing with file type detection

✅ **File Creation Operations**
- Implemented file creation, writing, and appending operations
- Added support for content injection during file creation
- Built utilities for handling text and binary file content

✅ **Simple Content Viewing**
- Implemented file content viewing with syntax highlighting
- Added support for binary file handling
- Created preview capabilities for large files

✅ **Enhanced Context with File Type Detection**
- Built a sophisticated file type detection system based on extensions, content, and markers
- Added programming language detection for source code files
- Implemented MIME type and binary detection

✅ **Basic Rollback Capability**
- Created an operation history tracking system
- Implemented file and directory backup before modifications
- Built rollback functionality to undo previous operations

## Architecture Overview

The Phase 3 implementation follows a modular architecture:

```
angela/
├── safety/
│   ├── classifier.py       # Risk classification system
│   ├── confirmation.py     # User confirmation interface
│   ├── preview.py          # Command preview generation
│   ├── validator.py        # Safety validation
│   └── __init__.py         # Unified safety interface
├── execution/
│   ├── engine.py           # Command execution engine
│   ├── filesystem.py       # File operations
│   └── rollback.py         # Operation tracking and rollback
├── context/
│   ├── manager.py          # Context management
│   └── file_detector.py    # File type detection
├── ai/
│   ├── file_integration.py # AI-to-file-ops bridge
│   └── prompts.py          # Enhanced prompts for file operations
└── cli/
    ├── files.py            # File operation commands
    └── main.py             # Main CLI interface
```

## Key Features and Innovations

### 1. Intelligent Safety System

The safety system intelligently classifies commands based on their risk level and potential impact. It analyzes commands to identify affected files and directories, and presents this information to the user in a clear, color-coded interface. For higher-risk operations, it requires explicit confirmation, showing exactly what will happen before execution.

### 2. Command Preview Generation

One of the most powerful features is the ability to preview what commands will do before they are executed. For example:

- When running `rm -rf directory`, it shows how many files will be deleted
- When running `mv source dest`, it shows whether the destination already exists and would be overwritten
- When running `mkdir -p path`, it shows what directories will be created

This preview system makes it much safer to run complex commands, reducing the risk of unintended consequences.

### 3. File Type Detection

The file detector can identify:

- Programming languages (Python, JavaScript, Ruby, etc.)
- Configuration files (JSON, YAML, TOML, etc.)
- Project-specific files (package.json, requirements.txt, etc.)
- Binary files with MIME type detection
- Files with shebang lines for executable scripts

This enhances the context awareness for operations and enables more intelligent file handling.

### 4. Rollback Capability

The rollback system keeps track of operations and creates backups of files and directories before modification. This allows users to undo changes if they realize they made a mistake. The rollback history is maintained across sessions, providing a safety net for operations.

### 5. File Operation Bridge

The file operation bridge extracts high-level file operations from shell commands, enabling Angela to understand the intent behind commands and execute them safely. This bridges the gap between natural language, shell commands, and actual file operations.

## Usage Examples

Here are some examples of how to use the new capabilities:

### File Operations Commands

```bash
# Create a directory
angela files mkdir my_project

# Create a file
angela files touch my_project/README.md

# Write content to a file
angela files write my_project/README.md -c "# My Project\n\nThis is a sample project."

# List directory contents with details
angela files ls my_project -l

# Display file content with syntax highlighting
angela files cat my_project/README.md

# Copy a file
angela files cp my_project/README.md my_project/README.backup.md

# Move a file
angela files mv my_project/README.backup.md my_project/docs/README.md

# Delete a file
angela files rm my_project/temp.txt

# Remove a directory
angela files rmdir my_project/temp_dir
```

### Natural Language Commands

```bash
# Create a project structure
angela "Create a Python project structure with src, tests, and docs directories"

# Find and manipulate files
angela "Find all Python files with TODO comments and list them"

# Edit files
angela "Change all instances of 'old_function' to 'new_function' in Python files"

# Set up configurations
angela "Create a basic .gitignore file for a Python project"
```

### Safety Features

```bash
# Dry run to see what would happen without making changes
angela --dry-run "Delete all log files older than 30 days"

# View recent operations
angela files rollback --list

# Rollback a previous operation
angela files rollback --id 5
```


## Phase 3 testing results
The core functionality of Angela-CLI seems intact despite these test failures. The failures are primarily due to:

Test environment limitations (stdin capture)
Mismatches between test expectations and implementation details
Test code issues (imports, mock expectations)

Since 22 tests passed and all the core component tests for context management, AI client functionality, and orchestration are working, you can confidently move forward with Phase 4 while gradually improving the tests as needed.


## results/output of some phase 3 test commands
angela --dry-run "Delete all log files older than 30 days"

Loading configuration from: /home/yoshi/.config/angela/config.toml
2025-05-06 01:03:02.477 | DEBUG    | angela.context.manager:_detect_project_root:67 - Project detected: git at /home/yoshi/test3/angela-cli                                   
2025-05-06 01:03:02.477 | DEBUG    | angela.context.manager:refresh_context:40 - Context refreshed: cwd=/home/yoshi/test3/angela-cli, project_root=/home/yoshi/test3/angela-cli                                                                                      
2025-05-06 01:03:02.811 | DEBUG    | angela.ai.client:_setup_client:42 - Gemini API client initialized with model: gemini-2.5-pro-exp-03-25                                   
2025-05-06 01:03:02 | INFO | Processing request: Delete all log files older than 30 days
2025-05-06 01:03:02 | INFO | Sending request to Gemini API
2025-05-06 01:03:07 | INFO | Received suggestion: find . -name '*.log' -type f -mtime +30 -delete
2025-05-06 01:03:07 | INFO | Dry run suggested command: find . -name '*.log' -type f -mtime +30 -delete
2025-05-06 01:03:07 | INFO | Preparing to execute command: find . -name '*.log' -type f -mtime +30 -delete


╭──────────────────── Command ────────────────────╮
│ find . -name '*.log' -type f -mtime +30 -delete │
╰─────────────────────────────────────────────────╯
Risk Level: SAFE
Reason: Finding files
                                    Impact Analysis                                    
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ Aspect                                               ┃ Details                      ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│ Operations                                           │ read                         │
│ Affected Files                                       │ +30                          │
│                                                      │ *.log                        │
│                                                      │ .                            │
│                                                      │ f                            │
└──────────────────────────────────────────────────────┴──────────────────────────────┘
╭──────────────── Command Preview ────────────────╮
│ Will search in: . (8318 files, 833 directories) │
│ Looking for files matching: *.log               │
│ Filtering by type: files                        │
╰─────────────────────────────────────────────────╯
╭─────────────────────────────────────────────╮
│ This is a dry run. No changes will be made. │
╰─────────────────────────────────────────────╯
2025-05-06 01:03:07 | INFO | Command execution cancelled by user: find . -name '*.log' -type f -mtime +30 -delete
2025-05-06 01:03:07 | WARNING | Command execution cancelled due to safety concerns: find . -name '*.log' -type f -mtime +30 -delete
I suggest using this command:
╭──────────────────── Command ────────────────────╮
│ find . -name '*.log' -type f -mtime +30 -delete │
╰─────────────────────────────────────────────────╯

Explanation:
This command finds all files ending with '.log' in the current directory and its 
subdirectories that were last modified more than 30 days ago and deletes them. Warning:
This operation is destructive and will permanently delete the matching files.

Command Output:
Command failed
╭────────────────────── Error ───────────────────────╮
│ Command execution cancelled due to safety concerns │
╰────────────────────────────────────────────────────╯

-----
venv)─(yoshi㉿kali)-[~/test3/angela-cli]
└─$ angela "Create a Python project structure with src, tests, and docs directories"

Loading configuration from: /home/yoshi/.config/angela/config.toml
2025-05-06 01:04:18.946 | DEBUG    | angela.context.manager:_detect_project_root:67 - Project detected: git at /home/yoshi/test3/angela-cli                                   
2025-05-06 01:04:18.946 | DEBUG    | angela.context.manager:refresh_context:40 - Context refreshed: cwd=/home/yoshi/test3/angela-cli, project_root=/home/yoshi/test3/angela-cli                                                                                      
2025-05-06 01:04:19.222 | DEBUG    | angela.ai.client:_setup_client:42 - Gemini API client initialized with model: gemini-2.5-pro-exp-03-25                                   
2025-05-06 01:04:19 | INFO | Processing request: Create a Python project structure with src, tests, and docs directories
2025-05-06 01:04:19 | INFO | Sending request to Gemini API
2025-05-06 01:04:24 | INFO | Received suggestion: mkdir src tests docs
I suggest using this command:
╭────── Command ───────╮
│ mkdir src tests docs │
╰──────────────────────╯

Explanation:
This command creates three directories named 'src', 'tests', and 'docs' in the current 
working directory, which is a common structure for Python projects.
-----
t initialized with model: gemini-2.5-pro-exp-03-25                                   
2025-05-06 01:04:43 | INFO | Processing request: Create a basic .gitignore file for a Python project
2025-05-06 01:04:43 | INFO | Sending request to Gemini API
2025-05-06 01:04:58 | INFO | Received suggestion: cat <<EOF > .gitignore
# Byte-compiled / optimized / DLL files
__pycache__/
*.py[cod]
*$py.class

# C extensions
*.so

# Distribution / packaging
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
pip-wheel-metadata/
share/python-wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# PyInstaller
*.manifest
*.spec

# Installer logs
pip-log.txt
pip-delete-this-directory.txt

# Unit test / coverage reports
htmlcov/
.tox/
.nox/
.coverage
.coverage.*
.cache
nosetests.xml
coverage.xml
*.cover
*.py,cover
.hypothesis/
.pytest_cache/

# Environments
.env
.venv
env/
venv/
ENV/
env.bak/
venv.bak/

# IDEs / Editors
.idea/
.vscode/
.project
.pydevproject
.settings/

# OS generated files
.DS_Store
Thumbs.db

# Jupyter Notebook
.ipynb_checkpoints
EOF
I suggest using this command:
╭──────────────── Command ────────────────╮
│ cat <<EOF > .gitignore                  │
│ # Byte-compiled / optimized / DLL files │
│ __pycache__/                            │
│ *.py[cod]                               │
│ *$py.class                              │
│                                         │
│ # C extensions                          │
│ *.so                                    │
│                                         │
│ # Distribution / packaging              │
│ .Python                                 │
│ build/                                  │
│ develop-eggs/                           │
│ dist/                                   │
│ downloads/                              │
│ eggs/                                   │
│ .eggs/                                  │
│ lib/                                    │
│ lib64/                                  │
│ parts/                                  │
│ sdist/                                  │
│ var/                                    │
│ wheels/                                 │
│ pip-wheel-metadata/                     │
│ share/python-wheels/                    │
│ *.egg-info/                             │
│ .installed.cfg                          │
│ *.egg                                   │
│ MANIFEST                                │
│                                         │
│ # PyInstaller                           │
│ *.manifest                              │
│ *.spec                                  │
│                                         │
│ # Installer logs                        │
│ pip-log.txt                             │
│ pip-delete-this-directory.txt           │
│                                         │
│ # Unit test / coverage reports          │
│ htmlcov/                                │
│ .tox/                                   │
│ .nox/                                   │
│ .coverage                               │
│ .coverage.*                             │
│ .cache                                  │
│ nosetests.xml                           │
│ coverage.xml                            │
│ *.cover                                 │
│ *.py,cover                              │
│ .hypothesis/                            │
│ .pytest_cache/                          │
│                                         │
│ # Environments                          │
│ .env                                    │
│ .venv                                   │
│ env/                                    │
│ venv/                                   │
│ ENV/                                    │
│ env.bak/                                │
│ venv.bak/                               │
│                                         │
│ # IDEs / Editors                        │
│ .idea/                                  │
│ .vscode/                                │
│ .project                                │
│ .pydevproject                           │
│ .settings/                              │
│                                         │
│ # OS generated files                    │
│ .DS_Store                               │
│ Thumbs.db                               │
│                                         │
│ # Jupyter Notebook                      │
│ .ipynb_checkpoints                      │
│ EOF                                     │
╰─────────────────────────────────────────╯

Explanation:
This command creates a `.gitignore` file in the current directory and populates it with
a standard set of rules for Python projects. These rules tell Git to ignore common 
files like bytecode (`__pycache__`, `*.pyc`), virtual environments (`venv/`, `env/`), 
distribution artifacts (`dist/`, `build/`), IDE configuration (`.idea/`, `.vscode/`), 
and OS-specific files (`.DS_Store`).

---
└─$ python -m angela request --help
Loading configuration from: /home/yoshi/.config/angela/config.toml
2025-05-06 01:06:08.886 | DEBUG    | angela.context.manager:_detect_project_root:67 - Project detected: git at /home/yoshi/test3/angela-cli                                   
2025-05-06 01:06:08.886 | DEBUG    | angela.context.manager:refresh_context:40 - Context refreshed: cwd=/home/yoshi/test3/angela-cli, project_root=/home/yoshi/test3/angela-cli                                                                                      
2025-05-06 01:06:09.165 | DEBUG    | angela.ai.client:_setup_client:42 - Gemini API client initialized with model: gemini-2.5-pro-exp-03-25                                   
                                                                                       
 Usage: python -m angela request [OPTIONS] REQUEST_TEXT...                             
                                                                                       
 Send a natural language request to Angela.                                            
                                                                                       
                                                                                       
╭─ Arguments ─────────────────────────────────────────────────────────────────────────╮
│ *    request_text      REQUEST_TEXT...  The natural language request for Angela.    │
│                                         [default: None]                             │
│                                         [required]                                  │
╰─────────────────────────────────────────────────────────────────────────────────────╯
╭─ Options ───────────────────────────────────────────────────────────────────────────╮
│ --execute  -e        Execute the suggested command.                                 │
│ --dry-run            Preview command execution without making changes.              │
│ --help               Show this message and exit.                                    │
╰─────────────────────────────────────────────────────────────────────────────────────╯



## Next Steps

***************************IMPORTANT IMPORTANT**************************************
# ****************MOVING FORWARD: PROJECT DIRECTION FOR ANGELA-CLI********************

## PHASE 4 PRIORITY SHIFT: SEAMLESS EXPERIENCE OVER EXCESSIVE SAFETY

As we continue development of Angela-CLI, we need to rebalance our priorities to better align with the core vision of the project. While the safety framework we've built in Phase 3 provides a solid foundation, we now need to pivot toward creating the seamless AI terminal agent experience that was always the project's ultimate goal.

### Key Direction Changes:

1. **Default to Execution**: Change the default behavior to execute commands rather than just suggesting them. Users should explicitly opt-out of execution rather than opt-in.

2. **Streamline Confirmation Flow**: Only require explicit confirmation for truly high-risk operations. Low and medium risk operations should execute automatically with minimal friction.

3. **Context-Aware Safety**: Develop smarter risk assessment that considers user history and project context. If a user regularly performs certain operations, reduce friction for those specific actions.

4. **Progressive Trust System**: Implement a system that builds trust with the user over time, gradually reducing confirmation requirements as patterns of use emerge with high trust to begin with unless told otherwise, which goes back to what we said about sers should explicitly opt-out of execution rather than opt-in. which also means users shoud lexplcicilty opt low trust but it begins with HIGH trust

5. **Remember User Preferences**: Develop persistent settings for execution preferences, allowing the experience to be customized to each user's comfort level.

6. **Task Continuity**: Enable Angela to maintain context across multiple requests, allowing for more conversational interactions rather than isolated commands.
### Step 4: Intelligent Interaction & Contextual Execution
(Focus: Make single commands/simple sequences smarter, faster, and provide richer feedback. Enhance immediate context use.)
Enhanced NLU & Tolerant Parsing: Implement more sophisticated Natural Language Understanding (ai/parser.py, intent/analyzer.py) to handle more complex or slightly misspelled/ambiguous single commands or simple sequences. Introduce interactive clarification (safety/confirmation.py using prompt_toolkit) but only when confidence is low (e.g., below ~70% match or high ambiguity); otherwise, attempt the most likely interpretation to maintain flow.
Rich Feedback & Asynchronous Streaming: Integrate rich and asyncio deeply (execution/engine.py, shell/formatter.py) for real-time, well-formatted feedback during command execution. Provide progress indicators (spinners/bars), stream stdout/stderr asynchronously, and give clear status updates, making Angela feel highly responsive. Capture all output cleanly.
Context-Aware Adaptive Confirmation: Leverage project type, recent activity, and command history (context/manager.py) to dynamically adjust confirmation needs (safety/classifier.py, orchestrator.py). Frequently used, low-risk commands in familiar contexts execute with minimal friction, while riskier operations still get detailed previews (safety/preview.py), balancing seamlessness with safety. Add detailed command history tracking (context/history.py).
Intelligent Error Analysis & Basic Fix Suggestions: When commands fail, use the AI (ai/parser.py, execution/engine.py) to analyze stderr in context. Proactively suggest potential fixes, relevant commands (e.g., ls if a file isn't found, permission checks), or documentation lookups based on the error message and command attempted.
Enhanced File/Directory Operations & Context: Implement more robust and complex file/directory operations (execution/filesystem.py) building on Phase 3 basics (e.g., recursive operations, pattern matching). Enhance context (context/filesystem.py) with reliable file type detection and basic metadata understanding to inform AI suggestions and operations.
### Implementation Priorities:

- Modify `request` function in `angela/cli/main.py` to default `execute=True`
- Create a more sophisticated confirmation UI that doesn't interrupt workflow for safe operations
- Develop a context-aware execution engine that adapts based on usage patterns
- Build a user preferences system that persists across sessions
- We will do this as we work on Phase 4

The ultimate vision for Angela-CLI is to be a true AI agent for the terminal - not just a suggestion engine or command translator, but a seamless extension of the user's intent and optimized user, a code agent, not a "helper" it will be able to haev full control to begin with unless explcicilty told oterhwise, Safety remains important, but it should never come at the expense of the fluid, natural experience that makes AI assistants valuable.
</file>

<file path="angela/cli/main.py">
"""
Main command-line interface for Angela CLI.
"""
import sys
import asyncio
from typing import List, Optional

import typer
from rich.console import Console
from rich.panel import Panel
from rich.syntax import Syntax
from rich import print as rich_print

from angela import __version__
from angela.config import config_manager
from angela.context import context_manager
from angela.orchestrator import orchestrator
from angela.execution.engine import execution_engine
from angela.utils.logging import setup_logging, get_logger
from angela.shell.formatter import terminal_formatter, OutputType
from angela.ai.analyzer import error_analyzer
from angela.context.session import session_manager

# Create the app
app = typer.Typer(help="Angela: AI-powered command-line assistant")
logger = get_logger(__name__)
console = Console()


def version_callback(value: bool):
    """Display version information and exit."""
    if value:
        console.print(f"Angela CLI version: {__version__}")
        sys.exit(0)


@app.callback()
def main(
    debug: bool = typer.Option(
        False, "--debug", "-d", help="Enable debug mode"
    ),
    version: bool = typer.Option(
        False, "--version", "-v", callback=version_callback, help="Show version and exit"
    ),
    monitor: bool = typer.Option(
        False, "--monitor", "-m", help="Enable background monitoring for proactive assistance"
    ),
):
    """Angela: AI-powered command-line assistant"""
    # Set debug mode
    config_manager.config.debug = debug
    
    # Configure logging
    setup_logging(debug=debug)
    
    # Start background monitoring if requested
    if monitor:
        # Import here to avoid circular imports
        from angela.monitoring.background import background_monitor
        background_monitor.start_monitoring()


@app.command()
def request(
    request_text: List[str] = typer.Argument(
        ..., help="The natural language request for Angela."
    ),
    suggest_only: bool = typer.Option(
        False, "--suggest-only", "-s", help="Only suggest commands without executing."
    ),
    dry_run: bool = typer.Option(
        False, "--dry-run", help="Preview command execution without making changes."
    ),
    force: bool = typer.Option(
        False, "--force", "-f", help="Execute without confirmation, even for risky operations."
    ),
):
    """Send a natural language request to Angela."""
    # Combine all arguments into a single request string
    full_request = " ".join(request_text)
    
    try:
        # If forcing execution, set this in the session
        if force:
            session_manager.add_entity("force_execution", "preference", "true")
        
        # Process the request - note execute=True is now the default
        # Only switch to false if suggest_only is True
        execute = not suggest_only
        
        # Call the orchestrator to process the request
        result = asyncio.run(orchestrator.process_request(
            full_request, execute=execute, dry_run=dry_run
        ))
        
        if "suggestion" in result:
            suggestion = result["suggestion"]
            
            # Display the suggestion with rich formatting
            terminal_formatter.print_command(suggestion.command, title="Command")
            
            # Show confidence if available
            if "confidence" in result:
                confidence = result["confidence"]
                confidence_color = "green" if confidence > 0.8 else "yellow" if confidence > 0.6 else "red"
                console.print(f"[bold]Confidence:[/bold] [{confidence_color}]{confidence:.2f}[/{confidence_color}]")
            
            # Show explanation
            console.print("\n[bold]Explanation:[/bold]")
            console.print(suggestion.explanation)
            
            # Show execution results if executed
            if "execution" in result:
                execution = result["execution"]
                console.print("\n[bold]Command Output:[/bold]")
                
                if execution["success"]:
                    if execution["stdout"].strip():
                        terminal_formatter.print_output(
                            execution["stdout"],
                            OutputType.STDOUT,
                            title="Output"
                        )
                    else:
                        console.print("[green]Command executed successfully with no output.[/green]")
                else:
                    console.print("[bold red]Command failed[/bold red]")
                    if execution["stderr"].strip():
                        terminal_formatter.print_output(
                            execution["stderr"],
                            OutputType.STDERR,
                            title="Error"
                        )
                    
                    # Show error analysis if available
                    if "error_analysis" in result:
                        terminal_formatter.print_error_analysis(result["error_analysis"])
                        
                    # Show fix suggestions if available
                    if "fix_suggestions" in execution:
                        console.print("\n[bold]Suggested fixes:[/bold]")
                        for suggestion in execution["fix_suggestions"]:
                            console.print(f"• {suggestion}")
            
        else:
            # Fall back to simple response if no suggestion
            panel_content = result.get("response", "I couldn't process that request.")
            console.print(Panel(panel_content, title="Angela", expand=False))
        
        # In debug mode, show context information
        if config_manager.config.debug:
            context_text = "\n".join([f"{k}: {v}" for k, v in result["context"].items()])
            rich_print("[bold blue]Context:[/bold blue]")
            rich_print(context_text)
            
    except Exception as e:
        logger.exception("Error processing request")
        console.print(f"[bold red]Error:[/bold red] {str(e)}")
        if config_manager.config.debug:
            import traceback
            console.print(traceback.format_exc())
        sys.exit(1)


@app.command()
def init():
    """Initialize Angela CLI with configuration."""
    console.print("Initializing Angela CLI...")
    
    # Check if API key is already set
    if config_manager.config.api.gemini_api_key:
        console.print("[green]API key already configured.[/green]")
    else:
        console.print("Google Gemini API key is required for Angela to function.")
        api_key = typer.prompt("Enter your Gemini API key", hide_input=True)
        config_manager.config.api.gemini_api_key = api_key
    
    # Configure safety options
    confirm_all = typer.confirm("Require confirmation for all operations?", default=False)
    config_manager.config.user.confirm_all_actions = confirm_all
    
    # Configure project root
    set_project_root = typer.confirm("Set a default project root?", default=False)
    if set_project_root:
        project_root = typer.prompt("Enter the path to your default project root")
        config_manager.config.user.default_project_root = project_root
    
    # Save the configuration
    config_manager.save_config()
    
    console.print("[green]Configuration saved successfully![/green]")
    console.print("\nAngela CLI is now initialized. You can use the following commands:")
    console.print("  [blue]angela <your request>[/blue] - Process a natural language request")
    console.print("  [blue]angela files <command>[/blue] - Perform file operations")
    console.print("  [blue]angela --help[/blue] - Show help")


@app.command("status")
def show_status():
    """Show the status of Angela CLI features and components."""
    from rich.table import Table
    
    # Get integration status
    from angela.integrations import phase_integration
    status = asyncio.run(phase_integration.status())
    
    # Display general status
    console.print(Panel(
        f"Angela CLI - Phase {status['phase']}\n"
        f"{status['description']}",
        title="Status",
        expand=False
    ))
    
    # Display enabled features
    if status.get("enabled_features"):
        console.print("\n[bold]Enabled Features:[/bold]")
        for feature in status["enabled_features"]:
            console.print(f"• {feature}")
    
    # Display project information if available
    if status.get("project"):
        project = status["project"]
        console.print("\n[bold]Project Information:[/bold]")
        console.print(f"• Type: {project['type']}")
        if project.get("frameworks"):
            console.print(f"• Frameworks: {', '.join(project['frameworks'])}")
        if "dependencies_count" in project:
            console.print(f"• Dependencies: {project['dependencies_count']}")
    
    # Display network monitoring status if available
    if status.get("network_monitoring"):
        network = status["network_monitoring"]
        console.print("\n[bold]Network Monitoring:[/bold]")
        console.print(f"• Status: {network['status']}")
        console.print(f"• Services Monitored: {network['services_monitored']}")
        console.print(f"• Dependency Updates: {network['dependency_updates']}")
    
    console.print("\n[bold]System Information:[/bold]")
    console.print(f"• Current Directory: {context_manager.cwd}")
    if context_manager.project_root:
        console.print(f"• Project Root: {context_manager.project_root}")


@app.command()
def shell():
    """Launch an interactive shell with Angela."""
    from prompt_toolkit import PromptSession
    from prompt_toolkit.history import FileHistory
    from prompt_toolkit.auto_suggest import AutoSuggestFromHistory
    from prompt_toolkit.completion import WordCompleter
    
    # Create a history file
    history_file = config_manager.CONFIG_DIR / "shell_history.txt"
    history_file.parent.mkdir(parents=True, exist_ok=True)
    
    # Create session with history
    session = PromptSession(
        history=FileHistory(str(history_file)),
        auto_suggest=AutoSuggestFromHistory(),
    )
    
    console.print(Panel(
        "Welcome to Angela's interactive shell!\n"
        "Type your requests directly and press Enter.\n"
        "Type 'exit' or press Ctrl+D to exit.",
        title="Angela Interactive Shell",
        expand=False
    ))
    
    # Main interaction loop
    while True:
        try:
            # Get input from the user
            text = session.prompt("angela> ")
            
            # Check for exit command
            if text.lower() in ("exit", "quit", "bye"):
                break
            
            # Skip empty input
            if not text.strip():
                continue
            
            # Process the request
            result = asyncio.run(orchestrator.process_request(text))
            
            # Display the response
            if "suggestion" in result:
                suggestion = result["suggestion"]
                
                # Show the command suggestion
                console.print("[bold]I suggest:[/bold]")
                command_syntax = Syntax(suggestion.command, "bash", theme="monokai")
                console.print(Panel(command_syntax, title="Command", expand=False))
                
                # Show explanation
                console.print(suggestion.explanation)
                
                # Ask if the user wants to execute the command
                execute_command = typer.confirm("Execute this command?", default=False)
                if execute_command:
                    # Execute the command
                    stdout, stderr, return_code = asyncio.run(
                        execution_engine.execute_command(suggestion.command)
                    )
                    
                    # Display the results
                    if return_code == 0:
                        if stdout:
                            console.print(Panel(stdout, title="Output", expand=False))
                        else:
                            console.print("[green]Command executed successfully with no output.[/green]")
                    else:
                        console.print("[bold red]Command failed:[/bold red]")
                        if stderr:
                            console.print(Panel(stderr, title="Error", expand=False))
            
            else:
                # Fall back to simple response
                console.print(result.get("response", "I couldn't process that request."))
            
            # Add a separator between interactions
            console.print("─" * console.width)
            
        except KeyboardInterrupt:
            # Handle Ctrl+C gracefully
            continue
        except EOFError:
            # Handle Ctrl+D (exit)
            break
        except Exception as e:
            logger.exception("Error in interactive shell")
            console.print(f"[bold red]Error:[/bold red] {str(e)}")
    
    console.print("Goodbye!")
</file>

<file path="angela/orchestrator.py">
"""
Main orchestration service for Angela CLI.

This module coordinates all the components of Angela CLI, from receiving
user requests to executing commands with safety checks.
"""
import asyncio
import re
from typing import Dict, Any, Optional, List, Tuple, Union
from pathlib import Path
from enum import Enum

from angela.ai.client import gemini_client, GeminiRequest
from angela.ai.prompts import build_prompt
from angela.ai.parser import parse_ai_response, CommandSuggestion
from angela.ai.file_integration import extract_file_operation, execute_file_operation
from angela.ai.content_analyzer import content_analyzer
from angela.context import context_manager
from angela.context.session import session_manager
from angela.context.history import history_manager
from angela.execution.engine import execution_engine
from angela.execution.adaptive_engine import adaptive_engine
from angela.ai.analyzer import error_analyzer
from angela.ai.intent_analyzer import intent_analyzer
from angela.ai.confidence import confidence_scorer
from angela.intent.planner import task_planner
from angela.workflows.manager import workflow_manager
from angela.utils.logging import get_logger
from angela.shell.formatter import terminal_formatter, OutputType
from angela.intent.advanced_planner import advanced_task_planner
from angela.execution.error_recovery import ErrorRecoveryManager
from angela.context.enhancer import context_enhancer
from angela.context.file_resolver import file_resolver
from angela.context.file_activity import file_activity_tracker, ActivityType
from angela.execution.hooks import execution_hooks

logger = get_logger(__name__)

class RequestType(Enum):
    """Types of requests that can be handled by the orchestrator."""
    COMMAND = "command"                # Single command suggestion
    MULTI_STEP = "multi_step"          # Multi-step operation
    FILE_CONTENT = "file_content"      # File content analysis/manipulation
    WORKFLOW_DEFINITION = "workflow"   # Define a new workflow
    WORKFLOW_EXECUTION = "run_workflow" # Execute a workflow
    CLARIFICATION = "clarification"    # Request for clarification
    UNKNOWN = "unknown"                # Unknown request type

class Orchestrator:
    """Main orchestration service for Angela CLI."""
    
    def __init__(self):
        """Initialize the orchestrator."""
        self._logger = logger
        self._background_tasks = set()
        self._error_recovery_manager = ErrorRecoveryManager()
    
    async def process_request(
        self, 
        request: str, 
        execute: bool = True,
        dry_run: bool = False
    ) -> Dict[str, Any]:
        '''
        Process a request from the user with enhanced context.
        
        Args:
            request: The user request
            execute: Whether to execute commands
            dry_run: Whether to simulate execution without making changes
            
        Returns:
            Dictionary with processing results
        '''
        # Refresh context to ensure we have the latest information
        context_manager.refresh_context()
        context = context_manager.get_context_dict()
        
        # Add session context for continuity across requests
        session_context = session_manager.get_context()
        context["session"] = session_context
        
        # Enhance context with project information, dependencies, and recent activity
        context = await context_enhancer.enrich_context(context)
        
        self._logger.info(f"Processing request: {request}")
        self._logger.debug(f"Enhanced context with {len(context)} keys")
        
        # Extract and resolve file references
        file_references = await file_resolver.extract_references(request, context)
        if file_references:
            # Add resolved file references to context
            context["resolved_files"] = [
                {"reference": ref, "path": str(path) if path else None}
                for ref, path in file_references
            ]
            self._logger.debug(f"Resolved {len(file_references)} file references")
        
        try:
            # Analyze the request to determine its type
            request_type = await self._determine_request_type(request, context)
            self._logger.info(f"Determined request type: {request_type.value}")
            
            # Process the request based on its type
            if request_type == RequestType.COMMAND:
                # Handle single command request
                return await self._process_command_request(request, context, execute, dry_run)
                
            elif request_type == RequestType.MULTI_STEP:
                # Handle multi-step operation
                return await self._process_multi_step_request(request, context, execute, dry_run)
                
            elif request_type == RequestType.FILE_CONTENT:
                # Handle file content analysis/manipulation
                return await self._process_file_content_request(request, context, execute, dry_run)
                
            elif request_type == RequestType.WORKFLOW_DEFINITION:
                # Handle workflow definition
                return await self._process_workflow_definition(request, context)
                
            elif request_type == RequestType.WORKFLOW_EXECUTION:
                # Handle workflow execution
                return await self._process_workflow_execution(request, context, execute, dry_run)
                
            elif request_type == RequestType.CLARIFICATION:
                # Handle request for clarification
                return await self._process_clarification_request(request, context)
                
            else:
                # Handle unknown request type
                return await self._process_unknown_request(request, context)
            
        except Exception as e:
            self._logger.exception(f"Error processing request: {str(e)}")
            # Fallback behavior
            return {
                "request": request,
                "response": f"An error occurred while processing your request: {str(e)}",
                "error": str(e),
                "context": context,
            }
    
    async def _determine_request_type(
        self, 
        request: str, 
        context: Dict[str, Any]
    ) -> RequestType:
        """
        Determine the type of request.
        
        Args:
            request: The user request
            context: Context information
            
        Returns:
            RequestType enum value
        """
        # Check for keywords and patterns indicating different request types
        
        # Workflow definition patterns
        workflow_def_patterns = [
            r'\b(?:define|create|make|add)\s+(?:a\s+)?(?:new\s+)?workflow\b',
            r'\bworkflow\s+(?:called|named)\b',
            r'\bsave\s+(?:this|these)\s+(?:as\s+(?:a\s+)?)?workflow\b',
        ]
        
        # Workflow execution patterns
        workflow_exec_patterns = [
            r'\brun\s+(?:the\s+)?workflow\b',
            r'\bexecute\s+(?:the\s+)?workflow\b',
            r'\bstart\s+(?:the\s+)?workflow\b',
        ]
        
        # File content patterns
        file_content_patterns = [
            r'\b(?:analyze|understand|summarize|examine)\s+(?:the\s+)?(?:content|code|text)\b',
            r'\b(?:modify|change|update|edit|refactor)\s+(?:the\s+)?(?:content|code|text|file)\b',
            r'\bfind\s+(?:in|inside|within)\s+(?:the\s+)?file\b',
        ]
        
        # Multi-step operation patterns
        multi_step_patterns = [
            r'\b(?:multiple steps|sequence|series|several|many)\b',
            r'\band then\b',
            r'\bafter that\b',
            r'\bone by one\b',
            r'\bstep by step\b',
            r'\bautomatically\b',
        ]
        
        # Check for workflow definition
        for pattern in workflow_def_patterns:
            if re.search(pattern, request, re.IGNORECASE):
                return RequestType.WORKFLOW_DEFINITION
        
        # Check for workflow execution
        for pattern in workflow_exec_patterns:
            if re.search(pattern, request, re.IGNORECASE):
                return RequestType.WORKFLOW_EXECUTION
        
        # Check for file content analysis/manipulation
        file_mentions = re.search(r'\b(?:file|code|script|document)\b', request, re.IGNORECASE)
        if file_mentions:
            for pattern in file_content_patterns:
                if re.search(pattern, request, re.IGNORECASE):
                    return RequestType.FILE_CONTENT
        
        # Check for multi-step operation
        complexity_indicators = sum(bool(re.search(pattern, request, re.IGNORECASE)) for pattern in multi_step_patterns)
        if complexity_indicators >= 1 or len(request.split()) > 15:
            # Complex requests or longer instructions often imply multi-step operations
            return RequestType.MULTI_STEP
        
        # Default to single command
        return RequestType.COMMAND
    
    async def _process_command_request(
        self, 
        request: str, 
        context: Dict[str, Any], 
        execute: bool, 
        dry_run: bool
    ) -> Dict[str, Any]:
        """
        Process a single command request.
        
        Args:
            request: The user request
            context: Context information
            execute: Whether to execute the command
            dry_run: Whether to simulate execution without making changes
            
        Returns:
            Dictionary with processing results
        """
        # Analyze intent with enhanced NLU
        intent_result = await intent_analyzer.analyze_intent(request)
        
        # Check if we've seen a similar request before
        similar_command = history_manager.search_similar_command(request)
        
        # Get command suggestion from AI
        suggestion = await self._get_ai_suggestion(
            request, 
            context, 
            similar_command, 
            intent_result
        )
        
        # Score confidence in the suggestion
        confidence = confidence_scorer.score_command_confidence(
            request=request,
            command=suggestion.command,
            context=context
        )
        
        # If confidence is low, offer clarification
        if confidence < 0.6 and not dry_run and not session_context.get("skip_clarification"):
            # Interactive clarification
            from prompt_toolkit.shortcuts import yes_no_dialog
            should_proceed = yes_no_dialog(
                title="Low Confidence Suggestion",
                text=f"I'm not very confident this is what you meant:\n\n{suggestion.command}\n\nWould you like to proceed with this command?",
            ).run()
            
            if not should_proceed:
                return {
                    "request": request,
                    "response": "Command cancelled due to low confidence.",
                    "context": context,
                }
        
        result = {
            "request": request,
            "suggestion": suggestion,
            "confidence": confidence,
            "intent": intent_result.intent_type if hasattr(intent_result, 'intent_type') else "unknown",
            "context": context,
            "type": "command"
        }
        
        # Execute the command if requested
        if execute or dry_run:
            self._logger.info(f"{'Dry run' if dry_run else 'Executing'} suggested command: {suggestion.command}")
            
            # Execute using the adaptive engine with rich feedback
            execution_result = await adaptive_engine.execute_command(
                command=suggestion.command,
                natural_request=request,
                explanation=suggestion.explanation,
                dry_run=dry_run
            )
            
            result["execution"] = execution_result
            
            # If execution failed, analyze errors and provide suggestions
            if not execution_result.get("success") and execution_result.get("stderr"):
                error_analysis = error_analyzer.analyze_error(
                    suggestion.command, 
                    execution_result["stderr"]
                )
                result["error_analysis"] = error_analysis
                
                # Generate fix suggestions
                fix_suggestions = error_analyzer.generate_fix_suggestions(
                    suggestion.command, 
                    execution_result["stderr"]
                )
                execution_result["fix_suggestions"] = fix_suggestions
                
                # Start background monitoring
                if not dry_run:
                    self._start_background_monitoring(suggestion.command, error_analysis)
        
        return result
    
    # Update in angela/orchestrator.py
    # Add this import at the top
    from angela.intent.advanced_planner import advanced_task_planner
    from angela.execution.error_recovery import ErrorRecoveryManager
    
    # Add this as a class member in the Orchestrator class
    def __init__(self):
        """Initialize the orchestrator."""
        self._logger = logger
        self._background_tasks = set()
        self._error_recovery_manager = ErrorRecoveryManager()
    
    # Update the _process_multi_step_request method
    async def _process_multi_step_request(
        self, 
        request: str, 
        context: Dict[str, Any], 
        execute: bool, 
        dry_run: bool
    ) -> Dict[str, Any]:
        """
        Process a multi-step operation request with advanced planning.
        
        Args:
            request: The user request
            context: Context information
            execute: Whether to execute the commands
            dry_run: Whether to simulate execution without making changes
            
        Returns:
            Dictionary with processing results
        """
        self._logger.info(f"Processing multi-step request: {request}")
        
        # Determine if we should use advanced planning
        complexity = await advanced_task_planner._determine_complexity(request)
        
        if complexity == "advanced":
            # Use the advanced planner for complex tasks
            plan = await advanced_task_planner.plan_task(request, context, complexity)
            
            # Create result with the plan
            if isinstance(plan, AdvancedTaskPlan):
                # Advanced plan with branching
                result = {
                    "request": request,
                    "type": "advanced_multi_step",
                    "context": context,
                    "plan": {
                        "id": plan.id,
                        "goal": plan.goal,
                        "description": plan.description,
                        "steps": [
                            {
                                "id": step_id,
                                "type": step.type,
                                "description": step.description,
                                "command": step.command,
                                "dependencies": step.dependencies,
                                "risk": step.estimated_risk
                            }
                            for step_id, step in plan.steps.items()
                        ],
                        "entry_points": plan.entry_points,
                        "step_count": len(plan.steps)
                    }
                }
                
                # Execute the plan if requested
                if execute or dry_run:
                    # Display the plan with rich formatting
                    await terminal_formatter.display_advanced_plan(plan)
                    
                    # Get confirmation for plan execution
                    confirmed = await self._confirm_advanced_plan(plan, dry_run)
                    
                    if confirmed or dry_run:
                        # Execute the plan
                        execution_results = await advanced_task_planner.execute_plan(plan, dry_run=dry_run)
                        result["execution_results"] = execution_results
                        result["success"] = execution_results.get("success", False)
                    else:
                        result["cancelled"] = True
                        result["success"] = False
            else:
                # Basic plan (fallback)
                result = await self._process_basic_multi_step(plan, request, context, execute, dry_run)
        else:
            # Use the basic planner for simple tasks
            plan = await task_planner.plan_task(request, context)
            result = await self._process_basic_multi_step(plan, request, context, execute, dry_run)
        
        return result
    
    async def _process_basic_multi_step(
        self,
        plan: TaskPlan,
        request: str,
        context: Dict[str, Any],
        execute: bool,
        dry_run: bool
    ) -> Dict[str, Any]:
        """Process a basic multi-step plan."""
        # Create result with the plan
        result = {
            "request": request,
            "type": "multi_step",
            "context": context,
            "plan": {
                "goal": plan.goal,
                "steps": [
                    {
                        "command": step.command,
                        "explanation": step.explanation,
                        "dependencies": step.dependencies,
                        "risk": step.estimated_risk
                    }
                    for step in plan.steps
                ],
                "step_count": len(plan.steps)
            }
        }
        
        # Execute the plan if requested
        if execute or dry_run:
            # Display the plan with rich formatting
            await self._display_plan(plan)
            
            # Get confirmation for plan execution
            confirmed = await self._confirm_plan_execution(plan, dry_run)
            
            if confirmed or dry_run:
                # Execute the plan
                execution_results = await task_planner.execute_plan(plan, dry_run=dry_run)
                result["execution_results"] = execution_results
                result["success"] = all(result.get("success", False) for result in execution_results)
                
                # Handle errors with recovery
                if not result["success"] and not dry_run:
                    recovered_results = await self._handle_execution_errors(plan, execution_results)
                    result["recovery_attempted"] = True
                    result["recovered_results"] = recovered_results
                    # Update success status if recovery was successful
                    if all(r.get("success", False) for r in recovered_results):
                        result["success"] = True
            else:
                result["cancelled"] = True
                result["success"] = False
        
        return result
    
    async def _handle_execution_errors(
        self,
        plan: TaskPlan,
        execution_results: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """
        Handle errors in multi-step execution with recovery.
        
        Args:
            plan: The task plan
            execution_results: Original execution results
            
        Returns:
            Updated execution results after recovery attempts
        """
        recovered_results = list(execution_results)  # Copy the original results
        
        # Find failed steps
        for i, result in enumerate(execution_results):
            if not result.get("success", False):
                # Get the corresponding step
                if i < len(plan.steps):
                    step = plan.steps[i]
                    
                    # Attempt recovery
                    recovery_result = await self._error_recovery_manager.handle_error(
                        step, result, {"plan": plan}
                    )
                    
                    # Update the result
                    if recovery_result.get("recovery_success", False):
                        recovered_results[i] = recovery_result
        
        return recovered_results
    
    async def _confirm_advanced_plan(self, plan: AdvancedTaskPlan, dry_run: bool) -> bool:
        """
        Get confirmation to execute an advanced plan.
        
        Args:
            plan: The advanced task plan
            dry_run: Whether this is a dry run
            
        Returns:
            True if confirmed, False otherwise
        """
        if dry_run:
            # No confirmation needed for dry run
            return True
        
        # Check if any steps are high risk
        has_high_risk = any(step.estimated_risk >= 3 for step in plan.steps.values())
        
        # Import here to avoid circular imports
        from rich.console import Console
        from prompt_toolkit.shortcuts import yes_no_dialog
        
        console = Console()
        
        if has_high_risk:
            # Use a more prominent warning for high-risk plans
            console.print(Panel(
                "⚠️  [bold red]This plan includes HIGH RISK operations[/bold red] ⚠️\n"
                "Some of these steps could make significant changes to your system.",
                border_style="red",
                expand=False
            ))
        
        # Show complexity warning for advanced plans
        console.print(Panel(
            "[bold yellow]This is an advanced plan with complex execution flow.[/bold yellow]\n"
            "It may include conditional branching and dependency-based execution.",
            border_style="yellow",
            expand=False
        ))
        
        # Get confirmation
        confirmed = yes_no_dialog(
            title="Confirm Advanced Plan Execution",
            text=f"Do you want to execute this {len(plan.steps)}-step advanced plan?",
        ).run()
        
        return confirmed
    
    async def _process_file_content_request(
        self, 
        request: str, 
        context: Dict[str, Any], 
        execute: bool, 
        dry_run: bool
    ) -> Dict[str, Any]:
        """
        Process a file content analysis/manipulation request.
        
        Args:
            request: The user request
            context: Context information
            execute: Whether to execute file operations
            dry_run: Whether to simulate execution without making changes
            
        Returns:
            Dictionary with processing results
        """
        self._logger.info(f"Processing file content request: {request}")
        
        # Extract file path from request using AI
        file_path = await self._extract_file_path(request, context)
        
        if not file_path:
            return {
                "request": request,
                "type": "file_content",
                "context": context,
                "error": "Could not determine file path from request",
                "response": "I couldn't determine which file you're referring to. Please specify the file path."
            }
        
        # Determine if this is analysis or manipulation
        operation_type = await self._determine_file_operation_type(request)
        
        result = {
            "request": request,
            "type": "file_content",
            "context": context,
            "file_path": str(file_path),
            "operation_type": operation_type
        }
        
        if operation_type == "analyze":
            # Analyze file content
            analysis_result = await content_analyzer.analyze_content(file_path, request)
            result["analysis"] = analysis_result
            
        elif operation_type == "summarize":
            # Summarize file content
            summary_result = await content_analyzer.summarize_content(file_path)
            result["summary"] = summary_result
            
        elif operation_type == "search":
            # Search file content
            search_result = await content_analyzer.search_content(file_path, request)
            result["search_results"] = search_result
            
        elif operation_type == "manipulate":
            # Manipulate file content
            manipulation_result = await content_analyzer.manipulate_content(file_path, request)
            result["manipulation"] = manipulation_result
            
            # Apply changes if requested
            if execute and not dry_run and manipulation_result["has_changes"]:
                # Get confirmation before applying changes
                confirmed = await self._confirm_file_changes(
                    file_path, 
                    manipulation_result["diff"]
                )
                
                if confirmed:
                    # Write the changes to the file
                    try:
                        with open(file_path, 'w', encoding='utf-8') as f:
                            f.write(manipulation_result["modified_content"])
                        result["changes_applied"] = True
                        result["success"] = True
                    except Exception as e:
                        self._logger.error(f"Error applying changes to {file_path}: {str(e)}")
                        result["error"] = f"Error applying changes: {str(e)}"
                        result["changes_applied"] = False
                        result["success"] = False
                else:
                    result["changes_applied"] = False
                    result["cancelled"] = True
            elif dry_run and manipulation_result["has_changes"]:
                result["changes_applied"] = False
                result["success"] = True
                result["dry_run"] = True
        
        return result
    
    async def _process_workflow_definition(
        self, 
        request: str, 
        context: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Process a workflow definition request.
        
        Args:
            request: The user request
            context: Context information
            
        Returns:
            Dictionary with processing results
        """
        self._logger.info(f"Processing workflow definition request: {request}")
        
        # Extract workflow information using AI
        workflow_info = await self._extract_workflow_info(request, context)
        
        if not workflow_info or "name" not in workflow_info:
            return {
                "request": request,
                "type": "workflow_definition",
                "context": context,
                "error": "Could not extract workflow information",
                "response": "I couldn't understand the workflow definition. Please provide a name and description."
            }
        
        # Define workflow
        workflow = await workflow_manager.define_workflow_from_natural_language(
            name=workflow_info["name"],
            description=workflow_info.get("description", ""),
            natural_language=workflow_info.get("steps", request),
            context=context
        )
        
        # Return result
        return {
            "request": request,
            "type": "workflow_definition",
            "context": context,
            "workflow": {
                "name": workflow.name,
                "description": workflow.description,
                "steps": [
                    {
                        "command": step.command,
                        "explanation": step.explanation,
                        "optional": step.optional,
                        "requires_confirmation": step.requires_confirmation
                    }
                    for step in workflow.steps
                ],
                "variables": workflow.variables,
                "step_count": len(workflow.steps)
            },
            "success": True,
            "response": f"Successfully defined workflow '{workflow.name}' with {len(workflow.steps)} steps."
        }
    
    async def _process_workflow_execution(
        self, 
        request: str, 
        context: Dict[str, Any], 
        execute: bool, 
        dry_run: bool
    ) -> Dict[str, Any]:
        """
        Process a workflow execution request.
        
        Args:
            request: The user request
            context: Context information
            execute: Whether to execute the workflow
            dry_run: Whether to simulate execution without making changes
            
        Returns:
            Dictionary with processing results
        """
        self._logger.info(f"Processing workflow execution request: {request}")
        
        # Extract workflow name and variables using AI
        workflow_execution_info = await self._extract_workflow_execution_info(request, context)
        
        if not workflow_execution_info or "name" not in workflow_execution_info:
            return {
                "request": request,
                "type": "workflow_execution",
                "context": context,
                "error": "Could not determine workflow name",
                "response": "I couldn't determine which workflow to run. Please specify the workflow name."
            }
        
        workflow_name = workflow_execution_info["name"]
        variables = workflow_execution_info.get("variables", {})
        
        # Check if workflow exists
        workflow = workflow_manager.get_workflow(workflow_name)
        if not workflow:
            available_workflows = workflow_manager.list_workflows()
            if available_workflows:
                workflow_list = ", ".join([w.name for w in available_workflows])
                return {
                    "request": request,
                    "type": "workflow_execution",
                    "context": context,
                    "error": f"Workflow '{workflow_name}' not found",
                    "response": f"Workflow '{workflow_name}' not found. Available workflows: {workflow_list}"
                }
            else:
                return {
                    "request": request,
                    "type": "workflow_execution",
                    "context": context,
                    "error": "No workflows defined",
                    "response": "No workflows have been defined yet. Use 'define workflow' to create one."
                }
        
        result = {
            "request": request,
            "type": "workflow_execution",
            "context": context,
            "workflow": {
                "name": workflow.name,
                "description": workflow.description,
                "steps": [
                    {
                        "command": step.command,
                        "explanation": step.explanation,
                        "optional": step.optional,
                        "requires_confirmation": step.requires_confirmation
                    }
                    for step in workflow.steps
                ],
                "variables": variables,
                "step_count": len(workflow.steps)
            }
        }
        
        # Execute the workflow if requested
        if execute or dry_run:
            # Display workflow with rich formatting
            await self._display_workflow(workflow, variables)
            
            # Get confirmation
            confirmed = await self._confirm_workflow_execution(workflow, variables, dry_run)
            
            if confirmed or dry_run:
                # Execute workflow
                execution_result = await workflow_manager.execute_workflow(
                    workflow_name=workflow_name,
                    variables=variables,
                    context=context,
                    dry_run=dry_run
                )
                
                result["execution_result"] = execution_result
                result["success"] = execution_result.get("success", False)
            else:
                result["cancelled"] = True
                result["success"] = False
        
        return result
    
    async def _process_clarification_request(
        self, 
        request: str, 
        context: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Process a request for clarification.
        
        Args:
            request: The user request
            context: Context information
            
        Returns:
            Dictionary with processing results
        """
        # Get session context for previous commands
        session = session_manager.get_context()
        recent_commands = session.get("recent_commands", [])
        
        # Build prompt for clarification
        prompt = f"""
You are Angela, an AI terminal assistant. The user is asking for clarification regarding a previous interaction.

Recent commands:
{recent_commands}

User request: {request}

Provide a helpful clarification or explanation about the recent commands or operations. Be concise but thorough.
If the user is asking about how to do something, explain the appropriate command or procedure.
"""
        
        # Call AI service
        api_request = GeminiRequest(prompt=prompt, max_tokens=2000)
        response = await gemini_client.generate_text(api_request)
        
        return {
            "request": request,
            "type": "clarification",
            "context": context,
            "response": response.text
        }
    
    async def _process_unknown_request(
        self, 
        request: str, 
        context: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Process an unknown request type.
        
        Args:
            request: The user request
            context: Context information
            
        Returns:
            Dictionary with processing results
        """
        # Try to get a general response from the AI
        prompt = f"""
You are Angela, an AI terminal assistant. The user has made a request that doesn't clearly match a command, multi-step operation, file manipulation, or workflow.

User request: {request}

Provide a helpful response. If appropriate, suggest what kinds of commands or operations might help with what they're trying to do.
Keep your response concise and focused.
"""
        
        # Call AI service
        api_request = GeminiRequest(prompt=prompt, max_tokens=2000)
        response = await gemini_client.generate_text(api_request)
        
        return {
            "request": request,
            "type": "unknown",
            "context": context,
            "response": response.text
        }
    
    async def _get_ai_suggestion(
        self, 
        request: str, 
        context: Dict[str, Any],
        similar_command: Optional[str] = None,
        intent_result: Optional[Any] = None
    ) -> CommandSuggestion:
        """Get a command suggestion from the AI service."""
        # Build prompt with context, including session context if available
        prompt = build_prompt(request, context, similar_command, intent_result)
        
        # Create a request to the Gemini API
        api_request = GeminiRequest(prompt=prompt)
        
        # Call the Gemini API
        self._logger.info("Sending request to Gemini API")
        api_response = await gemini_client.generate_text(api_request)
        
        # Parse the response
        suggestion = parse_ai_response(api_response.text)
        
        self._logger.info(f"Received suggestion: {suggestion.command}")
        return suggestion
    
    async def _extract_file_path(
        self, 
        request: str, 
        context: Dict[str, Any]
    ) -> Optional[Path]:
        """
        Extract a file path from a request using file_resolver.
        
        Args:
            request: The user request
            context: Context information
            
        Returns:
            Path object if found, None otherwise
        """
        self._logger.debug(f"Extracting file path from: {request}")
        
        # Try to extract file references
        file_references = await file_resolver.extract_references(request, context)
        
        # If we found any resolved references, return the first one
        for reference, path in file_references:
            if path:
                # Track as viewed file
                file_activity_tracker.track_file_viewing(path, None, {
                    "request": request,
                    "reference": reference
                })
                return path
        
        # If we found references but couldn't resolve them, use AI extraction as fallback
        if file_references:
            for reference, _ in file_references:
                # Try to resolve with a broader scope
                path = await file_resolver.resolve_reference(
                    reference, 
                    context,
                    search_scope="project"
                )
                if path:
                    # Track as viewed file
                    file_activity_tracker.track_file_viewing(path, None, {
                        "request": request,
                        "reference": reference
                    })
                    return path
        
        # If all else fails, fall back to the original AI method
        # [Existing AI extraction code]
        return None
    
    async def _determine_file_operation_type(self, request: str) -> str:
        """
        Determine the type of file operation requested.
        
        Args:
            request: The user request
            
        Returns:
            String indicating the operation type: "analyze", "summarize", "search", or "manipulate"
        """
        # Check for keywords indicating different operation types
        
        # Manipulation keywords
        manipulation_keywords = [
            "change", "modify", "update", "edit", "replace", "rename", "refactor",
            "convert", "transform", "add", "remove", "delete", "fix"
        ]
        
        # Analysis keywords
        analysis_keywords = [
            "analyze", "explain", "understand", "evaluate", "assess", "examine",
            "review", "check", "audit"
        ]
        
        # Summarization keywords
        summarization_keywords = [
            "summarize", "summary", "overview", "brief", "digress", "gist"
        ]
        
        # Search keywords
        search_keywords = [
            "find", "search", "locate", "grep", "look for", "identify", "pinpoint"
        ]
        
        # Normalize request text
        normalized = request.lower()
        
        # Check for each type of operation
        for keyword in manipulation_keywords:
            if keyword in normalized:
                return "manipulate"
        
        for keyword in summarization_keywords:
            if keyword in normalized:
                return "summarize"
        
        for keyword in search_keywords:
            if keyword in normalized:
                return "search"
        
        # Default to analysis
        return "analyze"
    
    async def _extract_workflow_info(
        self, 
        request: str, 
        context: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Extract workflow information from a request.
        
        Args:
            request: The user request
            context: Context information
            
        Returns:
            Dictionary with workflow name, description, and steps
        """
        prompt = f"""
Extract information about a workflow definition from this user request:
"{request}"

Return a JSON object with:
1. name: The workflow name
2. description: A brief description of what the workflow does
3. steps: The sequence of steps or commands to include in the workflow

Format:
{{
  "name": "workflow_name",
  "description": "What this workflow does",
  "steps": "Detailed description of steps"
}}

Include only the JSON object with no additional text.
"""
        
        api_request = GeminiRequest(prompt=prompt, max_tokens=1000)
        response = await gemini_client.generate_text(api_request)
        
        try:
            # Extract JSON from the response
            import json
            import re
            
            # Try to find JSON in the response
            json_match = re.search(r'```(?:json)?\s*(.*?)\s*```', response.text, re.DOTALL)
            if json_match:
                json_str = json_match.group(1)
            else:
                # Assume the entire response is JSON
                json_str = response.text
            
            # Parse JSON
            workflow_info = json.loads(json_str)
            return workflow_info
            
        except Exception as e:
            self._logger.error(f"Error extracting workflow info: {str(e)}")
            return {}
    
    async def _extract_workflow_execution_info(
        self, 
        request: str, 
        context: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Extract workflow execution information from a request.
        
        Args:
            request: The user request
            context: Context information
            
        Returns:
            Dictionary with workflow name and variables
        """
        # Get list of available workflows
        available_workflows = workflow_manager.list_workflows()
        workflow_names = [w.name for w in available_workflows]
        
        prompt = f"""
Extract information about a workflow execution from this user request:
"{request}"

Available workflows: {", ".join(workflow_names) if workflow_names else "None"}

Return a JSON object with:
1. name: The workflow name to execute
2. variables: Any variable values to use (as key-value pairs)

Format:
{{
  "name": "workflow_name",
  "variables": {{
    "var1": "value1",
    "var2": "value2"
  }}
}}

Include only the JSON object with no additional text.
"""
        
        api_request = GeminiRequest(prompt=prompt, max_tokens=1000)
        response = await gemini_client.generate_text(api_request)
        
        try:
            # Extract JSON from the response
            import json
            import re
            
            # Try to find JSON in the response
            json_match = re.search(r'```(?:json)?\s*(.*?)\s*```', response.text, re.DOTALL)
            if json_match:
                json_str = json_match.group(1)
            else:
                # Assume the entire response is JSON
                json_str = response.text
            
            # Parse JSON
            execution_info = json.loads(json_str)
            return execution_info
            
        except Exception as e:
            self._logger.error(f"Error extracting workflow execution info: {str(e)}")
            return {}
    
    async def _display_plan(self, plan: Any) -> None:
        """
        Display a task plan with rich formatting.
        
        Args:
            plan: The task plan to display
        """
        # Use the terminal formatter to display the plan
        from angela.shell.formatter import terminal_formatter
        await terminal_formatter.display_task_plan(plan)
    
    async def _confirm_plan_execution(self, plan: Any, dry_run: bool) -> bool:
        """
        Get confirmation to execute a plan.
        
        Args:
            plan: The task plan to execute
            dry_run: Whether this is a dry run
            
        Returns:
            True if confirmed, False otherwise
        """
        if dry_run:
            # No confirmation needed for dry run
            return True
        
        # Check if any steps are high risk
        has_high_risk = any(step.estimated_risk >= 3 for step in plan.steps)
        
        # Import here to avoid circular imports
        from rich.console import Console
        from prompt_toolkit.shortcuts import yes_no_dialog
        
        console = Console()
        
        if has_high_risk:
            # Use a more prominent warning for high-risk plans
            console.print(Panel(
                "⚠️  [bold red]This plan includes HIGH RISK operations[/bold red] ⚠️\n"
                "Some of these steps could make significant changes to your system.",
                border_style="red",
                expand=False
            ))
        
        # Get confirmation
        confirmed = yes_no_dialog(
            title="Confirm Plan Execution",
            text=f"Do you want to execute this {len(plan.steps)}-step plan?",
        ).run()
        
        return confirmed
    
    async def _display_workflow(self, workflow: Any, variables: Dict[str, Any]) -> None:
        """
        Display a workflow with rich formatting.
        
        Args:
            workflow: The workflow to display
            variables: Variables for the workflow
        """
        # Import here to avoid circular imports
        from rich.console import Console
        from rich.table import Table
        from rich.panel import Panel
        from rich.syntax import Syntax
        
        console = Console()
        
        # Create a table for the workflow steps
        table = Table(title=f"Workflow: {workflow.name}")
        table.add_column("#", style="cyan")
        table.add_column("Command", style="green")
        table.add_column("Explanation", style="white")
        table.add_column("Options", style="yellow")
        
        # Add steps to the table
        for i, step in enumerate(workflow.steps):
            # Apply variable substitution
            command = step.command
            for var_name, var_value in variables.items():
                # Remove leading $ if present
                clean_name = var_name[1:] if var_name.startswith('$') else var_name
                
                # Substitute ${VAR} syntax
                command = command.replace(f"${{{clean_name}}}", str(var_value))
                
                # Substitute $VAR syntax
                command = command.replace(f"${clean_name}", str(var_value))
            
            options = []
            if step.optional:
                options.append("Optional")
            if step.requires_confirmation:
                options.append("Requires Confirmation")
            
            table.add_row(
                str(i + 1),
                Syntax(command, "bash", theme="monokai", word_wrap=True).markup,
                step.explanation,
                ", ".join(options) if options else ""
            )
        
        # Display the table
        console.print("\n")
        console.print(Panel(
            workflow.description,
            title=f"Workflow: {workflow.name}",
            border_style="blue"
        ))
        console.print(table)
        
        # Display variables if any
        if variables:
            var_table = Table(title="Variables")
            var_table.add_column("Name", style="cyan")
            var_table.add_column("Value", style="green")
            
            for var_name, var_value in variables.items():
                var_table.add_row(var_name, str(var_value))
            
            console.print(var_table)
    
    async def _confirm_workflow_execution(
        self, 
        workflow: Any, 
        variables: Dict[str, Any], 
        dry_run: bool
    ) -> bool:
        """
        Get confirmation to execute a workflow.
        
        Args:
            workflow: The workflow to execute
            variables: Variables for the workflow
            dry_run: Whether this is a dry run
            
        Returns:
            True if confirmed, False otherwise
        """
        if dry_run:
            # No confirmation needed for dry run
            return True
        
        # Check if any steps require confirmation
        requires_confirmation = any(step.requires_confirmation for step in workflow.steps)
        
        # Import here to avoid circular imports
        from rich.console import Console
        from prompt_toolkit.shortcuts import yes_no_dialog
        
        console = Console()
        
        if requires_confirmation:
            # Use a more prominent warning for confirmation-required workflows
            console.print(Panel(
                "⚠️  [bold yellow]This workflow includes steps that require confirmation[/bold yellow] ⚠️\n"
                "Some of these steps could make significant changes.",
                border_style="yellow",
                expand=False
            ))
        
        # Get confirmation
        confirmed = yes_no_dialog(
            title="Confirm Workflow Execution",
            text=f"Do you want to execute workflow '{workflow.name}' with {len(workflow.steps)} steps?",
        ).run()
        
        return confirmed
    
    async def _confirm_file_changes(self, file_path: Path, diff: str) -> bool:
        """
        Get confirmation for file changes.
        
        Args:
            file_path: Path to the file being changed
            diff: Unified diff of the changes
            
        Returns:
            True if confirmed, False otherwise
        """
        # Import here to avoid circular imports
        from rich.console import Console
        from rich.panel import Panel
        from rich.syntax import Syntax
        from prompt_toolkit.shortcuts import yes_no_dialog
        
        console = Console()
        
        # Display the diff
        console.print("\n")
        console.print(Panel(
            f"Proposed changes to {file_path}:",
            title="File Changes",
            border_style="yellow"
        ))
        console.print(Syntax(diff, "diff", theme="monokai"))
        
        # Get confirmation
        confirmed = yes_no_dialog(
            title="Confirm File Changes",
            text=f"Do you want to apply these changes to {file_path}?",
        ).run()
        
        return confirmed
    
    def _start_background_monitoring(self, command: str, error_analysis: Dict[str, Any]) -> None:
        """
        Start background monitoring for a failed command.
        
        Args:
            command: The failed command
            error_analysis: Analysis of the error
        """
        # Create and start a background task
        task = asyncio.create_task(
            self._monitor_for_suggestions(command, error_analysis)
        )
        
        # Add the task to our set of background tasks
        self._background_tasks.add(task)
        # Remove the task when it's done
        task.add_done_callback(self._background_tasks.discard)
    
    async def _monitor_for_suggestions(self, command: str, error_analysis: Dict[str, Any]) -> None:
        """
        Monitor and provide suggestions for a failed command.
        
        Args:
            command: The failed command
            error_analysis: Analysis of the error
        """
        # Wait a short time before offering suggestions
        await asyncio.sleep(2)
        
        # Import here to avoid circular imports
        from rich.console import Console
        
        console = Console()
        
        # Generate potential fix suggestions
        suggestions = []
        
        # Add suggestions from error analysis
        if "fix_suggestions" in error_analysis:
            suggestions.extend(error_analysis["fix_suggestions"])
        
        # Add historical fixes if available
        if "historical_fixes" in error_analysis:
            suggestions.extend(error_analysis["historical_fixes"])
        
        # If we have suggestions, offer them
        if suggestions:
            console.print("\n")
            console.print("[bold blue]Suggestion:[/bold blue] Try one of these commands to fix the issue:")
            
            for i, suggestion in enumerate(suggestions[:3], 1):  # Limit to top 3
                console.print(f"  {i}. {suggestion}")
            
            console.print("\nUse 'angela try fix 1' to execute the first suggestion, etc.")
    
    async def process_file_operation(
        self, 
        operation: str, 
        parameters: Dict[str, Any],
        dry_run: bool = False
    ) -> Dict[str, Any]:
        """
        Process a file operation request.
        
        Args:
            operation: The type of file operation (e.g., 'create_file', 'read_file').
            parameters: Parameters for the operation.
            dry_run: Whether to simulate the operation without making changes.
            
        Returns:
            A dictionary with the operation results.
        """
        # Execute the file operation
        return await execute_file_operation(operation, parameters, dry_run=dry_run)


    async def execute_command(
        self, 
        command: str,
        natural_request: str,
        explanation: Optional[str] = None,
        dry_run: bool = False
    ) -> Dict[str, Any]:
        '''
        Execute a command with adaptive behavior based on user context.
        
        Args:
            command: The command to execute
            natural_request: The original natural language request
            explanation: AI explanation of what the command does
            dry_run: Whether to simulate the command without execution
            
        Returns:
            Dictionary with execution results
        '''
        self._logger.info(f"Preparing to execute command: {command}")
        
        # Get current context for hooks
        context = context_manager.get_context_dict()
        
        # Call pre-execution hook
        await execution_hooks.pre_execute_command(command, context)
        
        # Analyze command risk and impact
        risk_level, risk_reason = classify_command_risk(command)
        impact = analyze_command_impact(command)
        
        # Add to session context
        session_manager.add_command(command)
        
        # Generate command preview if needed
        from angela.safety.preview import generate_preview
        preview = await generate_preview(command) if preferences_manager.preferences.ui.show_command_preview else None
        
        # Get adaptive confirmation based on risk level and user history
        confirmed = await get_adaptive_confirmation(
            command=command,
            risk_level=risk_level,
            risk_reason=risk_reason,
            impact=impact,
            preview=preview,
            explanation=explanation,
            natural_request=natural_request,
            dry_run=dry_run
        )
        
        if not confirmed and not dry_run:
            self._logger.info(f"Command execution cancelled by user: {command}")
            return {
                "command": command,
                "success": False,
                "cancelled": True,
                "stdout": "",
                "stderr": "Command execution cancelled by user",
                "return_code": 1,
                "dry_run": dry_run
            }
        
        # Execute the command
        result = await self._execute_with_feedback(command, dry_run)
        
        # Call post-execution hook
        await execution_hooks.post_execute_command(command, result, context)
        
        # Add to history
        history_manager.add_command(
            command=command,
            natural_request=natural_request,
            success=result["success"],
            output=result.get("stdout", ""),
            error=result.get("stderr", ""),
            risk_level=risk_level
        )
        
        # If execution failed, analyze error and suggest fixes
        if not result["success"] and result.get("stderr"):
            result["error_analysis"] = error_analyzer.analyze_error(command, result["stderr"])
            result["fix_suggestions"] = error_analyzer.generate_fix_suggestions(command, result["stderr"])
        
        # Offer to learn from successful executions
        if result["success"] and risk_level > 0:
            from angela.safety.adaptive_confirmation import offer_command_learning
            await offer_command_learning(command)
        
        return resul



# Global orchestrator instance
orchestrator = Orchestrator()

# Synchronous wrapper for backwards compatibility
def process_request(request: str) -> Dict[str, Any]:
    """Synchronous wrapper for processing a request."""
    return asyncio.run(orchestrator.process_request(request))
</file>

<file path="README.md">
# Angela-CLI
--
```bash
.
├── MD
│   ├── Info.md
│   ├── NextSteps.md
│   ├── Phase1.md
│   ├── Phase2.md
│   ├── Phase3.md
│   ├── Phase4.md
│   ├── Phase5.md
│   ├── Phase6.md
│   ├── Phase7.md
│   ├── context.md
│   └── tree.md
├── Makefile
├── README.md
├── angela
│   ├── __init__.py
│   ├── __main__.py
│   ├── ai
│   │   ├── analyzer.py
│   │   ├── client.py
│   │   ├── confidence.py
│   │   ├── content_analyzer.py
│   │   ├── content_analyzer_extensions.py
│   │   ├── file_integration.py
│   │   ├── intent_analyzer.py
│   │   ├── parser.py
│   │   ├── prompts.py
│   │   └── prompts_update.py
│   ├── cli
│   │   ├── __init__.py
│   │   ├── files.py
│   │   ├── files_extensions.py
│   │   ├── generation.py
│   │   ├── main.py
│   │   └── workflows.py
│   ├── cli.py
│   ├── config.py
│   ├── constants.py
│   ├── context
│   │   ├── __init__.py
│   │   ├── enhancer.py
│   │   ├── file_activity.py
│   │   ├── file_detector.py
│   │   ├── file_resolver.py
│   │   ├── history.py
│   │   ├── manager.py
│   │   ├── preferences.py
│   │   ├── project_inference.py
│   │   └── session.py
│   ├── execution
│   │   ├── adaptive_engine.py
│   │   ├── engine.py
│   │   ├── error_recovery.py
│   │   ├── filesystem.py
│   │   ├── hooks.py
│   │   └── rollback.py
│   ├── generation
│   │   ├── architecture.py
│   │   ├── documentation.py
│   │   ├── engine.py
│   │   ├── frameworks.py
│   │   ├── planner.py
│   │   └── validators.py
│   ├── integrations
│   │   ├── integrations5.py
│   │   └── integrations6.py
│   ├── intent
│   │   ├── advanced_planner.py
│   │   ├── models.py
│   │   └── planner.py
│   ├── monitoring
│   │   ├── __init__.py
│   │   ├── background.py
│   │   └── network_monitor.py
│   ├── orchestrator.py
│   ├── review
│   │   ├── diff_manager.py
│   │   └── feedback.py
│   ├── safety
│   │   ├── __init__.py
│   │   ├── adaptive_confirmation.py
│   │   ├── classifier.py
│   │   ├── confirmation.py
│   │   ├── preview.py
│   │   └── validator.py
│   ├── shell
│   │   ├── angela.bash
│   │   ├── angela.zsh
│   │   └── formatter.py
│   ├── toolchain
│   │   ├── ci_cd.py
│   │   ├── git.py
│   │   └── package_managers.py
│   ├── utils
│   │   ├── __init__.py
│   │   └── logging.py
│   └── workflows
│       ├── __init__.py
│       ├── manager.py
│       └── sharing.py
├── pyproject.toml
├── pytest.ini
├── requirements.txt
├── scripts
│   ├── install.sh
│   └── uninstall.sh
├── setup.py
├── test_frameworks.py
└── tests
    ├── __init__.py
    ├── conftest.py
    ├── test_ai_client.py
    ├── test_context.py
    ├── test_context_enhancer.py
    ├── test_execution.py
    ├── test_file_activity.py
    ├── test_file_detector.py
    ├── test_file_resolver.py
    ├── test_filesystem.py
    ├── test_integration.py
    ├── test_orchestration.py
    ├── test_prompt_building.py
    ├── test_response_parsing.py
    └── test_safety.py

19 directories, 106 files
                             

```
**Project Title:** **Angela-CLI** 

**Core Concept:**

Angela Acts as an intelligent copilot for terminal operations and imprtaly Code generation autonomously adn at a very very high level, it should be able to create entire webistes an even run them, enhancing productivity, reducing errors, and lowering the barrier to entry for complex tasks and very very high level large scale code generation and deployment.

**Benefits:**

*   **Increased Productivity:** Faster execution of complex or unfamiliar tasks.
*   **Reduced Cognitive Load:** No need to memorize obscure command syntax.
*   **Lower Barrier to Entry:** Makes powerful shell capabilities accessible to less experienced users.
*   **Learning Tool:** Users can see the actual commands generated by the AI, helping them learn.
*   **Automation of Tedious Tasks:** Simplifies repetitive file management or setup processes.
*   **MASSIVE CODE GENERATION AUTOMATICALLY ADN AUTONMOUSLY EVEN WITH CI/CD AUTOMATICALLY ADN AUTOTOMOUSLY WITH MASSIVE CONTEXT STORAGE**
*   **THE AI AGENT IS INTERTWINED AND ESSENTIALLY LIVES IN THE SHELL AS TEH HIGH INTELLEGENT BRAIN AND CONDUCTOR OF THE SHELL/TERMINAL**

**Challenges:**
*   **Accuracy:** Ensuring the AI correctly interprets intent and generates the correct. commands/code.
*   **Context Management:** Effectively tracking the relevant state (directory, environment variables, previous actions) at a very large and high level.
*   **Ambiguity Resolution:** Handling unclear or underspecified user requests gracefully while maintaining focusing on a seamless user experience.
*   **Performance:** Latency of AI model responses impacting the interactive feel.
*   **Integration Complexity:** Modifying or wrapping shells reliably can be tricky.

This in-depth description outlines a powerful tool that blends natural language processing with core system operations, potentially transforming how users interact with their command-line environment.
---------

## Tech stack:
Python, Ubuntu Linux
--
## *********WHAT I WANT TO ACHEIVE (BRIEF OVERVIEW-- IT WILL BE SOEM OF THIS BUT EVEN MORE AND AT AN EVEN HIGHER LEVEL, WERE ESSENRTIALLY RECREATING TEH MOST INTELLEGENT AND CAPABLE OPERATING SYSTEM, TERMINAL, SOFTWARE DEVELOPER, DEVOPS ENGINEER, AI AGENT, AND MORE< WERE CREATING AGI BUT IN A TERMINAL. TEH WORLDS FIRST AGI WILL BE CREATED BY ME AND WILL LIVE IN A TERMINAL*****

1.  **I want an AI partner so deeply woven into my shell that its presence feels almost ambient, yet instantly responsive.** It's more than just a keyword trigger; I want the boundary between my standard shell commands and my instructions to "Angela" to blur. When I type `Angela refactor the main loop in processor.py for clarity`, I want the shell's response mechanism itself to feel like it *understands* this isn't a literal command named "Angela" but an invocation of this embedded intelligence. The transition should be frictionless, immediate, and devoid of the clunkiness of launching a separate process or waiting for a distinct interface. It should feel less like I'm *running a tool* and more like the shell itself has gained a natural language understanding layer.

2.  **I want Angela's contextual awareness to be profound and dynamic.** Defining a project root is just the start. I want her to potentially infer the *type* of project (Is this a Node.js app? A Python library? A Hugo static site?) and leverage that knowledge. If I say `Angela add a dependency for 'requests'`, she should know to use `pip install requests` and update `requirements.txt` in a Python project, or `npm install requests` and update `package.json` in a Node project. Her "mental map" shouldn't just be static file paths; ideally, it incorporates an understanding of common project layouts, configuration file locations, and maybe even recent files I've worked on within that project, allowing for even more concise instructions like `Angela edit the file I was just working on`.

3.  **I want to express complex, multi-step intentions, not just simple tasks.** My goal isn't just mapping single sentences to single commands. I want to be able to articulate workflows: `Angela, create a new feature branch named 'user-auth', switch to it, create a 'auth.py' file in the 'services' directory with a basic Flask blueprint structure, and then stage that new file.` Angela should be able to decompose this complex request into the necessary sequence of `git checkout -b`, `cd`, `touch`, code generation, and `git add` commands, presenting the entire plan for my approval. I want her to handle conditional logic implicitly, like `Angela, find all images larger than 1MB in the assets folder and optimize them`, where she figures out the `find` command and then applies an appropriate optimization tool (like `optipng` or `jpegoptim`) to each result.

4.  **I want Angela's versatility to extend across my entire development ecosystem.** She shouldn't just be limited to file operations and code generation. I want her to be my natural language interface to other CLI tools I use daily:
    *   **Version Control:** `Angela, show me the differences in the last commit`, `Angela, revert the changes to 'config.yaml'`, `Angela, squash the last 3 commits into one`.
    *   **Containers:** `Angela, restart the 'webserver' docker container`, `Angela, show me the logs for the database container`.
    *   **Cloud Services:** `Angela, list my S3 buckets`, `Angela, deploy the latest changes to the staging environment` (invoking the necessary `gcloud`, `aws`, or `az` commands).
    *   **Databases:** (With appropriate configuration/safety) `Angela, show me the schema for the 'users' table`.
    She should become the universal translator for the myriad of CLI tools I interact with.

5.  **I want the reduction in cognitive load to foster a state of creative 'flow'.** The constant mental context-switching between my high-level goal and the low-level syntax required to achieve it breaks concentration and introduces friction. By offloading the syntactical burden to Angela, I want to stay focused on the *design*, the *logic*, the *problem-solving*. The terminal should transform from a potential source of frustration ("What was that flag again?") into a smooth conduit for my intentions. This isn't just about saving time; it's about changing my *relationship* with the command line from potentially adversarial to truly collaborative and empowering. I want it to feel less like giving orders and more like having a conversation with a highly competent assistant.

Ultimately, **I'm aiming for nothing less than a paradigm shift in command-line interaction.** I want to build an AI entity that lives within my terminal, understands my projects and my natural language goals deeply, and translates those goals into actions across my entire digital workspace. It's about creating an environment where the power of the command line is accessible through intuitive conversation, making me fundamentally more effective, creative, and less encumbered by technical minutiae. It's about building the command-line partner I've always wished I had. Imagine only a very very advanced genius programmer could create such a project

**Act as an expert Principal Software Architect.**

**Your Task:** You are tasked with designing the initial high-level technical strategy and architecture for a new project called **Angela-CLI**.

**Key Desired Characteristics (for the AI to consider):**

1.  **Seamless Integration:** Feels like part of the shell, not a separate clunky tool. Triggering Angela should be natural and immediate.
2.  **Natural Language Understanding:** Interprets user intent, not just keywords. Handles multi-step requests and workflows.
3.  **Contextual Awareness:** Understands the current directory, project structure (potentially inferred or user-defined), and maybe recent activity to resolve ambiguity.
4.  **Versatile Action Capabilities:** Translates intent into:
    *   Shell command generation & execution.
    *   File/directory creation, manipulation, and *content-aware editing*.
    *   Code snippet/boilerplate generation.
    *   Interfacing with common developer tools (Git, Docker, potentially cloud CLIs).
    *   **MASSIVE CODE GENERATION AUTOMATICALLY ADN AUTONMOUSLY EVEN WITH CI/CD AUTOMATICALLY ADN AUTOTOMOUSLY WITH MASSIVE CONTEXT STORAGE**
*   **THE AI AGENT IS INTERTWINED AND ESSENTIALLY LIVES IN THE SHELL AS TEH HIGH INTELLEGENT BRAIN AND CONDUCTOR OF THE SHELL/TERMINAL**

**Technical Constraints:**

*   **Platform:** Linux (Ubuntu focus), targeting Bash & Zsh shells.
*   **Core AI Engine:** Google Gemini API.
*   **Primary Implementation Language:** Python.

# *********WHAT I WANT TO ACHEIVE (BRIEF OVERVIEW) - IT WILL BE SOEM OF THIS BUT EVEN MORE AND AT AN EVEN HIGHER LEVEL, WERE ESSENRTIALLY RECREATING TEH MOST INTELLEGENT AND CAPABLE OPERATING SYSTEM, TERMINAL, SOFTWARE DEVELOPER, DEVOPS ENGINEER, AI AGENT, AND MORE< WERE CREATING AGI BUT IN A TERMINAL. TEH WORLDS FIRST AGI WILL BE CREATED BY ME AND WILL LIVE IN A TERMINAL*****

---------------
## Phases-- This is just a core struccture but will be expaned on 10x fold
--
# Angela-CLI: Detailed Technical Blueprint

## Technology Stack Specification

### Core Foundation
- **Python 3.9+**: Primary implementation language
- **Bash/Zsh**: Target shells for integration
- **Google Gemini API**: AI service backbone
- **Ubuntu Linux**: Primary target platform


## Brief Roadmap


## Implementation Plan

### Step 1: Project Setup & Shell Hook
1. Initialize project structure with core directories
2. Implement basic configuration loading (API keys)
3. Create shell function in `angela.bash`/`angela.zsh`:
   ```bash
   # Basic shell hook
   angela() {
     python -m angela "$@"
   }
   ```
4. Implement CLI entry point with argument parsing
5. Create simple echo capability that passes request to Python backend

### Step 2: Orchestration & Context
1. Build orchestrator to manage request flow
2. Implement working directory tracking
3. Create project root detection via markers (.git, etc.)
4. Add basic logging and error handling
5. Design data models for requests/responses
6. Implement testing framework

### Step 3: Gemini API Integration
1. Create Gemini API client class
2. Design initial prompt templates with context injection
3. Implement response parsing and error handling
4. Build basic intent classification (command vs. file operation)
5. Add simple command suggestion capability (non-executing)

### Step 4: Intelligent Interaction & Contextual Execution
(Focus: Make single commands/simple sequences smarter, faster, and provide richer feedback. Enhance immediate context use.)
Enhanced NLU & Tolerant Parsing: Implement more sophisticated Natural Language Understanding (ai/parser.py, intent/analyzer.py) to handle more complex or slightly misspelled/ambiguous single commands or simple sequences. Introduce interactive clarification (safety/confirmation.py using prompt_toolkit) but only when confidence is low (e.g., below ~70% match or high ambiguity); otherwise, attempt the most likely interpretation to maintain flow.
Rich Feedback & Asynchronous Streaming: Integrate rich and asyncio deeply (execution/engine.py, shell/formatter.py) for real-time, well-formatted feedback during command execution. Provide progress indicators (spinners/bars), stream stdout/stderr asynchronously, and give clear status updates, making Angela feel highly responsive. Capture all output cleanly.
Context-Aware Adaptive Confirmation: Leverage project type, recent activity, and command history (context/manager.py) to dynamically adjust confirmation needs (safety/classifier.py, orchestrator.py). Frequently used, low-risk commands in familiar contexts execute with minimal friction, while riskier operations still get detailed previews (safety/preview.py), balancing seamlessness with safety. Add detailed command history tracking (context/history.py).
Intelligent Error Analysis & Basic Fix Suggestions: When commands fail, use the AI (ai/parser.py, execution/engine.py) to analyze stderr in context. Proactively suggest potential fixes, relevant commands (e.g., ls if a file isn't found, permission checks), or documentation lookups based on the error message and command attempted.
Enhanced File/Directory Operations & Context: Implement more robust and complex file/directory operations (execution/filesystem.py) building on Phase 3 basics (e.g., recursive operations, pattern matching). Enhance context (context/filesystem.py) with reliable file type detection and basic metadata understanding to inform AI suggestions and operations.

### Step 5: Autonomous Task Orchestration & Proactive Assistance
(Focus: Enable high-level goal execution, deep content understanding, learning user workflows, and proactive behaviour.)
High-Level Goal Decomposition & Multi-Step Orchestration: Empower the AI (intent/planner.py, orchestrator.py) to break down complex user goals ("Deploy latest dev to staging") into sequences of commands/actions. Plan dependencies, visualize the execution flow (shell/formatter.py with rich), gain confirmation, and execute the orchestrated plan, monitoring progress and handling intermediate steps/errors gracefully.
Conversational Context & Session Memory: Implement robust session memory (context/manager.py, orchestrator.py) allowing Angela to understand follow-up commands referencing entities (files, outputs, errors) from the current interaction ("Try that again with sudo", "Analyze those errors").
AI-Powered File Content Comprehension & Manipulation: Integrate AI (ai/client.py, potentially new ai/content_analyzer.py) to understand the content of files (code functions, config values, text). Enable natural language requests for content-aware tasks like refactoring simple functions, updating configuration entries, or summarizing logs (execution/filesystem.py, safety/preview.py showing diffs). Create underlying utilities for safe content manipulation.
User-Defined Workflows via Natural Language: Allow users to teach Angela reusable multi-step workflows ("Define 'publish package' as: run tests, bump version, build, upload"). Angela (intent/planner.py, new workflows/manager.py) translates, confirms, saves, and allows invocation by the user-defined name.
Proactive Monitoring, Suggestions & Advanced Rollback: Implement optional background monitoring (orchestrator.py, asyncio) for contextual nudges (lint errors, git status, process crashes) via shell/formatter.py. Offer proactive suggestions/autofill based on deeper context (context/*, ai/*). Enhance rollback mechanisms (safety/*, execution/*) to specifically support undoing multi-step or content-manipulation actions where feasible, maintaining safety without hindering the autonomous capabilities.

### Step 6: Enhanced Project Context
1. Implement project type inference
2. Add dependency detection in projects
3. Create file reference resolution from natural language
4. Implement recent activity tracking
5. massivly Enhance prompt engineering with project context

### Step 7: Developer Tool Integration (MAIN ASPECTY OF THIS WHOLE THING WERE IT COMES ALL TOGETHOR)
1. Add Git commands integration
2. Implement Docker support
3. Create code generation flow. it should be able to create 8000 word code files, or small websites/apps etc etc. its essntially a code agent capapbale of great coding stregths. if teh user sasy "create me a porfolio website" it shoud be able to udnertand that and go ahead and create a whole directory/tree structure with files and even code those files in full and have it fully ready for developement.




This will establish the core infrastructure before integrating AI capabilities, ensuring a solid foundation for the more complex features to follow, to ocomplish IT WILL BE SOEM OF THIS BUT EVEN MORE AND AT AN EVEN HIGHER LEVEL, WERE ESSENRTIALLY RECREATING TEH MOST INTELLEGENT AND CAPABLE OPERATING SYSTEM, TERMINAL, SOFTWARE DEVELOPER, DEVOPS ENGINEER, AI AGENT, AND MORE< WERE CREATING AGI BUT IN A TERMINAL. TEH WORLDS FIRST AGI WILL BE CREATED BY ME AND WILL LIVE IN A TERMINAL*****
</file>

</files>


*****END OF ANGELA-CLI CODEBASE****
