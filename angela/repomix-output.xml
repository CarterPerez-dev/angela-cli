This file is a merged representation of a subset of the codebase, containing files not matching ignore patterns, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
4. Repository files, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching these patterns are excluded: **/__pycache__/**, **/.git/**, **/node_modules/**, **/.venv/**, **/target/**, **/dist/**, **/build/**
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

<additional_info>

</additional_info>

</file_summary>

<directory_structure>
api/
  __init__.py
  ai.py
  cli.py
  context.py
  execution.py
  generation.py
  intent.py
  interfaces.py
  monitoring.py
  review.py
  safety.py
  shell.py
  toolchain.py
  workflows.py
cli/
  __init__.py
components/
  ai/
    __init__.py
    analyzer.py
    client.py
    confidence.py
    content_analyzer_extensions.py
    content_analyzer.py
    enhanced_prompts.py
    file_integration.py
    intent_analyzer.py
    parser.py
    prompts.py
    semantic_analyzer.py
  cli/
    __init__.py
    docker.py
    files_extensions.py
    files.py
    generation.py
    main.py
    workflows.py
  context/
    __init__.py
    enhanced_file_activity.py
    enhancer.py
    file_activity.py
    file_detector.py
    file_resolver.py
    history.py
    manager.py
    preferences.py
    project_inference.py
    project_state_analyzer.py
    semantic_context_manager.py
    session.py
  execution/
    __init__.py
    adaptive_engine.py
    engine.py
    error_recovery.py
    filesystem.py
    hooks.py
    rollback_commands.py
    rollback.py
  generation/
    __init__.py
    architecture.py
    context_manager.py
    documentation.py
    engine.py
    frameworks.py
    models.py
    planner.py
    refiner.py
    validators.py
  intent/
    __init__.py
    complex_workflow_planner.py
    enhanced_task_planner.py
    models.py
    planner.py
    semantic_task_planner.py
  interfaces/
    __init__.py
    execution.py
    safety.py
  monitoring/
    __init__.py
    background.py
    network_monitor.py
    notification_handler.py
    proactive_assistant.py
  review/
    __init__.py
    diff_manager.py
    feedback.py
  safety/
    __init__.py
    adaptive_confirmation.py
    classifier.py
    confirmation.py
    preview.py
    validator.py
  shell/
    __init__.py
    advanced_formatter.py
    angela_enhanced.bash
    angela_enhanced.zsh
    angela.bash
    angela.tmux
    angela.zsh
    completion.py
    formatter.py
    inline_feedback.py
  toolchain/
    __init__.py
    ci_cd.py
    cross_tool_workflow_engine.py
    docker.py
    enhanced_universal_cli.py
    git.py
    package_managers.py
    test_frameworks.py
    universal_cli.py
  utils/
    __init__.py
    enhanced_logging.py
    logging.py
  workflows/
    __init__.py
    manager.py
    sharing.py
core/
  __init__.py
  events.py
  registry.py
utils/
  async_utils.py
  logging.py
__init__.py
__main__.py
config.py
constants.py
orchestrator.py
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="api/__init__.py">
# angela/api/__init__.py
"""
Public API for Angela CLI.

This module provides a clean, stable interface to access all Angela CLI components.
Each sub-module provides access to a specific category of functionality.
"""

# Import all API modules to make them available through the API package
from angela.api import cli
from angela.api import ai
from angela.api import context
from angela.api import execution
from angela.api import generation
from angela.api import intent
from angela.api import monitoring
from angela.api import review
from angela.api import safety
from angela.api import shell
from angela.api import toolchain
from angela.api import workflows
from angela.api import interfaces

# Define the public API
__all__ = [
    'cli',
    'ai',
    'context',
    'execution',
    'generation',
    'intent',
    'interfaces',
    'monitoring',
    'review',
    'safety',
    'shell',
    'toolchain',
    'workflows'
]
</file>

<file path="api/ai.py">
"""
Public API for the AI components.

This module provides functions to access AI components with lazy initialization.
"""
from typing import Optional, Type, Any, Callable

from angela.core.registry import registry
from angela.components.ai.client import gemini_client, GeminiRequest
from angela.components.ai.parser import parse_ai_response, CommandSuggestion
from angela.components.ai.prompts import build_prompt


# Gemini Client API
def get_gemini_client():
    """Get the Gemini API client instance."""
    from angela.components.ai.client import GeminiClient, gemini_client 
    return registry.get_or_create("gemini_client", GeminiClient, factory=lambda: gemini_client)

def get_gemini_request_class() -> Type[Any]: 
    """Get the GeminiRequest class."""
    from angela.components.ai.client import GeminiRequest 
    return GeminiRequest

# Parser API
def get_command_suggestion_class() -> Type[Any]: 
    """Get the CommandSuggestion class."""
    from angela.components.ai.parser import CommandSuggestion 
    return CommandSuggestion

def get_parse_ai_response_func() -> Callable:
    """Get the parse_ai_response function."""
    from angela.components.ai.parser import parse_ai_response 
    return parse_ai_response

# Prompt API
def get_build_prompt_func() -> Callable:
    """Get the build_prompt function."""
    from angela.components.ai.prompts import build_prompt 
    return build_prompt

# Analyzer API
def get_error_analyzer():
    """Get the error analyzer instance."""
    from angela.components.ai.analyzer import ErrorAnalyzer, error_analyzer 
    return registry.get_or_create("error_analyzer", ErrorAnalyzer, factory=lambda: error_analyzer)

# Confidence API
def get_confidence_scorer():
    """Get the confidence scorer instance."""
    from angela.components.ai.confidence import ConfidenceScorer, confidence_scorer 
    return registry.get_or_create("confidence_scorer", ConfidenceScorer, factory=lambda: confidence_scorer)

# Content Analyzer API
def get_content_analyzer():
    """Get the content analyzer instance."""
    from angela.components.ai.content_analyzer import ContentAnalyzer, content_analyzer 
    return registry.get_or_create("content_analyzer", ContentAnalyzer, factory=lambda: content_analyzer)

# Intent Analyzer API
def get_intent_analyzer():
    """Get the intent analyzer instance."""
    from angela.components.ai.intent_analyzer import IntentAnalyzer, intent_analyzer 
    return registry.get_or_create("intent_analyzer", IntentAnalyzer, factory=lambda: intent_analyzer)

# Semantic Analyzer API
def get_semantic_analyzer():
    """Get the semantic analyzer instance."""
    from angela.components.ai.semantic_analyzer import SemanticAnalyzer, semantic_analyzer 
    return registry.get_or_create("semantic_analyzer", SemanticAnalyzer, factory=lambda: semantic_analyzer)
</file>

<file path="api/cli.py">
"""
Public API for CLI components.

This module provides functions to access CLI components with lazy initialization.
"""
from typing import Optional, Any, Dict
import typer

from angela.core.registry import registry

# Import CLI apps but don't expose them directly
from angela.components.cli.main import app as main_app
from angela.components.cli.files import app as files_app
from angela.components.cli.workflows import app as workflows_app
from angela.components.cli.generation import app as generation_app
from angela.components.cli.docker import app as docker_app
from angela.components.execution.rollback_commands import app as rollback_app

# Main CLI App
def get_main_app():
    """Get the main CLI app instance."""
    return registry.get_or_create("main_app", typer.Typer, factory=lambda: main_app)

def get_files_app():
    """Get the files CLI app instance."""
    return registry.get_or_create("files_app", typer.Typer, factory=lambda: files_app)

def get_workflows_app():
    """Get the workflows CLI app instance."""
    return registry.get_or_create("workflows_app", typer.Typer, factory=lambda: workflows_app)

def get_generation_app():
    """Get the generation CLI app instance."""
    return registry.get_or_create("generation_app", typer.Typer, factory=lambda: generation_app)

def get_docker_app():
    """Get the docker CLI app instance."""
    return registry.get_or_create("docker_app", typer.Typer, factory=lambda: docker_app)

def get_rollback_app():
    """Get the rollback commands CLI app instance."""
    return registry.get_or_create("rollback_app", typer.Typer, factory=lambda: rollback_app)

# Unified App Interface
def get_app():
    """
    Get the complete CLI app with all subcommands registered.
    
    This is the main entry point for the CLI interface.
    """
    # Retrieve (or create) the main app
    app = get_main_app()
    
    # Add subcommands if they're not already registered
    _ensure_subcommands_registered(app)
    
    return app

def _ensure_subcommands_registered(app):
    """
    Ensure all subcommands are registered with the main app.
    
    This function checks the app's registered commands and adds
    any missing subcommands.
    """
    # Get the subcommand typer apps
    files = get_files_app()
    workflows = get_workflows_app() 
    generation = get_generation_app()
    docker = get_docker_app()
    rollback = get_rollback_app()
    
    # Check if subcommands are already registered
    registered_commands = getattr(app, "registered_commands", {})
    
    # Add each subcommand if not already registered
    if "files" not in registered_commands:
        app.add_typer(files, name="files", help="File and directory operations")
    
    if "workflows" not in registered_commands:
        app.add_typer(workflows, name="workflows", help="Workflow management")
    
    if "generate" not in registered_commands:
        app.add_typer(generation, name="generate", help="Code generation")
    
    if "rollback" not in registered_commands:
        app.add_typer(rollback, name="rollback", help="Rollback operations and transactions")
    
    if "docker" not in registered_commands:
        app.add_typer(docker, name="docker", help="Docker and Docker Compose operations")
        
        
app = get_app()
</file>

<file path="api/context.py">
# angela/api/context.py
"""
Public API for context components.

This module provides functions to access context components with lazy initialization.
"""
from typing import Optional, Type, Any, Dict, List, Union, Callable
from pathlib import Path

from angela.core.registry import registry

# Context Manager API
def get_context_manager():
    """Get the context manager instance."""
    from angela.components.context.manager import ContextManager, context_manager
    return registry.get_or_create("context_manager", ContextManager, factory=lambda: context_manager)

# Session Manager API
def get_session_manager():
    """Get the session manager instance."""
    from angela.components.context.session import SessionManager, session_manager 
    return registry.get_or_create("session_manager", SessionManager, factory=lambda: session_manager)

# History Manager API
def get_history_manager():
    """Get the history manager instance."""
    from angela.components.context.history import HistoryManager, history_manager
    return registry.get_or_create("history_manager", HistoryManager, factory=lambda: history_manager)

# Preferences Manager API
def get_preferences_manager():
    """Get the preferences manager instance."""
    from angela.components.context.preferences import PreferencesManager, preferences_manager 
    return registry.get_or_create("preferences_manager", PreferencesManager, factory=lambda: preferences_manager)

# File Activity API
def get_file_activity_tracker():
    """Get the file activity tracker instance."""
    from angela.components.context.file_activity import FileActivityTracker, file_activity_tracker
    return registry.get_or_create("file_activity_tracker", FileActivityTracker, factory=lambda: file_activity_tracker)

def get_activity_type():
    """Get the ActivityType enum from file_activity."""
    from angela.components.context.file_activity import ActivityType
    return ActivityType

# Enhanced File Activity API
def get_enhanced_file_activity_tracker():
    """Get the enhanced file activity tracker instance."""
    from angela.components.context.enhanced_file_activity import EnhancedFileActivityTracker, enhanced_file_activity_tracker 
    return registry.get_or_create("enhanced_file_activity_tracker", EnhancedFileActivityTracker, factory=lambda: enhanced_file_activity_tracker)

def get_entity_type():
    """Get the EntityType enum from enhanced_file_activity."""
    from angela.components.context.enhanced_file_activity import EntityType
    return EntityType

# File Detector API
def get_file_detector():
    """Get the file detection functions."""
    from angela.components.context.file_detector import detect_file_type, get_content_preview
    
    class FileDetector: # This class is defined locally
        def detect_file_type(self, path: Path) -> Dict[str, Any]:
            return detect_file_type(path)
            
        def get_content_preview(self, path: Path, max_lines: int = 10, max_chars: int = 1000) -> Optional[str]:
            return get_content_preview(path, max_lines, max_chars)
    
    return registry.get_or_create("file_detector", FileDetector, factory=lambda: FileDetector()) 

# File Resolver API
def get_file_resolver():
    """Get the file resolver instance."""
    from angela.components.context.file_resolver import FileResolver, file_resolver 
    return registry.get_or_create("file_resolver", FileResolver, factory=lambda: file_resolver)

# Project Inference API
def get_project_inference():
    """Get the project inference instance."""
    from angela.components.context.project_inference import ProjectInference, project_inference 
    return registry.get_or_create("project_inference", ProjectInference, factory=lambda: project_inference)

# Project State Analyzer API
def get_project_state_analyzer():
    """Get the project state analyzer instance."""
    from angela.components.context.project_state_analyzer import ProjectStateAnalyzer, project_state_analyzer 
    return registry.get_or_create("project_state_analyzer", ProjectStateAnalyzer, factory=lambda: project_state_analyzer)

# Semantic Context Manager API
def get_semantic_context_manager():
    """Get the semantic context manager instance."""
    from angela.components.context.semantic_context_manager import SemanticContextManager, semantic_context_manager 
    return registry.get_or_create("semantic_context_manager", SemanticContextManager, factory=lambda: semantic_context_manager)

# Context Enhancer API
def get_context_enhancer():
    """Get the context enhancer instance."""
    from angela.components.context.enhancer import ContextEnhancer, context_enhancer 
    return registry.get_or_create("context_enhancer", ContextEnhancer, factory=lambda: context_enhancer)

# Get file detector function
def get_file_detector_func():
    """Get the file detection function."""
    from angela.components.context.file_detector import detect_file_type
    return detect_file_type

# Initialize functions
def initialize_project_inference():
    """Initialize project inference for the current project in background."""
    import threading
    project_root = get_context_manager().project_root
    
    # Only attempt to run the inference if a project root is detected
    if project_root:
        from angela.utils.async_utils import run_async_background
        from angela.utils.logging import get_logger
        logger = get_logger(__name__)
        
        logger.debug(f"Starting background project inference for {project_root}")
        
        # Start project inference in background without waiting for results
        run_async_background(
            get_project_inference().infer_project_info(project_root),
            callback=lambda _: logger.debug(f"Project inference completed for {project_root}"),
            error_callback=lambda e: logger.error(f"Project inference failed: {str(e)}")
        )
        
        # Also start semantic context refresh in background
        run_async_background(
            get_semantic_context_manager().refresh_context(force=True),
            callback=lambda _: logger.debug("Semantic context refresh completed"),
            error_callback=lambda e: logger.error(f"Semantic context refresh failed: {str(e)}")
        )
</file>

<file path="api/execution.py">
# angela/api/execution.py
"""
Public API for the execution components.

This module provides functions to access execution components with lazy initialization.
"""
from typing import Optional, Type, Any, Dict, List, Tuple, Union, Callable, Coroutine # Added Coroutine
from pathlib import Path

from angela.core.registry import registry

def get_error_recovery_manager():
    """Get the error recovery manager instance."""
    from angela.components.execution.error_recovery import ErrorRecoveryManager, error_recovery_manager
    return registry.get_or_create("error_recovery_manager", ErrorRecoveryManager, factory=lambda: error_recovery_manager)

# Execution Engine API
def get_execution_engine():
    """Get the execution engine instance."""
    from angela.components.execution.engine import ExecutionEngine, execution_engine
    return registry.get_or_create("execution_engine", ExecutionEngine, factory=lambda: execution_engine)

# Adaptive Engine API
def get_adaptive_engine():
    """Get the adaptive execution engine instance."""
    from angela.components.execution.adaptive_engine import AdaptiveExecutionEngine, adaptive_engine
    return registry.get_or_create("adaptive_engine", AdaptiveExecutionEngine, factory=lambda: adaptive_engine)

# Rollback API
def get_rollback_manager():
    """Get the rollback manager instance."""
    from angela.components.execution.rollback import RollbackManager, rollback_manager
    return registry.get_or_create("rollback_manager", RollbackManager, factory=lambda: rollback_manager)

# Execution Hooks API
def get_execution_hooks():
    """Get the execution hooks instance."""
    from angela.components.execution.hooks import ExecutionHooks, execution_hooks
    return registry.get_or_create("execution_hooks", ExecutionHooks, factory=lambda: execution_hooks)

# --- Filesystem Function Getters ---
def get_create_directory_func() -> Callable[..., Coroutine[Any, Any, bool]]:
    """Get the create_directory function."""
    from angela.components.execution.filesystem import create_directory
    return create_directory

def get_delete_directory_func() -> Callable[..., Coroutine[Any, Any, bool]]:
    """Get the delete_directory function."""
    from angela.components.execution.filesystem import delete_directory
    return delete_directory

def get_create_file_func() -> Callable[..., Coroutine[Any, Any, bool]]:
    """Get the create_file function."""
    from angela.components.execution.filesystem import create_file
    return create_file

def get_read_file_func() -> Callable[..., Coroutine[Any, Any, Union[str, bytes]]]:
    """Get the read_file function."""
    from angela.components.execution.filesystem import read_file
    return read_file

def get_write_file_func() -> Callable[..., Coroutine[Any, Any, bool]]:
    """Get the write_file function."""
    from angela.components.execution.filesystem import write_file
    return write_file

def get_delete_file_func() -> Callable[..., Coroutine[Any, Any, bool]]:
    """Get the delete_file function."""
    from angela.components.execution.filesystem import delete_file
    return delete_file

def get_copy_file_func() -> Callable[..., Coroutine[Any, Any, bool]]:
    """Get the copy_file function."""
    from angela.components.execution.filesystem import copy_file
    return copy_file

def get_move_file_func() -> Callable[..., Coroutine[Any, Any, bool]]:
    """Get the move_file function."""
    from angela.components.execution.filesystem import move_file
    return move_file

# Filesystem API (This returns a wrapper class, which is different from individual function getters)
def get_filesystem_functions():
    """Get filesystem functions wrapper."""
    from angela.components.execution.filesystem import (
        create_directory, delete_directory,
        create_file, read_file, write_file, delete_file,
        copy_file, move_file, FileSystemError, BACKUP_DIR
    )
    
    class FilesystemFunctions:
        def __init__(self):
            self.create_directory = create_directory
            self.delete_directory = delete_directory
            self.create_file = create_file
            self.read_file = read_file
            self.write_file = write_file
            self.delete_file = delete_file
            self.copy_file = copy_file
            self.move_file = move_file
            self.FileSystemError = FileSystemError
            self.BACKUP_DIR = BACKUP_DIR
    
    return registry.get_or_create("filesystem_functions_wrapper", FilesystemFunctions, factory=lambda: FilesystemFunctions())


# Access constants directly
def get_backup_dir():
    """Get the backup directory path."""
    from angela.components.execution.filesystem import BACKUP_DIR
    return BACKUP_DIR

# Get FileSystemError class
def get_filesystem_error_class():
    """Get the FileSystemError class."""
    from angela.components.execution.filesystem import FileSystemError
    return FileSystemError
</file>

<file path="api/generation.py">
"""
Public API for the generation components.

This module provides functions to access generation components with lazy initialization.
"""
from typing import Optional, Type, Any, Dict, List, Union, Callable

from angela.core.registry import registry

# Architecture Analyzer API
def get_architectural_analyzer():
    """Get the architectural analyzer instance."""
    from angela.components.generation.architecture import architectural_analyzer
    return registry.get_or_create("architectural_analyzer", lambda: architectural_analyzer)

def analyze_project_architecture(project_path, context=None):
    """Analyze project architecture wrapper function."""
    from angela.components.generation.architecture import analyze_project_architecture as _analyze
    return _analyze(project_path, context)

# Code Generation Engine API
def get_code_generation_engine():
    """Get the code generation engine instance."""
    from angela.components.generation.engine import code_generation_engine
    return registry.get_or_create("code_generation_engine", lambda: code_generation_engine)

# Documentation Generator API
def get_documentation_generator():
    """Get the documentation generator instance."""
    from angela.components.generation.documentation import documentation_generator
    return registry.get_or_create("documentation_generator", lambda: documentation_generator)

# Framework Generator API
def get_framework_generator():
    """Get the framework generator instance."""
    from angela.components.generation.frameworks import framework_generator
    return registry.get_or_create("framework_generator", lambda: framework_generator)

# Interactive Refiner API
def get_interactive_refiner():
    """Get the interactive refiner instance."""
    from angela.components.generation.refiner import interactive_refiner
    return registry.get_or_create("interactive_refiner", lambda: interactive_refiner)

# Project Planner API
def get_project_planner():
    """Get the project planner instance."""
    from angela.components.generation.planner import project_planner
    return registry.get_or_create("project_planner", lambda: project_planner)

# Generation Context Manager API
def get_generation_context_manager():
    """Get the generation context manager instance."""
    from angela.components.generation.context_manager import generation_context_manager
    return registry.get_or_create("generation_context_manager", lambda: generation_context_manager)

# Code Validator API
def validate_code(content, file_path):
    """Validate code wrapper function."""
    from angela.components.generation.validators import validate_code as _validate
    return _validate(content, file_path)

# Models
def get_code_file_class():
    """Get the CodeFile class."""
    from angela.components.generation.models import CodeFile
    return CodeFile

def get_code_project_class():
    """Get the CodeProject class."""
    from angela.components.generation.models import CodeProject
    return CodeProject

def get_project_architecture_class():
    """Get the ProjectArchitecture class."""
    from angela.components.generation.planner import ProjectArchitecture
    return ProjectArchitecture
</file>

<file path="api/intent.py">
# angela/api/intent.py
"""
Public API for the intent components.

This module provides functions to access intent components with lazy initialization.
"""
from typing import Optional, Type, Any, Dict, List, Union, Callable

from angela.core.registry import registry

# Intent Models API
def get_intent_model_classes():
    """Get the intent model classes."""
    from angela.components.intent.models import Intent, IntentType, ActionPlan
    return Intent, IntentType, ActionPlan

# Task Planner API
def get_task_planner():
    """Get the task planner instance."""
    from angela.components.intent.planner import TaskPlanner, task_planner 
    return registry.get_or_create("task_planner", TaskPlanner, factory=lambda: task_planner)

# Enhanced Task Planner API
def get_enhanced_task_planner():
    """Get the enhanced task planner instance."""
    from angela.components.intent.enhanced_task_planner import EnhancedTaskPlanner, enhanced_task_planner 
    return registry.get_or_create("enhanced_task_planner", EnhancedTaskPlanner, factory=lambda: enhanced_task_planner)

# Semantic Task Planner API
def get_semantic_task_planner():
    """Get the semantic task planner instance."""
    from angela.components.intent.semantic_task_planner import SemanticTaskPlanner, semantic_task_planner 
    return registry.get_or_create("semantic_task_planner", SemanticTaskPlanner, factory=lambda: semantic_task_planner)

def get_intent_clarification_class():
    """Get the IntentClarification class."""
    from angela.components.intent.semantic_task_planner import IntentClarification
    return IntentClarification

# Complex Workflow Planner API
def get_complex_workflow_planner():
    """Get the complex workflow planner instance."""
    from angela.components.intent.complex_workflow_planner import ComplexWorkflowPlanner, complex_workflow_planner 
    return registry.get_or_create("complex_workflow_planner", ComplexWorkflowPlanner, factory=lambda: complex_workflow_planner)

def get_workflow_step_type_enum():
    """Get the WorkflowStepType enum."""
    from angela.components.intent.complex_workflow_planner import WorkflowStepType
    return WorkflowStepType

def get_complex_workflow_plan_class():
    """Get the ComplexWorkflowPlan class."""
    from angela.components.intent.complex_workflow_planner import ComplexWorkflowPlan
    return ComplexWorkflowPlan


def get_advanced_task_plan_class():
    """Get the AdvancedTaskPlan class."""
    from angela.components.intent.planner import AdvancedTaskPlan
    return AdvancedTaskPlan


def get_plan_step_type_enum():
    """Get the PlanStepType enum."""
    from angela.components.intent.planner import PlanStepType
    return PlanStepType


def get_plan_model_classes():
    """Get the plan model classes."""
    from angela.components.intent.planner import (
        PlanStep, TaskPlan, PlanStepType, AdvancedPlanStep, AdvancedTaskPlan
    )
    return PlanStep, TaskPlan, PlanStepType, AdvancedPlanStep, AdvancedTaskPlan


def create_action_plan(task_plan: Any) -> Any:
    """
    Create an action plan from a task plan.
    
    Args:
        task_plan: The task plan to convert
        
    Returns:
        An ActionPlan ready for execution
    """

    from angela.components.intent.planner import task_planner as base_task_planner
    return base_task_planner.create_action_plan(task_plan)
</file>

<file path="api/interfaces.py">
# angela/api/interfaces.py
"""
Public API for interface components.

This module provides functions to access interface components.
"""

def get_command_executor_class():
    """Get the CommandExecutor abstract base class."""
    from angela.components.interfaces.execution import CommandExecutor
    return CommandExecutor

def get_adaptive_executor_class():
    """Get the AdaptiveExecutor abstract base class."""
    from angela.components.interfaces.execution import AdaptiveExecutor
    return AdaptiveExecutor

def get_safety_validator_class():
    """Get the SafetyValidator abstract base class."""
    from angela.components.interfaces.safety import SafetyValidator
    return SafetyValidator
</file>

<file path="api/monitoring.py">
# angela/api/monitoring.py
"""
Public API for the monitoring components.

This module provides functions to access monitoring components with lazy initialization.
"""
from typing import Optional, Type, Any, Dict, List, Union, Callable, Awaitable

from angela.core.registry import registry

# Background Monitor API
def get_background_monitor():
    """Get the background monitor instance."""
    from angela.components.monitoring.background import BackgroundMonitor, background_monitor 
    return registry.get_or_create("background_monitor", BackgroundMonitor, factory=lambda: background_monitor)

# Network Monitor API
def get_network_monitor():
    """Get the network monitor instance."""
    from angela.components.monitoring.network_monitor import NetworkMonitor, network_monitor 
    return registry.get_or_create("network_monitor", NetworkMonitor, factory=lambda: network_monitor)

# Notification Handler API
def get_notification_handler():
    """Get the notification handler instance."""
    from angela.components.monitoring.notification_handler import NotificationHandler, notification_handler 
    return registry.get_or_create("notification_handler", NotificationHandler, factory=lambda: notification_handler)

# Proactive Assistant API
def get_proactive_assistant():
    """Get the proactive assistant instance."""
    from angela.components.monitoring.proactive_assistant import ProactiveAssistant, proactive_assistant 
    return registry.get_or_create("proactive_assistant", ProactiveAssistant, factory=lambda: proactive_assistant)
</file>

<file path="api/review.py">
"""
Public API for the review components.

This module provides functions to access review components with lazy initialization.
"""
from typing import Optional, Type, Any, Dict, List, Union, Callable
from pathlib import Path

from angela.core.registry import registry

# Diff Manager API
def get_diff_manager():
    """Get the diff manager instance."""
    from angela.components.review.diff_manager import diff_manager
    return registry.get_or_create("diff_manager", lambda: diff_manager)

# Feedback Manager API
def get_feedback_manager():
    """Get the feedback manager instance."""
    from angela.components.review.feedback import feedback_manager
    return registry.get_or_create("feedback_manager", lambda: feedback_manager)
</file>

<file path="api/safety.py">
"""
Public API for safety components.

This module provides functions to access safety components with lazy initialization.
"""
from typing import Optional, Type, Any, Dict, List, Union, Callable, Tuple

from angela.core.registry import registry
from angela.components.safety.adaptive_confirmation import offer_command_learning

# Command Validator API
def get_command_validator():
    """Get the command validator instance."""
    from angela.components.safety.validator import CommandValidator, command_validator
    return registry.get_or_create("command_validator", CommandValidator, factory=lambda: command_validator)


# Command Risk Classifier API
def get_command_risk_classifier():
    """Get the command risk classifier instance."""
    from angela.components.safety.classifier import CommandRiskClassifier, command_risk_classifier
    return registry.get_or_create("command_risk_classifier", CommandRiskClassifier, factory=lambda: command_risk_classifier)

# Confirmation Helper API
def get_confirmation_helper():
    """Get the confirmation helper instance."""
    from angela.components.safety.confirmation import ConfirmationHelper, confirmation_helper
    return registry.get_or_create("confirmation_helper", ConfirmationHelper, factory=lambda: confirmation_helper)

# Adaptive Confirmation API
def get_adaptive_confirmation():
    """Get the adaptive confirmation handler function."""
    from angela.components.safety.adaptive_confirmation import get_adaptive_confirmation as confirmation_func
    return confirmation_func

# Command Preview API
def get_command_preview_generator():
    """Get the command preview generator instance."""
    from angela.components.safety.preview import CommandPreviewGenerator, command_preview_generator
    return registry.get_or_create("command_preview_generator", CommandPreviewGenerator, factory=lambda: command_preview_generator)

# Additional functions needed by components
def get_validate_command_safety_func():
    """Get the validate_command_safety function."""
    from angela.components.safety import validate_command_safety
    return validate_command_safety

def get_operation_safety_checker():
    """Get the check_operation_safety function."""
    from angela.components.safety import check_operation_safety
    return check_operation_safety

def get_command_learning_handler():
    """Get the command learning handler function."""
    from angela.components.safety.adaptive_confirmation import offer_command_learning
    return offer_command_learning

def get_command_impact_analyzer():
    """Get the command impact analyzer."""
    from angela.components.safety.classifier import analyze_command_impact
    return analyze_command_impact

def get_adaptive_confirmation_handler():
    """Get the adaptive confirmation handler."""
    from angela.components.safety.adaptive_confirmation import get_adaptive_confirmation
    return get_adaptive_confirmation

# Helper functions for direct validation
def validate_command(command: str) -> Tuple[bool, Optional[str]]:
    """
    Validate a command for safety.
    
    Args:
        command: Command to validate
        
    Returns:
        Tuple of (is_safe, error_message)
    """
    validator = get_command_validator()
    return validator(command)

def classify_command_risk(command: str) -> Tuple[int, str]:
    """ API: Classify the risk level of a command. """
    from angela.components.safety import classify_command_risk as classify_fn
    return classify_fn(command)

def analyze_command_impact(command: str) -> Dict[str, Any]:
    """ API: Analyze the potential impact of a command. """
    from angela.components.safety import analyze_command_impact as analyze_fn 
    return analyze_fn(command)

def generate_command_preview(command: str) -> Dict[str, Any]:
    """
    Generate a preview of what a command will do.
    
    Args:
        command: Command to preview
        
    Returns:
        Dictionary with preview information
    """
    preview_generator = get_command_preview_generator()
    return preview_generator.generate_preview(command)
</file>

<file path="api/shell.py">
# angela/api/shell.py
"""
Public API for shell components.

This module provides functions to access shell components with lazy initialization.
"""
from typing import Optional, Type, Any, Dict, List, Union, Callable, Awaitable, Tuple

from angela.core.registry import registry

# Terminal Formatter API
def get_terminal_formatter():
    """Get the terminal formatter instance."""
    from angela.components.shell.formatter import TerminalFormatter, terminal_formatter # Import Class and instance
    return registry.get_or_create("terminal_formatter", TerminalFormatter, factory=lambda: terminal_formatter)

# Output Type Enum
def get_output_type_enum():
    """Get the OutputType enum from formatter."""
    from angela.components.shell.formatter import OutputType
    return OutputType

# Inline Feedback API
def get_inline_feedback():
    """Get the inline feedback instance."""
    from angela.components.shell.inline_feedback import InlineFeedback, inline_feedback # Import Class and instance
    return registry.get_or_create("inline_feedback", InlineFeedback, factory=lambda: inline_feedback)

# Completion Handler API
def get_completion_handler():
    """Get the completion handler instance."""
    from angela.components.shell.completion import CompletionHandler, completion_handler # Import Class and instance
    return registry.get_or_create("completion_handler", CompletionHandler, factory=lambda: completion_handler)

async def display_command_learning(base_command: str, count: int) -> None:
    """Display command learning notification."""
    from angela.components.shell.formatter import terminal_formatter
    await terminal_formatter.display_command_learning(base_command, count)

async def display_auto_execution_notice(command: str, risk_level: int, preview: Optional[str]) -> None:
    """Display notice for auto-execution."""
    from angela.components.shell.formatter import terminal_formatter
    await terminal_formatter.display_auto_execution_notice(command, risk_level, preview)

async def display_command_preview(command: str, preview: str) -> None:
    """Display command preview."""
    from angela.components.shell.formatter import terminal_formatter
    await terminal_formatter.display_command_preview(command, preview)

async def display_trust_added_message(command: str) -> None:
    """Display message when command is added to trusted list."""
    from angela.components.shell.formatter import terminal_formatter
    await terminal_formatter.display_trust_added_message(command)

# Advanced Formatter API Functions
async def display_advanced_plan(plan: Any) -> None:
    """
    Display an advanced task plan with rich formatting.
    
    Args:
        plan: The advanced task plan to display
    """
    from angela.components.shell.advanced_formatter import display_advanced_plan as _display_advanced_plan
    await _display_advanced_plan(plan)

async def display_execution_results(plan: Any, results: Dict[str, Any]) -> None:
    """
    Display execution results for an advanced task plan.
    
    Args:
        plan: The executed advanced task plan
        results: The execution results
    """
    from angela.components.shell.advanced_formatter import display_execution_results as _display_execution_results
    await _display_execution_results(plan, results)

async def display_step_details(step_id: str, result: Dict[str, Any], plan: Optional[Any] = None) -> None:
    """
    Display detailed results for a specific step.
    
    Args:
        step_id: ID of the step
        result: The step's execution result
        plan: Optional plan for context
    """
    from angela.components.shell.advanced_formatter import display_step_details as _display_step_details
    await _display_step_details(step_id, result, plan)

async def display_step_error(step_id: str, error: str, step_type: str, description: str) -> None:
    """
    Display an error that occurred during step execution.
    
    Args:
        step_id: ID of the failed step
        error: Error message
        step_type: Type of the step
        description: Step description
    """
    from angela.components.shell.advanced_formatter import display_step_error as _display_step_error
    await _display_step_error(step_id, error, step_type, description)
    
    
    
async def display_pre_confirmation_info(
    command: str,
    risk_level: int,
    risk_reason: str,
    impact: Dict[str, Any],
    explanation: Optional[str] = None,
    preview: Optional[str] = None,
    confidence_score: Optional[float] = None,
    execution_time: Optional[float] = None
) -> None:
    """
    Display a comprehensive pre-confirmation information block.
    
    Args:
        command: The command to be executed
        risk_level: Risk level (0-4)
        risk_reason: Reason for the risk assessment
        impact: Impact analysis dictionary
        explanation: Optional explanation of the command
        preview: Optional preview of command execution
        confidence_score: Optional AI confidence score (0-1)
        execution_time: Optional execution time if this is post-execution
    """
    from angela.components.shell.formatter import terminal_formatter
    await terminal_formatter.display_pre_confirmation_info(
        command=command,
        risk_level=risk_level,
        risk_reason=risk_reason,
        impact=impact,
        explanation=explanation,
        preview=preview,
        confidence_score=confidence_score,
        execution_time=execution_time
    )


async def display_inline_confirmation(
    prompt_text: str = "Proceed with execution?"
) -> bool:
    """
    Display an inline confirmation prompt and get user input.
    
    Args:
        prompt_text: The confirmation prompt text
        
    Returns:
        True if confirmed, False otherwise
    """
    from angela.components.shell.formatter import terminal_formatter
    return await terminal_formatter.display_inline_confirmation(prompt_text)

async def display_execution_timer(
    command: str,
    with_philosophy: bool = True
) -> Tuple[str, str, int, float]:
    """
    Display a command execution timer with philosophy quotes.
    
    Args:
        command: The command being executed
        with_philosophy: Whether to display philosophy quotes
        
    Returns:
        Tuple of (stdout, stderr, return_code, execution_time)
    """
    from angela.components.shell.formatter import terminal_formatter
    return await terminal_formatter.display_execution_timer(command, with_philosophy)

async def display_loading_timer(
    message: str,
    with_philosophy: bool = True
) -> None:
    """
    Display a loading timer with optional philosophy quotes.
    
    Args:
        message: The loading message to display
        with_philosophy: Whether to display philosophy quotes
    """
    from angela.components.shell.formatter import terminal_formatter
    await terminal_formatter.display_loading_timer(message, with_philosophy)

def get_inline_feedback():
    """Get the inline feedback instance."""
    from angela.components.shell.inline_feedback import InlineFeedback, inline_feedback
    return inline_feedback
</file>

<file path="api/toolchain.py">
# angela/api/toolchain.py
"""
Public API for toolchain components.

This module provides functions to access toolchain components with lazy initialization.
"""
from typing import Optional, Type, Any, Dict, List, Union, Callable

from angela.core.registry import registry

# Git Integration API
def get_git_integration():
    """Get the git integration instance."""
    from angela.components.toolchain.git import GitIntegration, git_integration
    return registry.get_or_create("git_integration", GitIntegration, factory=lambda: git_integration)

# Package Manager Integration API
def get_package_manager_integration():
    """Get the package manager integration instance."""
    from angela.components.toolchain.package_managers import PackageManagerIntegration, package_manager_integration
    return registry.get_or_create("package_manager_integration", PackageManagerIntegration, factory=lambda: package_manager_integration)

# Docker Integration API
def get_docker_integration():
    """Get the docker integration instance."""
    from angela.components.toolchain.docker import DockerIntegration, docker_integration
    return registry.get_or_create("docker_integration", DockerIntegration, factory=lambda: docker_integration)

# Universal CLI Translator API
def get_universal_cli_translator():
    """Get the universal CLI translator instance."""
    from angela.components.toolchain.universal_cli import UniversalCLITranslator, universal_cli_translator
    return registry.get_or_create("universal_cli_translator", UniversalCLITranslator, factory=lambda: universal_cli_translator)

# CI/CD Integration API
def get_ci_cd_integration():
    """Get the CI/CD integration instance."""
    from angela.components.toolchain.ci_cd import CiCdIntegration, ci_cd_integration
    return registry.get_or_create("ci_cd_integration", CiCdIntegration, factory=lambda: ci_cd_integration)

# Enhanced Universal CLI API
def get_enhanced_universal_cli():
    """Get the enhanced universal CLI instance."""
    from angela.components.toolchain.enhanced_universal_cli import EnhancedUniversalCLI, enhanced_universal_cli
    return registry.get_or_create("enhanced_universal_cli", EnhancedUniversalCLI, factory=lambda: enhanced_universal_cli)

# Cross Tool Workflow Engine API
def get_cross_tool_workflow_engine():
    """Get the cross tool workflow engine instance."""
    from angela.components.toolchain.cross_tool_workflow_engine import CrossToolWorkflowEngine, cross_tool_workflow_engine
    return registry.get_or_create("cross_tool_workflow_engine", CrossToolWorkflowEngine, factory=lambda: cross_tool_workflow_engine)

# Test Framework Integration API
def get_test_framework_integration():
    """Get the test framework integration instance."""
    from angela.components.toolchain.test_frameworks import TestFrameworkIntegration, test_framework_integration
    return registry.get_or_create("test_framework_integration", TestFrameworkIntegration, factory=lambda: test_framework_integration)


def create_cross_tool_workflow(request: str, context: Dict[str, Any], tools: Optional[List[str]] = None) -> Any:
    """
    Create a cross-tool workflow from a natural language request.
    """
    engine = get_cross_tool_workflow_engine() 
    return engine.create_workflow(request, context, tools)

async def execute_cross_tool_workflow(workflow: Any, variables: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
    """
    Execute a cross-tool workflow.
    """
    engine = get_cross_tool_workflow_engine() # This will get the instance correctly now
    return await engine.execute_workflow(workflow, variables)
</file>

<file path="api/workflows.py">
# angela/api/workflows.py
"""
Public API for the workflow components.

This module provides functions to access workflow components with lazy initialization.
"""
from typing import Optional, Type, Any, Dict, List, Union, Callable
from pathlib import Path

from angela.core.registry import registry

# Workflow Manager API
def get_workflow_manager():
    """Get the workflow manager instance."""
    from angela.components.workflows.manager import WorkflowManager, workflow_manager 
    return registry.get_or_create("workflow_manager", WorkflowManager, factory=lambda: workflow_manager)

# Workflow Sharing API
def get_workflow_sharing_manager():
    """Get the workflow sharing manager instance."""
    from angela.components.workflows.sharing import WorkflowSharingManager, workflow_sharing_manager 
    return registry.get_or_create("workflow_sharing_manager", WorkflowSharingManager, factory=lambda: workflow_sharing_manager)

# Models
def get_workflow_model_classes():
    """Get the workflow model classes."""
    from angela.components.workflows.manager import Workflow, WorkflowStep
    return Workflow, WorkflowStep
</file>

<file path="cli/__init__.py">
# angela/cli/__init__.py
"""
CLI forwarding module for Angela CLI.

This module re-exports CLI components for backward compatibility.
"""
from angela.components.cli import app, main_app, files_app, workflows_app, generation_app, docker_app

__all__ = ['app', 'main_app', 'files_app', 'workflows_app', 'generation_app', 'docker_app']
</file>

<file path="components/ai/__init__.py">
# angela/components/ai/__init__.py
"""
AI components for Angela CLI.

This package provides AI-powered functionalities including content analysis,
command generation, intent understanding, and semantic code comprehension.
"""

# Core AI infrastructure - these don't create circular imports
from .client import gemini_client, GeminiRequest
from .parser import parse_ai_response, CommandSuggestion
from .prompts import build_prompt

# Import confidence_scorer directly instead of through the API layer
from .confidence import confidence_scorer

# Define __all__ with the directly imported components
__all__ = [
    # Core AI
    'gemini_client', 'GeminiRequest', 'parse_ai_response', 
    'CommandSuggestion', 'build_prompt',
    'confidence_scorer',  # Add direct export
]

# Lazy loading functions - these prevent circular imports
def get_error_analyzer():
    from .analyzer import error_analyzer
    return error_analyzer

def get_intent_analyzer():
    from .intent_analyzer import intent_analyzer
    return intent_analyzer

def get_content_analyzer():
    from .content_analyzer import content_analyzer
    return content_analyzer

def get_semantic_analyzer():
    from .semantic_analyzer import semantic_analyzer
    return semantic_analyzer

# Update __all__ to include the getter functions
__all__ += [
    'get_error_analyzer', 'get_intent_analyzer', 'get_content_analyzer',
    'get_semantic_analyzer'
]
</file>

<file path="components/ai/analyzer.py">
# angela/ai/analyzer.py

import re
import os
import sys
import shlex
from pathlib import Path
from typing import Dict, Any, Optional, List, Tuple, Union

from angela.utils.logging import get_logger
from angela.api.context import get_history_manager

logger = get_logger(__name__)

class ErrorAnalyzer:
    """Analyzer for command errors with fix suggestions."""
    
    # Common error patterns and their explanations/fixes
    ERROR_PATTERNS = [
        # File not found
        (r'No such file or directory', 'The specified file or directory does not exist', [
            'Check if the path is correct',
            'Use ls to view available files',
            'Use find to search for the file'
        ]),
        # Permission denied
        (r'Permission denied', 'You don\'t have sufficient permissions', [
            'Check file permissions with ls -l',
            'Use sudo for operations requiring elevated privileges',
            'Change permissions with chmod'
        ]),
        # Command not found
        (r'command not found', 'The command is not installed or not in PATH', [
            'Check if the command is installed',
            'Install the package containing the command',
            'Check your PATH environment variable'
        ]),
        # Syntax errors
        (r'syntax error', 'There\'s a syntax error in the command', [
            'Check for missing quotes or brackets',
            'Check the command documentation for correct syntax',
            'Simplify the command and try again'
        ]),
        # Network errors
        (r'(Connection refused|Network is unreachable)', 'Network connection issue', [
            'Check if the host is reachable',
            'Verify network connectivity',
            'Check if the service is running on the target host'
        ])
    ]
    
    def __init__(self):
        """Initialize the error analyzer."""
        self._logger = logger
    
    def analyze_error(self, command: str, error: str) -> Dict[str, Any]:
        """
        Analyze a command error and provide fix suggestions.
        
        Args:
            command: The failed command
            error: The error output
            
        Returns:
            Dictionary with analysis and suggestions
        """
        self._logger.debug(f"Analyzing error for command: {command}")
        
        # Extract the important parts of the error
        error_short = self._extract_key_error(error)
        
        # Check for known error patterns
        pattern_match = self._match_error_pattern(error)
        
        # Check command history for similar errors and their fixes
        historical_fixes = get_history_manager().find_error_patterns(error_short)
        
        # Analyze command structure for potential issues
        command_issues = self._analyze_command_structure(command)
        
        # Check for missing files or directories
        file_issues = self._check_file_references(command, error)
        
        # Build the response
        result = {
            "error_summary": error_short,
            "possible_cause": pattern_match[1] if pattern_match else "Unknown error",
            "fix_suggestions": pattern_match[2] if pattern_match else [],
            "historical_fixes": [fix for _, fix in historical_fixes],
            "command_issues": command_issues,
            "file_issues": file_issues
        }
        
        return result
    
    def _extract_key_error(self, error: str) -> str:
        """
        Extract the key part of an error message.
        
        Args:
            error: The full error output
            
        Returns:
            A shorter, more focused error message
        """
        # Split by lines and remove empty ones
        lines = [line.strip() for line in error.splitlines() if line.strip()]
        
        if not lines:
            return "Unknown error"
        
        # Check for common error patterns in the first few lines
        for line in lines[:3]:
            # Look for lines with "error" or "ERROR"
            if "error" in line.lower():
                return line
            
            # Look for lines with common error indicators
            for pattern, _, _ in self.ERROR_PATTERNS:
                if re.search(pattern, line, re.IGNORECASE):
                    return line
        
        # If no clear error pattern is found, return the first line
        return lines[0]
    
    def _match_error_pattern(self, error: str) -> Optional[Tuple[str, str, List[str]]]:
        """
        Match an error against known patterns.
        
        Args:
            error: The error output
            
        Returns:
            Tuple of (pattern, explanation, fixes) or None if no match
        """
        for pattern, explanation, fixes in self.ERROR_PATTERNS:
            if re.search(pattern, error, re.IGNORECASE):
                return (pattern, explanation, fixes)
        
        return None
    
    def _analyze_command_structure(self, command: str) -> List[str]:
        """
        Analyze command structure for potential issues.
        
        Args:
            command: The command string
            
        Returns:
            List of potential issues
        """
        issues = []
        
        # Check if the command is empty
        if not command.strip():
            issues.append("Command is empty")
            return issues
        
        try:
            # Try to parse the command with shlex
            tokens = shlex.split(command)
            
            # Check for basic command structure
            if not tokens:
                issues.append("Command parsing failed")
                return issues
            
            base_cmd = tokens[0]
            
            # Check for redirect without command
            if base_cmd in ['>', '>>', '<']:
                issues.append("Redirect symbol used as command")
            
            # Check for pipe without command
            if base_cmd == '|':
                issues.append("Pipe symbol used as command")
            
            # Check for unbalanced quotes (shlex would have raised an error)
            
            # Check for missing arguments in common commands
            if len(tokens) == 1:
                if base_cmd in ['cp', 'mv', 'ln']:
                    issues.append(f"{base_cmd} requires source and destination arguments")
                elif base_cmd in ['grep', 'sed', 'awk']:
                    issues.append(f"{base_cmd} requires a pattern and input")
            
            # Check for potentially incorrect flag formats
            for token in tokens[1:]:
                if token.startswith('-') and len(token) > 2 and not token.startswith('--'):
                    # Might be combining single-letter flags incorrectly
                    if any(not c.isalpha() for c in token[1:]):
                        issues.append(f"Potentially malformed flag: {token}")
            
        except ValueError as e:
            # This typically happens with unbalanced quotes
            issues.append(f"Command parsing error: {str(e)}")
        
        return issues
    
    def _check_file_references(self, command: str, error: str) -> List[Dict[str, Any]]:
        """
        Check for file references in the command that might be causing issues.
        
        Args:
            command: The command string
            error: The error output
            
        Returns:
            List of file issues
        """
        issues = []
        
        # Extract potential file paths from the command
        try:
            tokens = shlex.split(command)
            
            # Skip the command name
            potential_paths = []
            for token in tokens[1:]:
                # Skip options
                if token.startswith('-'):
                    continue
                
                # Skip operators
                if token in ['|', '>', '>>', '<', '&&', '||', ';']:
                    continue
                
                # Consider as potential path
                potential_paths.append(token)
            
            # Check if the paths exist
            for path_str in potential_paths:
                path = Path(path_str)
                
                # Only check if it looks like a path
                if '/' in path_str or '.' in path_str:
                    issue = {"path": path_str}
                    
                    if not path.exists():
                        issue["exists"] = False
                        issue["suggestion"] = f"File/directory does not exist: {path_str}"
                        
                        # Check for common typos
                        parent = path.parent
                        if parent.exists():
                            # Check for similar files in the same directory
                            similar_files = []
                            try:
                                for p in parent.iterdir():
                                    # Simple similarity: Levenshtein distance approximation
                                    if p.name.startswith(path.name[:2]) or p.name.endswith(path.name[-2:]):
                                        similar_files.append(p.name)
                            except (PermissionError, OSError):
                                pass
                            
                            if similar_files:
                                issue["similar_files"] = similar_files[:3]  # Limit to top 3
                        
                        issues.append(issue)
                    else:
                        # Check for permission issues
                        issue["exists"] = True
                        if "Permission denied" in error and not os.access(path, os.R_OK):
                            issue["permission"] = False
                            issue["suggestion"] = f"Permission denied for: {path_str}"
                            issues.append(issue)
        
        except Exception as e:
            logger.exception(f"Error checking file references: {str(e)}")
        
        return issues
    
    def generate_fix_suggestions(self, command: str, error: str) -> List[str]:
        """
        Generate fix suggestions for a failed command.
        
        Args:
            command: The failed command
            error: The error output
            
        Returns:
            List of suggested fixes
        """
        # Analyze the error
        analysis = self.analyze_error(command, error)
        
        # Combine all suggestions
        suggestions = []
        
        # Add suggestions from pattern matching
        suggestions.extend(analysis["fix_suggestions"])
        
        # Add suggestions from historical fixes
        if analysis["historical_fixes"]:
            for i, fix in enumerate(analysis["historical_fixes"], 1):
                suggestions.append(f"Previous fix: {fix}")
        
        # Add suggestions from command issues
        for issue in analysis["command_issues"]:
            if issue == "Command parsing failed":
                suggestions.append("Check for unbalanced quotes or special characters")
            elif "requires" in issue:
                suggestions.append(issue)  # Already a suggestion
            elif "flag" in issue:
                suggestions.append("Check command flags format")
        
        # Add suggestions from file issues
        for issue in analysis["file_issues"]:
            if "suggestion" in issue:
                suggestions.append(issue["suggestion"])
            
            if "similar_files" in issue:
                similar = ", ".join(issue["similar_files"])
                suggestions.append(f"Did you mean one of these: {similar}?")
        
        # Deduplicate suggestions
        unique_suggestions = []
        seen = set()
        for suggestion in suggestions:
            suggestion_key = suggestion.lower()
            if suggestion_key not in seen:
                seen.add(suggestion_key)
                unique_suggestions.append(suggestion)
        
        return unique_suggestions

# Global error analyzer instance
error_analyzer = ErrorAnalyzer()
</file>

<file path="components/ai/client.py">
# angela/ai/client.py
import asyncio
from typing import Dict, Any, Optional

import google.generativeai as genai
from pydantic import BaseModel

from angela.config import config_manager
from angela.constants import GEMINI_MODEL, GEMINI_MAX_TOKENS, GEMINI_TEMPERATURE
from angela.utils.logging import get_logger

logger = get_logger(__name__)

class GeminiRequest(BaseModel):
    """Model for a request to the Gemini API."""
    prompt: str
    temperature: float = GEMINI_TEMPERATURE
    max_output_tokens: int = GEMINI_MAX_TOKENS
    
class GeminiResponse(BaseModel):
    """Model for a response from the Gemini API."""
    text: str
    generated_text: str
    raw_response: Dict[str, Any]

class GeminiClient:
    """Client for interacting with the Google Gemini API."""
    
    def __init__(self):
        """Initialize the Gemini API client."""
        self._setup_client()
        
    def _setup_client(self):
        """Set up the Gemini API client."""

        api_key = config_manager.config.api.gemini_api_key
        if not api_key:
            logger.error("Gemini API key is not configured.")
            raise ValueError("Gemini API key is not configured. Run 'angela init' to set it up.")
        
        genai.configure(api_key=api_key)
        self.model = genai.GenerativeModel(GEMINI_MODEL)
        logger.debug(f"Gemini API client initialized with model: {GEMINI_MODEL}")
        
    # Update angela/ai/client.py
    async def generate_text(self, request: GeminiRequest) -> GeminiResponse:
        """Generate text using the Gemini API."""
        try:
            # Configure the generation configuration
            generation_config = genai.types.GenerationConfig(
                temperature=request.temperature,
                max_output_tokens=request.max_output_tokens,
            )
            
            # Call the Gemini API
            response = await asyncio.to_thread(
                self.model.generate_content,
                request.prompt,
                generation_config=generation_config,
            )
            
            # Process the response
            if not response.text:
                # Don't wrap this in a try/except - let it propagate directly
                raise ValueError("Empty response from Gemini API.")
            
            # Create a structured response - handle different response structures
            try:
                # Try to adapt to different response formats
                if hasattr(response, 'candidates') and response.candidates:
                    # Convert candidate to dict if possible
                    if hasattr(response.candidates[0], '__dict__'):
                        raw_response = response.candidates[0].__dict__
                    elif hasattr(response.candidates[0], 'to_dict'):
                        raw_response = response.candidates[0].to_dict()
                    else:
                        # Fallback - create a simple dict with text
                        raw_response = {"text": response.text}
                else:
                    # Fallback to a simpler format if candidates not available
                    raw_response = {"text": response.text}
                    
                result = GeminiResponse(
                    text=response.text,
                    generated_text=response.text,
                    raw_response=raw_response,
                )
            except Exception as format_error:
                logger.exception(f"Error formatting Gemini response: {str(format_error)}")
                # Even if formatting fails, still provide a valid response
                result = GeminiResponse(
                    text=response.text,
                    generated_text=response.text,
                    raw_response={"text": response.text},
                )
            
            logger.debug(f"Gemini API response received. Length: {len(result.text)}")
            return result
        
        except ValueError as ve:
            # Let ValueError propagate directly
            logger.exception(f"Gemini API returned empty response: {str(ve)}")
            raise
        except Exception as e:
            # Wrap other exceptions
            logger.exception(f"Error calling Gemini API: {str(e)}")
            raise RuntimeError(f"Failed to generate text with Gemini API: {str(e)}")

# Global client instance
gemini_client = GeminiClient()
</file>

<file path="components/ai/confidence.py">
# angela/ai/confidence.py

import re
from typing import Dict, Any, List, Tuple, Optional

from angela.api.context import get_history_manager
from angela.utils.logging import get_logger

logger = get_logger(__name__)

class ConfidenceScorer:
    """
    System for scoring confidence in natural language understanding
    and command suggestions.
    """
    
    def __init__(self):
        """Initialize the confidence scorer."""
        self._logger = logger
    
    def score_command_confidence(
        self, 
        request: str, 
        command: str, 
        context: Dict[str, Any]
    ) -> float:
        """
        Score confidence in a command suggestion.
        
        Args:
            request: The original request
            command: The suggested command
            context: Context information
            
        Returns:
            Confidence score (0.0-1.0) representing how well the command matches the user's request
        """
        # Base confidence starts at 0.7 (moderate default)
        confidence = 0.7
        
        # 1. Check if similar commands have been used before
        historical_confidence = self._check_history(command)
        
        # 2. Check command complexity vs. request complexity
        complexity_confidence = self._check_complexity(request, command)
        
        # 3. Check for entity matches
        entity_confidence = self._check_entities(request, command, context)
        
        # 4. Check for flags/options that seem out of place
        flags_confidence = self._check_command_flags(command)
        
        # Combine all factors (with weights)
        confidence = (
            0.3 * historical_confidence + 
            0.3 * complexity_confidence + 
            0.3 * entity_confidence + 
            0.1 * flags_confidence
        )
        
        # Ensure we stay in valid range
        confidence = min(1.0, max(0.0, confidence))
        

        
        return confidence
    
    def _check_history(self, command: str) -> float:
        """
        Check if similar commands have been used before.
        
        Args:
            command: The suggested command
            
        Returns:
            Confidence score component (0.0-1.0)
        """
        # Extract the base command (first word)
        base_command = command.split()[0] if command else ""
        
        # Get frequency of this base command
        history_manager = get_history_manager()
        frequency = history_manager.get_command_frequency(base_command)
        
        # Get success rate
        success_rate = history_manager.get_command_success_rate(base_command)
        
        # Calculate confidence based on frequency and success rate
        if frequency == 0:
            return 0.5  # Neutral for new commands
            
        # Scale based on frequency (up to 10 uses)
        frequency_factor = min(frequency / 10.0, 1.0)
        
        # Combine with success rate
        return 0.5 + (0.5 * frequency_factor * success_rate)
    
    def _check_complexity(self, request: str, command: str) -> float:
        """
        Check if command complexity matches request complexity.
        
        Args:
            request: The original request
            command: The suggested command
            
        Returns:
            Confidence score component (0.0-1.0)
        """
        # Simple heuristic: count tokens in request and command
        request_tokens = len(request.split())
        command_tokens = len(command.split())
        
        # Very simple requests should lead to simple commands
        if request_tokens <= 3 and command_tokens > 10:
            return 0.4  # Low confidence for complex command from simple request
            
        # Complex requests might lead to complex commands
        if request_tokens >= 10 and command_tokens <= 3:
            return 0.6  # Moderate confidence for simple command from complex request
            
        # Ideal ratio is roughly 1:1 to 1:2
        ratio = command_tokens / max(1, request_tokens)
        if 0.5 <= ratio <= 2.0:
            return 0.9  # High confidence when complexity matches
        elif 0.25 <= ratio <= 4.0:
            return 0.7  # Moderate confidence for reasonable mismatch
        else:
            return 0.5  # Low confidence for significant mismatch
    
    def _check_entities(
        self, 
        request: str, 
        command: str, 
        context: Dict[str, Any]
    ) -> float:
        """
        Check if entities in the request match those in the command.
        
        Args:
            request: The original request
            command: The suggested command
            context: Context information
            
        Returns:
            Confidence score component (0.0-1.0)
        """
        # Extract potential entities from request (simple approach)
        request_words = set(request.lower().split())
        
        # Check for important entities
        file_mentions = any(word in request_words for word in ["file", "files", "document", "text"])
        dir_mentions = any(word in request_words for word in ["directory", "folder", "dir"])
        
        # Check if command matches the entity types mentioned
        if file_mentions and not any(ext in command for ext in [".txt", ".md", ".py", ".js", ".html"]):
            return 0.5  # Request mentions files but command doesn't seem to deal with files
            
        if dir_mentions and not any(cmd in command for cmd in ["cd", "mkdir", "rmdir", "ls"]):
            return 0.6  # Request mentions directories but command doesn't seem to deal with directories
        
        # Check for specific paths or filenames
        # This is a simplified approach - real implementation would use regex
        path_pattern = r'[\w/\.-]+'
        request_paths = re.findall(path_pattern, request)
        command_paths = re.findall(path_pattern, command)
        
        if request_paths and not any(rp in command for rp in request_paths):
            return 0.7  # Paths mentioned in request don't appear in command
        
        # Default - reasonable confidence
        return 0.8
    
    def _check_command_flags(self, command: str) -> float:
        """
        Check for unusual flag combinations or invalid options.
        
        Args:
            command: The suggested command
            
        Returns:
            Confidence score component (0.0-1.0)
        """
        # This would ideally have a database of valid flags for common commands
        # For now, just do some basic checks
        
        # Check for potentially conflicting flags
        if "-r" in command and "--no-recursive" in command:
            return 0.3  # Conflicting flags
            
        if "-f" in command and "--interactive" in command:
            return 0.4  # Potentially conflicting (force vs. interactive)
        
        # Check for unusual combinations
        if "rm" in command and "-p" in command:
            return 0.5  # Unusual flag for rm
            
        if "cp" in command and "-l" in command:
            return 0.6  # Unusual flag for cp
        
        # Default - high confidence
        return 0.9

# Global confidence scorer instance
confidence_scorer = ConfidenceScorer()
</file>

<file path="components/ai/content_analyzer_extensions.py">
# angela/ai/content_analyzer_extensions.py

from typing import Dict, Any, List, Optional, Union
from pathlib import Path
import re
import json

from angela.components.ai.content_analyzer import ContentAnalyzer, content_analyzer
from angela.utils.logging import get_logger

logger = get_logger(__name__)

class EnhancedContentAnalyzer(ContentAnalyzer):
    """Extended content analyzer with support for additional file types and languages."""
    
    # Language-specific analysis handlers 
    LANGUAGE_HANDLERS = {
        # Existing handlers
        "Python": "_analyze_python",
        "JavaScript": "_analyze_javascript",
        "HTML": "_analyze_html",
        "CSS": "_analyze_css",
        
        # New handlers
        "TypeScript": "_analyze_typescript",
        "Java": "_analyze_java",
        "Rust": "_analyze_rust",
        "Go": "_analyze_go",
        "Ruby": "_analyze_ruby",
        "PHP": "_analyze_php",
        "C": "_analyze_c",
        "CPP": "_analyze_cpp",
        "CSharp": "_analyze_csharp",
        "Swift": "_analyze_swift",
        "Kotlin": "_analyze_kotlin",
        
        # Data formats
        "JSON": "_analyze_json",
        "YAML": "_analyze_yaml",
        "XML": "_analyze_xml",
        "CSV": "_analyze_csv",
        
        # Config files
        "Dockerfile": "_analyze_dockerfile",
        "Makefile": "_analyze_makefile",
    }
    
    async def analyze_content(self, file_path, request=None):
        """Override the base analyze_content method to use specialized analyzers."""
        result = await super().analyze_content(file_path, request)
        
        # If we got an error, return it
        if "error" in result:
            return result
        
        # Check if we have a specialized analyzer for this file type
        file_type = result.get("type", "unknown")
        language = result.get("language", "unknown").lower()
        
        specialized_analyzer = self._specialized_analyzers.get(language)
        if specialized_analyzer:
            try:
                enhanced_result = await specialized_analyzer(file_path, request)
                if enhanced_result:
                    # Merge the enhanced result with the base result
                    result.update(enhanced_result)
            except Exception as e:
                self._logger.error(f"Error in specialized analyzer for {language}: {str(e)}")
        
        return result
    
    async def _analyze_python(self, file_path, request=None):
        """Specialized analyzer for Python files."""
        # Example implementation - you would expand this
        import ast
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # Parse the AST
            tree = ast.parse(content)
            
            # Extract classes and functions
            classes = []
            functions = []
            
            for node in ast.walk(tree):
                if isinstance(node, ast.ClassDef):
                    classes.append({
                        "name": node.name,
                        "line": node.lineno,
                        "methods": [m.name for m in node.body if isinstance(m, ast.FunctionDef)]
                    })
                elif isinstance(node, ast.FunctionDef):
                    if not any(isinstance(parent, ast.ClassDef) for parent in ast.iter_path(tree, node)):
                        functions.append({
                            "name": node.name,
                            "line": node.lineno,
                            "args": [a.arg for a in node.args.args]
                        })
            
            return {
                "classes": classes,
                "functions": functions,
                "imports": self._extract_python_imports(content)
            }
        except Exception as e:
            self._logger.error(f"Error analyzing Python file: {str(e)}")
            return None
    
    def _extract_python_imports(self, content):
        """Extract import statements from Python code."""
        import_pattern = r'^(?:from\s+(\S+)\s+)?import\s+(.+)$'
        imports = []
        
        for line in content.splitlines():
            line = line.strip()
            match = re.match(import_pattern, line)
            if match:
                from_module, imported = match.groups()
                imports.append({
                    "from_module": from_module,
                    "imported": [name.strip() for name in imported.split(',')]
                })
        
        return imports
    
    async def _analyze_typescript(self, file_path, request=None):
        """Specialized analyzer for TypeScript files."""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # Extract interfaces and types
            types = self._extract_typescript_types(content)
            
            # For more complex analysis, we can use the AI
            prompt = f"""
                Analyze this TypeScript file and extract key information:
                
                ```typescript
                {content[:20000]}  # Limit for large files
                ```
                
                Identify and describe:
                1. Interfaces and their properties
                2. Type definitions
                3. Classes and their methods
                4. Key functions and their purposes
                5. Design patterns used
                6. Dependencies and imports
                
                Format your response as a structured analysis.
                """
                
            # Call AI for analysis
            response = await self._get_ai_analysis(prompt)
            
            return {
                "types": types,
                "ai_analysis": response
            }
        except Exception as e:
            self._logger.error(f"Error analyzing TypeScript file: {str(e)}")
            return None
    
    def _extract_typescript_types(self, content: str) -> List[Dict[str, Any]]:
        """Extract interface and type definitions from TypeScript code."""
        interface_pattern = r'interface\s+(\w+)(?:\s+extends\s+(\w+))?\s*\{([^}]*)\}'
        type_pattern = r'type\s+(\w+)\s*=\s*(.+?);'
        
        interfaces = []
        for match in re.finditer(interface_pattern, content, re.DOTALL):
            name, extends, body = match.groups()
            properties = {}
            
            # Parse properties
            prop_pattern = r'(\w+)(?:\?)?:\s*([^;]+);'
            for prop_match in re.finditer(prop_pattern, body):
                prop_name, prop_type = prop_match.groups()
                properties[prop_name] = prop_type.strip()
            
            interfaces.append({
                "name": name,
                "extends": extends,
                "properties": properties
            })
        
        types = []
        for match in re.finditer(type_pattern, content, re.DOTALL):
            name, definition = match.groups()
            types.append({
                "name": name,
                "definition": definition.strip()
            })
        
        return interfaces + types
    
    async def _analyze_javascript(self, file_path, request=None):
        """Specialized analyzer for JavaScript files."""
        # Similar implementation to TypeScript but without type information
        pass
    
    async def _analyze_json(self, file_path, request=None):
        """Specialized analyzer for JSON files."""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # Parse the JSON
            data = json.loads(content)
            
            # Infer schema
            schema = self._infer_json_schema(data)
            
            return {
                "schema": schema,
                "keys": list(data.keys()) if isinstance(data, dict) else [],
                "array_length": len(data) if isinstance(data, list) else None,
                "data_preview": str(data)[:1000] + "..." if len(str(data)) > 1000 else str(data)
            }
        except Exception as e:
            self._logger.error(f"Error analyzing JSON file: {str(e)}")
            return None
    
    def _infer_json_schema(self, data):
        """Infer a simple schema from JSON data."""
        if isinstance(data, dict):
            schema = {}
            for key, value in data.items():
                schema[key] = self._get_type(value)
            return {"type": "object", "properties": schema}
        elif isinstance(data, list):
            if not data:
                return {"type": "array", "items": {"type": "unknown"}}
            
            # Get the type of the first item
            first_item_type = self._get_type(data[0])
            
            # Check if all items have the same type
            same_type = all(self._get_type(item) == first_item_type for item in data)
            
            if same_type:
                return {"type": "array", "items": first_item_type}
            else:
                return {"type": "array", "items": {"type": "mixed"}}
        else:
            return self._get_type(data)
    
    def _get_type(self, value):
        """Get the type of a JSON value."""
        if value is None:
            return {"type": "null"}
        elif isinstance(value, bool):
            return {"type": "boolean"}
        elif isinstance(value, int):
            return {"type": "integer"}
        elif isinstance(value, float):
            return {"type": "number"}
        elif isinstance(value, str):
            return {"type": "string"}
        elif isinstance(value, dict):
            schema = {}
            for key, val in value.items():
                schema[key] = self._get_type(val)
            return {"type": "object", "properties": schema}
        elif isinstance(value, list):
            if not value:
                return {"type": "array", "items": {"type": "unknown"}}
            return {"type": "array", "items": self._get_type(value[0])}
        else:
            return {"type": "unknown"}
    
    async def _analyze_yaml(self, file_path, request=None):
        """Specialized analyzer for YAML files."""
        # Implementation similar to JSON
        pass
    
    async def _analyze_markdown(self, file_path, request=None):
        """Specialized analyzer for Markdown files."""
        # Extract headings, links, etc.
        pass
    
    async def _analyze_html(self, file_path, request=None):
        """Specialized analyzer for HTML files."""
        # Extract elements, links, scripts, etc.
        pass
    
    async def _analyze_css(self, file_path, request=None):
        """Specialized analyzer for CSS files."""
        # Extract selectors, properties, etc.
        pass
    
    async def _analyze_sql(self, file_path, request=None):
        """Specialized analyzer for SQL files."""
        # Extract tables, queries, etc.
        pass
    
    async def _get_ai_analysis(self, prompt):
        """Get analysis from the AI service."""
        from angela.api.ai import get_gemini_client, get_gemini_request_class
        
        api_request = get_gemini_request_class()(
            prompt=prompt,
            max_tokens=4000
        )
        
        client = get_gemini_client()
        response = await client.generate_text(api_request)
        return response.text
</file>

<file path="components/ai/content_analyzer.py">
# angela/components/ai/content_analyzer.py
"""
File content analysis and manipulation for Angela CLI.

This module provides AI-powered capabilities for understanding and
manipulating file contents based on natural language requests.
"""
import os
import re
import difflib
from pathlib import Path
from typing import Dict, Any, List, Tuple, Optional, Union

from angela.api.review import get_diff_manager
from angela.api.context import get_context_manager, get_file_detector_func
from angela.config import config_manager
from angela.utils.logging import get_logger

logger = get_logger(__name__)

class ContentAnalyzer:
    """
    Analyzer for file content with AI-powered understanding and manipulation.
    
    This class provides:
    1. Content understanding - extract meaning from code or text
    2. Content summarization - generate concise summaries
    3. Content manipulation - make targeted changes based on natural language requests
    4. Content search - find relevant sections or patterns
    """
    
    def __init__(self):
        """Initialize the content analyzer."""
        self._logger = logger
    
    async def analyze_content(
        self, 
        file_path: Union[str, Path], 
        request: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Analyze file content to extract meaningful information.
        
        Args:
            file_path: Path to the file to analyze
            request: Optional specific analysis request
            
        Returns:
            Dictionary with analysis results
        """
        # Import here to avoid circular imports
        from angela.api.ai import get_gemini_client, get_gemini_request_class
        
        path_obj = Path(file_path)
        
        # Check if file exists
        if not path_obj.exists():
            return {"error": f"File not found: {path_obj}"}
        
        # Get file info to determine type and appropriate analysis
        detect_file_type = get_file_detector_func()
        file_info = detect_file_type(path_obj)
        
        # Read file content
        try:
            with open(path_obj, 'r', encoding='utf-8', errors='replace') as f:
                content = f.read()
        except Exception as e:
            self._logger.error(f"Error reading file: {str(e)}")
            return {"error": f"Error reading file: {str(e)}"}
        
        # Generate analysis prompt based on file type and request
        prompt = self._build_analysis_prompt(content, file_info, request)
        
        # Call AI service
        GeminiRequest = get_gemini_request_class()
        api_request = GeminiRequest(
            prompt=prompt,
            max_tokens=4000
        )
        
        gemini_client = get_gemini_client()
        response = await gemini_client.generate_text(api_request)
        
        # Structure the analysis results
        result = {
            "path": str(path_obj),
            "type": file_info.get("type", "unknown"),
            "language": file_info.get("language"),
            "analysis": response.text,
            "request": request
        }
        
        return result
    
    async def summarize_content(
        self, 
        file_path: Union[str, Path], 
        max_length: int = 500
    ) -> Dict[str, Any]:
        """
        Generate a concise summary of file content.
        
        Args:
            file_path: Path to the file to summarize
            max_length: Maximum length of the summary in characters
            
        Returns:
            Dictionary with summary results
        """
        # Import here to avoid circular imports
        from angela.api.ai import get_gemini_client, get_gemini_request_class
        
        path_obj = Path(file_path)
        
        # Check if file exists
        if not path_obj.exists():
            return {"error": f"File not found: {path_obj}"}
        
        # Get file info
        detect_file_type = get_file_detector_func()
        file_info = detect_file_type(path_obj)
        
        # Read file content
        try:
            with open(path_obj, 'r', encoding='utf-8', errors='replace') as f:
                content = f.read()
        except Exception as e:
            self._logger.error(f"Error reading file: {str(e)}")
            return {"error": f"Error reading file: {str(e)}"}
        
        prompt = f"""
Provide a concise summary of the following {file_info.get('language', 'text')} file. 
Focus on the main purpose, structure, and key components.
Keep the summary under {max_length} characters.
{content[:20000]}  # Limit to first 20K chars for very large files

Summary:
"""
        
        # Call AI service
        # Call AI service
        GeminiRequest = get_gemini_request_class()
        api_request = GeminiRequest(
            prompt=prompt,
            max_tokens=1000
        )
        
        gemini_client = get_gemini_client()
        response = await gemini_client.generate_text(api_request)
        
        # Return the summary
        return {
            "path": str(path_obj),
            "type": file_info.get("type", "unknown"),
            "language": file_info.get("language"),
            "summary": response.text,
            "content_length": len(content)
        }
    
    async def manipulate_content(
        self, 
        file_path: Union[str, Path], 
        instruction: str,
        transaction_id: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Manipulate file content based on a natural language instruction with rollback support.
        
        Args:
            file_path: Path to the file to manipulate
            instruction: Natural language instruction for the manipulation
            transaction_id: Optional transaction ID for rollback tracking
            
        Returns:
            Dictionary with manipulation results including old and new content
        """
        # Import here to avoid circular imports
        from angela.api.ai import get_gemini_client, get_gemini_request_class
        
        path_obj = Path(file_path)
        
        # Check if file exists
        if not path_obj.exists():
            return {"error": f"File not found: {path_obj}"}
        
        # Get file info
        detect_file_type = get_file_detector_func()
        file_info = detect_file_type(path_obj)
        
        # Check if this is a text file that can be manipulated
        if file_info.get("binary", False):
            return {"error": f"Cannot manipulate binary file: {path_obj}"}
        
        # Read current content
        try:
            with open(path_obj, 'r', encoding='utf-8', errors='replace') as f:
                original_content = f.read()
        except Exception as e:
            self._logger.error(f"Error reading file: {str(e)}")
            return {"error": f"Error reading file: {str(e)}"}
        
        # Generate manipulation prompt
        prompt = self._build_manipulation_prompt(original_content, file_info, instruction)
        
        # Call AI service
        GeminiRequest = get_gemini_request_class()
        api_request = GeminiRequest(
            prompt=prompt,
            max_tokens=20000  # Large token limit for returning the full modified content
        )
        
        gemini_client = get_gemini_client()
        response = await gemini_client.generate_text(api_request)
        
        # Extract the modified content from the response
        modified_content = self._extract_modified_content(response.text, original_content)
        
        # Generate diff
        diff_manager = get_diff_manager()
        diff = diff_manager.generate_diff(original_content, modified_content)
        
        # Return the results
        result = {
            "path": str(path_obj),
            "type": file_info.get("type", "unknown"),
            "language": file_info.get("language"),
            "instruction": instruction,
            "original_content": original_content,
            "modified_content": modified_content,
            "diff": diff,
            "has_changes": original_content != modified_content
        }
        
        return result
    
    async def search_content(
        self,
        file_path: Union[str, Path],
        query: str,
        context_lines: int = 2
    ) -> Dict[str, Any]:
        """
        Search for relevant sections in a file based on a query.
        
        Args:
            file_path: Path to the file to search
            query: Natural language search query
            context_lines: Number of context lines to include around matches
            
        Returns:
            Dictionary with search results
        """
        # Import here to avoid circular imports
        from angela.api.ai import get_gemini_client, get_gemini_request_class
        
        path_obj = Path(file_path)
        
        # Check if file exists
        if not path_obj.exists():
            return {"error": f"File not found: {path_obj}"}
        
        # Get file info
        detect_file_type = get_file_detector_func()
        file_info = detect_file_type(path_obj)
        
        # Check if this is a text file that can be searched
        if file_info.get("binary", False):
            return {"error": f"Cannot search binary file: {path_obj}"}
        
        # Read content
        try:
            with open(path_obj, 'r', encoding='utf-8', errors='replace') as f:
                content = f.read()
        except Exception as e:
            self._logger.error(f"Error reading file: {str(e)}")
            return {"error": f"Error reading file: {str(e)}"}
        
        # Generate search prompt
        prompt = f"""
Search the following {file_info.get('language', 'text')} file for sections that match this query: "{query}"

For each matching section, provide:
1. Line numbers (approximate)
2. The relevant code/text section
3. A brief explanation of why it matches the query
{content[:50000]}  # Limit to first 50K chars for very large files

Search results:
"""
        
        # Call AI service
        # Call AI service
        GeminiRequest = get_gemini_request_class()
        api_request = GeminiRequest(
            prompt=prompt,
            max_tokens=4000
        )
        
        gemini_client = get_gemini_client()
        response = await gemini_client.generate_text(api_request)
        
        # Parse the search results to extract matches
        matches = self._parse_search_results(response.text, content, context_lines)
        
        # Return the results
        return {
            "path": str(path_obj),
            "type": file_info.get("type", "unknown"),
            "language": file_info.get("language"),
            "query": query,
            "matches": matches,
            "match_count": len(matches)
        }
    
    def _build_analysis_prompt(
        self, 
        content: str, 
        file_info: Dict[str, Any], 
        request: Optional[str]
    ) -> str:
        """
        Build a prompt for content analysis.
        
        Args:
            content: The file content
            file_info: Information about the file
            request: Specific analysis request
            
        Returns:
            A prompt string for the AI service
        """
        file_type = file_info.get("type", "unknown")
        language = file_info.get("language", "unknown")
        
        # Adjust analysis based on file type and language
        analysis_focus = ""
        if language == "Python":
            analysis_focus = """
- Identify main functions and classes
- Describe the overall code structure
- Note any imports and dependencies
- Identify potential bugs or code issues
- Suggest improvements or best practices
"""
        elif language == "JavaScript" or language == "TypeScript":
            analysis_focus = """
- Identify key functions and modules
- Note any imports, frameworks, or libraries used
- Analyze the code structure and patterns
- Identify potential bugs or code issues
- Suggest improvements or best practices
"""
        elif language == "HTML" or language == "CSS":
            analysis_focus = """
- Describe the document structure
- Identify key components or sections
- Note any external resources or dependencies
- Analyze accessibility and best practices
- Suggest improvements
"""
        elif file_type == "document" or language == "Markdown":
            analysis_focus = """
- Summarize the main topics and sections
- Identify key points and arguments
- Note the document structure and organization
- Analyze clarity and coherence
- Suggest improvements
"""
        elif "config" in file_type.lower() or file_type in ["JSON", "YAML", "TOML"]:
            analysis_focus = """
- Identify key configuration settings
- Explain the purpose of important parameters
- Note any environment-specific settings
- Identify potential issues or missing values
- Suggest improvements or best practices
"""
        

        if request:
            prompt = f"""
Analyze the following {language} file with this specific request: "{request}"
{content[:50000]}  # Limit to first 50K chars for very large files

Analysis:
"""
        else:
            prompt = f"""
Analyze the following {language} file.

{analysis_focus}
{content[:50000]}  # Limit to first 50K chars for very large files

Analysis:
"""
        
        return prompt
    
    def _build_manipulation_prompt(
        self, 
        content: str, 
        file_info: Dict[str, Any], 
        instruction: str
    ) -> str:
        """
        Build a prompt for content manipulation.
        
        Args:
            content: The original file content
            file_info: Information about the file
            instruction: Manipulation instruction
            
        Returns:
            A prompt string for the AI service
        """
        language = file_info.get("language", "unknown")
        
        prompt = f"""
You are given a {language} file and a request to modify it.

File content:
{content}

Request: {instruction}

Your task is to modify the file according to the request, maintaining the integrity, style, and purpose of the original file.
Return the ENTIRE modified content, not just the changed parts.
Only make changes that directly address the request.

Modified file content:
"""
        return prompt
    
    def _extract_modified_content(self, response: str, original_content: str) -> str:
        """
        Extract the modified content from the AI response.
        
        Args:
            response: The AI service response
            original_content: The original file content
            
        Returns:
            The modified content
        """
        # Try to extract content between ```
        match = re.search(r'```(?:.*?)\n(.*?)```', response, re.DOTALL)
        if match:
            return match.group(1)
        
        # If no code block, look for specific patterns indicating the start of content
        patterns = [
            r'Modified file content:\n(.*)',
            r'MODIFIED CONTENT:\n(.*)',
            r'Here\'s the modified content:\n(.*)',
        ]
        
        for pattern in patterns:
            match = re.search(pattern, response, re.DOTALL)
            if match:
                return match.group(1).strip()
        
        # If no clear content found, return the full response
        # (or the original content if response clearly isn't just content)
        if len(response.splitlines()) < 5 or len(response) > len(original_content) * 1.5:
            self._logger.warning("Could not clearly identify modified content, using original")
            return original_content
        
        return response.strip()
    
    def _parse_search_results(
        self, 
        response: str, 
        content: str, 
        context_lines: int
    ) -> List[Dict[str, Any]]:
        """
        Parse search results from the AI response.
        
        Args:
            response: The AI service response
            content: The original file content
            context_lines: Number of context lines to include
            
        Returns:
            A list of match dictionaries
        """
        # Split content into lines for context
        content_lines = content.splitlines()
        
        # Look for patterns like "Lines 10-15" or "Line 20"
        line_patterns = [
            r'Lines? (\d+)(?:-(\d+))?',  # Standard "Line X" or "Lines X-Y"
            r'L(\d+)(?:-L(\d+))?',       # Shortened "LX" or "LX-LY"
            r'(\d+)(?:-(\d+))?\s*:',     # Line numbers with colon "X:" or "X-Y:"
        ]
        
        # Matches to return
        matches = []
        
        # Process multi-line response sections separately
        sections = re.split(r'\n\s*\n', response)
        
        for section in sections:
            # Skip empty sections
            if not section.strip():
                continue
                
            # Look for line number patterns
            line_start = None
            line_end = None
            
            for pattern in line_patterns:
                line_match = re.search(pattern, section)
                if line_match:
                    line_start = int(line_match.group(1))
                    if line_match.group(2):
                        line_end = int(line_match.group(2))
                    else:
                        line_end = line_start
                    break
            
            # If no line numbers found, continue to next section
            if line_start is None:
                continue
            
            # Extract code block if present
            code_block = None
            code_match = re.search(r'```(?:.*?)\n(.*?)```', section, re.DOTALL)
            if code_match:
                code_block = code_match.group(1)
            
            # Extract explanation
            explanation = section
            if code_block:
                explanation = re.sub(r'```(?:.*?)\n.*?```', '', explanation, flags=re.DOTALL)
            explanation = re.sub(r'Lines? \d+(?:-\d+)?:?', '', explanation, flags=re.DOTALL)
            explanation = explanation.strip()
            
            # Get context lines
            context_start = max(0, line_start - 1 - context_lines)
            context_end = min(len(content_lines) - 1, line_end - 1 + context_lines)
            
            # Get the actual content with context
            match_content = '\n'.join(content_lines[context_start:context_end + 1])
            
            # Create match entry
            match = {
                "line_start": line_start,
                "line_end": line_end,
                "context_start": context_start + 1,  # 1-indexed for display
                "context_end": context_end + 1,      # 1-indexed for display
                "content": match_content,
                "explanation": explanation
            }
            
            matches.append(match)
        
        return matches
    
content_analyzer = ContentAnalyzer()
</file>

<file path="components/ai/enhanced_prompts.py">
# angela/ai/enhanced_prompts.py
"""
Enhanced prompt engineering for Angela CLI with semantic awareness.

This module extends Angela's prompting capabilities with semantic code understanding,
detailed project state, and nuanced user history for significantly more informed responses.
"""
import os
import re
import json
from pathlib import Path
from typing import Dict, Any, List, Tuple, Optional, Set, Union


from angela.api.ai import (
    build_prompt, SYSTEM_INSTRUCTIONS, EXAMPLES, FILE_OPERATION_EXAMPLES,
    ENHANCED_PROJECT_CONTEXT, ERROR_ANALYSIS_PROMPT, MULTI_STEP_OPERATION_PROMPT,
    CODE_GENERATION_PROMPT, RECENT_FILES_CONTEXT, RESOLVED_FILES_CONTEXT,
    FILE_OPERATION_PROMPT_TEMPLATE
)

from angela.utils.logging import get_logger
from angela.api.context import get_file_detector_func
from angela.api.ai import get_gemini_client, get_gemini_request_class
from angela.api.context import get_semantic_analyzer, get_project_state_analyzer

logger = get_logger(__name__)

# Enhanced system instructions that highlight semantic understanding
ENHANCED_SYSTEM_INSTRUCTIONS = """
You are Angela, an AI-powered command-line assistant with deep semantic code understanding. 
You are integrated into the user's terminal shell and possess detailed awareness of the code structure, 
project state, and development history.

Your capabilities include:
1. Understanding code at a semantic level - functions, classes, dependencies, and architectural patterns
2. Being aware of the project's state including Git status, pending migrations, and build health
3. Tracking specific code entities (functions, classes, methods) across files
4. Interpreting user intentions in the context of their recent activity
5. Providing intelligent suggestions based on comprehensive project understanding
6. Translating high-level goals into precise, context-appropriate actions

You prioritize:
1. Precision in commands and file operations based on semantic understanding
2. Context-awareness that leverages project structure, state, and dependencies
3. Intelligent code modifications with an understanding of potential impacts
4. Helpful explanations that leverage code entity relationships
5. Proactive suggestions informed by project state (test failures, code quality issues, etc.)
"""

# Semantic code context template
SEMANTIC_CODE_CONTEXT = """
## Semantic Code Understanding
{entity_type}: {entity_name}
Location: {filename}:{line_start}-{line_end}
Summary: {summary}

Related Entities:
{related_entities}

Dependencies:
{dependencies}
"""

# Project state context template
PROJECT_STATE_CONTEXT = """
## Project State
Git Status: {git_status}
Branch: {branch} {remote_state}
Changes: {has_changes} {change_details}

Build Status: {build_status}
Tests: {test_status}
Dependencies: {dependencies_status}

Issues:
{issues_summary}
"""

# Enhanced task planning prompt template
SEMANTIC_TASK_PLANNING_PROMPT = """
I'm going to help you plan a complex task with deep awareness of the project's semantic structure and state.

Project Code Context:
{semantic_code_context}

Project State:
{project_state_context}

Recent Activity:
{recent_activity}

For this request:
"{request}"

Let me break this down into well-structured, semantically-aware steps that account for:
1. Dependencies between code components
2. Potential impacts of changes
3. Current project state considerations
4. Error handling for each step
5. Verification steps after critical operations

Here's my detailed plan:
"""

# Enhanced code manipulation prompt template
SEMANTIC_CODE_MANIPULATION_PROMPT = """
I need to modify code with a detailed understanding of its semantic structure and implications.

## Entity to Modify
{entity_type}: {entity_name}
Purpose: {entity_summary}
Dependencies: {entity_dependencies}
Referenced by: {entity_references}

## Desired Modification
"{instruction}"

## Approach
I'll make this change while:
1. Preserving the function's contract with callers
2. Maintaining consistent error handling
3. Respecting existing patterns
4. Updating related documentation
5. Considering potential impacts on dependent code

## Modified Code
```{language}
{modified_code}
```
"""

# Template for summarizing recent coding activity
CODING_HISTORY_CONTEXT = """
## Recent Coding Activity
Recently Modified Entities:
{recent_entities}

Coding Patterns:
{coding_patterns}

Frequent Operations:
{common_operations}
"""

async def build_enhanced_prompt(
    request: str, 
    context: Dict[str, Any],
    similar_command: Optional[str] = None,
    intent_result: Optional[Dict[str, Any]] = None,
    entity_name: Optional[str] = None
) -> str:
    """
    Build an enhanced prompt for the Gemini API with semantic code understanding
    and project state awareness.
    
    Args:
        request: The user request
        context: Context information about the current environment
        similar_command: Optional similar command from history
        intent_result: Optional intent analysis result
        entity_name: Optional specific code entity to focus on
        
    Returns:
        A prompt string for the AI service with enhanced semantic context
    """
    logger.debug("Building enhanced prompt with semantic awareness")
    
    # Start with basic context information
    enhanced_context = f"Current working directory: {context.get('cwd', 'unknown')}\n"
    
    # Add project root if available
    project_root = context.get('project_root')
    if project_root:
        enhanced_context += f"Project root: {project_root}\n"
        
        # Add project type if available
        project_type = context.get('project_type', 'unknown')
        enhanced_context += f"Project type: {project_type}\n"
        
        # Get enhanced project state if available
        try:
            project_state_analyzer = get_project_state_analyzer()
            project_state = await project_state_analyzer.get_project_state(project_root)
            
            # Add Git status
            git_state = project_state.get('git_state', {})
            if git_state.get('is_git_repo', False):
                # Format Git status information
                branch = git_state.get('current_branch', 'unknown')
                has_changes = git_state.get('has_changes', False)
                
                # Format remote state information
                remote_state = git_state.get('remote_state', {})
                remote_info = ""
                if remote_state:
                    ahead = remote_state.get('ahead', 0)
                    behind = remote_state.get('behind', 0)
                    
                    if ahead > 0 and behind > 0:
                        remote_info = f"(ahead {ahead}, behind {behind})"
                    elif ahead > 0:
                        remote_info = f"(ahead {ahead})"
                    elif behind > 0:
                        remote_info = f"(behind {behind})"
                
                # Format change details
                change_details = ""
                if has_changes:
                    modified_count = len(git_state.get('modified_files', []))
                    untracked_count = len(git_state.get('untracked_files', []))
                    staged_count = len(git_state.get('staged_files', []))
                    
                    details = []
                    if modified_count > 0:
                        details.append(f"{modified_count} modified")
                    if untracked_count > 0:
                        details.append(f"{untracked_count} untracked")
                    if staged_count > 0:
                        details.append(f"{staged_count} staged")
                    
                    change_details = f"({', '.join(details)})"
                
                # Add Git information to context
                enhanced_context += PROJECT_STATE_CONTEXT.format(
                    git_status="Active repository" if git_state.get('is_git_repo', False) else "Not a Git repository",
                    branch=branch,
                    remote_state=remote_info,
                    has_changes="With uncommitted changes" if has_changes else "Clean working directory",
                    change_details=change_details,
                    build_status=f"System: {project_state.get('build_status', {}).get('system', 'unknown')}",
                    test_status=f"Framework: {project_state.get('test_status', {}).get('framework', 'unknown')}",
                    dependencies_status=f"Manager: {project_state.get('dependencies', {}).get('package_manager', 'unknown')}",
                    issues_summary=_format_issues_summary(project_state)
                )
            
            # Add build status
            build_status = project_state.get('build_status', {})
            if build_status.get('build_system_detected', False):
                enhanced_context += f"Build system: {build_status.get('system', 'unknown')}\n"
                
                if build_status.get('last_build'):
                    enhanced_context += f"Last build: {build_status.get('last_build')}\n"
            
            # Add test status
            test_status = project_state.get('test_status', {})
            if test_status.get('test_framework_detected', False):
                enhanced_context += f"Test framework: {test_status.get('framework', 'unknown')}\n"
                enhanced_context += f"Test files: {test_status.get('test_files_count', 0)}\n"
                
                if test_status.get('coverage'):
                    enhanced_context += f"Test coverage: {test_status.get('coverage', {}).get('percentage')}%\n"
            
            # Add dependency information
            dependencies = project_state.get('dependencies', {})
            if dependencies.get('has_dependencies', False):
                enhanced_context += f"Package manager: {dependencies.get('package_manager', 'unknown')}\n"
                enhanced_context += f"Dependencies: {dependencies.get('dependencies_count', 0)} main, {dependencies.get('dev_dependencies_count', 0)} dev\n"
                
                if dependencies.get('outdated_packages'):
                    outdated_count = len(dependencies.get('outdated_packages', []))
                    enhanced_context += f"Outdated packages: {outdated_count}\n"
        
        except Exception as e:
            logger.error(f"Error getting project state: {str(e)}")
    
    # Add semantic code information if a specific entity is provided
    if entity_name and project_root:
        try:
            semantic_analyzer = get_semantic_analyzer()
            entity_info = await semantic_analyzer.analyze_entity_usage(entity_name, project_root)
            
            if entity_info.get('found', False):
                entity_type = entity_info.get('type', 'unknown')
                filename = entity_info.get('filename', 'unknown')
                line_start = entity_info.get('line_start', 0)
                line_end = entity_info.get('line_end', 0)
                
                # Generate a summary of the entity
                summary = await semantic_analyzer.summarize_code_entity(entity_name, project_root)
                
                # Format related entities
                related = entity_info.get('related_entities', [])
                related_entities = ""
                if related:
                    related_entities = "\n".join([
                        f"- {r.get('name')} ({r.get('relationship')})"
                        for r in related[:5]  # Limit to 5 for brevity
                    ])
                else:
                    related_entities = "None detected"
                
                # Format dependencies
                dependencies = entity_info.get('details', {}).get('dependencies', [])
                dependencies_str = ", ".join(dependencies) if dependencies else "None detected"
                
                # Add semantic information to context
                enhanced_context += SEMANTIC_CODE_CONTEXT.format(
                    entity_type=entity_type.capitalize(),
                    entity_name=entity_name,
                    filename=Path(filename).name,
                    line_start=line_start,
                    line_end=line_end,
                    summary=summary,
                    related_entities=related_entities,
                    dependencies=dependencies_str
                )
        
        except Exception as e:
            logger.error(f"Error getting semantic code information: {str(e)}")
    
    # Add information about the current file if available
    current_file = context.get('current_file')
    if current_file:
        file_path = current_file.get('path')
        enhanced_context += f"Current file: {file_path}\n"
        
        # Try to get semantic information about the current file
        if project_root and file_path:
            try:
                file_path_obj = Path(file_path)
                semantic_analyzer = get_semantic_analyzer()
                module = await semantic_analyzer.analyze_file(file_path_obj)
                
                if module:
                    # Add basic module information
                    enhanced_context += f"File type: {module.language} module\n"
                    enhanced_context += f"Functions: {len(module.functions)}\n"
                    enhanced_context += f"Classes: {len(module.classes)}\n"
                    
                    # Add key entities in the file
                    if module.functions or module.classes:
                        enhanced_context += "Key entities:\n"
                        
                        # List top classes
                        for class_name in list(module.classes.keys())[:3]:
                            cls = module.classes[class_name]
                            method_count = len(cls.methods)
                            enhanced_context += f"- Class {class_name} ({method_count} methods)\n"
                        
                        # List top functions
                        for func_name in list(module.functions.keys())[:3]:
                            func = module.functions[func_name]
                            enhanced_context += f"- Function {func_name}({', '.join(func.params)})\n"
            except Exception as e:
                logger.error(f"Error analyzing current file: {str(e)}")
    
    # Add recent file activity information
    recent_files = context.get('recent_files', {})
    if recent_files:
        accessed_files = recent_files.get('accessed', [])
        active_files = recent_files.get('active_files', [])
        
        if accessed_files or active_files:
            enhanced_context += "Recent file activity:\n"
            
            if accessed_files:
                enhanced_context += f"- Accessed: {', '.join([Path(f).name for f in accessed_files[:3]])}\n"
            
            if active_files:
                enhanced_context += f"- Most active: {', '.join([f.get('name', 'unknown') for f in active_files[:3]])}\n"
    
    # Add intent analysis if available
    if intent_result:
        enhanced_context += "\nIntent analysis:\n"
        enhanced_context += f"- Intent type: {intent_result.get('intent_type', 'unknown')}\n"
        enhanced_context += f"- Confidence: {intent_result.get('confidence', 0.0):.2f}\n"
        
        # Add extracted entities
        if intent_result.get("entities"):
            enhanced_context += "- Extracted entities:\n"
            for key, value in intent_result.get("entities", {}).items():
                enhanced_context += f"  - {key}: {value}\n"
    
    # Add similar command suggestion if available
    if similar_command:
        enhanced_context += f"\nYou previously suggested this similar command: {similar_command}\n"
    
    # Add examples for few-shot learning
    examples = "\nExamples:\n"
    
    # Add standard examples
    for example in EXAMPLES:
        examples += f"\nUser request: {example['request']}\n"
        examples += f"Context: {example['context']}\n"
        examples += f"Response: {example['response']}\n"
    
    # Define the expected response format
    response_format = """
Expected response format (valid JSON):
{
    "intent": "the_classified_intent",
    "command": "the_suggested_command",
    "explanation": "explanation of what the command does, including semantic considerations",
    "confidence": 0.85, /* Optional confidence score from 0.0 to 1.0 */
    "semantic_insights": "insights about code/project impacts (optional)",
    "additional_info": "any additional information (optional)"
}
"""
    
    # Build the complete prompt
    prompt = f"{ENHANCED_SYSTEM_INSTRUCTIONS}\n\n{enhanced_context}\n\n{examples}\n\n{response_format}\n\nUser request: {request}\n\nResponse:"
    
    logger.debug(f"Built enhanced prompt with length: {len(prompt)}")
    return prompt

def _format_issues_summary(project_state: Dict[str, Any]) -> str:
    """Format a summary of project issues for the prompt."""
    todo_items = project_state.get('todo_items', [])
    code_quality = project_state.get('code_quality', {})
    issues_count = code_quality.get('issues_count', 0)
    high_priority_issues = code_quality.get('high_priority_issues', [])
    
    summary = []
    
    if todo_items:
        todo_count = len(todo_items)
        fixme_count = sum(1 for item in todo_items if item.get('type') == 'FIXME')
        summary.append(f"{todo_count} TODOs ({fixme_count} FIXMEs)")
    
    if issues_count > 0:
        summary.append(f"{issues_count} linting issues")
    
    if high_priority_issues:
        summary.append(f"{len(high_priority_issues)} high-priority issues")
    
    if not summary:
        return "No significant issues detected"
    
    return ", ".join(summary)

async def build_semantic_code_manipulation_prompt(
    entity_name: str,
    instruction: str,
    project_root: Union[str, Path],
    modified_code: Optional[str] = None
) -> str:
    """
    Build a prompt for modifying code with semantic understanding.
    
    Args:
        entity_name: Name of the entity to modify
        instruction: The modification instruction
        project_root: Path to the project root
        modified_code: Optional modified code to include
        
    Returns:
        A prompt string for code manipulation
    """
    logger.debug(f"Building semantic code manipulation prompt for {entity_name}")
    
    try:
        # Get semantic information about the entity
        semantic_analyzer = get_semantic_analyzer()
        entity_info = await semantic_analyzer.analyze_entity_usage(entity_name, project_root)
        
        if not entity_info.get('found', False):
            return f"Could not find entity '{entity_name}' in the project."
        
        # Get entity information
        entity_type = entity_info.get('type', 'unknown')
        filename = entity_info.get('filename', 'unknown')
        line_start = entity_info.get('line_start', 0)
        line_end = entity_info.get('line_end', 0)
        
        # Get a summary of the entity
        entity_summary = await semantic_analyzer.summarize_code_entity(entity_name, project_root)
        
        # Get the entity's code if not provided
        if not modified_code:
            original_code = await semantic_analyzer.get_entity_code(entity_name, project_root)
            modified_code = original_code
        
        # Get file type
        file_detector_func = get_file_detector_func()
        file_info = file_detector_func(Path(filename))
        language = file_info.get('language', '').lower()
        
        # Format references
        related = entity_info.get('related_entities', [])
        references = []
        
        for r in related:
            if r.get('relationship') == 'called_by':
                references.append(f"{r.get('name')} calls this {entity_type}")
            elif r.get('relationship') == 'extended_by':
                references.append(f"{r.get('name')} extends this {entity_type}")
        
        references_str = "\n".join([f"- {ref}" for ref in references]) if references else "None detected"
        
        # Format dependencies
        dependencies = []
        
        if entity_type == 'function' or entity_type == 'method':
            for called in entity_info.get('details', {}).get('called_functions', []):
                dependencies.append(f"Calls {called}")
        
        elif entity_type == 'class':
            for base in entity_info.get('base_classes', []):
                dependencies.append(f"Extends {base}")
        
        dependencies_str = "\n".join([f"- {dep}" for dep in dependencies]) if dependencies else "None detected"
        
        # Build the prompt
        prompt = SEMANTIC_CODE_MANIPULATION_PROMPT.format(
            entity_type=entity_type.capitalize(),
            entity_name=entity_name,
            entity_summary=entity_summary,
            entity_dependencies=dependencies_str,
            entity_references=references_str,
            instruction=instruction,
            language=language,
            modified_code=modified_code
        )
        
        return prompt
    
    except Exception as e:
        logger.error(f"Error building semantic code manipulation prompt: {str(e)}")
        return f"Error building semantic code manipulation prompt: {str(e)}"

async def build_semantic_task_planning_prompt(
    request: str,
    context: Dict[str, Any],
    entity_names: Optional[List[str]] = None
) -> str:
    """
    Build a prompt for task planning with semantic understanding.
    
    Args:
        request: The user request
        context: Context information
        entity_names: Optional list of entity names to focus on
        
    Returns:
        A prompt string for task planning
    """
    logger.debug("Building semantic task planning prompt")
    
    project_root = context.get('project_root')
    if not project_root:
        from angela.ai.prompts import build_prompt
        return build_prompt(request, context)  # Fall back to regular prompt
    
    # Build semantic code context
    semantic_code_context = ""
    
    if entity_names:
        semantic_analyzer = get_semantic_analyzer()
        for entity_name in entity_names:
            try:
                entity_info = await semantic_analyzer.analyze_entity_usage(entity_name, project_root)
                
                if entity_info.get('found', False):
                    entity_type = entity_info.get('type', 'unknown')
                    filename = entity_info.get('filename', 'unknown')
                    
                    # Get a summary of the entity
                    summary = await semantic_analyzer.summarize_code_entity(entity_name, project_root)
                    
                    semantic_code_context += f"{entity_type.capitalize()}: {entity_name} in {Path(filename).name}\n"
                    semantic_code_context += f"Summary: {summary}\n\n"
            except Exception as e:
                logger.error(f"Error getting semantic info for {entity_name}: {str(e)}")
    
    # Get project state context
    project_state_context = ""
    
    try:
        # Get project state
        project_state_analyzer = get_project_state_analyzer()
        project_state = await project_state_analyzer.get_project_state(project_root)
        
        # Format Git information
        git_state = project_state.get('git_state', {})
        if git_state.get('is_git_repo', False):
            branch = git_state.get('current_branch', 'unknown')
            has_changes = git_state.get('has_changes', False)
            
            project_state_context += f"Git: Branch {branch}, "
            project_state_context += "Uncommitted changes" if has_changes else "Clean working directory"
            project_state_context += "\n"
        
        # Add build status
        build_status = project_state.get('build_status', {})
        if build_status.get('build_system_detected', False):
            project_state_context += f"Build: {build_status.get('system', 'unknown')}"
            
            if build_status.get('last_build'):
                project_state_context += f", Last build at {build_status.get('last_build')}"
            
            project_state_context += "\n"
        
        # Add test status
        test_status = project_state.get('test_status', {})
        if test_status.get('test_framework_detected', False):
            project_state_context += f"Tests: {test_status.get('framework', 'unknown')}, "
            project_state_context += f"{test_status.get('test_files_count', 0)} test files"
            
            if test_status.get('coverage'):
                project_state_context += f", {test_status.get('coverage', {}).get('percentage')}% coverage"
            
            project_state_context += "\n"
        
        # Add code quality issues
        code_quality = project_state.get('code_quality', {})
        if code_quality.get('linting_setup_detected', False):
            project_state_context += f"Linting: {code_quality.get('linter', 'unknown')}, "
            project_state_context += f"{code_quality.get('issues_count', 0)} issues"
            project_state_context += "\n"
        
        # Add TODO items
        todo_items = project_state.get('todo_items', [])
        if todo_items:
            project_state_context += f"TODOs: {len(todo_items)} items "
            
            # Count by type
            todo_counts = {}
            for item in todo_items:
                item_type = item.get('type', 'unknown')
                if item_type not in todo_counts:
                    todo_counts[item_type] = 0
                todo_counts[item_type] += 1
            
            type_counts = [f"{count} {type_}" for type_, count in todo_counts.items()]
            project_state_context += f"({', '.join(type_counts)})"
            project_state_context += "\n"
    
    except Exception as e:
        logger.error(f"Error getting project state: {str(e)}")
    
    # Get recent activity information
    recent_activity = ""
    
    # Add recent file activity
    recent_files = context.get('recent_files', {})
    if recent_files:
        accessed = recent_files.get('accessed', [])
        if accessed:
            recent_activity += "Recently accessed files:\n"
            for file_path in accessed[:5]:
                recent_activity += f"- {Path(file_path).name}\n"
    
    # Add recent commands
    session = context.get('session', {})
    recent_commands = session.get('recent_commands', [])
    if recent_commands:
        recent_activity += "\nRecent commands:\n"
        for cmd in recent_commands[:5]:
            recent_activity += f"- {cmd}\n"
    
    # Build the prompt
    prompt = SEMANTIC_TASK_PLANNING_PROMPT.format(
        semantic_code_context=semantic_code_context or "No specific code entities focused on.",
        project_state_context=project_state_context or "No detailed project state available.",
        recent_activity=recent_activity or "No recent activity recorded.",
        request=request
    )
    
    return prompt
</file>

<file path="components/ai/file_integration.py">
# angela/ai/file_integration.py
"""
Integration module for AI-powered file operations.

This module bridges the AI suggestions with actual file operations,
extracting file operations from commands and executing them safely.
"""
import re
import shlex
from pathlib import Path
from typing import Dict, Any, List, Tuple, Optional

from angela.api.execution import (
    get_filesystem_error_class,
    get_create_directory_func,
    get_delete_directory_func,
    get_create_file_func,
    get_read_file_func,
    get_write_file_func,
    get_delete_file_func,
    get_copy_file_func,
    get_move_file_func
)
from angela.utils.logging import get_logger

logger = get_logger(__name__)

# Patterns for file operation commands
FILE_OPERATION_PATTERNS = [
    # mkdir patterns
    (r"^mkdir\s+(-p\s+)?(.+)$", "create_directory"),
    
    # rmdir and rm -r patterns
    (r"^rmdir\s+(.+)$", "delete_directory"),
    (r"^rm\s+(-r|-rf|--recursive)\s+(.+)$", "delete_directory"),
    
    # touch patterns
    (r"^touch\s+(.+)$", "create_file"),
    
    # cat/less/more patterns (read)
    (r"^(cat|less|more|head|tail)\s+(.+)$", "read_file"),
    
    # echo > patterns (write)
    (r"^echo\s+(.+)\s+>\s+(.+)$", "write_file"),
    (r"^echo\s+(.+)\s+>>\s+(.+)$", "append_file"),
    
    # rm patterns
    (r"^rm\s+(?!-r|--recursive)(.+)$", "delete_file"),
    
    # cp patterns
    (r"^cp\s+(?!-r|--recursive)(.+?)\s+(.+)$", "copy_file"),
    
    # mv patterns
    (r"^mv\s+(.+?)\s+(.+)$", "move_file"),
]

# Specific operation extractors
async def extract_mkdir_operation(command: str) -> Tuple[str, Dict[str, Any]]:
    """Extract mkdir operation parameters from a command."""
    tokens = shlex.split(command)
    
    # Check for -p/--parents flag
    parents = "-p" in tokens or "--parents" in tokens
    
    # Get directory paths
    paths = []
    for arg in tokens[1:]:
        if not arg.startswith("-"):
            paths.append(arg)
    
    return "create_directory", {"path": paths[0] if paths else ".", "parents": parents}


async def extract_rmdir_operation(command: str) -> Tuple[str, Dict[str, Any]]:
    """Extract rmdir operation parameters from a command."""
    tokens = shlex.split(command)
    
    # Check for recursive flag in rm commands
    recursive = any(flag in tokens for flag in ["-r", "-rf", "--recursive", "-R"])
    force = any(flag in tokens for flag in ["-f", "-rf", "--force"])
    
    # Get directory paths
    paths = []
    for arg in tokens[1:]:
        if not arg.startswith("-"):
            paths.append(arg)
    
    return "delete_directory", {
        "path": paths[0] if paths else ".",
        "recursive": recursive,
        "force": force
    }


async def extract_touch_operation(command: str) -> Tuple[str, Dict[str, Any]]:
    """Extract touch operation parameters from a command."""
    tokens = shlex.split(command)
    
    # Get file paths
    paths = []
    for arg in tokens[1:]:
        if not arg.startswith("-"):
            paths.append(arg)
    
    return "create_file", {"path": paths[0] if paths else ".", "content": None}


async def extract_cat_operation(command: str) -> Tuple[str, Dict[str, Any]]:
    """Extract cat operation parameters from a command."""
    tokens = shlex.split(command)
    
    # Get file paths
    paths = []
    for arg in tokens[1:]:
        if not arg.startswith("-"):
            paths.append(arg)
    
    # Check for binary flag
    binary = "-b" in tokens or "--binary" in tokens
    
    return "read_file", {"path": paths[0] if paths else ".", "binary": binary}


async def extract_echo_write_operation(command: str) -> Tuple[str, Dict[str, Any]]:
    """Extract echo write operation parameters from a command."""
    # Determine if this is append (>>) or overwrite (>)
    append = ">>" in command
    
    # Split by redirection operator
    parts = command.split(">>" if append else ">", 1)
    
    # Extract the echo part and the file path
    echo_part = parts[0].strip()[5:]  # Remove 'echo ' prefix
    file_path = parts[1].strip()
    
    # Handle quoted content
    if echo_part.startswith('"') and echo_part.endswith('"'):
        content = echo_part[1:-1]
    elif echo_part.startswith("'") and echo_part.endswith("'"):
        content = echo_part[1:-1]
    else:
        content = echo_part
    
    return "write_file", {
        "path": file_path,
        "content": content,
        "append": append
    }


async def extract_rm_operation(command: str) -> Tuple[str, Dict[str, Any]]:
    """Extract rm operation parameters from a command."""
    tokens = shlex.split(command)
    
    # Check for force flag
    force = "-f" in tokens or "--force" in tokens
    
    # Get file paths
    paths = []
    for arg in tokens[1:]:
        if not arg.startswith("-"):
            paths.append(arg)
    
    return "delete_file", {"path": paths[0] if paths else ".", "force": force}


async def extract_cp_operation(command: str) -> Tuple[str, Dict[str, Any]]:
    """Extract cp operation parameters from a command."""
    tokens = shlex.split(command)
    
    # Check for force/overwrite flag
    overwrite = "-f" in tokens or "--force" in tokens
    
    # Get source and destination
    args = [arg for arg in tokens[1:] if not arg.startswith("-")]
    
    if len(args) >= 2:
        source = args[0]
        destination = args[-1]  # Last argument is always the destination
    else:
        # Not enough arguments
        raise ValueError("cp command requires source and destination")
    
    return "copy_file", {
        "source": source,
        "destination": destination,
        "overwrite": overwrite
    }


async def extract_mv_operation(command: str) -> Tuple[str, Dict[str, Any]]:
    """Extract mv operation parameters from a command."""
    tokens = shlex.split(command)
    
    # Check for force/overwrite flag
    overwrite = "-f" in tokens or "--force" in tokens
    
    # Get source and destination
    args = [arg for arg in tokens[1:] if not arg.startswith("-")]
    
    if len(args) >= 2:
        source = args[0]
        destination = args[-1]  # Last argument is always the destination
    else:
        # Not enough arguments
        raise ValueError("mv command requires source and destination")
    
    return "move_file", {
        "source": source,
        "destination": destination,
        "overwrite": overwrite
    }


# Operation extractors mapping
OPERATION_EXTRACTORS = {
    "mkdir": extract_mkdir_operation,
    "rmdir": extract_rmdir_operation,
    "rm": extract_rmdir_operation if "-r" in "{command}" or "--recursive" in "{command}" else extract_rm_operation,
    "touch": extract_touch_operation,
    "cat": extract_cat_operation,
    "less": extract_cat_operation,
    "more": extract_cat_operation,
    "head": extract_cat_operation,
    "tail": extract_cat_operation,
    "echo": extract_echo_write_operation,
    "cp": extract_cp_operation,
    "mv": extract_mv_operation,
}


async def extract_file_operation(command: str) -> Optional[Tuple[str, Dict[str, Any]]]:
    """
    Extract file operation details from a command string.
    
    Args:
        command: The shell command to analyze.
        
    Returns:
        A tuple of (operation_type, parameters) or None if not a file operation.
    """
    try:
        # Get the base command
        tokens = shlex.split(command)
        if not tokens:
            return None
        
        base_cmd = tokens[0]
        
        # Check if this is a known file operation
        if base_cmd in OPERATION_EXTRACTORS:
            # Use the specific extractor for this command
            extractor = OPERATION_EXTRACTORS[base_cmd]
            
            # For rm, we need to check if it's recursive
            if base_cmd == "rm":
                if any(flag in tokens for flag in ["-r", "-rf", "--recursive", "-R"]):
                    return await extract_rmdir_operation(command)
                else:
                    return await extract_rm_operation(command)
            
            # For other commands, use the registered extractor
            return await extractor(command)
        
        # Fall back to pattern matching
        for pattern, operation_type in FILE_OPERATION_PATTERNS:
            match = re.match(pattern, command)
            if match:
                # Basic extraction based on pattern groups
                if operation_type == "create_directory":
                    return operation_type, {"path": match.group(2), "parents": bool(match.group(1))}
                elif operation_type == "delete_directory":
                    return operation_type, {"path": match.group(1), "recursive": "-r" in command or "-rf" in command}
                elif operation_type == "create_file":
                    return operation_type, {"path": match.group(1), "content": None}
                elif operation_type == "read_file":
                    return operation_type, {"path": match.group(2)}
                elif operation_type == "write_file":
                    return operation_type, {"path": match.group(2), "content": match.group(1), "append": False}
                elif operation_type == "append_file":
                    return operation_type, {"path": match.group(2), "content": match.group(1), "append": True}
                elif operation_type == "delete_file":
                    return operation_type, {"path": match.group(1)}
                elif operation_type == "copy_file":
                    parts = match.group(1).rsplit(" ", 1)
                    if len(parts) == 2:
                        return operation_type, {"source": parts[0], "destination": parts[1]}
                elif operation_type == "move_file":
                    parts = match.group(1).rsplit(" ", 1)
                    if len(parts) == 2:
                        return operation_type, {"source": parts[0], "destination": parts[1]}
        
        # Not a file operation
        return None
    
    except Exception as e:
        logger.exception(f"Error extracting file operation from '{command}': {str(e)}")
        return None


async def execute_file_operation(
    operation_type: str, 
    parameters: Dict[str, Any],
    dry_run: bool = False
) -> Dict[str, Any]:
    """
    Execute a file operation based on type and parameters.
    
    Args:
        operation_type: The type of file operation.
        parameters: Parameters for the operation.
        dry_run: Whether to simulate the operation without making changes.
        
    Returns:
        A dictionary with the operation results.
    """
    try:
        logger.info(f"Executing file operation: {operation_type}")
        logger.debug(f"Parameters: {parameters}")
        
        result = {
            "operation": operation_type,
            "parameters": parameters,
            "success": False,
            "dry_run": dry_run,
        }
        
        # Execute the appropriate operation
        if operation_type == "create_directory":
            path = parameters.get("path")
            parents = parameters.get("parents", True)
            
            create_directory_func = get_create_directory_func()
            success = await create_directory_func(path, parents=parents, dry_run=dry_run)
            result["success"] = success
            
        elif operation_type == "delete_directory":
            path = parameters.get("path")
            recursive = parameters.get("recursive", False)
            force = parameters.get("force", False)
            
            delete_directory_func = get_delete_directory_func()
            success = await delete_directory_func(
                path, recursive=recursive, force=force, dry_run=dry_run
            )
            result["success"] = success
            
        elif operation_type == "create_file":
            path = parameters.get("path")
            content = parameters.get("content")
            
            create_file_func = get_create_file_func()
            success = await create_file_func(path, content=content, dry_run=dry_run)
            result["success"] = success
            
        elif operation_type == "read_file":
            path = parameters.get("path")
            binary = parameters.get("binary", False)
            
            read_file_func = get_read_file_func()
            content = await read_file_func(path, binary=binary)
            result["content"] = content
            result["success"] = True
            
        elif operation_type == "write_file":
            path = parameters.get("path")
            content = parameters.get("content", "")
            append = parameters.get("append", False)
            
            write_file_func = get_write_file_func()
            success = await write_file_func(
                path, content, append=append, dry_run=dry_run
            )
            result["success"] = success
            
        elif operation_type == "delete_file":
            path = parameters.get("path")
            force = parameters.get("force", False)
            
            delete_file_func = get_delete_file_func()
            success = await delete_file_func(path, force=force, dry_run=dry_run)
            result["success"] = success
            
        elif operation_type == "copy_file":
            source = parameters.get("source")
            destination = parameters.get("destination")
            overwrite = parameters.get("overwrite", False)
            
            copy_file_func = get_copy_file_func()
            success = await copy_file_func(
                source, destination, overwrite=overwrite, dry_run=dry_run
            )
            result["success"] = success
            
        elif operation_type == "move_file":
            source = parameters.get("source")
            destination = parameters.get("destination")
            overwrite = parameters.get("overwrite", False)
            
            move_file_func = get_move_file_func()
            success = await move_file_func(
                source, destination, overwrite=overwrite, dry_run=dry_run
            )
            result["success"] = success
            
        else:
            logger.warning(f"Unknown file operation: {operation_type}")
            result["error"] = f"Unknown file operation: {operation_type}"
        
        return result
        
    except get_filesystem_error_class() as e:
        logger.exception(f"Error executing file operation: {str(e)}")
        return {
            "operation": operation_type,
            "parameters": parameters,
            "success": False,
            "error": str(e),
            "dry_run": dry_run,
        }
    except Exception as e:
        logger.exception(f"Unexpected error in file operation: {str(e)}")
        return {
            "operation": operation_type,
            "parameters": parameters,
            "success": False,
            "error": f"Unexpected error: {str(e)}",
            "dry_run": dry_run,
        }
</file>

<file path="components/ai/intent_analyzer.py">
# angela/ai/intent_analyzer.py

import re
import difflib
from typing import Dict, Any, List, Tuple, Optional
from pydantic import BaseModel

from angela.utils.logging import get_logger
from angela.components.ai.confidence import confidence_scorer
from angela.api.context import get_history_manager

logger = get_logger(__name__)

class IntentAnalysisResult(BaseModel):
    """Model for intent analysis results."""
    original_request: str
    normalized_request: str
    intent_type: str
    confidence: float
    entities: Dict[str, Any] = {}
    disambiguation_needed: bool = False
    possible_intents: List[Tuple[str, float]] = []

class IntentAnalyzer:
    """
    Enhanced intent analyzer with fuzzy matching and tolerance for
    misspellings and ambiguity.
    """
    
    # Define known intent patterns with examples
    INTENT_PATTERNS = {
        "file_search": [
            "find files", "search for files", "locate files", 
            "show me files", "list files matching"
        ],
        "directory_operation": [
            "create directory", "make folder", "create folder", 
            "remove directory", "delete folder"
        ],
        "file_operation": [
            "create file", "edit file", "delete file", "write to file",
            "read file", "show file contents", "copy file", "move file"
        ],
        "system_info": [
            "show system info", "check disk space", "memory usage",
            "cpu usage", "system status", "show processes"
        ],
        "git_operation": [
            "git status", "git commit", "git push", "git pull",
            "create branch", "switch branch", "merge branch"
        ],
        # Add more intent patterns as needed
    }
    
    # Common misspellings and variations
    SPELLING_VARIATIONS = {
        "directory": ["dir", "folder", "direcotry", "directroy"],
        "file": ["flie", "fil", "document"],
        "create": ["make", "creat", "crate", "new"],
        "delete": ["remove", "del", "rm", "erase"],
        "search": ["find", "look for", "locate", "seek"],
        # Add more variations
    }
    
    def __init__(self):
        """Initialize the intent analyzer."""
        self._logger = logger
    
    def normalize_request(self, request: str) -> str:
        """
        Normalize the request by fixing common misspellings and variations.
        
        Args:
            request: The original request string
            
        Returns:
            Normalized request string
        """
        normalized = request.lower()
        
        # Replace common variations with standard terms
        for standard, variations in self.SPELLING_VARIATIONS.items():
            for variation in variations:
                # Use word boundary regex to avoid partial replacements
                pattern = r'\b' + re.escape(variation) + r'\b'
                normalized = re.sub(pattern, standard, normalized)
        
        self._logger.debug(f"Normalized request: '{request}' -> '{normalized}'")
        return normalized
    
    def analyze_intent(self, request: str) -> IntentAnalysisResult:
        """
        Analyze the intent of a request with enhanced tolerance for
        variations and ambiguity.
        
        Args:
            request: The original request string
            
        Returns:
            IntentAnalysisResult with the analysis
        """
        # Normalize the request
        normalized = self.normalize_request(request)
        
        # Find closest matching intents
        matches = []
        for intent_type, patterns in self.INTENT_PATTERNS.items():
            # Calculate best match score for this intent type
            best_score = 0
            for pattern in patterns:
                similarity = difflib.SequenceMatcher(None, normalized, pattern).ratio()
                if similarity > best_score:
                    best_score = similarity
            
            # Add to matches if score is above threshold
            if best_score > 0.6:  # Adjust threshold as needed
                matches.append((intent_type, best_score))
        
        # Sort matches by confidence score
        matches.sort(key=lambda x: x[1], reverse=True)
        
        # Check if we have a clear winner or need disambiguation
        if not matches:
            # No clear intent - low confidence fallback to generic
            return IntentAnalysisResult(
                original_request=request,
                normalized_request=normalized,
                intent_type="unknown",
                confidence=0.3,
                disambiguation_needed=True
            )
        
        top_intent, top_score = matches[0]
        
        # Extract entities based on the intent type
        entities = self._extract_entities(normalized, top_intent)
        
        # Check if disambiguation is needed
        disambiguation_needed = False
        if len(matches) > 1:
            second_intent, second_score = matches[1]
            # If top two scores are close, disambiguation might be needed
            if top_score - second_score < 0.15:
                disambiguation_needed = True
                self._logger.debug(f"Ambiguous intent: {top_intent} ({top_score:.2f}) vs {second_intent} ({second_score:.2f})")
        
        # Create the result
        result = IntentAnalysisResult(
            original_request=request,
            normalized_request=normalized,
            intent_type=top_intent,
            confidence=top_score,
            entities=entities,
            disambiguation_needed=disambiguation_needed,
            possible_intents=matches[:3]  # Keep top 3 for disambiguation
        )
        
        self._logger.info(f"Intent analysis: {top_intent} (confidence: {top_score:.2f})")
        return result
    
    def _extract_entities(self, normalized: str, intent_type: str) -> Dict[str, Any]:
        """
        Extract entities from the request based on intent type.
        
        Args:
            normalized: The normalized request string
            intent_type: The type of intent
            
        Returns:
            Dictionary of extracted entities
        """
        entities = {}
        
        # Extract entities based on intent type
        if intent_type == "file_search":
            # Extract file patterns
            pattern_match = re.search(r'matching (.+?)(?: in | with | containing |$)', normalized)
            if pattern_match:
                entities["pattern"] = pattern_match.group(1)
            
            # Extract directory to search in
            dir_match = re.search(r'in (?:directory |folder |)([\w\./]+)', normalized)
            if dir_match:
                entities["directory"] = dir_match.group(1)
                
        elif intent_type == "file_operation" or intent_type == "directory_operation":
            # Extract file/directory names
            path_match = re.search(r'(?:file|directory|folder) (?:called |named |)["\'"]?([\w\./]+)["\'"]?', normalized)
            if path_match:
                entities["path"] = path_match.group(1)
                
            # Extract content if applicable
            content_match = re.search(r'with (?:content |text |)["\'](.*?)["\']', normalized)
            if content_match:
                entities["content"] = content_match.group(1)
        
        # Add more entity extraction rules for other intent types
        
        return entities
    
    async def get_interactive_disambiguation(self, result: IntentAnalysisResult) -> IntentAnalysisResult:
        """
        Get user clarification for ambiguous intent.
        
        Args:
            result: The initial analysis result
            
        Returns:
            Updated analysis result after disambiguation
        """
        # Only disambiguate if confidence is low or explicitly needed
        if result.confidence > 0.7 and not result.disambiguation_needed:
            return result
        
        self._logger.info(f"Getting disambiguation for intent: {result.intent_type}")
        
        # Import here to avoid circular imports
        from prompt_toolkit.shortcuts import radiolist_dialog
        from prompt_toolkit.styles import Style
        
        # Create options for disambiguation
        options = []
        for intent_type, score in result.possible_intents:
            # Create a human-readable description for each intent
            description = self._get_intent_description(intent_type, result.entities)
            options.append((intent_type, description))
        
        # Add a "none of these" option
        options.append(("none", "None of these - let me rephrase"))
        
        # Create dialog style
        dialog_style = Style.from_dict({
            'dialog': 'bg:#222222',
            'dialog.body': 'bg:#222222 #ffffff',
            'dialog.border': '#888888',
            'button': 'bg:#222222 #ffffff',
            'button.focused': 'bg:#0969DA #ffffff',
        })
        
        # Show the dialog
        selected_intent = radiolist_dialog(
            title="Clarification Needed",
            text=f"I'm not sure what you meant by: '{result.original_request}'\nPlease select what you intended:",
            values=options,
            style=dialog_style
        ).run()
        
        # If user selected a specific intent, update the result
        if selected_intent and selected_intent != "none":
            # Find the score for the selected intent
            selected_score = 0.85  # Default to high confidence since user confirmed
            for intent, score in result.possible_intents:
                if intent == selected_intent:
                    selected_score = max(0.85, score)  # At least 0.85 confidence
                    break
                    
            # Update the result
            result.intent_type = selected_intent
            result.confidence = selected_score
            result.disambiguation_needed = False
            
            # Re-extract entities based on the new intent
            result.entities = self._extract_entities(result.normalized_request, selected_intent)
            
            self._logger.info(f"Intent clarified: {selected_intent} (confidence: {selected_score:.2f})")
        
        return result
    
    def _get_intent_description(self, intent_type: str, entities: Dict[str, Any]) -> str:
        """
        Create a human-readable description for an intent.
        
        Args:
            intent_type: The type of intent
            entities: Extracted entities
            
        Returns:
            Human-readable description
        """
        if intent_type == "file_search":
            pattern = entities.get("pattern", "files")
            directory = entities.get("directory", "current directory")
            return f"Search for {pattern} in {directory}"
            
        elif intent_type == "file_operation":
            path = entities.get("path", "a file")
            if "create" in path or "make" in path:
                return f"Create a new file: {path}"
            elif "delete" in path or "remove" in path:
                return f"Delete file: {path}"
            else:
                return f"Perform operation on file: {path}"
                
        elif intent_type == "directory_operation":
            path = entities.get("path", "a directory")
            if "create" in path or "make" in path:
                return f"Create a new directory: {path}"
            elif "delete" in path or "remove" in path:
                return f"Delete directory: {path}"
            else:
                return f"Perform operation on directory: {path}"
                
        elif intent_type == "system_info":
            return "Show system information"
            
        elif intent_type == "git_operation":
            return "Perform Git operation"
            
        # Fallback for unknown intent types
        return f"Intent: {intent_type}"

# Global intent analyzer instance
intent_analyzer = IntentAnalyzer()
</file>

<file path="components/ai/parser.py">
# angela/ai/parser.py
import json
from typing import Dict, Any, Optional

from pydantic import BaseModel, Field, ValidationError

from angela.utils.logging import get_logger

logger = get_logger(__name__)

class CommandSuggestion(BaseModel):
    """Model for a command suggestion from the AI."""
    intent: str = Field(..., description="The classified intent of the user's request")
    command: str = Field(..., description="The suggested shell command")
    explanation: str = Field(..., description="Explanation of what the command does")
    additional_info: Optional[str] = Field(None, description="Any additional information")

# Update in angela/ai/parser.py
def parse_ai_response(response_text: str) -> CommandSuggestion:
    """Parse the AI response into a structured format."""
    try:
        # Try to extract JSON from the response
        json_str = None
        
        # Check for JSON in markdown code block with language specifier
        if "```json" in response_text and "```" in response_text.split("```json")[1]:
            json_str = response_text.split("```json")[1].split("```")[0].strip()
        # Check for JSON in regular markdown code block
        elif "```" in response_text and "```" in response_text.split("```")[1]:
            # Try without language specifier
            json_str = response_text.split("```")[1].strip()
        else:
            # Assume the entire response is JSON
            json_str = response_text.strip()
        
        # Parse the JSON
        data = json.loads(json_str)
        
        # Validate with Pydantic model
        suggestion = CommandSuggestion(**data)
        
        logger.debug(f"Successfully parsed AI response: {suggestion}")
        return suggestion
    
    except (json.JSONDecodeError, ValidationError) as e:
        logger.error(f"Failed to parse AI response: {str(e)}")
        logger.debug(f"Raw response: {response_text}")
        
        # Fallback: Try to extract just the command if JSON parsing fails
        try:
            import re
            # Improve the regex pattern to better match different formats
            command_match = re.search(r'command["\']?\s*:\s*["\']?(.*?)["\']?[,}]', response_text)
            if command_match:
                # Extract just the command value, not the whole match
                command = command_match.group(1).strip()
                # Remove any trailing quotes
                if command.endswith('"') or command.endswith("'"):
                    command = command[:-1]
                logger.debug(f"Extracted command using regex: {command}")
                return CommandSuggestion(
                    intent="unknown",
                    command=command,
                    explanation="Command extracted from incomplete response."
                )
        except Exception as regex_error:
            logger.error(f"Regex extraction also failed: {str(regex_error)}")
        
        raise ValueError(f"Could not parse AI response: {str(e)}")
</file>

<file path="components/ai/prompts.py">
# angela/ai/prompts.py
"""
Prompt engineering for Angela CLI.

This module provides a comprehensive collection of prompts and templates for the Gemini API
with enhanced context information about the current environment, project structure, file activities,
and resolved references. All prompt building functionality is centralized in this file.

The module offers various specialized prompt templates for different use cases, including:
- Command generation with rich context awareness
- File operation prompts with project-specific knowledge
- Multi-step operation planning
- Error analysis and recovery suggestions
- Code generation and manipulation prompts
"""
from typing import Dict, Any, Optional, List, Tuple, Union
from pathlib import Path
import logging

from angela.utils.logging import get_logger

logger = get_logger(__name__)

# Base system instructions
SYSTEM_INSTRUCTIONS = """
You are Angela, an AI-powered command-line assistant integrated into the user's terminal shell.
Your goal is to help users by interpreting their natural language requests and translating them into appropriate shell commands or file operations.

Follow these guidelines:
1. Prioritize standard Linux shell commands that work across different distributions.
2. Focus on practical and efficient solutions that work in a terminal environment.
3. Be clear and direct about what suggested commands will do.
4. For file operations, prefer using built-in commands like mkdir, touch, rm, etc.
5. Format your responses in a structured JSON format for consistent parsing.
6. Consider the user's context, history, and project environment in your suggestions.
7. Offer informative explanations that help users learn terminal skills over time.
"""

# Examples for few-shot learning
EXAMPLES = [
    {
        "request": "Find all Python files in this project",
        "context": {"project_root": "/home/user/project", "project_type": "python"},
        "response": {
            "intent": "search_files",
            "command": "find . -name '*.py'",
            "explanation": "This command searches for files with the .py extension in the current directory and all subdirectories."
        }
    },
    {
        "request": "Show me disk usage for the current directory",
        "context": {"cwd": "/home/user/project"},
        "response": {
            "intent": "disk_usage",
            "command": "du -sh .",
            "explanation": "This command shows the disk usage (-s) in a human-readable format (-h) for the current directory."
        }
    },
    {
        "request": "Create a directory called 'test' and a file inside it",
        "context": {"cwd": "/home/user/project"},
        "response": {
            "intent": "file_creation",
            "command": "mkdir -p test && touch test/example.txt",
            "explanation": "This command creates a directory named 'test' and an empty file named 'example.txt' inside it. The -p flag ensures parent directories are created if needed."
        }
    },
    {
        "request": "Delete all temporary files in the current directory",
        "context": {"cwd": "/home/user/project"},
        "response": {
            "intent": "file_deletion",
            "command": "find . -name '*.tmp' -type f -delete",
            "explanation": "This command finds and deletes all files with the .tmp extension in the current directory and its subdirectories. Be careful as this will permanently delete matching files."
        }
    },
    {
        "request": "Move all JavaScript files to the src directory",
        "context": {"cwd": "/home/user/project", "project_type": "node"},
        "response": {
            "intent": "file_movement",
            "command": "mkdir -p src && find . -maxdepth 1 -name '*.js' -type f -exec mv {} src/ \\;",
            "explanation": "This command creates the src directory if it doesn't exist, then finds all JavaScript files in the current directory and moves them to the src directory."
        }
    }
]

# Additional examples for file operations
FILE_OPERATION_EXAMPLES = [
    {
        "request": "Edit a file and change all instances of 'old' to 'new'",
        "context": {"cwd": "/home/user/project"},
        "response": {
            "intent": "file_edit",
            "command": "sed -i 's/old/new/g' filename.txt",
            "explanation": "This command uses sed to replace all occurrences of 'old' with 'new' in the file. The -i flag makes the changes in-place."
        }
    },
    {
        "request": "Display the first 10 lines of a log file",
        "context": {"cwd": "/home/user/project"},
        "response": {
            "intent": "file_view",
            "command": "head -n 10 logfile.log",
            "explanation": "This command displays the first 10 lines of the specified log file."
        }
    },
    {
        "request": "Create a backup of my configuration file",
        "context": {"cwd": "/home/user"},
        "response": {
            "intent": "file_backup",
            "command": "cp ~/.config/app/config.yaml ~/.config/app/config.yaml.bak",
            "explanation": "This command creates a backup copy of the configuration file by appending .bak to the filename."
        }
    }
]

# Enhanced project context template
ENHANCED_PROJECT_CONTEXT = """
## Enhanced Project Information
Project Type: {project_type}
Frameworks: {frameworks}
Main Dependencies: {dependencies}
Important Files: {important_files}
Project Structure:
- Main Directories: {main_directories}
- Total Files: {total_files}
"""

# Error analysis prompts - for generating error explanations and fixes
ERROR_ANALYSIS_PROMPT = """
Analyze the following command error and provide helpful debugging information:

Command: {command}
Error Output:
{error_output}

Consider:
1. Common syntax errors or misused flags
2. Missing dependencies or prerequisites
3. Permission issues or path problems
4. Similar commands that might work instead
5. Step-by-step debugging approach

Provide a concise explanation of the error and actionable suggestions to fix it.
"""

# Multi-step operation prompt - for complex tasks requiring multiple commands
MULTI_STEP_OPERATION_PROMPT = """
Your task is to create a sequence of commands to accomplish this goal:
{goal}

Project context:
{project_context}

Consider creating a plan that:
1. Breaks down the task into clear sequential steps
2. Handles potential errors or edge cases
3. Uses variables or temporary files when needed
4. Incorporates proper checks between steps
5. Follows best practices for the user's environment

Return a JSON object with an array of command objects, each having:
- command: the shell command to execute
- purpose: brief explanation of this step
- dependencies: list of previous step indices this depends on
- estimated_risk: number from 0-4 indicating risk level
"""

# Code generation prompt - for creating code files or snippets
CODE_GENERATION_PROMPT = """
Create code for the following purpose:
{purpose}

Language: {language}
Project Type: {project_type}
File Path: {file_path}

Requirements:
{requirements}

Include:
- Appropriate imports and dependencies
- Clear documentation and comments
- Error handling and input validation
- Best practices for {language}
- Consistency with the project's coding style

The code should be production-ready, following modern standards and design patterns.
"""

# Workflow automation prompt - for creating sequences of reusable operations
WORKFLOW_AUTOMATION_PROMPT = """
Create a reusable workflow for this scenario:
{scenario}

User's environment:
{environment}

The workflow should:
1. Be parameterizable with variables like ${FILE} or ${DIR}
2. Include appropriate error handling
3. Be efficient and avoid unnecessary steps
4. Follow best practices for shell scripting
5. Include clear documentation for each step

Format the workflow as a sequence of commands with explanations.
"""

# Recent file activity template 
RECENT_FILES_CONTEXT = """
## Recent File Activity
Recently Accessed Files:
{recent_files}

Most Active Files:
{active_files}
"""

# Resolved file references template
RESOLVED_FILES_CONTEXT = """
## Resolved File References
The following file references were resolved from your request:
{resolved_files}
"""

# Enhanced file operation prompt template
FILE_OPERATION_PROMPT_TEMPLATE = """
You are asked to perform an operation on a file with enhanced context awareness.

## File Information
Path: {file_path}
Type: {file_type}
Language: {language}
Size: {size}

## Project Context
Project Type: {project_type}
Project Root: {project_root}

## Request
{request}

Consider all this context when generating your response. If the file is part of a project,
consider how this operation might affect the project as a whole.
"""

# Advanced debugging prompt - for complex system troubleshooting
ADVANCED_DEBUGGING_PROMPT = """
Help debug this complex system issue:
{issue_description}

System Information:
- OS: {os_info}
- Environment: {environment}
- Related Components: {components}

Error logs:
{error_logs}

Recent system changes:
{recent_changes}

Provide a comprehensive debugging approach including:
1. Root cause analysis with multiple potential explanations
2. Diagnostic commands to gather more information
3. Potential solutions ranked by likelihood of success
4. Prevention strategies for future occurrences
5. Explanation of the underlying system mechanisms
"""

# Data transformation prompt - for processing and converting data
DATA_TRANSFORMATION_PROMPT = """
Transform the data according to these requirements:
{requirements}

Source data format: {source_format}
Target data format: {target_format}

Sample data:
{sample_data}

Generate a command or script that will:
1. Handle the full data set efficiently
2. Validate input and provide error handling
3. Produce the output in exactly the specified format
4. Preserve data integrity and type safety
5. Include appropriate logging or progress indication
"""

# Security audit prompt - for analyzing security implications
SECURITY_AUDIT_PROMPT = """
Perform a security audit of this command or script:
{command_or_script}

Context:
{context}

In your audit, consider:
1. Potential injection vulnerabilities or escape issues
2. Permissions and privilege escalation risks
3. Data exposure or leakage concerns
4. Resource exhaustion possibilities
5. Best practice recommendations for secure usage

Provide a security risk assessment and suggested improvements.
"""

def build_prompt(
    request: str, 
    context: Dict[str, Any],
    similar_command: Optional[str] = None,
    intent_result: Optional[Any] = None
) -> str:
    """
    Build a prompt for the Gemini API with enhanced context information.
    
    Args:
        request: The user request
        context: Context information about the current environment
        similar_command: Optional similar command from history
        intent_result: Optional intent analysis result
        
    Returns:
        A prompt string for the AI service
    """
    # Create a context description
    context_str = "Current context:\n"
    if context.get("cwd"):
        context_str += f"- Current working directory: {context['cwd']}\n"
    if context.get("project_root"):
        context_str += f"- Project root: {context['project_root']}\n"
    if context.get("project_type"):
        context_str += f"- Project type: {context['project_type']}\n"
    if context.get("relative_path"):
        context_str += f"- Path relative to project root: {context['relative_path']}\n"
    
    # Add information about the current file if available
    if context.get("current_file"):
        file_info = context["current_file"]
        context_str += f"- Current file: {file_info.get('path')}\n"
        if file_info.get("language"):
            context_str += f"- File language: {file_info.get('language')}\n"
        if file_info.get("type"):
            context_str += f"- File type: {file_info.get('type')}\n"
    
    # Add enhanced project information if available
    if context.get("enhanced_project"):
        project_info = context["enhanced_project"]
        
        # Format frameworks information
        frameworks_str = "None detected"
        if project_info.get("frameworks"):
            framework_names = list(project_info["frameworks"].keys())
            frameworks_str = ", ".join(framework_names[:5])
            if len(framework_names) > 5:
                frameworks_str += f" and {len(framework_names) - 5} more"
        
        # Format dependencies information
        dependencies_str = "None detected"
        if project_info.get("dependencies") and project_info["dependencies"].get("top_dependencies"):
            dependencies_str = ", ".join(project_info["dependencies"]["top_dependencies"][:5])
            if len(project_info["dependencies"]["top_dependencies"]) > 5:
                dependencies_str += f" and {len(project_info['dependencies']['top_dependencies']) - 5} more"
            
            # Add counts information
            if project_info["dependencies"].get("counts"):
                dependencies_str += f" (Total: {project_info['dependencies'].get('total', 0)})"
        
        # Format important files information
        important_files_str = "None detected"
        if project_info.get("important_files") and project_info["important_files"].get("paths"):
            important_files_str = ", ".join(project_info["important_files"]["paths"][:5])
            if len(project_info["important_files"]["paths"]) > 5:
                important_files_str += f" and {len(project_info['important_files']['paths']) - 5} more"
        
        # Format main directories information
        main_directories_str = "None detected"
        if project_info.get("structure") and project_info["structure"].get("main_directories"):
            main_directories_str = ", ".join(project_info["structure"]["main_directories"])
        
        # Format total files information
        total_files_str = "Unknown"
        if project_info.get("structure") and "total_files" in project_info["structure"]:
            total_files_str = str(project_info["structure"]["total_files"])
        
        # Add to context string
        context_str += ENHANCED_PROJECT_CONTEXT.format(
            project_type=project_info.get("type", "Unknown"),
            frameworks=frameworks_str,
            dependencies=dependencies_str,
            important_files=important_files_str,
            main_directories=main_directories_str,
            total_files=total_files_str
        )
    
    # Add recent file activity if available
    if context.get("recent_files"):
        recent_files = context["recent_files"]
        
        # Format recent files information
        recent_files_str = "None"
        if recent_files.get("accessed"):
            # Extract filenames only for brevity
            recent_filenames = [Path(path).name for path in recent_files["accessed"][:5]]
            recent_files_str = ", ".join(recent_filenames)
            if len(recent_files["accessed"]) > 5:
                recent_files_str += f" and {len(recent_files['accessed']) - 5} more"
        
        # Format active files information
        active_files_str = "None"
        if recent_files.get("activities"):
            active_files_str = ", ".join([a.get("name", "unknown") for a in recent_files["activities"][:3]])
            if len(recent_files["activities"]) > 3:
                active_files_str += f" and {len(recent_files['activities']) - 3} more"
        
        # Add to context string
        context_str += RECENT_FILES_CONTEXT.format(
            recent_files=recent_files_str,
            active_files=active_files_str
        )
    
    # Add resolved file references if available
    if context.get("resolved_files"):
        resolved_files = context["resolved_files"]
        
        # Format resolved files information
        resolved_files_str = ""
        for ref_info in resolved_files:
            reference = ref_info.get("reference", "")
            path = ref_info.get("path", "Not found")
            resolved_files_str += f"- '{reference}' → {path}\n"
        
        # Add to context string
        if resolved_files_str:
            context_str += RESOLVED_FILES_CONTEXT.format(
                resolved_files=resolved_files_str
            )
    
    # Add conversation context
    if "session" in context:
        session = context["session"]
        
        # Add recent commands for continuity
        if session.get("recent_commands"):
            context_str += "Recent commands:\n"
            for i, cmd in enumerate(session.get("recent_commands", []), 1):
                context_str += f"- Command {i}: {cmd}\n"
        
        # Add recent results for reference
        if session.get("recent_results"):
            context_str += "Recent command results:\n"
            for i, result in enumerate(session.get("recent_results", []), 1):
                # Truncate long results
                if len(result) > 200:
                    result = result[:200] + "..."
                context_str += f"- Result {i}: {result}\n"
        
        # Add entities for reference resolution
        if session.get("entities"):
            context_str += "Referenced entities:\n"
            for name, entity in session.get("entities", {}).items():
                context_str += f"- {name}: {entity.get('type')} - {entity.get('value')}\n"
    
    # Add intent analysis if available - FIXED SECTION
    if intent_result:
        context_str += "\nIntent analysis:\n"
        
        # Check if intent_result is a dictionary or a Pydantic model
        if hasattr(intent_result, "__dict__") and not hasattr(intent_result, "get"):
            # It's a Pydantic model - use attribute access
            context_str += f"- Intent type: {getattr(intent_result, 'intent_type', 'unknown')}\n"
            context_str += f"- Confidence: {getattr(intent_result, 'confidence', 0.0):.2f}\n"
            
            # Add extracted entities safely
            if hasattr(intent_result, "entities") and intent_result.entities:
                context_str += "- Extracted entities:\n"
                for key, value in intent_result.entities.items():
                    context_str += f"  - {key}: {value}\n"
        else:
            # Fall back to dictionary-style access for backwards compatibility
            context_str += f"- Intent type: {intent_result.get('intent_type', 'unknown')}\n"
            context_str += f"- Confidence: {intent_result.get('confidence', 0.0):.2f}\n"
            
            # Add extracted entities
            if intent_result.get("entities"):
                context_str += "- Extracted entities:\n"
                for key, value in intent_result.get("entities", {}).items():
                    context_str += f"  - {key}: {value}\n"
    
    # Add similar command suggestion if available
    if similar_command:
        context_str += f"\nYou previously suggested this similar command: {similar_command}\n"
    
    # Add examples for few-shot learning
    examples_str = "Examples:\n"
    
    # Add standard examples
    for example in EXAMPLES:
        examples_str += f"\nUser request: {example['request']}\n"
        examples_str += f"Context: {example['context']}\n"
        examples_str += f"Response: {example['response']}\n"
    
    # Add file operation examples
    for example in FILE_OPERATION_EXAMPLES:
        examples_str += f"\nUser request: {example['request']}\n"
        examples_str += f"Context: {example['context']}\n"
        examples_str += f"Response: {example['response']}\n"
    
    # Define the expected response format - now with confidence indicator
    response_format = """
Expected response format (valid JSON):
{
    "intent": "the_classified_intent",
    "command": "the_suggested_command",
    "explanation": "explanation of what the command does",
    "confidence": 0.85, /* Optional confidence score from 0.0 to 1.0 */
    "additional_info": "any additional information (optional)"
}
"""
    
    # Build the complete prompt
    prompt = f"{SYSTEM_INSTRUCTIONS}\n\n{context_str}\n\n{examples_str}\n\n{response_format}\n\nUser request: {request}\n\nResponse:"
    
    logger.debug(f"Built prompt with length: {len(prompt)}")
    return prompt


# Terminal customization prompt - for personalized environment setup
TERMINAL_CUSTOMIZATION_PROMPT = """
Create a customization plan for the user's terminal environment:

Current setup:
{current_setup}

User preferences:
{preferences}

Usage patterns:
{usage_patterns}

The customization should include:
1. Shell configuration recommendations (.bashrc, .zshrc, etc.)
2. Prompt styling and information display
3. Aliases and functions for common operations
4. Productivity tools and utilities
5. Performance optimization settings

Provide detailed implementation instructions with code snippets.
"""

# System administration prompt - for advanced system tasks
SYSTEM_ADMINISTRATION_PROMPT = """
Provide a solution for this system administration task:
{task_description}

System details:
{system_details}

Requirements:
{requirements}

The solution should:
1. Be robust and handle edge cases
2. Include appropriate logging and monitoring
3. Consider security implications
4. Be efficient and scalable
5. Follow system administration best practices

Provide detailed implementation steps with commands and configuration.
"""

# Project analysis prompt - for codebase understanding
PROJECT_ANALYSIS_PROMPT = """
Analyze this software project:
{project_summary}

Key files:
{key_files}

Key questions:
{questions}

Provide an in-depth analysis covering:
1. Architecture and design patterns
2. Component relationships and dependencies
3. Potential technical debt or improvement areas
4. Performance and scalability considerations
5. Security posture and risk assessment

The analysis should be actionable and prioritized by impact.
"""

def build_file_operation_prompt(
    operation: str, 
    parameters: Dict[str, Any], 
    context: Dict[str, Any]
) -> str:
    """
    Build a prompt for generating a file operation command.
    
    Args:
        operation: The type of file operation (e.g., 'create_file', 'delete_directory').
        parameters: Parameters for the operation.
        context: Context information about the current environment.
        
    Returns:
        A prompt string for the Gemini API.
    """
    # Create file information
    file_path = parameters.get("path", "Unknown")
    
    # Get file info if available
    file_info = {}
    if file_path != "Unknown":
        try:
            from angela.context.manager import context_manager
            file_info = context_manager.get_file_info(Path(file_path))
        except Exception as e:
            logger.debug(f"Error getting file info: {str(e)}")
    
    file_info_str = f"""
File: {file_path}
Type: {file_info.get('type', 'Unknown')}
Language: {file_info.get('language', 'Unknown')}
"""
    
    # Create a description of the requested operation
    operation_str = f"""
Requested file operation: {operation}
Parameters:
"""
    for key, value in parameters.items():
        operation_str += f"- {key}: {value}\n"
    
    # Create enhanced project context
    project_context_str = "Project Context:\n"
    
    if context.get("enhanced_project"):
        project_info = context["enhanced_project"]
        project_context_str += f"- Project Type: {project_info.get('type', 'Unknown')}\n"
        
        # Add frameworks if available
        if project_info.get("frameworks"):
            frameworks = list(project_info["frameworks"].keys())
            project_context_str += f"- Frameworks: {', '.join(frameworks[:3])}"
            if len(frameworks) > 3:
                project_context_str += f" and {len(frameworks) - 3} more"
            project_context_str += "\n"
        
        # Add file's relationship to project
        if file_path != "Unknown" and context.get("project_root"):
            try:
                rel_path = Path(file_path).relative_to(Path(context["project_root"]))
                project_context_str += f"- Relative Path: {rel_path}\n"
            except ValueError:
                # File is not within project root
                project_context_str += f"- Note: File is outside project root\n"
    
    # Define the task
    task_str = f"""
Your task is to generate a shell command that will perform the requested file operation.
The command should be safe, efficient, and follow best practices for Linux/Unix shell environments.
Consider the file type, language, and project context when generating the command.
"""
    
    # Define the expected response format
    response_format = """
Expected response format (valid JSON):
{
    "command": "the_shell_command",
    "explanation": "explanation of what the command does",
    "risk_level": "SAFE|LOW|MEDIUM|HIGH|CRITICAL",
    "destructive": true|false
}
"""
    
    # Build the complete prompt
    prompt = f"{SYSTEM_INSTRUCTIONS}\n\n{operation_str}\n\n{file_info_str}\n\n{project_context_str}\n\n{task_str}\n\n{response_format}\n\nResponse:"
    
    logger.debug(f"Built file operation prompt with length: {len(prompt)}")
    return prompt
</file>

<file path="components/ai/semantic_analyzer.py">
# angela/ai/semantic_analyzer.py
"""
Semantic code analysis for Angela CLI.

This module provides deep code understanding capabilities, extracting semantic
information from source code files to enable context-aware assistance.
"""
import os
import re
import ast
import json
import asyncio
import importlib.util
from pathlib import Path
from typing import Dict, Any, List, Tuple, Optional, Set, Union, NamedTuple
from collections import defaultdict

from angela.utils.logging import get_logger
from angela.api.context import get_file_detector_func
from angela.api.ai import get_gemini_client, get_gemini_request_class

logger = get_logger(__name__)

class CodeEntity:
    """Base class for code entities like functions, classes, and variables."""
    
    def __init__(self, name: str, line_start: int, line_end: int, filename: str):
        self.name = name
        self.line_start = line_start
        self.line_end = line_end
        self.filename = filename
        self.references: List[Tuple[str, int]] = []  # (filename, line)
        self.dependencies: List[str] = []  # Names of other entities this depends on
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for serialization."""
        return {
            "name": self.name,
            "type": self.__class__.__name__,
            "line_start": self.line_start,
            "line_end": self.line_end,
            "filename": self.filename,
            "references": self.references,
            "dependencies": self.dependencies
        }
    
    def __str__(self) -> str:
        return f"{self.__class__.__name__}(name={self.name}, file={Path(self.filename).name}:{self.line_start}-{self.line_end})"


class Function(CodeEntity):
    """Represents a function or method in code."""
    
    def __init__(self, name: str, line_start: int, line_end: int, filename: str, 
                 params: List[str], docstring: Optional[str] = None,
                 is_method: bool = False, decorators: List[str] = None,
                 return_type: Optional[str] = None, class_name: Optional[str] = None):
        super().__init__(name, line_start, line_end, filename)
        self.params = params
        self.docstring = docstring
        self.is_method = is_method
        self.decorators = decorators or []
        self.return_type = return_type
        self.class_name = class_name
        self.called_functions: List[str] = []
        self.complexity: Optional[int] = None  # Cyclomatic complexity
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for serialization."""
        result = super().to_dict()
        result.update({
            "params": self.params,
            "docstring": self.docstring,
            "is_method": self.is_method,
            "decorators": self.decorators,
            "return_type": self.return_type,
            "class_name": self.class_name,
            "called_functions": self.called_functions,
            "complexity": self.complexity
        })
        return result


class Class(CodeEntity):
    """Represents a class in code."""
    
    def __init__(self, name: str, line_start: int, line_end: int, filename: str,
                 docstring: Optional[str] = None, base_classes: List[str] = None,
                 decorators: List[str] = None):
        super().__init__(name, line_start, line_end, filename)
        self.docstring = docstring
        self.base_classes = base_classes or []
        self.decorators = decorators or []
        self.methods: Dict[str, Function] = {}
        self.attributes: Dict[str, Variable] = {}
        self.nested_classes: Dict[str, 'Class'] = {}
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for serialization."""
        result = super().to_dict()
        result.update({
            "docstring": self.docstring,
            "base_classes": self.base_classes,
            "decorators": self.decorators,
            "methods": {name: method.to_dict() for name, method in self.methods.items()},
            "attributes": {name: attr.to_dict() for name, attr in self.attributes.items()},
            "nested_classes": {name: cls.to_dict() for name, cls in self.nested_classes.items()}
        })
        return result


class Variable(CodeEntity):
    """Represents a variable or attribute in code."""
    
    def __init__(self, name: str, line_start: int, line_end: int, filename: str,
                 var_type: Optional[str] = None, value: Optional[str] = None,
                 is_attribute: bool = False, class_name: Optional[str] = None,
                 is_constant: bool = False):
        super().__init__(name, line_start, line_end, filename)
        self.var_type = var_type
        self.value = value
        self.is_attribute = is_attribute
        self.class_name = class_name
        self.is_constant = is_constant
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for serialization."""
        result = super().to_dict()
        result.update({
            "var_type": self.var_type,
            "value": self.value,
            "is_attribute": self.is_attribute,
            "class_name": self.class_name,
            "is_constant": self.is_constant
        })
        return result


class Import(CodeEntity):
    """Represents an import statement."""
    
    def __init__(self, name: str, line_start: int, line_end: int, filename: str,
                 import_path: str, is_from: bool = False, alias: Optional[str] = None):
        super().__init__(name, line_start, line_end, filename)
        self.import_path = import_path
        self.is_from = is_from
        self.alias = alias
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for serialization."""
        result = super().to_dict()
        result.update({
            "import_path": self.import_path,
            "is_from": self.is_from,
            "alias": self.alias
        })
        return result


class Module:
    """Represents a code module (file) with its entities."""
    
    def __init__(self, filename: str):
        self.filename = filename
        self.imports: Dict[str, Import] = {}
        self.functions: Dict[str, Function] = {}
        self.classes: Dict[str, Class] = {}
        self.variables: Dict[str, Variable] = {}
        self.docstring: Optional[str] = None
        self.language: Optional[str] = None
        self.dependencies: List[str] = []
        self.last_modified: Optional[float] = None
        self.code_metrics: Dict[str, Any] = {}
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for serialization."""
        return {
            "filename": self.filename,
            "imports": {name: imp.to_dict() for name, imp in self.imports.items()},
            "functions": {name: func.to_dict() for name, func in self.functions.items()},
            "classes": {name: cls.to_dict() for name, cls in self.classes.items()},
            "variables": {name: var.to_dict() for name, var in self.variables.items()},
            "docstring": self.docstring,
            "language": self.language,
            "dependencies": self.dependencies,
            "last_modified": self.last_modified,
            "code_metrics": self.code_metrics
        }

    def get_summary(self) -> Dict[str, Any]:
        """Get a simplified summary of the module."""
        return {
            "filename": self.filename,
            "name": Path(self.filename).name,
            "language": self.language,
            "class_count": len(self.classes),
            "function_count": len(self.functions),
            "import_count": len(self.imports),
            "docstring": self.docstring[:100] + "..." if self.docstring and len(self.docstring) > 100 else self.docstring,
            "classes": list(self.classes.keys()),
            "key_functions": list(self.functions.keys())[:5] + (["..."] if len(self.functions) > 5 else []),
            "dependencies": self.dependencies
        }


class SemanticAnalyzer:
    """
    Semantic code analyzer that extracts deeper meaning from source files.
    
    This class provides:
    1. Extraction of code structure (functions, classes, variables)
    2. Analysis of dependencies and references
    3. Code metrics and complexity information
    4. Integration with the LLM for deeper insights
    """
    
    def __init__(self):
        self._logger = logger
        self._modules: Dict[str, Module] = {}
        self._language_analyzers: Dict[str, callable] = {
            "python": self._analyze_python_file,
            "javascript": self._analyze_javascript_file,
            "typescript": self._analyze_typescript_file,
            "java": self._analyze_with_llm,
            "c#": self._analyze_with_llm,
            "c++": self._analyze_with_llm,
            "ruby": self._analyze_with_llm,
            "go": self._analyze_with_llm,
            "rust": self._analyze_with_llm
        }
        self._cache_valid_time = 300  # Seconds before a cached analysis is considered stale
    
    async def analyze_file(self, file_path: Union[str, Path]) -> Optional[Module]:
        """
        Analyze a source code file to extract semantic information.
        
        Args:
            file_path: Path to the file to analyze
            
        Returns:
            Module object with semantic information or None if analysis failed
        """
        path_obj = Path(file_path)
        
        # Check if we have a recent cached analysis
        if str(path_obj) in self._modules:
            module = self._modules[str(path_obj)]
            if module.last_modified and path_obj.stat().st_mtime <= module.last_modified:
                self._logger.debug(f"Using cached analysis for {path_obj}")
                return module
        
        # Check if file exists
        if not path_obj.exists():
            self._logger.warning(f"File not found for semantic analysis: {path_obj}")
            return None
        
        # Detect file type using the API layer
        detect_file_type = get_file_detector_func()
        file_info = detect_file_type(path_obj)
        language = file_info.get("language", "").lower()
        
        # Skip if this isn't a supported code file
        if not language or language.lower() not in self._language_analyzers:
            self._logger.debug(f"Unsupported language for semantic analysis: {language} in {path_obj}")
            return None
        
        # Create a new module
        module = Module(str(path_obj))
        module.language = language
        module.last_modified = path_obj.stat().st_mtime
        
        try:
            # Call the appropriate analyzer based on language
            analyzer = self._language_analyzers.get(language.lower(), self._analyze_with_llm)
            
            if asyncio.iscoroutinefunction(analyzer):
                result = await analyzer(path_obj, module)
            else:
                result = analyzer(path_obj, module)
            
            if result:
                self._modules[str(path_obj)] = module
                self._logger.info(f"Completed semantic analysis of {path_obj}")
                return module
        except Exception as e:
            self._logger.exception(f"Error analyzing {path_obj}: {str(e)}")
        
        return None
    
    async def analyze_project_files(self, project_root: Union[str, Path], max_files: int = 100) -> Dict[str, Module]:
        """
        Analyze multiple source files within a project.
        
        Args:
            project_root: Root directory of the project
            max_files: Maximum number of files to analyze
            
        Returns:
            Dictionary of file paths to Module objects
        """
        root_path = Path(project_root)
        
        # Find source code files
        source_files = []
        
        for language, _ in self._language_analyzers.items():
            extensions = self._get_extensions_for_language(language)
            for ext in extensions:
                source_files.extend(list(root_path.glob(f"**/*{ext}")))
        
        # Exclude files that shouldn't be analyzed
        exclude_patterns = [
            "**/node_modules/**", "**/venv/**", "**/.venv/**",
            "**/.git/**", "**/build/**", "**/dist/**",
            "**/__pycache__/**", "**/.pytest_cache/**"
        ]
        
        for pattern in exclude_patterns:
            source_files = [f for f in source_files if not self._matches_glob_pattern(str(f), pattern)]
        
        # Limit to max files
        source_files = source_files[:max_files]
        
        # Analyze each file
        analysis_results = {}
        
        for file_path in source_files:
            module = await self.analyze_file(file_path)
            if module:
                analysis_results[str(file_path)] = module
        
        # Analyze references between modules
        self._analyze_cross_module_references(analysis_results)
        
        return analysis_results
    
    def _get_extensions_for_language(self, language: str) -> List[str]:
        """Get file extensions for a given language."""
        extensions_map = {
            "python": [".py", ".pyi", ".pyx"],
            "javascript": [".js", ".jsx", ".mjs"],
            "typescript": [".ts", ".tsx"],
            "java": [".java"],
            "c#": [".cs"],
            "c++": [".cpp", ".cc", ".h", ".hpp"],
            "ruby": [".rb"],
            "go": [".go"],
            "rust": [".rs"]
        }
        
        return extensions_map.get(language.lower(), [])
    
    def _matches_glob_pattern(self, path: str, pattern: str) -> bool:
        """Check if a path matches a glob pattern."""
        import fnmatch
        return fnmatch.fnmatch(path, pattern)
    
    def _analyze_python_file(self, file_path: Path, module: Module) -> bool:
        """
        Analyze a Python file using the ast module.
        
        Args:
            file_path: Path to the Python file
            module: Module object to populate
            
        Returns:
            True if analysis was successful, False otherwise
        """
        # Read the file content
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
        except Exception as e:
            self._logger.error(f"Error reading Python file {file_path}: {str(e)}")
            return False
        
        # Parse the AST
        try:
            tree = ast.parse(content, filename=str(file_path))
            
            # Get module docstring
            module.docstring = ast.get_docstring(tree)
            
            # Visit all nodes in the AST to extract information
            for node in ast.walk(tree):
                # Extract imports
                if isinstance(node, ast.Import):
                    for name in node.names:
                        import_name = name.asname or name.name
                        module.imports[import_name] = Import(
                            name=import_name,
                            line_start=node.lineno,
                            line_end=node.lineno,
                            filename=str(file_path),
                            import_path=name.name,
                            is_from=False,
                            alias=name.asname
                        )
                        module.dependencies.append(name.name)
                
                elif isinstance(node, ast.ImportFrom):
                    module_name = node.module or ""
                    for name in node.names:
                        import_name = name.asname or name.name
                        full_path = f"{module_name}.{name.name}" if module_name else name.name
                        module.imports[import_name] = Import(
                            name=import_name,
                            line_start=node.lineno,
                            line_end=node.lineno,
                            filename=str(file_path),
                            import_path=full_path,
                            is_from=True,
                            alias=name.asname
                        )
                        module.dependencies.append(full_path)
                
                # Extract functions
                elif isinstance(node, ast.FunctionDef) or isinstance(node, ast.AsyncFunctionDef):
                    # Check if we're inside a class
                    parent_class = None
                    for ancestor in ast.walk(tree):
                        if isinstance(ancestor, ast.ClassDef):
                            # Check if node is in the body of this class
                            if any(node is child for child in ancestor.body):
                                parent_class = ancestor.name
                                break
                    
                    # Get function parameters
                    params = []
                    for arg in node.args.args:
                        params.append(arg.arg)
                    
                    # Get decorators
                    decorators = []
                    for decorator in node.decorator_list:
                        if isinstance(decorator, ast.Name):
                            decorators.append(decorator.id)
                        elif isinstance(decorator, ast.Attribute):
                            decorators.append(f"{decorator.value.id}.{decorator.attr}")
                        elif isinstance(decorator, ast.Call):
                            if isinstance(decorator.func, ast.Name):
                                decorators.append(decorator.func.id)
                            elif isinstance(decorator.func, ast.Attribute):
                                decorators.append(f"{decorator.func.value.id}.{decorator.func.attr}")
                    
                    # Get return type annotation if available
                    return_type = None
                    if node.returns:
                        return_type = self._get_type_annotation(node.returns)
                    
                    # Create function entity
                    function = Function(
                        name=node.name,
                        line_start=node.lineno,
                        line_end=self._get_last_line(node),
                        filename=str(file_path),
                        params=params,
                        docstring=ast.get_docstring(node),
                        is_method=parent_class is not None,
                        decorators=decorators,
                        return_type=return_type,
                        class_name=parent_class
                    )
                    
                    # Extract called functions
                    for child in ast.walk(node):
                        if isinstance(child, ast.Call):
                            if isinstance(child.func, ast.Name):
                                function.called_functions.append(child.func.id)
                            elif isinstance(child.func, ast.Attribute):
                                if isinstance(child.func.value, ast.Name):
                                    function.called_functions.append(f"{child.func.value.id}.{child.func.attr}")
                    
                    # Calculate cyclomatic complexity
                    function.complexity = self._calculate_complexity(node)
                    
                    # Store the function in the appropriate place
                    if parent_class and parent_class in module.classes:
                        module.classes[parent_class].methods[node.name] = function
                    else:
                        module.functions[node.name] = function
                
                # Extract classes
                elif isinstance(node, ast.ClassDef):
                    # Get base classes
                    base_classes = []
                    for base in node.bases:
                        if isinstance(base, ast.Name):
                            base_classes.append(base.id)
                        elif isinstance(base, ast.Attribute):
                            base_classes.append(f"{base.value.id}.{base.attr}")
                    
                    # Get decorators
                    decorators = []
                    for decorator in node.decorator_list:
                        if isinstance(decorator, ast.Name):
                            decorators.append(decorator.id)
                    
                    # Create class entity
                    class_entity = Class(
                        name=node.name,
                        line_start=node.lineno,
                        line_end=self._get_last_line(node),
                        filename=str(file_path),
                        docstring=ast.get_docstring(node),
                        base_classes=base_classes,
                        decorators=decorators
                    )
                    
                    # Look for class attributes
                    for child in node.body:
                        if isinstance(child, ast.Assign):
                            for target in child.targets:
                                if isinstance(target, ast.Name):
                                    # Get value as string
                                    value = None
                                    if isinstance(child.value, ast.Constant):
                                        value = str(child.value.value)
                                    
                                    # Create attribute entity
                                    attribute = Variable(
                                        name=target.id,
                                        line_start=child.lineno,
                                        line_end=child.lineno,
                                        filename=str(file_path),
                                        var_type=None,  # No type annotation in assign
                                        value=value,
                                        is_attribute=True,
                                        class_name=node.name
                                    )
                                    
                                    class_entity.attributes[target.id] = attribute
                        
                        elif isinstance(child, ast.AnnAssign) and isinstance(child.target, ast.Name):
                            # Get type annotation
                            var_type = self._get_type_annotation(child.annotation)
                            
                            # Get value as string
                            value = None
                            if child.value and isinstance(child.value, ast.Constant):
                                value = str(child.value.value)
                            
                            # Create attribute entity
                            attribute = Variable(
                                name=child.target.id,
                                line_start=child.lineno,
                                line_end=child.lineno,
                                filename=str(file_path),
                                var_type=var_type,
                                value=value,
                                is_attribute=True,
                                class_name=node.name
                            )
                            
                            class_entity.attributes[child.target.id] = attribute
                    
                    module.classes[node.name] = class_entity
                
                # Extract global variables
                elif isinstance(node, ast.Assign) and all(isinstance(target, ast.Name) for target in node.targets):
                    for target in node.targets:
                        # Skip private variables
                        if target.id.startswith('_'):
                            continue
                        
                        # Get value as string
                        value = None
                        if isinstance(node.value, ast.Constant):
                            value = str(node.value.value)
                        
                        # Check if this is a constant
                        is_constant = target.id.isupper()
                        
                        # Create variable entity
                        variable = Variable(
                            name=target.id,
                            line_start=node.lineno,
                            line_end=node.lineno,
                            filename=str(file_path),
                            var_type=None,  # No type annotation in assign
                            value=value,
                            is_constant=is_constant
                        )
                        
                        module.variables[target.id] = variable
                
                elif isinstance(node, ast.AnnAssign) and isinstance(node.target, ast.Name):
                    # Skip private variables
                    if node.target.id.startswith('_'):
                        continue
                    
                    # Get type annotation
                    var_type = self._get_type_annotation(node.annotation)
                    
                    # Get value as string
                    value = None
                    if node.value and isinstance(node.value, ast.Constant):
                        value = str(node.value.value)
                    
                    # Check if this is a constant
                    is_constant = node.target.id.isupper()
                    
                    # Create variable entity
                    variable = Variable(
                        name=node.target.id,
                        line_start=node.lineno,
                        line_end=node.lineno,
                        filename=str(file_path),
                        var_type=var_type,
                        value=value,
                        is_constant=is_constant
                    )
                    
                    module.variables[node.target.id] = variable
            
            # Calculate code metrics
            module.code_metrics = {
                "total_lines": len(content.splitlines()),
                "code_lines": len([line for line in content.splitlines() if line.strip() and not line.strip().startswith('#')]),
                "comment_lines": len([line for line in content.splitlines() if line.strip().startswith('#')]),
                "blank_lines": len([line for line in content.splitlines() if not line.strip()]),
                "function_count": len(module.functions),
                "class_count": len(module.classes),
                "import_count": len(module.imports),
                "complexity": sum(func.complexity or 0 for func in module.functions.values()),
                "average_function_size": sum(func.line_end - func.line_start for func in module.functions.values()) / len(module.functions) if module.functions else 0
            }
            
            return True
        
        except SyntaxError as e:
            self._logger.warning(f"Syntax error in Python file {file_path}: {str(e)}")
            return False
            
        except Exception as e:
            self._logger.error(f"Error parsing Python file {file_path}: {str(e)}")
            return False
    
    def _get_type_annotation(self, annotation) -> Optional[str]:
        """Extract type annotation string from AST node."""
        if isinstance(annotation, ast.Name):
            return annotation.id
        elif isinstance(annotation, ast.Attribute):
            if isinstance(annotation.value, ast.Name):
                return f"{annotation.value.id}.{annotation.attr}"
            return annotation.attr
        elif isinstance(annotation, ast.Subscript):
            if isinstance(annotation.value, ast.Name):
                if isinstance(annotation.slice, ast.Name):
                    return f"{annotation.value.id}[{annotation.slice.id}]"
                elif isinstance(annotation.slice, ast.Constant):
                    return f"{annotation.value.id}[{annotation.slice.value}]"
                return f"{annotation.value.id}[...]"
            return "..."
        return None
    
    def _get_last_line(self, node) -> int:
        """Get the last line number of an AST node."""
        # If the node has an end_lineno attribute (Python 3.8+), use it
        if hasattr(node, 'end_lineno'):
            return node.end_lineno
        
        # Otherwise, find the maximum lineno in the node and its children
        max_lineno = node.lineno
        for child in ast.iter_child_nodes(node):
            max_lineno = max(max_lineno, self._get_last_line(child))
        return max_lineno
    
    def _calculate_complexity(self, node) -> int:
        """Calculate cyclomatic complexity of a function."""
        complexity = 1  # Start with 1 (default path)
        
        # Count branches
        for child in ast.walk(node):
            if isinstance(child, (ast.If, ast.While, ast.For, ast.IfExp)):
                complexity += 1
            elif isinstance(child, ast.BoolOp) and isinstance(child.op, ast.And):
                complexity += len(child.values) - 1
            elif isinstance(child, ast.BoolOp) and isinstance(child.op, ast.Or):
                complexity += len(child.values) - 1
            elif isinstance(child, ast.Try):
                complexity += len(child.handlers)  # Count except blocks
        
        return complexity
    
    async def _analyze_javascript_file(self, file_path: Path, module: Module) -> bool:
        """
        Analyze a JavaScript file using a simple regex-based approach or LLM.
        
        Args:
            file_path: Path to the JavaScript file
            module: Module object to populate
            
        Returns:
            True if analysis was successful, False otherwise
        """
        # For non-Python files, we'll use a simple regex-based approach for now
        # In a real implementation, you might want to use language-specific parsers
        
        try:
            # Read the file content
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # Extract imports/requires
            import_patterns = [
                r'import\s+{([^}]+)}\s+from\s+[\'"]([^\'"]+)[\'"]',  # import { x, y } from 'module'
                r'import\s+(\w+)\s+from\s+[\'"]([^\'"]+)[\'"]',  # import x from 'module'
                r'import\s+[\'"]([^\'"]+)[\'"]',  # import 'module'
                r'const\s+{([^}]+)}\s+=\s+require\([\'"]([^\'"]+)[\'"]\)',  # const { x, y } = require('module')
                r'const\s+(\w+)\s+=\s+require\([\'"]([^\'"]+)[\'"]\)'  # const x = require('module')
            ]
            
            line_num = 1
            for line in content.splitlines():
                for pattern in import_patterns:
                    for match in re.finditer(pattern, line):
                        if len(match.groups()) == 2:
                            names, module_path = match.groups()
                            if ',' in names:
                                # Multiple imports
                                for name in names.split(','):
                                    name = name.strip()
                                    if name:
                                        module.imports[name] = Import(
                                            name=name,
                                            line_start=line_num,
                                            line_end=line_num,
                                            filename=str(file_path),
                                            import_path=f"{module_path}.{name}",
                                            is_from=True
                                        )
                                        module.dependencies.append(module_path)
                            else:
                                # Single import
                                name = names.strip()
                                module.imports[name] = Import(
                                    name=name,
                                    line_start=line_num,
                                    line_end=line_num,
                                    filename=str(file_path),
                                    import_path=module_path,
                                    is_from=True
                                )
                                module.dependencies.append(module_path)
                        else:
                            # Simple import
                            module_path = match.group(1)
                            module.dependencies.append(module_path)
                line_num += 1
            
            # Extract functions
            function_patterns = [
                r'function\s+(\w+)\s*\(([^)]*)\)',  # function name(params)
                r'const\s+(\w+)\s*=\s*(?:async\s*)?\([^)]*\)\s*=>\s*{',  # const name = (params) => {
                r'let\s+(\w+)\s*=\s*(?:async\s*)?\([^)]*\)\s*=>\s*{',  # let name = (params) => {
                r'var\s+(\w+)\s*=\s*(?:async\s*)?\([^)]*\)\s*=>\s*{',  # var name = (params) => {
                r'async\s+function\s+(\w+)\s*\(([^)]*)\)'  # async function name(params)
            ]
            
            for pattern in function_patterns:
                for match in re.finditer(pattern, content, re.MULTILINE):
                    name = match.group(1)
                    params = []
                    if len(match.groups()) > 1:
                        params = [p.strip() for p in match.group(2).split(',') if p.strip()]
                    
                    start_line = content[:match.start()].count('\n') + 1
                    end_line = start_line + content[match.start():].split('{', 1)[1].count('\n')
                    
                    function = Function(
                        name=name,
                        line_start=start_line,
                        line_end=end_line if end_line > start_line else start_line + 5,  # Estimate if we couldn't find the end
                        filename=str(file_path),
                        params=params
                    )
                    
                    module.functions[name] = function
            
            # Extract classes
            class_pattern = r'class\s+(\w+)(?:\s+extends\s+(\w+))?\s*{'
            for match in re.finditer(class_pattern, content, re.MULTILINE):
                name = match.group(1)
                base_classes = []
                if match.group(2):
                    base_classes.append(match.group(2))
                
                start_line = content[:match.start()].count('\n') + 1
                
                # Try to find the end of the class
                class_content = content[match.start():]
                open_braces = 0
                for i, char in enumerate(class_content):
                    if char == '{':
                        open_braces += 1
                    elif char == '}':
                        open_braces -= 1
                        if open_braces == 0:
                            end_line = start_line + class_content[:i+1].count('\n')
                            break
                else:
                    end_line = start_line + 20  # Estimate if we couldn't find the end
                
                class_entity = Class(
                    name=name,
                    line_start=start_line,
                    line_end=end_line,
                    filename=str(file_path),
                    base_classes=base_classes
                )
                
                module.classes[name] = class_entity
            
            # Calculate code metrics
            module.code_metrics = {
                "total_lines": len(content.splitlines()),
                "code_lines": len([line for line in content.splitlines() if line.strip() and not line.strip().startswith('//')]),
                "comment_lines": len([line for line in content.splitlines() if line.strip().startswith('//')]),
                "blank_lines": len([line for line in content.splitlines() if not line.strip()]),
                "function_count": len(module.functions),
                "class_count": len(module.classes),
                "import_count": len(module.imports)
            }
            
            return True
            
        except Exception as e:
            self._logger.error(f"Error analyzing JavaScript file {file_path}: {str(e)}")
            return False
    
    async def _analyze_typescript_file(self, file_path: Path, module: Module) -> bool:
        """
        Analyze a TypeScript file.
        
        Args:
            file_path: Path to the TypeScript file
            module: Module object to populate
            
        Returns:
            True if analysis was successful, False otherwise
        """
        # Start with JavaScript analysis
        js_result = await self._analyze_javascript_file(file_path, module)
        
        if not js_result:
            return False
        
        try:
            # Read the file content
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # Extract interfaces
            interface_pattern = r'interface\s+(\w+)(?:\s+extends\s+(\w+))?\s*{'
            for match in re.finditer(interface_pattern, content, re.MULTILINE):
                name = match.group(1)
                base_classes = []
                if match.group(2):
                    base_classes.append(match.group(2))
                
                start_line = content[:match.start()].count('\n') + 1
                
                # Try to find the end of the interface
                interface_content = content[match.start():]
                open_braces = 0
                for i, char in enumerate(interface_content):
                    if char == '{':
                        open_braces += 1
                    elif char == '}':
                        open_braces -= 1
                        if open_braces == 0:
                            end_line = start_line + interface_content[:i+1].count('\n')
                            break
                else:
                    end_line = start_line + 10  # Estimate if we couldn't find the end
                
                # Treat interfaces as classes for simplicity
                class_entity = Class(
                    name=name,
                    line_start=start_line,
                    line_end=end_line,
                    filename=str(file_path),
                    base_classes=base_classes
                )
                
                module.classes[name] = class_entity
            
            # Extract types
            type_pattern = r'type\s+(\w+)\s*=\s*\{[^}]*\}'
            for match in re.finditer(type_pattern, content, re.MULTILINE):
                name = match.group(1)
                
                start_line = content[:match.start()].count('\n') + 1
                end_line = start_line + content[match.start():match.end()].count('\n')
                
                # For simplicity, we'll store types as variables
                variable = Variable(
                    name=name,
                    line_start=start_line,
                    line_end=end_line,
                    filename=str(file_path),
                    var_type="type",
                    is_constant=True
                )
                
                module.variables[name] = variable
            
            # Update functions and class methods with type information
            for name, function in module.functions.items():
                # Try to find the function with type annotations
                function_pattern = fr'function\s+{re.escape(name)}\s*\([^)]*\)\s*:\s*(\w+)'
                match = re.search(function_pattern, content)
                if match:
                    function.return_type = match.group(1)
            
            return True
            
        except Exception as e:
            self._logger.error(f"Error analyzing TypeScript file {file_path}: {str(e)}")
            return False
    
    async def _analyze_with_llm(self, file_path: Path, module: Module) -> bool:
        """
        Use the LLM to analyze files when language-specific parsers aren't available.
        
        Args:
            file_path: Path to the file
            module: Module object to populate
            
        Returns:
            True if analysis was successful, False otherwise
        """
        try:
            # Read the file content
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # Limit content size for LLM
            if len(content) > 20000:
                self._logger.warning(f"File {file_path} is too large for LLM analysis, truncating")
                content = content[:20000] + "\n... [truncated]"
            
            # Detect language
            detect_file_type = get_file_detector_func()
            file_info = detect_file_type(file_path)
            language = file_info.get("language", "unknown")
            
            # Build prompt for the LLM
            prompt = f"""
Analyze this {language} source code and extract the key semantic information:

```{language}
{content}
```

Please return a JSON response with the following structure:
{{
  "imports": [
    {{ "name": "import_name", "path": "import_path", "line": line_number }}
  ],
  "functions": [
    {{ "name": "function_name", "start_line": start_line, "end_line": end_line, "params": ["param1", "param2"], "return_type": "return_type", "complexity": estimated_complexity }}
  ],
  "classes": [
    {{ 
      "name": "class_name", 
      "start_line": start_line, 
      "end_line": end_line, 
      "base_classes": ["base1", "base2"],
      "methods": [
        {{ "name": "method_name", "start_line": start_line, "end_line": end_line, "params": ["param1", "param2"] }}
      ],
      "attributes": [
        {{ "name": "attr_name", "type": "attr_type", "line": line_number }}
      ]
    }}
  ],
  "variables": [
    {{ "name": "var_name", "type": "var_type", "line": line_number, "is_constant": true_or_false }}
  ],
  "docstring": "module_level_docstring_if_any",
  "code_metrics": {{
    "total_lines": total_line_count,
    "function_count": number_of_functions,
    "class_count": number_of_classes,
    "complexity": estimated_overall_complexity
  }}
}}

Ensure your JSON is valid. Don't include any comments or explanations outside the JSON.
"""
            GeminiRequest = get_gemini_request_class()
            api_request = GeminiRequest(
                prompt=prompt,
                max_tokens=4000,
                temperature=0.1  # Low temperature for more deterministic output
            )
            
            gemini_client = get_gemini_client()
            response = await gemini_client.generate_text(api_request)
            

            # Parse the response
            try:
                # Try to extract JSON from the response
                response_text = response.text
                
                # Look for JSON block
                json_match = re.search(r'```(?:json)?\s*(.*?)\s*```', response_text, re.DOTALL)
                if json_match:
                    response_text = json_match.group(1)
                
                # Try to parse the JSON
                data = json.loads(response_text)
                
                # Populate the module with the extracted information
                
                # Module docstring
                if "docstring" in data:
                    module.docstring = data["docstring"]
                
                # Imports
                for imp in data.get("imports", []):
                    name = imp.get("name", "")
                    if name:
                        module.imports[name] = Import(
                            name=name,
                            line_start=imp.get("line", 1),
                            line_end=imp.get("line", 1),
                            filename=str(file_path),
                            import_path=imp.get("path", "")
                        )
                        if imp.get("path"):
                            module.dependencies.append(imp.get("path"))
                
                # Functions
                for func in data.get("functions", []):
                    name = func.get("name", "")
                    if name:
                        module.functions[name] = Function(
                            name=name,
                            line_start=func.get("start_line", 1),
                            line_end=func.get("end_line", 1),
                            filename=str(file_path),
                            params=func.get("params", []),
                            return_type=func.get("return_type"),
                            complexity=func.get("complexity")
                        )
                
                # Classes
                for cls in data.get("classes", []):
                    name = cls.get("name", "")
                    if name:
                        class_entity = Class(
                            name=name,
                            line_start=cls.get("start_line", 1),
                            line_end=cls.get("end_line", 1),
                            filename=str(file_path),
                            base_classes=cls.get("base_classes", [])
                        )
                        
                        # Add methods
                        for method in cls.get("methods", []):
                            method_name = method.get("name", "")
                            if method_name:
                                class_entity.methods[method_name] = Function(
                                    name=method_name,
                                    line_start=method.get("start_line", 1),
                                    line_end=method.get("end_line", 1),
                                    filename=str(file_path),
                                    params=method.get("params", []),
                                    is_method=True,
                                    class_name=name
                                )
                        
                        # Add attributes
                        for attr in cls.get("attributes", []):
                            attr_name = attr.get("name", "")
                            if attr_name:
                                class_entity.attributes[attr_name] = Variable(
                                    name=attr_name,
                                    line_start=attr.get("line", 1),
                                    line_end=attr.get("line", 1),
                                    filename=str(file_path),
                                    var_type=attr.get("type"),
                                    is_attribute=True,
                                    class_name=name
                                )
                        
                        module.classes[name] = class_entity
                
                # Variables
                for var in data.get("variables", []):
                    name = var.get("name", "")
                    if name:
                        module.variables[name] = Variable(
                            name=name,
                            line_start=var.get("line", 1),
                            line_end=var.get("line", 1),
                            filename=str(file_path),
                            var_type=var.get("type"),
                            is_constant=var.get("is_constant", False)
                        )
                
                # Code metrics
                if "code_metrics" in data:
                    module.code_metrics = data["code_metrics"]
                
                return True
                
            except json.JSONDecodeError as e:
                self._logger.error(f"Error parsing LLM response as JSON: {str(e)}")
                return False
                
        except Exception as e:
            self._logger.error(f"Error in LLM analysis for {file_path}: {str(e)}")
            return False
    
    def _analyze_cross_module_references(self, modules: Dict[str, Module]) -> None:
        """
        Analyze references between modules to build a dependency graph.
        
        Args:
            modules: Dictionary of modules to analyze
        """
        # Build a map of entity names to their modules
        entity_map = {}
        
        for module_path, module in modules.items():
            # Add functions
            for func_name in module.functions:
                entity_map[func_name] = module_path
            
            # Add classes
            for class_name in module.classes:
                entity_map[class_name] = module_path
        
        # Look for references
        for module_path, module in modules.items():
            # Check function calls
            for func_name, func in module.functions.items():
                for called_func in func.called_functions:
                    # Ignore method calls (with dots)
                    if '.' in called_func:
                        continue
                    
                    if called_func in entity_map and entity_map[called_func] != module_path:
                        # Found a reference to a function in another module
                        target_module = modules[entity_map[called_func]]
                        if called_func in target_module.functions:
                            target_func = target_module.functions[called_func]
                            target_func.references.append((module_path, func.line_start))
                            func.dependencies.append(called_func)
            
            # Check class inheritance
            for class_name, cls in module.classes.items():
                for base_class in cls.base_classes:
                    # Ignore qualified base classes
                    if '.' in base_class:
                        continue
                    
                    if base_class in entity_map and entity_map[base_class] != module_path:
                        # Found a reference to a class in another module
                        target_module = modules[entity_map[base_class]]
                        if base_class in target_module.classes:
                            target_class = target_module.classes[base_class]
                            target_class.references.append((module_path, cls.line_start))
                            cls.dependencies.append(base_class)
                            
    
    def find_related_entities(self, entity_name: str, project_files: Dict[str, Module]) -> List[Dict[str, Any]]:
        """
        Find entities related to a given entity in the project.
        
        Args:
            entity_name: Name of the entity to find relations for
            project_files: Dictionary of modules in the project
            
        Returns:
            List of related entities with relationship information
        """
        related_entities = []
        
        # Look for entities that reference or are referenced by the target entity
        for module_path, module in project_files.items():
            # Check functions
            for func_name, func in module.functions.items():
                # Check if this is our target entity
                if func_name == entity_name:
                    # Find functions that call this one
                    for other_module_path, other_module in project_files.items():
                        for other_func_name, other_func in other_module.functions.items():
                            if entity_name in other_func.called_functions:
                                related_entities.append({
                                    "name": other_func_name,
                                    "type": "function",
                                    "relationship": "calls",
                                    "filename": other_module_path,
                                    "line": other_func.line_start
                                })
                
                # Check if this function calls our target entity
                if entity_name in func.called_functions:
                    related_entities.append({
                        "name": func_name,
                        "type": "function",
                        "relationship": "called_by",
                        "filename": module_path,
                        "line": func.line_start
                    })
            
            # Check classes
            for class_name, cls in module.classes.items():
                # Check if this is our target entity
                if class_name == entity_name:
                    # Find classes that inherit from this one
                    for other_module_path, other_module in project_files.items():
                        for other_class_name, other_class in other_module.classes.items():
                            if entity_name in other_class.base_classes:
                                related_entities.append({
                                    "name": other_class_name,
                                    "type": "class",
                                    "relationship": "inherits_from",
                                    "filename": other_module_path,
                                    "line": other_class.line_start
                                })
                
                # Check if this class inherits from our target entity
                if entity_name in cls.base_classes:
                    related_entities.append({
                        "name": class_name,
                        "type": "class",
                        "relationship": "extended_by",
                        "filename": module_path,
                        "line": cls.line_start
                    })
                
                # Check class methods
                for method_name, method in cls.methods.items():
                    if entity_name in method.called_functions:
                        related_entities.append({
                            "name": f"{class_name}.{method_name}",
                            "type": "method",
                            "relationship": "called_by",
                            "filename": module_path,
                            "line": method.line_start
                        })
        
        return related_entities
    
    async def analyze_entity_usage(self, entity_name: str, project_root: Union[str, Path], depth: int = 1) -> Dict[str, Any]:
        """
        Analyze how a specific entity is used throughout the project.
        
        Args:
            entity_name: Name of the entity to analyze
            project_root: Root directory of the project
            depth: Relationship depth to explore
            
        Returns:
            Dictionary with entity usage information
        """
        root_path = Path(project_root)
        
        # Analyze project files first
        project_files = await self.analyze_project_files(root_path)
        
        # Find the entity in the project
        entity_info = None
        entity_module = None
        entity_type = None
        
        for module_path, module in project_files.items():
            # Check functions
            if entity_name in module.functions:
                entity_info = module.functions[entity_name].to_dict()
                entity_module = module
                entity_type = "function"
                break
            
            # Check classes
            if entity_name in module.classes:
                entity_info = module.classes[entity_name].to_dict()
                entity_module = module
                entity_type = "class"
                break
            
            # Check variables
            if entity_name in module.variables:
                entity_info = module.variables[entity_name].to_dict()
                entity_module = module
                entity_type = "variable"
                break
            
            # Check for class methods
            for class_name, cls in module.classes.items():
                if entity_name in cls.methods:
                    entity_info = cls.methods[entity_name].to_dict()
                    entity_info["class_name"] = class_name
                    entity_module = module
                    entity_type = "method"
                    break
                
                # Check for full qualified method name (class.method)
                if "." in entity_name:
                    class_part, method_part = entity_name.split(".", 1)
                    if class_name == class_part and method_part in cls.methods:
                        entity_info = cls.methods[method_part].to_dict()
                        entity_info["class_name"] = class_name
                        entity_module = module
                        entity_type = "method"
                        break
        
        if not entity_info:
            return {
                "entity_name": entity_name,
                "found": False,
                "message": f"Entity '{entity_name}' not found in the project"
            }
        
        # Find related entities
        related = self.find_related_entities(entity_name, project_files)
        
        # For methods, also check the class name if it's a qualified name
        if "." in entity_name and not related:
            class_part = entity_name.split(".", 1)[0]
            class_related = self.find_related_entities(class_part, project_files)
            related.extend(class_related)
        
        # Get recursive related entities if depth > 1
        if depth > 1:
            next_level = []
            for related_entity in related:
                name = related_entity["name"]
                if "." in name:  # Skip qualified names for simplicity
                    continue
                    
                sub_related = self.find_related_entities(name, project_files)
                for sub in sub_related:
                    if sub not in next_level and sub not in related:
                        sub["relationship_depth"] = 2
                        next_level.append(sub)
            
            related.extend(next_level)
        
        # Build result
        result = {
            "entity_name": entity_name,
            "found": True,
            "type": entity_type,
            "filename": entity_info["filename"],
            "line_start": entity_info["line_start"],
            "line_end": entity_info["line_end"],
            "related_entities": related,
            "details": entity_info
        }
        
        if entity_type == "function" or entity_type == "method":
            # Add information about function parameters
            result["parameters"] = entity_info.get("params", [])
            result["return_type"] = entity_info.get("return_type")
            result["complexity"] = entity_info.get("complexity")
            
            # If it's a method, add class information
            if entity_type == "method":
                result["class_name"] = entity_info.get("class_name")
                
                # Get class info
                if entity_module and entity_info.get("class_name") in entity_module.classes:
                    class_info = entity_module.classes[entity_info["class_name"]].to_dict()
                    result["class_details"] = {
                        "base_classes": class_info.get("base_classes", []),
                        "method_count": len(class_info.get("methods", {})),
                        "attribute_count": len(class_info.get("attributes", {}))
                    }
        
        elif entity_type == "class":
            # Add class-specific information
            result["base_classes"] = entity_info.get("base_classes", [])
            result["method_count"] = len(entity_info.get("methods", {}))
            result["attribute_count"] = len(entity_info.get("attributes", {}))
            result["methods"] = list(entity_info.get("methods", {}).keys())
            result["attributes"] = list(entity_info.get("attributes", {}).keys())
        
        elif entity_type == "variable":
            # Add variable-specific information
            result["var_type"] = entity_info.get("var_type")
            result["value"] = entity_info.get("value")
            result["is_constant"] = entity_info.get("is_constant", False)
        
        return result
    
    async def summarize_code_entity(self, entity_name: str, project_root: Union[str, Path]) -> str:
        """
        Generate a natural language summary of a code entity.
        
        Args:
            entity_name: Name of the entity to summarize
            project_root: Root directory of the project
            
        Returns:
            String with a natural language summary
        """
        # First, get the entity usage information
        usage_info = await self.analyze_entity_usage(entity_name, project_root)
        
        if not usage_info.get("found", False):
            return f"Could not find entity '{entity_name}' in the project."
        
        # Create a detailed prompt for the LLM
        entity_type = usage_info.get("type", "unknown")
        filename = usage_info.get("filename", "unknown")
        
        # Read the actual file content around the entity definition
        try:
            with open(filename, 'r', encoding='utf-8') as f:
                file_content = f.read()
            
            lines = file_content.splitlines()
            start_line = max(0, usage_info.get("line_start", 1) - 1)  # Lines are 1-indexed
            end_line = min(len(lines), usage_info.get("line_end", start_line + 10))
            
            entity_code = "\n".join(lines[start_line:end_line])
            
            # Get the language
            detect_file_type = get_file_detector_func()
            file_info = detect_file_type(Path(filename))
            language = file_info.get("language", "").lower()
            
            # Build the prompt
            prompt = f"""
You are reviewing code and need to provide a clear, concise summary of a specific code entity.

Entity Name: {entity_name}
Entity Type: {entity_type}
File: {Path(filename).name}
Language: {language}

Here is the code for this entity:
```{language}
{entity_code}
```

Additional Information:
"""
            
            if entity_type == "function" or entity_type == "method":
                params = usage_info.get("parameters", [])
                return_type = usage_info.get("return_type", "unknown")
                complexity = usage_info.get("complexity", "unknown")
                
                prompt += f"""
- Parameters: {', '.join(params)}
- Return Type: {return_type}
- Complexity: {complexity}
"""
                
                if entity_type == "method":
                    class_name = usage_info.get("class_name", "unknown")
                    prompt += f"- Part of Class: {class_name}\n"
                    
                    class_details = usage_info.get("class_details", {})
                    if class_details:
                        base_classes = class_details.get("base_classes", [])
                        if base_classes:
                            prompt += f"- Class Inherits From: {', '.join(base_classes)}\n"
            
            elif entity_type == "class":
                base_classes = usage_info.get("base_classes", [])
                methods = usage_info.get("methods", [])
                attributes = usage_info.get("attributes", [])
                
                prompt += f"""
- Base Classes: {', '.join(base_classes) if base_classes else 'None'}
- Methods: {', '.join(methods[:5]) + ('...' if len(methods) > 5 else '') if methods else 'None'}
- Attributes: {', '.join(attributes[:5]) + ('...' if len(attributes) > 5 else '') if attributes else 'None'}
"""
            
            elif entity_type == "variable":
                var_type = usage_info.get("var_type", "unknown")
                value = usage_info.get("value", "unknown")
                is_constant = usage_info.get("is_constant", False)
                
                prompt += f"""
- Type: {var_type}
- Value: {value}
- Is Constant: {is_constant}
"""
            
            # Add relationship information
            related = usage_info.get("related_entities", [])
            if related:
                callers = [r["name"] for r in related if r.get("relationship") == "calls"]
                called = [r["name"] for r in related if r.get("relationship") == "called_by"]
                inherits = [r["name"] for r in related if r.get("relationship") == "inherits_from"]
                extends = [r["name"] for r in related if r.get("relationship") == "extended_by"]
                
                prompt += "\nRelationships:\n"
                
                if callers:
                    prompt += f"- Called by: {', '.join(callers[:5]) + ('...' if len(callers) > 5 else '')}\n"
                
                if called:
                    prompt += f"- Calls: {', '.join(called[:5]) + ('...' if len(called) > 5 else '')}\n"
                
                if inherits:
                    prompt += f"- Inherits from: {', '.join(inherits)}\n"
                
                if extends:
                    prompt += f"- Extended by: {', '.join(extends[:5]) + ('...' if len(extends) > 5 else '')}\n"
            
            prompt += """
Based on the code and information provided, give a concise, useful summary of what this entity does,
its role in the codebase, and any notable design patterns or implementation details. Keep the summary
focused and to-the-point - ideally 3-5 sentences.
"""
            
            # Call AI service
            GeminiRequest = get_gemini_request_class()
            api_request = GeminiRequest(
                prompt=prompt,
                max_tokens=1000,
                temperature=0.3
            )
            
            gemini_client = get_gemini_client()
            response = await gemini_client.generate_text(api_request)
            
            return response.text.strip()
            
        except Exception as e:
            self._logger.error(f"Error generating summary for {entity_name}: {str(e)}")
            return f"Error generating summary for {entity_name}: {str(e)}"
    
    async def get_entity_code(self, entity_name: str, project_root: Union[str, Path]) -> Optional[str]:
        """
        Get the source code for a specific entity.
        
        Args:
            entity_name: Name of the entity to get code for
            project_root: Root directory of the project
            
        Returns:
            String with the entity's source code or None if not found
        """
        # First, get the entity usage information
        usage_info = await self.analyze_entity_usage(entity_name, project_root)
        
        if not usage_info.get("found", False):
            return None
        
        # Get the file path and line range
        filename = usage_info.get("filename")
        start_line = usage_info.get("line_start", 1)
        end_line = usage_info.get("line_end", start_line)
        
        # Read the file content
        try:
            with open(filename, 'r', encoding='utf-8') as f:
                lines = f.readlines()
                
            # Get the specified line range (adjust for 0-based indexing)
            start_idx = max(0, start_line - 1)
            end_idx = min(len(lines), end_line)
            
            return "".join(lines[start_idx:end_idx])
            
        except Exception as e:
            self._logger.error(f"Error getting code for {entity_name}: {str(e)}")
            return None
    
    def get_module_dependencies(self, modules: Dict[str, Module]) -> Dict[str, List[str]]:
        """
        Get a map of module dependencies.
        
        Args:
            modules: Dictionary of modules to analyze
            
        Returns:
            Dictionary with module paths as keys and lists of dependencies as values
        """
        dependencies = {}
        
        for module_path, module in modules.items():
            dependencies[module_path] = module.dependencies
        
        return dependencies
    
    def calculate_project_metrics(self, modules: Dict[str, Module]) -> Dict[str, Any]:
        """
        Calculate metrics for the entire project.
        
        Args:
            modules: Dictionary of modules to analyze
            
        Returns:
            Dictionary with project metrics
        """
        total_lines = 0
        code_lines = 0
        comment_lines = 0
        blank_lines = 0
        function_count = 0
        class_count = 0
        complexity = 0
        
        for module in modules.values():
            metrics = module.code_metrics
            total_lines += metrics.get("total_lines", 0)
            code_lines += metrics.get("code_lines", 0)
            comment_lines += metrics.get("comment_lines", 0)
            blank_lines += metrics.get("blank_lines", 0)
            function_count += metrics.get("function_count", 0) + sum(len(cls.methods) for cls in module.classes.values())
            class_count += metrics.get("class_count", 0)
            complexity += metrics.get("complexity", 0)
        
        # Calculate percentages
        comment_ratio = comment_lines / code_lines if code_lines > 0 else 0
        blank_ratio = blank_lines / total_lines if total_lines > 0 else 0
        average_function_complexity = complexity / function_count if function_count > 0 else 0
        
        return {
            "total_lines": total_lines,
            "code_lines": code_lines,
            "comment_lines": comment_lines,
            "blank_lines": blank_lines,
            "function_count": function_count,
            "class_count": class_count,
            "complexity": complexity,
            "comment_ratio": comment_ratio,
            "blank_ratio": blank_ratio,
            "average_function_complexity": average_function_complexity,
            "module_count": len(modules),
            "file_count": len(modules)
        }

# Global semantic analyzer instance
semantic_analyzer = SemanticAnalyzer()
</file>

<file path="components/cli/__init__.py">
# angela/cli/__init__.py
"""
CLI components for Angela CLI.

This package provides the command-line interface for interacting with Angela,
including the main application and various subcommands for different functionality.
"""
from angela.components.cli.main import app as main_app
from angela.components.cli.files import app as files_app
from angela.components.cli.workflows import app as workflows_app
from angela.components.cli.generation import app as generation_app
from angela.components.execution.rollback_commands import app as rollback_app
from angela.components.cli.docker import app as docker_app

# Add subcommands to the main app
main_app.add_typer(files_app, name="files", help="File and directory operations")
main_app.add_typer(workflows_app, name="workflows", help="Workflow management")
main_app.add_typer(generation_app, name="generate", help="Code generation")
main_app.add_typer(rollback_app, name="rollback", help="Rollback operations and transactions")
main_app.add_typer(docker_app, name="docker", help="Docker and Docker Compose operations")

# Export the main app
app = main_app

__all__ = ['app']
</file>

<file path="components/cli/docker.py">
# angela/cli/docker.py
"""
CLI commands for Docker integration in Angela CLI.

This module provides CLI commands for interacting with Docker and Docker Compose
through Angela CLI. It allows users to manage containers, images, and Docker-related
files from the command line.
"""
import asyncio
import os
from pathlib import Path
from typing import List, Optional

import typer
from rich.console import Console
from rich.table import Table
from rich.panel import Panel
from rich.syntax import Syntax
from rich.progress import Progress, SpinnerColumn, TextColumn

from angela.api.toolchain import get_docker_integration
from angela.api.context import get_context_manager
from angela.utils.logging import get_logger
from angela.api.execution import get_execution_engine


logger = get_logger(__name__)
console = Console()

# Create the Typer app for Docker commands
app = typer.Typer(help="Docker and Docker Compose commands")


@app.command("status")
async def docker_status():
    """Show Docker and Docker Compose availability status."""
    with Progress(
        SpinnerColumn(),
        TextColumn("[bold blue]Checking Docker status...[/bold blue]"),
        console=console
    ) as progress:
        task = progress.add_task("Checking", total=1)
        
        # Check Docker availability
        docker_integration = get_docker_integration()
        docker_available = await docker_integration.is_docker_available()
        
        # Check Docker Compose availability
        compose_available = await docker_integration.is_docker_compose_available()
        
        progress.update(task, completed=1)
    
    # Display status
    table = Table(title="Docker Status")
    table.add_column("Component", style="cyan")
    table.add_column("Status", style="green")
    
    table.add_row(
        "Docker",
        "[green]Available[/green]" if docker_available else "[red]Not Available[/red]"
    )
    table.add_row(
        "Docker Compose",
        "[green]Available[/green]" if compose_available else "[red]Not Available[/red]"
    )
    
    console.print(table)
    
    if not docker_available:
        console.print("[yellow]Docker is not available. Install Docker to use these features.[/yellow]")
    elif not compose_available:
        console.print("[yellow]Docker Compose is not available. Install Docker Compose for multi-container support.[/yellow]")


@app.command("ps")
async def list_containers(
    all_containers: bool = typer.Option(False, "--all", "-a", help="Show all containers (including stopped)"),
    quiet: bool = typer.Option(False, "--quiet", "-q", help="Only display container IDs")
):
    """List Docker containers."""
    with Progress(
        SpinnerColumn(),
        TextColumn("[bold blue]Listing containers...[/bold blue]"),
        console=console
    ) as progress:
        task = progress.add_task("Listing", total=1)
        
        docker_integration = get_docker_integration()
        result = await docker_integration.list_containers(all_containers)
        
        progress.update(task, completed=1)
    
    if result["success"]:
        containers = result["containers"]
        
        if not containers:
            console.print("[yellow]No containers found.[/yellow]")
            return
        
        if quiet:
            for container in containers:
                if "id" in container:
                    console.print(container["id"])
        else:
            table = Table(title=f"{'All' if all_containers else 'Running'} Containers")
            
            # Determine columns based on the first container's structure
            if containers:
                first_container = containers[0]
                
                # Common columns
                table.add_column("ID", style="cyan", no_wrap=True)
                table.add_column("Name", style="green")
                table.add_column("Image", style="blue")
                table.add_column("Status", style="yellow")
                
                # Additional columns if available
                if "ports" in first_container:
                    table.add_column("Ports", style="magenta")
                
                # Add rows
                for container in containers:
                    # Extract values (handle different formats)
                    container_id = container.get("id", container.get("ID", ""))
                    # Show short ID
                    if len(container_id) > 12:
                        container_id = container_id[:12]
                    
                    name = container.get("names", container.get("Names", "")).lstrip('/')
                    if isinstance(name, list):
                        name = ", ".join(name)
                    
                    image = container.get("image", container.get("Image", ""))
                    status = container.get("status", container.get("Status", ""))
                    
                    # Build row
                    row = [container_id, name, image, status]
                    
                    # Add ports if available
                    if "ports" in first_container:
                        ports = container.get("ports", container.get("Ports", ""))
                        if isinstance(ports, list):
                            ports = ", ".join(ports)
                        row.append(ports)
                    
                    table.add_row(*row)
            
            console.print(table)
    else:
        console.print(f"[red]Error listing containers: {result.get('error', 'Unknown error')}[/red]")


@app.command("logs")
async def show_container_logs(
    container: str = typer.Argument(..., help="Container ID or name"),
    follow: bool = typer.Option(False, "--follow", "-f", help="Follow log output"),
    tail: Optional[int] = typer.Option(None, "--tail", "-n", help="Number of lines to show from the end"),
    timestamps: bool = typer.Option(False, "--timestamps", "-t", help="Show timestamps")
):
    """Show logs from a Docker container."""
    with Progress(
        SpinnerColumn(),
        TextColumn(f"[bold blue]Getting logs for container {container}...[/bold blue]"),
        console=console
    ) as progress:
        task = progress.add_task("Getting logs", total=1)
        
        docker_integration = get_docker_integration()
        result = await docker_integration.get_container_logs(
            container_id_or_name=container,
            tail=tail,
            follow=follow,
            timestamps=timestamps
        )
        
        progress.update(task, completed=1)
    
    if result["success"]:
        logs = result.get("logs", "")
        
        if not logs.strip():
            console.print("[yellow]No logs available for this container.[/yellow]")
            return
        
        console.print(Panel(
            logs,
            title=f"Logs for container: {container}",
            expand=False,
            border_style="blue"
        ))
        
        if result.get("truncated", False):
            console.print("[yellow]Log output truncated due to size or streaming timeout.[/yellow]")
    else:
        console.print(f"[red]Error getting container logs: {result.get('error', 'Unknown error')}[/red]")

@app.command("start")
async def start_container(container: str = typer.Argument(..., help="Container ID or name")):
    """Start a Docker container."""
    with Progress(
        SpinnerColumn(),
        TextColumn(f"[bold blue]Starting container {container}...[/bold blue]"),
        console=console
    ) as progress:
        task = progress.add_task("Starting", total=1)
        
        docker_integration = get_docker_integration()
        result = await docker_integration.start_container(container)
        
        progress.update(task, completed=1)
    
    if result["success"]:
        console.print(f"[green]Container {container} started successfully.[/green]")
    else:
        console.print(f"[red]Error starting container: {result.get('error', 'Unknown error')}[/red]")


@app.command("stop")
async def stop_container(
    container: str = typer.Argument(..., help="Container ID or name"),
    timeout: int = typer.Option(10, "--timeout", "-t", help="Timeout in seconds before killing the container")
):
    """Stop a Docker container."""
    with Progress(
        SpinnerColumn(),
        TextColumn(f"[bold blue]Stopping container {container}...[/bold blue]"),
        console=console
    ) as progress:
        task = progress.add_task("Stopping", total=1)
        
        docker_integration = get_docker_integration()
        result = await docker_integration.stop_container(container, timeout)
        
        progress.update(task, completed=1)
    
    if result["success"]:
        console.print(f"[green]Container {container} stopped successfully.[/green]")
    else:
        console.print(f"[red]Error stopping container: {result.get('error', 'Unknown error')}[/red]")


@app.command("restart")
async def restart_container(
    container: str = typer.Argument(..., help="Container ID or name"),
    timeout: int = typer.Option(10, "--timeout", "-t", help="Timeout in seconds before killing the container")
):
    """Restart a Docker container."""
    with Progress(
        SpinnerColumn(),
        TextColumn(f"[bold blue]Restarting container {container}...[/bold blue]"),
        console=console
    ) as progress:
        task = progress.add_task("Restarting", total=1)
        
        docker_integration = get_docker_integration()
        result = await docker_integration.restart_container(container, timeout)
        
        progress.update(task, completed=1)
    
    if result["success"]:
        console.print(f"[green]Container {container} restarted successfully.[/green]")
    else:
        console.print(f"[red]Error restarting container: {result.get('error', 'Unknown error')}[/red]")


@app.command("rm")
async def remove_container(
    container: str = typer.Argument(..., help="Container ID or name"),
    force: bool = typer.Option(False, "--force", "-f", help="Force removal of running container"),
    volumes: bool = typer.Option(False, "--volumes", "-v", help="Remove anonymous volumes")
):
    """Remove a Docker container."""
    with Progress(
        SpinnerColumn(),
        TextColumn(f"[bold blue]Removing container {container}...[/bold blue]"),
        console=console
    ) as progress:
        task = progress.add_task("Removing", total=1)
        
        docker_integration = get_docker_integration()
        result = await docker_integration.remove_container(container, force, volumes)
        
        progress.update(task, completed=1)
    
    if result["success"]:
        console.print(f"[green]Container {container} removed successfully.[/green]")
    else:
        console.print(f"[red]Error removing container: {result.get('error', 'Unknown error')}[/red]")


@app.command("images")
async def list_images(
    all_images: bool = typer.Option(False, "--all", "-a", help="Show all images (including intermediates)"),
    quiet: bool = typer.Option(False, "--quiet", "-q", help="Only display image IDs")
):
    """List Docker images."""
    with Progress(
        SpinnerColumn(),
        TextColumn("[bold blue]Listing images...[/bold blue]"),
        console=console
    ) as progress:
        task = progress.add_task("Listing", total=1)
        
        docker_integration = get_docker_integration()
        result = await docker_integration.list_images(all_images)
        
        progress.update(task, completed=1)
    
    if result["success"]:
        images = result["images"]
        
        if not images:
            console.print("[yellow]No images found.[/yellow]")
            return
        
        if quiet:
            for image in images:
                if "id" in image:
                    console.print(image["id"])
        else:
            table = Table(title="Docker Images")
            
            # Add columns
            table.add_column("Repository", style="cyan")
            table.add_column("Tag", style="green")
            table.add_column("ID", style="blue", no_wrap=True)
            table.add_column("Created", style="yellow")
            table.add_column("Size", style="magenta")
            
            # Add rows
            for image in images:
                # Extract values (handle different formats)
                repository = image.get("repository", image.get("Repository", "<none>"))
                tag = image.get("tag", image.get("Tag", "<none>"))
                image_id = image.get("id", image.get("ID", ""))
                # Show short ID
                if len(image_id) > 12:
                    image_id = image_id[:12]
                
                created = image.get("created", image.get("Created", ""))
                size = image.get("size", image.get("Size", ""))
                
                table.add_row(repository, tag, image_id, created, size)
            
            console.print(table)
    else:
        console.print(f"[red]Error listing images: {result.get('error', 'Unknown error')}[/red]")


@app.command("rmi")
async def remove_image(
    image: str = typer.Argument(..., help="Image ID or name"),
    force: bool = typer.Option(False, "--force", "-f", help="Force removal")
):
    """Remove a Docker image."""
    with Progress(
        SpinnerColumn(),
        TextColumn(f"[bold blue]Removing image {image}...[/bold blue]"),
        console=console
    ) as progress:
        task = progress.add_task("Removing", total=1)
        
        docker_integration = get_docker_integration()
        result = await docker_integration.remove_image(image, force)
        
        progress.update(task, completed=1)
    
    if result["success"]:
        console.print(f"[green]Image {image} removed successfully.[/green]")
    else:
        console.print(f"[red]Error removing image: {result.get('error', 'Unknown error')}[/red]")
        
        
@app.command("pull")
async def pull_image(image: str = typer.Argument(..., help="Image name to pull")):
    """Pull a Docker image."""
    with Progress(
        SpinnerColumn(),
        TextColumn(f"[bold blue]Pulling image {image}...[/bold blue]"),
        console=console
    ) as progress:
        task = progress.add_task("Pulling", total=1)
        
        docker_integration = get_docker_integration()
        result = await docker_integration.pull_image(image)
        
        progress.update(task, completed=1)
    
    if result["success"]:
        console.print(f"[green]Image {image} pulled successfully.[/green]")
    else:
        console.print(f"[red]Error pulling image: {result.get('error', 'Unknown error')}[/red]")


@app.command("build")
async def build_image(
    context_path: str = typer.Argument(".", help="Path to build context"),
    tag: Optional[str] = typer.Option(None, "--tag", "-t", help="Tag for the built image"),
    dockerfile: Optional[str] = typer.Option(None, "--file", "-f", help="Path to Dockerfile"),
    no_cache: bool = typer.Option(False, "--no-cache", help="Do not use cache when building")
):
    """Build a Docker image."""
    # Resolve path
    path = Path(context_path).absolute()
    if not path.exists() or not path.is_dir():
        console.print(f"[red]Error: Build context does not exist or is not a directory: {path}[/red]")
        return
    
    with Progress(
        SpinnerColumn(),
        TextColumn(f"[bold blue]Building Docker image from {path}...[/bold blue]"),
        console=console
    ) as progress:
        task = progress.add_task("Building", total=1)
        
        docker_integration = get_docker_integration()
        result = await docker_integration.build_image(
            context_path=path,
            tag=tag,
            dockerfile=dockerfile,
            no_cache=no_cache
        )
        
        progress.update(task, completed=1)
    
    if result["success"]:
        image_id = result.get("image_id", "")
        tag_str = f" with tag {tag}" if tag else ""
        
        console.print(f"[green]Image built successfully{tag_str}.[/green]")
        if image_id:
            console.print(f"[green]Image ID: {image_id}[/green]")
    else:
        console.print(f"[red]Error building image: {result.get('error', 'Unknown error')}[/red]")


@app.command("run")
async def run_container(
    image: str = typer.Argument(..., help="Docker image to run"),
    command: Optional[str] = typer.Argument(None, help="Command to run in the container"),
    name: Optional[str] = typer.Option(None, "--name", help="Container name"),
    ports: List[str] = typer.Option([], "--port", "-p", help="Port mappings (host:container)"),
    volumes: List[str] = typer.Option([], "--volume", "-v", help="Volume mappings (host:container)"),
    environment: List[str] = typer.Option([], "--env", "-e", help="Environment variables (KEY=VALUE)"),
    detach: bool = typer.Option(True, "--detach", "-d", help="Run in background"),
    remove: bool = typer.Option(False, "--rm", help="Remove container when it exits"),
    network: Optional[str] = typer.Option(None, "--network", help="Connect to network"),
    interactive: bool = typer.Option(False, "--interactive", "-i", help="Interactive mode")
):
    """Run a Docker container."""
    # Parse environment variables
    env_dict = {}
    for env in environment:
        parts = env.split("=", 1)
        if len(parts) == 2:
            env_dict[parts[0]] = parts[1]
    
    with Progress(
        SpinnerColumn(),
        TextColumn(f"[bold blue]Running container from image {image}...[/bold blue]"),
        console=console
    ) as progress:
        task = progress.add_task("Running", total=1)
        
        docker_integration = get_docker_integration()
        result = await docker_integration.run_container(
            image=image,
            command=command,
            name=name,
            ports=ports,
            volumes=volumes,
            environment=env_dict,
            detach=detach,
            remove=remove,
            network=network,
            interactive=interactive
        )
        
        progress.update(task, completed=1)
    
    if result["success"]:
        if detach:
            container_id = result.get("container_id", "")
            if container_id:
                console.print(f"[green]Container started in detached mode with ID: {container_id}[/green]")
            else:
                console.print(f"[green]Container started in detached mode.[/green]")
        else:
            console.print(f"[green]Container executed successfully.[/green]")
    else:
        console.print(f"[red]Error running container: {result.get('error', 'Unknown error')}[/red]")


@app.command("exec")
async def exec_in_container(
    container: str = typer.Argument(..., help="Container ID or name"),
    command: str = typer.Argument(..., help="Command to execute"),
    interactive: bool = typer.Option(False, "--interactive", "-i", help="Interactive mode")
):
    """Execute a command in a running container."""
    with Progress(
        SpinnerColumn(),
        TextColumn(f"[bold blue]Executing command in container {container}...[/bold blue]"),
        console=console
    ) as progress:
        task = progress.add_task("Executing", total=1)
        
        docker_integration = get_docker_integration()
        result = await docker_integration.exec_in_container(
            container_id_or_name=container,
            command=command,
            interactive=interactive
        )
        
        progress.update(task, completed=1)
    
    if result["success"]:
        output = result.get("output", "")
        if output.strip():
            console.print(Panel(
                output,
                title=f"Command output from {container}",
                expand=False,
                border_style="blue"
            ))
        else:
            console.print(f"[green]Command executed successfully with no output.[/green]")
    else:
        console.print(f"[red]Error executing command: {result.get('error', 'Unknown error')}[/red]")

@app.command("compose-up")
async def compose_up(
    compose_file: Optional[str] = typer.Option(None, "--file", "-f", help="Path to docker-compose.yml"),
    directory: Optional[str] = typer.Option(None, "--dir", help="Project directory (default: current directory)"),
    detach: bool = typer.Option(True, "--detach", "-d", help="Run in background"),
    build: bool = typer.Option(False, "--build", help="Build images before starting"),
    no_recreate: bool = typer.Option(False, "--no-recreate", help="Don't recreate containers"),
    force_recreate: bool = typer.Option(False, "--force-recreate", help="Force recreate containers"),
    services: List[str] = typer.Argument(None, help="Services to start (default: all)")
):
    """Start services using Docker Compose."""
    # Resolve directory
    if directory:
        dir_path = Path(directory).absolute()
    else:
        context_manager = get_context_manager()
        dir_path = context_manager.cwd
    
    if not dir_path.exists() or not dir_path.is_dir():
        console.print(f"[red]Error: Project directory does not exist or is not a directory: {dir_path}[/red]")
        return
    
    with Progress(
        SpinnerColumn(),
        TextColumn("[bold blue]Starting Docker Compose services...[/bold blue]"),
        console=console
    ) as progress:
        task = progress.add_task("Starting", total=1)
        
        docker_integration = get_docker_integration()
        result = await docker_integration.compose_up(
            compose_file=compose_file,
            project_directory=dir_path,
            detach=detach,
            build=build,
            no_recreate=no_recreate,
            force_recreate=force_recreate,
            services=services
        )
        
        progress.update(task, completed=1)
    
    if result["success"]:
        console.print(f"[green]Docker Compose services started successfully.[/green]")
        console.print(f"[green]Use 'angela docker compose-ps' to check service status.[/green]")
    else:
        console.print(f"[red]Error starting Docker Compose services: {result.get('error', 'Unknown error')}[/red]")
        
        # Show command for debugging
        if "command" in result:
            console.print(f"[yellow]Command: {result['command']}[/yellow]")


@app.command("compose-down")
async def compose_down(
    compose_file: Optional[str] = typer.Option(None, "--file", "-f", help="Path to docker-compose.yml"),
    directory: Optional[str] = typer.Option(None, "--dir", help="Project directory (default: current directory)"),
    remove_images: bool = typer.Option(False, "--rmi", help="Remove images"),
    remove_volumes: bool = typer.Option(False, "--volumes", "-v", help="Remove volumes"),
    remove_orphans: bool = typer.Option(False, "--remove-orphans", help="Remove orphaned containers")
):
    """Stop and remove Docker Compose services."""
    # Resolve directory
    if directory:
        dir_path = Path(directory).absolute()
    else:
        context_manager = get_context_manager()
        dir_path = context_manager.cwd
    
    if not dir_path.exists() or not dir_path.is_dir():
        console.print(f"[red]Error: Project directory does not exist or is not a directory: {dir_path}[/red]")
        return
    
    with Progress(
        SpinnerColumn(),
        TextColumn("[bold blue]Stopping Docker Compose services...[/bold blue]"),
        console=console
    ) as progress:
        task = progress.add_task("Stopping", total=1)
        
        docker_integration = get_docker_integration()
        result = await docker_integration.compose_down(
            compose_file=compose_file,
            project_directory=dir_path,
            remove_images=remove_images,
            remove_volumes=remove_volumes,
            remove_orphans=remove_orphans
        )
        
        progress.update(task, completed=1)
    
    if result["success"]:
        console.print(f"[green]Docker Compose services stopped successfully.[/green]")
    else:
        console.print(f"[red]Error stopping Docker Compose services: {result.get('error', 'Unknown error')}[/red]")


@app.command("compose-ps")
async def compose_ps(
    compose_file: Optional[str] = typer.Option(None, "--file", "-f", help="Path to docker-compose.yml"),
    directory: Optional[str] = typer.Option(None, "--dir", help="Project directory (default: current directory)"),
    all_services: bool = typer.Option(False, "--all", "-a", help="Show stopped services"),
    services: List[str] = typer.Argument(None, help="Services to show (default: all)")
):
    """List Docker Compose services."""
    # Resolve directory
    if directory:
        dir_path = Path(directory).absolute()
    else:
        context_manager = get_context_manager()
        dir_path = context_manager.cwd
    
    if not dir_path.exists() or not dir_path.is_dir():
        console.print(f"[red]Error: Project directory does not exist or is not a directory: {dir_path}[/red]")
        return
    
    with Progress(
        SpinnerColumn(),
        TextColumn("[bold blue]Listing Docker Compose services...[/bold blue]"),
        console=console
    ) as progress:
        task = progress.add_task("Listing", total=1)
        
        docker_integration = get_docker_integration()
        result = await docker_integration.compose_ps(
            compose_file=compose_file,
            project_directory=dir_path,
            services=services,
            all_services=all_services
        )
        
        progress.update(task, completed=1)
    
    if result["success"]:
        if not result.get("services"):
            console.print("[yellow]No Docker Compose services found.[/yellow]")
            return
        
        # Display raw output for simplicity, as format can vary
        console.print(result["raw_output"])
    else:
        console.print(f"[red]Error listing Docker Compose services: {result.get('error', 'Unknown error')}[/red]")


@app.command("compose-logs")
async def compose_logs(
    compose_file: Optional[str] = typer.Option(None, "--file", "-f", help="Path to docker-compose.yml"),
    directory: Optional[str] = typer.Option(None, "--dir", help="Project directory (default: current directory)"),
    follow: bool = typer.Option(False, "--follow", help="Follow log output"),
    tail: Optional[int] = typer.Option(None, "--tail", help="Number of lines to show from the end"),
    timestamps: bool = typer.Option(False, "--timestamps", help="Show timestamps"),
    services: List[str] = typer.Argument(None, help="Services to show logs for (default: all)")
):
    """View logs from Docker Compose services."""
    # Resolve directory
    if directory:
        dir_path = Path(directory).absolute()
    else:
        context_manager = get_context_manager()
        dir_path = context_manager.cwd
    
    if not dir_path.exists() or not dir_path.is_dir():
        console.print(f"[red]Error: Project directory does not exist or is not a directory: {dir_path}[/red]")
        return
    
    with Progress(
        SpinnerColumn(),
        TextColumn("[bold blue]Getting Docker Compose logs...[/bold blue]"),
        console=console
    ) as progress:
        task = progress.add_task("Getting logs", total=1)
        
        docker_integration = get_docker_integration()
        result = await docker_integration.compose_logs(
            compose_file=compose_file,
            project_directory=dir_path,
            services=services,
            follow=follow,
            tail=tail,
            timestamps=timestamps
        )
        
        progress.update(task, completed=1)
    
    if result["success"]:
        logs = result.get("logs", "")
        
        if not logs.strip():
            console.print("[yellow]No logs available for these services.[/yellow]")
            return
        
        console.print(Panel(
            logs,
            title="Docker Compose Logs",
            expand=False,
            border_style="blue"
        ))
        
        if result.get("truncated", False):
            console.print("[yellow]Log output truncated due to size or streaming timeout.[/yellow]")
    else:
        console.print(f"[red]Error getting Docker Compose logs: {result.get('error', 'Unknown error')}[/red]")


@app.command("generate-dockerfile")
async def generate_dockerfile(
    directory: str = typer.Argument(".", help="Project directory"),
    output: Optional[str] = typer.Option(None, "--output", "-o", help="Output file path"),
    overwrite: bool = typer.Option(False, "--overwrite", help="Overwrite existing file")
):
    """Generate a Dockerfile for a project."""
    # Resolve directory
    dir_path = Path(directory).absolute()
    if not dir_path.exists() or not dir_path.is_dir():
        console.print(f"[red]Error: Project directory does not exist or is not a directory: {dir_path}[/red]")
        return
    
    with Progress(
        SpinnerColumn(),
        TextColumn(f"[bold blue]Generating Dockerfile for {dir_path}...[/bold blue]"),
        console=console
    ) as progress:
        task = progress.add_task("Generating", total=1)
        
        docker_integration = get_docker_integration()
        result = await docker_integration.generate_dockerfile(
            project_directory=dir_path,
            output_file=output,
            overwrite=overwrite
        )
        
        progress.update(task, completed=1)
    
    if result["success"]:
        dockerfile_path = result["dockerfile_path"]
        content = result.get("content", "")
        
        console.print(f"[green]Dockerfile generated successfully at {dockerfile_path}[/green]")
        
        # Display the content
        console.print(Syntax(
            content,
            "dockerfile",
            theme="monokai",
            word_wrap=True,
            line_numbers=True
        ))
    else:
        if "already exists" in result.get("error", ""):
            console.print(f"[yellow]{result['error']} Use --overwrite to replace it.[/yellow]")
        else:
            console.print(f"[red]Error generating Dockerfile: {result.get('error', 'Unknown error')}[/red]")


@app.command("generate-compose")
async def generate_docker_compose(
    directory: str = typer.Argument(".", help="Project directory"),
    output: Optional[str] = typer.Option(None, "--output", "-o", help="Output file path"),
    overwrite: bool = typer.Option(False, "--overwrite", help="Overwrite existing file"),
    include_databases: bool = typer.Option(True, "--databases/--no-databases", help="Include detected database services")
):
    """Generate a docker-compose.yml file for a project."""
    # Resolve directory
    dir_path = Path(directory).absolute()
    if not dir_path.exists() or not dir_path.is_dir():
        console.print(f"[red]Error: Project directory does not exist or is not a directory: {dir_path}[/red]")
        return
    
    with Progress(
        SpinnerColumn(),
        TextColumn(f"[bold blue]Generating docker-compose.yml for {dir_path}...[/bold blue]"),
        console=console
    ) as progress:
        task = progress.add_task("Generating", total=1)
        
        docker_integration = get_docker_integration()
        result = await docker_integration.generate_docker_compose(
            project_directory=dir_path,
            output_file=output,
            overwrite=overwrite,
            include_databases=include_databases
        )
        
        progress.update(task, completed=1)
    
    if result["success"]:
        compose_file_path = result["compose_file_path"]
        content = result.get("content", "")
        services = result.get("services_included", [])
        
        console.print(f"[green]docker-compose.yml generated successfully at {compose_file_path}[/green]")
        console.print(f"[green]Services included: {', '.join(services)}[/green]")
        
        # Display the content
        console.print(Syntax(
            content,
            "yaml",
            theme="monokai",
            word_wrap=True,
            line_numbers=True
        ))
    else:
        if "already exists" in result.get("error", ""):
            console.print(f"[yellow]{result['error']} Use --overwrite to replace it.[/yellow]")
        else:
            console.print(f"[red]Error generating docker-compose.yml: {result.get('error', 'Unknown error')}[/red]")


@app.command("generate-dockerignore")
async def generate_dockerignore(
    directory: str = typer.Argument(".", help="Project directory"),
    output: Optional[str] = typer.Option(None, "--output", "-o", help="Output file path"),
    overwrite: bool = typer.Option(False, "--overwrite", help="Overwrite existing file")
):
    """Generate a .dockerignore file for a project."""
    # Resolve directory
    dir_path = Path(directory).absolute()
    if not dir_path.exists() or not dir_path.is_dir():
        console.print(f"[red]Error: Project directory does not exist or is not a directory: {dir_path}[/red]")
        return
    
    with Progress(
        SpinnerColumn(),
        TextColumn(f"[bold blue]Generating .dockerignore for {dir_path}...[/bold blue]"),
        console=console
    ) as progress:
        task = progress.add_task("Generating", total=1)
        
        docker_integration = get_docker_integration()
        result = await docker_integration.generate_dockerignore(
            project_directory=dir_path,
            output_file=output,
            overwrite=overwrite
        )
        
        progress.update(task, completed=1)
    
    if result["success"]:
        dockerignore_path = result["path"]
        content = result.get("content", "")
        
        console.print(f"[green].dockerignore generated successfully at {dockerignore_path}[/green]")
        
        # Display the content
        console.print(Syntax(
            content,
            "gitignore",
            theme="monokai",
            word_wrap=True,
            line_numbers=True
        ))
    else:
        if "already exists" in result.get("error", ""):
            console.print(f"[yellow]{result['error']} Use --overwrite to replace it.[/yellow]")
        else:
            console.print(f"[red]Error generating .dockerignore: {result.get('error', 'Unknown error')}[/red]")


@app.command("setup")
async def setup_docker_project(
    directory: str = typer.Argument(".", help="Project directory"),
    dockerfile: bool = typer.Option(True, "--dockerfile/--no-dockerfile", help="Generate Dockerfile"),
    compose: bool = typer.Option(True, "--compose/--no-compose", help="Generate docker-compose.yml"),
    dockerignore: bool = typer.Option(True, "--dockerignore/--no-dockerignore", help="Generate .dockerignore"),
    overwrite: bool = typer.Option(False, "--overwrite", help="Overwrite existing files"),
    databases: bool = typer.Option(True, "--databases/--no-databases", help="Include detected database services"),
    build: bool = typer.Option(False, "--build", help="Build Docker image after setup")
):
    """Set up a complete Docker environment for a project."""
    # Resolve directory
    dir_path = Path(directory).absolute()
    if not dir_path.exists() or not dir_path.is_dir():
        console.print(f"[red]Error: Project directory does not exist or is not a directory: {dir_path}[/red]")
        return
    
    with Progress(
        SpinnerColumn(),
        TextColumn(f"[bold blue]Setting up Docker environment for {dir_path}...[/bold blue]"),
        console=console
    ) as progress:
        task = progress.add_task("Setting up", total=1)
        
        docker_integration = get_docker_integration()
        result = await docker_integration.setup_docker_project(
            project_directory=dir_path,
            generate_dockerfile=dockerfile,
            generate_compose=compose,
            generate_dockerignore=dockerignore,
            overwrite=overwrite,
            include_databases=databases,
            build_image=build
        )
        
        progress.update(task, completed=1)
    
    if result["success"]:
        files_generated = result.get("files_generated", [])
        console.print(f"[green]Docker environment setup completed successfully.[/green]")
        
        if files_generated:
            console.print("[green]Files generated:[/green]")
            for file_path in files_generated:
                console.print(f"[green]- {file_path}[/green]")
        
        if build and result.get("build_image", {}).get("success", False):
            console.print("[green]Docker image built successfully.[/green]")
        elif build and result.get("build_warnings"):
            console.print(f"[yellow]Warning during image build: {result['build_warnings']}[/yellow]")
    else:
        console.print(f"[red]Error setting up Docker environment: {result.get('error', 'Unknown error')}[/red]")
        
        # Show detailed results if available
        if "dockerfile" in result and not result["dockerfile"].get("success", False):
            console.print(f"[red]Dockerfile error: {result['dockerfile'].get('error', 'Unknown error')}[/red]")
        
        if "docker_compose" in result and not result["docker_compose"].get("success", False):
            console.print(f"[red]docker-compose.yml error: {result['docker_compose'].get('error', 'Unknown error')}[/red]")
        
        if "dockerignore" in result and not result["dockerignore"].get("success", False):
            console.print(f"[red].dockerignore error: {result['dockerignore'].get('error', 'Unknown error')}[/red]")


@app.command("info")
async def info():
    """Display detailed information about the Docker setup."""
    with Progress(
        SpinnerColumn(),
        TextColumn("[bold blue]Getting Docker information...[/bold blue]"),
        console=console
    ) as progress:
        task = progress.add_task("Getting info", total=1)
        
        # Check Docker and Docker Compose availability
        docker_integration = get_docker_integration()
        docker_available = await docker_integration.is_docker_available()
        compose_available = await docker_integration.is_docker_compose_available()
        
        # Execute docker info command if available
        docker_info_output = ""
        if docker_available:
            execution_engine = get_execution_engine()
            stdout, stderr, exit_code = await execution_engine.execute_command(
                "docker info",
                check_safety=True
            )
            if exit_code == 0:
                docker_info_output = stdout
        
        progress.update(task, completed=1)
    
    # Display information
    if docker_available:
        console.print(Panel(
            docker_info_output,
            title="Docker Information",
            expand=False,
            border_style="blue"
        ))
        
        console.print(f"Docker Compose: {'[green]Available[/green]' if compose_available else '[red]Not Available[/red]'}")
    else:
        console.print("[red]Docker is not available. Install Docker to use these features.[/red]")


# Run the app directly when this module is executed
if __name__ == "__main__":
    app()
</file>

<file path="components/cli/files_extensions.py">
# angela/cli/file_extensions.py
"""
CLI extensions for file resolution and activity tracking.

This module extends the files command with advanced file resolution features.
"""
import asyncio
from pathlib import Path
from typing import Optional, List

import typer
from rich.console import Console
from rich.panel import Panel
from rich.table import Table
from rich.text import Text
from rich import print as rich_print

from angela.api.context import (
    get_context_enhancer,
    get_context_manager,
    get_file_resolver,
    get_file_activity_tracker,
    get_activity_type_enum
)

from angela.utils.logging import get_logger

logger = get_logger(__name__)
console = Console()

# Create a Typer app for files extensions
app = typer.Typer(name="files", help="Advanced file operations")


@app.command("resolve")
def resolve_file(
    reference: str = typer.Argument(..., help="File reference to resolve"),
    scope: str = typer.Option(
        "all", "--scope", "-s", 
        help="Search scope (project, directory, all)"
    ),
):
    """Resolve a file reference to an actual file path."""
    # Get the current context
    context_manager = get_context_manager()
    context = context_manager.get_context_dict()
    
    # Convert scope
    search_scope = None
    if scope == "project":
        search_scope = "project"
    elif scope == "directory":
        search_scope = "directory"
    
    # Run the resolver
    try:
        file_resolver = get_file_resolver()
        path = asyncio.run(file_resolver.resolve_reference(
            reference,
            context,
            search_scope=search_scope
        ))
        
        if path:
            # Show the resolved path
            console.print(Panel(
                f"Resolved '[bold]{reference}[/bold]' to:\n[green]{path}[/green]",
                title="File Resolution",
                border_style="green"
            ))
            
            # Track as viewed file
            file_activity_tracker = get_file_activity_tracker()
            file_activity_tracker.track_file_viewing(path, None, {
                "reference": reference,
                "resolved_via": "cli"
            })
        else:
            # Show not found message
            console.print(Panel(
                f"Could not resolve '[bold]{reference}[/bold]' to a file or directory.",
                title="File Resolution",
                border_style="yellow"
            ))
            
            # Show suggestions based on the scope
            if search_scope == "project" and context.get("project_root"):
                # Suggest listing files in project
                console.print("Try using 'angela files find' to search for files in the project.")
            elif search_scope == "directory":
                # Suggest listing files in directory
                console.print("Try using 'angela files ls' to list files in the current directory.")
            else:
                # Suggest other scopes
                console.print("Try using '--scope project' or '--scope directory' to narrow the search.")
    
    except Exception as e:
        logger.exception(f"Error resolving file reference: {str(e)}")
        console.print(f"[bold red]Error:[/bold red] {str(e)}")


@app.command("extract")
def extract_references(
    text: str = typer.Argument(..., help="Text containing file references"),
):
    """Extract and resolve file references from text."""
    # Get the current context
    context_manager = get_context_manager()
    context = context_manager.get_context_dict()
    
    # Run the extractor
    try:
        file_resolver = get_file_resolver()
        references = asyncio.run(file_resolver.extract_references(text, context))
        
        if references:
            # Create a table for the results
            table = Table(title="Extracted File References")
            table.add_column("Reference", style="cyan")
            table.add_column("Resolved Path", style="green")
            table.add_column("Status", style="yellow")
            
            for reference, path in references:
                if path:
                    status = "[green]Found[/green]"
                    path_str = str(path)
                    
                    # Track as viewed file
                    file_activity_tracker = get_file_activity_tracker()
                    file_activity_tracker.track_file_viewing(path, None, {
                        "reference": reference,
                        "extracted_via": "cli"
                    })
                else:
                    status = "[yellow]Not Found[/yellow]"
                    path_str = "[italic]Not resolved[/italic]"
                
                table.add_row(reference, path_str, status)
            
            console.print(table)
        else:
            # Show not found message
            console.print(Panel(
                f"No file references found in the text.",
                title="File References",
                border_style="yellow"
            ))
    
    except Exception as e:
        logger.exception(f"Error extracting file references: {str(e)}")
        console.print(f"[bold red]Error:[/bold red] {str(e)}")


@app.command("recent")
def recent_files(
    limit: int = typer.Option(10, "--limit", "-n", help="Number of files to show"),
    activity_type: Optional[str] = typer.Option(
        None, "--type", "-t", 
        help="Filter by activity type (viewed, created, modified, deleted)"
    ),
):
    """Show recently accessed files."""
    try:
        # Convert activity type
        activity_types = None
        if activity_type:
            try:
                ActivityType = get_activity_type_enum()
                activity_types = [ActivityType(activity_type)]
            except ValueError:
                console.print(f"[yellow]Invalid activity type: {activity_type}[/yellow]")
                console.print("Available types: viewed, created, modified, deleted")
                return
        
        # Get recent activities
        file_activity_tracker = get_file_activity_tracker()
        activities = file_activity_tracker.get_recent_activities(
            limit=limit,
            activity_types=activity_types
        )
        
        if activities:
            # Create a table for the results
            table = Table(title=f"Recent File Activities (Last {len(activities)})")
            table.add_column("File", style="cyan")
            table.add_column("Activity", style="green")
            table.add_column("Time", style="yellow")
            table.add_column("Command", style="blue")
            
            for activity in activities:
                # Format the file name and path
                file_name = activity.get("name", "unknown")
                file_path = activity.get("path", "unknown")
                file_str = f"{file_name}\n[dim]{file_path}[/dim]"
                
                # Format activity type
                activity_type = activity.get("activity_type", "unknown")
                if activity_type == "viewed":
                    activity_str = "[blue]Viewed[/blue]"
                elif activity_type == "created":
                    activity_str = "[green]Created[/green]"
                elif activity_type == "modified":
                    activity_str = "[yellow]Modified[/yellow]"
                elif activity_type == "deleted":
                    activity_str = "[red]Deleted[/red]"
                else:
                    activity_str = activity_type.capitalize()
                
                # Format time
                time_str = activity.get("datetime", "unknown")
                if "T" in time_str:
                    time_str = time_str.replace("T", " ").split(".")[0]  # Simplify timestamp
                
                # Format command (truncate if too long)
                command = activity.get("command", "")
                if command and len(command) > 40:
                    command = command[:37] + "..."
                
                table.add_row(file_str, activity_str, time_str, command)
            
            console.print(table)
        else:
            # Show no activities message
            console.print(Panel(
                f"No file activities tracked yet.",
                title="Recent Files",
                border_style="yellow"
            ))
            
            # Show help message
            console.print("File activities are tracked when you interact with files using Angela.")
            console.print("Try viewing or manipulating some files first.")
    
    except Exception as e:
        logger.exception(f"Error retrieving recent files: {str(e)}")
        console.print(f"[bold red]Error:[/bold red] {str(e)}")


@app.command("active")
def most_active_files(
    limit: int = typer.Option(5, "--limit", "-n", help="Number of files to show"),
):
    """Show most actively used files."""
    try:
        # Get most active files
        file_activity_tracker = get_file_activity_tracker()
        active_files = file_activity_tracker.get_most_active_files(limit=limit)
        
        if active_files:
            # Create a table for the results
            table = Table(title=f"Most Active Files (Top {len(active_files)})")
            table.add_column("File", style="cyan")
            table.add_column("Activity Count", style="green")
            table.add_column("Last Activity", style="yellow")
            table.add_column("Activity Types", style="blue")
            
            for file_info in active_files:
                # Format the file name and path
                file_name = file_info.get("name", "unknown")
                file_path = file_info.get("path", "unknown")
                file_str = f"{file_name}\n[dim]{file_path}[/dim]"
                
                # Format activity count
                count = file_info.get("count", 0)
                
                # Format last activity time
                last_activity = file_info.get("last_activity", 0)
                if last_activity:
                    from datetime import datetime
                    time_str = datetime.fromtimestamp(last_activity).isoformat()
                    if "T" in time_str:
                        time_str = time_str.replace("T", " ").split(".")[0]  # Simplify timestamp
                else:
                    time_str = "Unknown"
                
                # Format activity types
                activities = file_info.get("activities", [])
                activities_str = ", ".join(a.capitalize() for a in activities)
                
                table.add_row(file_str, str(count), time_str, activities_str)
            
            console.print(table)
        else:
            # Show no activities message
            console.print(Panel(
                f"No file activities tracked yet.",
                title="Most Active Files",
                border_style="yellow"
            ))
            
            # Show help message
            console.print("File activities are tracked when you interact with files using Angela.")
            console.print("Try viewing or manipulating some files first.")
    
    except Exception as e:
        logger.exception(f"Error retrieving most active files: {str(e)}")
        console.print(f"[bold red]Error:[/bold red] {str(e)}")


@app.command("project")
def show_project_info():
    """Show detected project information."""
    try:
        # Get the current context
        context_manager = get_context_manager()
        context = context_manager.get_context_dict()
        
        # Check if in a project
        if not context.get("project_root"):
            console.print(Panel(
                "Not currently in a project directory.",
                title="Project Information",
                border_style="yellow"
            ))
            return
        
        # Get enhanced project info
        context_enhancer = get_context_enhancer()
        
        # Enrich context
        enriched = asyncio.run(context_enhancer.enrich_context(context))
        project_info = enriched.get("enhanced_project", {})
        
        if project_info:
            # Create a panel for the project information
            project_type = project_info.get("type", "Unknown")
            project_root = context.get("project_root", "Unknown")
            
            content = f"[bold]Project Type:[/bold] {project_type}\n"
            content += f"[bold]Project Root:[/bold] {project_root}\n\n"
            
            # Add frameworks if available
            if project_info.get("frameworks"):
                frameworks = list(project_info["frameworks"].keys())
                content += f"[bold]Frameworks:[/bold] {', '.join(frameworks)}\n"
            
            # Add dependencies if available
            if project_info.get("dependencies") and project_info["dependencies"].get("top_dependencies"):
                deps = project_info["dependencies"]["top_dependencies"]
                content += f"[bold]Top Dependencies:[/bold] {', '.join(deps[:5])}"
                if len(deps) > 5:
                    content += f" and {len(deps) - 5} more"
                content += f" (Total: {project_info['dependencies'].get('total', 0)})\n"
            
            # Add important files if available
            if project_info.get("important_files") and project_info["important_files"].get("paths"):
                files = project_info["important_files"]["paths"]
                content += f"[bold]Important Files:[/bold]\n"
                for f in files[:5]:
                    content += f"• {f}\n"
                if len(files) > 5:
                    content += f"• ... and {len(files) - 5} more\n"
            
            # Add structure info if available
            if project_info.get("structure"):
                structure = project_info["structure"]
                content += f"\n[bold]Project Structure:[/bold]\n"
                content += f"• Total Files: {structure.get('total_files', 'Unknown')}\n"
                
                if structure.get("main_directories"):
                    content += f"• Main Directories: {', '.join(structure['main_directories'])}\n"
                
                if structure.get("file_counts"):
                    content += f"• Top File Types:\n"
                    sorted_types = sorted(
                        structure["file_counts"].items(), 
                        key=lambda x: x[1], 
                        reverse=True
                    )
                    for ext, count in sorted_types[:5]:
                        content += f"  - {ext}: {count}\n"
            
            console.print(Panel(
                content,
                title=f"Project Information: {project_type}",
                border_style="green",
                expand=False
            ))
        else:
            # Show no project info message
            console.print(Panel(
                f"No enhanced project information available.",
                title="Project Information",
                border_style="yellow"
            ))
            
            # Show basic project info
            console.print(f"[bold]Project Root:[/bold] {context.get('project_root', 'Unknown')}")
            console.print(f"[bold]Project Type:[/bold] {context.get('project_type', 'Unknown')}")
    
    except Exception as e:
        logger.exception(f"Error retrieving project information: {str(e)}")
        console.print(f"[bold red]Error:[/bold red] {str(e)}")
</file>

<file path="components/cli/files.py">
# angela/cli/files.py
"""
File operation commands for Angela CLI.

This module provides CLI commands for file and directory operations.
"""
import os
import sys
import asyncio
from pathlib import Path
from typing import List, Optional, Tuple

import typer
from rich.console import Console
from rich.table import Table
from rich.panel import Panel
from rich.syntax import Syntax
from rich.markup import escape
from rich.text import Text
from rich.prompt import Prompt, Confirm
from rich.filesize import decimal as format_size

from angela.api.context import get_context_manager
from angela.api.execution import (
    get_filesystem_error_class,
    get_create_directory_func,
    get_delete_directory_func,
    get_create_file_func,
    get_read_file_func,
    get_write_file_func,
    get_delete_file_func,
    get_copy_file_func,
    get_move_file_func,
    get_rollback_manager
)

from angela.utils.logging import get_logger

logger = get_logger(__name__)
console = Console()

# Create the file operations app
app = typer.Typer(help="Angela's file operations")


@app.command("ls")
def list_directory(
    path: str = typer.Argument(
        None, help="Directory to list (defaults to current directory)"
    ),
    all: bool = typer.Option(
        False, "--all", "-a", help="Include hidden files"
    ),
    long: bool = typer.Option(
        False, "--long", "-l", help="Show detailed information"
    ),
):
    """List directory contents with enhanced formatting."""
    try:
        context_manager = get_context_manager()
        dir_path = Path(path) if path else context_manager.cwd
        
        if not dir_path.exists():
            console.print(f"[bold red]Error:[/bold red] Path not found: {dir_path}")
            sys.exit(1)
        
        if not dir_path.is_dir():
            console.print(f"[bold red]Error:[/bold red] Not a directory: {dir_path}")
            sys.exit(1)
        
        # Get directory contents
        contents = context_manager.get_directory_contents(dir_path, include_hidden=all)
        
        if not contents:
            console.print(f"Directory is empty: {dir_path}")
            return
        
        # Display in table format if long listing is requested
        if long:
            table = Table(title=f"Contents of {dir_path}")
            table.add_column("Name", style="cyan")
            table.add_column("Type", style="green")
            table.add_column("Size", style="blue", justify="right")
            table.add_column("Language", style="magenta")
            
            for item in contents:
                name = item["name"]
                
                # Add indicator for directories
                if item["is_dir"]:
                    name = f"{name}/"
                
                # Format size
                size = format_size(item["size"]) if "size" in item else ""
                
                # Get type and language
                item_type = item.get("type", "unknown")
                language = item.get("language", "")
                
                table.add_row(name, item_type, size, language)
            
            console.print(table)
        
        # Simple listing
        else:
            for item in contents:
                name = item["name"]
                
                # Color and format based on type
                if item["is_dir"]:
                    console.print(f"[bold blue]{name}/[/bold blue]", end="  ")
                elif item.get("language"):
                    console.print(f"[green]{name}[/green]", end="  ")
                elif item.get("binary", False):
                    console.print(f"[dim]{name}[/dim]", end="  ")
                else:
                    console.print(name, end="  ")
            
            # End with a newline
            console.print()
        
    except Exception as e:
        logger.exception(f"Error listing directory: {str(e)}")
        console.print(f"[bold red]Error:[/bold red] {str(e)}")
        sys.exit(1)


@app.command("mkdir")
def make_directory(
    path: str = typer.Argument(..., help="Directory to create"),
    parents: bool = typer.Option(
        True, "--parents/--no-parents", "-p", help="Create parent directories if needed"
    ),
    dry_run: bool = typer.Option(
        False, "--dry-run", help="Show what would happen without making changes"
    ),
):
    """Create a directory."""
    try:
        # Run the operation
        create_directory_func = get_create_directory_func()
        success = asyncio.run(create_directory_func(path, parents=parents, dry_run=dry_run))
        
        if success:
            if dry_run:
                console.print(f"[bold blue]DRY RUN:[/bold blue] Would create directory: {path}")
            else:
                console.print(f"[bold green]Created directory:[/bold green] {path}")
        else:
            console.print(f"[bold yellow]Operation cancelled.[/bold yellow]")
            sys.exit(1)
        
    except get_filesystem_error_class() as e:
        logger.exception(f"Error creating directory: {str(e)}")
        console.print(f"[bold red]Error:[/bold red] {str(e)}")
        sys.exit(1)

@app.command("rmdir")
def remove_directory(
    path: str = typer.Argument(..., help="Directory to remove"),
    recursive: bool = typer.Option(
        False, "--recursive", "-r", help="Recursively remove directories and their contents"
    ),
    force: bool = typer.Option(
        False, "--force", "-f", help="Ignore nonexistent files"
    ),
    dry_run: bool = typer.Option(
        False, "--dry-run", help="Show what would happen without making changes"
    ),
):
    """Remove a directory."""
    try:
        # Run the operation
        delete_directory_func = get_delete_directory_func()
        success = asyncio.run(delete_directory_func(
            path, recursive=recursive, force=force, dry_run=dry_run
        ))
        
        if success:
            if dry_run:
                console.print(f"[bold blue]DRY RUN:[/bold blue] Would remove directory: {path}")
            else:
                console.print(f"[bold green]Removed directory:[/bold green] {path}")
        else:
            console.print(f"[bold yellow]Operation cancelled.[/bold yellow]")
            sys.exit(1)
        
    except get_filesystem_error_class() as e:
        logger.exception(f"Error removing directory: {str(e)}")
        console.print(f"[bold red]Error:[/bold red] {str(e)}")
        sys.exit(1)


@app.command("touch")
def touch_file(
    path: str = typer.Argument(..., help="File to create or update timestamp"),
    dry_run: bool = typer.Option(
        False, "--dry-run", help="Show what would happen without making changes"
    ),
):
    """Create a new file or update file timestamp."""
    try:
        # Run the operation (with no content for touch)
        create_file_func = get_create_file_func()
        success = asyncio.run(create_file_func(path, content=None, dry_run=dry_run))
        
        if success:
            if dry_run:
                console.print(f"[bold blue]DRY RUN:[/bold blue] Would touch file: {path}")
            else:
                console.print(f"[bold green]Touched file:[/bold green] {path}")
        else:
            console.print(f"[bold yellow]Operation cancelled.[/bold yellow]")
            sys.exit(1)
        
    except get_filesystem_error_class() as e:
        logger.exception(f"Error touching file: {str(e)}")
        console.print(f"[bold red]Error:[/bold red] {str(e)}")
        sys.exit(1)


@app.command("cat")
def cat_file(
    path: str = typer.Argument(..., help="File to display"),
    binary: bool = typer.Option(
        False, "--binary", "-b", help="Display binary content"
    ),
    syntax: bool = typer.Option(
        True, "--syntax/--no-syntax", help="Syntax highlighting"
    ),
):
    """Display file contents."""
    try:
        file_path = Path(path)
        
        # Get file info to determine syntax highlighting
        context_manager = get_context_manager()
        file_info = context_manager.get_file_info(file_path)
        
        # Read the file
        read_file_func = get_read_file_func()
        content = asyncio.run(read_file_func(path, binary=binary))
        
        # Display the content
        if binary:
            # For binary files, just show a hexdump-like output
            console.print(Panel(
                escape(repr(content[:1000])) + ("..." if len(content) > 1000 else ""),
                title=f"Binary content of {path}",
                subtitle=f"Showing first 1000 bytes of {len(content)} total bytes",
                expand=False
            ))
        elif syntax and file_info.get("language") and not binary:
            # Determine the language for syntax highlighting
            lang = file_info.get("language", "").lower()
            if "python" in lang:
                lang = "python"
            elif "javascript" in lang:
                lang = "javascript"
            elif "html" in lang:
                lang = "html"
            elif "css" in lang:
                lang = "css"
            elif "json" in lang:
                lang = "json"
            elif "yaml" in lang:
                lang = "yaml"
            elif "markdown" in lang:
                lang = "markdown"
            elif "bash" in lang or "shell" in lang:
                lang = "bash"
            else:
                lang = "text"
            
            # Display with syntax highlighting
            console.print(Syntax(
                content,
                lang,
                theme="monokai",
                line_numbers=True,
                word_wrap=True
            ))
        else:
            # Simple text display
            console.print(content)
        
    except get_filesystem_error_class() as e:
        logger.exception(f"Error reading file: {str(e)}")
        console.print(f"[bold red]Error:[/bold red] {str(e)}")
        sys.exit(1)


@app.command("rm")
def remove_file(
    path: str = typer.Argument(..., help="File to remove"),
    force: bool = typer.Option(
        False, "--force", "-f", help="Ignore nonexistent files"
    ),
    dry_run: bool = typer.Option(
        False, "--dry-run", help="Show what would happen without making changes"
    ),
):
    """Remove a file."""
    try:
        # Run the operation
        delete_file_func = get_delete_file_func()
        success = asyncio.run(delete_file_func(path, force=force, dry_run=dry_run))
        
        if success:
            if dry_run:
                console.print(f"[bold blue]DRY RUN:[/bold blue] Would remove file: {path}")
            else:
                console.print(f"[bold green]Removed file:[/bold green] {path}")
        else:
            console.print(f"[bold yellow]Operation cancelled.[/bold yellow]")
            sys.exit(1)
        
    except get_filesystem_error_class() as e:
        logger.exception(f"Error removing file: {str(e)}")
        console.print(f"[bold red]Error:[/bold red] {str(e)}")
        sys.exit(1)


@app.command("cp")
def copy_file_command(
    source: str = typer.Argument(..., help="Source file to copy"),
    destination: str = typer.Argument(..., help="Destination path"),
    force: bool = typer.Option(
        False, "--force", "-f", help="Overwrite destination if it exists"
    ),
    dry_run: bool = typer.Option(
        False, "--dry-run", help="Show what would happen without making changes"
    ),
):
    """Copy a file."""
    try:
        # Run the operation
        copy_file_func = get_copy_file_func()
        success = asyncio.run(copy_file_func(
            source, destination, overwrite=force, dry_run=dry_run
        ))
        
        if success:
            if dry_run:
                console.print(f"[bold blue]DRY RUN:[/bold blue] Would copy {source} to {destination}")
            else:
                console.print(f"[bold green]Copied:[/bold green] {source} -> {destination}")
        else:
            console.print(f"[bold yellow]Operation cancelled.[/bold yellow]")
            sys.exit(1)
        
    except get_filesystem_error_class() as e:
        logger.exception(f"Error copying file: {str(e)}")
        console.print(f"[bold red]Error:[/bold red] {str(e)}")
        sys.exit(1)


@app.command("mv")
def move_file_command(
    source: str = typer.Argument(..., help="Source file to move"),
    destination: str = typer.Argument(..., help="Destination path"),
    force: bool = typer.Option(
        False, "--force", "-f", help="Overwrite destination if it exists"
    ),
    dry_run: bool = typer.Option(
        False, "--dry-run", help="Show what would happen without making changes"
    ),
):
    """Move a file."""
    try:
        # Run the operation
        move_file_func = get_move_file_func()
        success = asyncio.run(move_file_func(
            source, destination, overwrite=force, dry_run=dry_run
        ))
        
        if success:
            if dry_run:
                console.print(f"[bold blue]DRY RUN:[/bold blue] Would move {source} to {destination}")
            else:
                console.print(f"[bold green]Moved:[/bold green] {source} -> {destination}")
        else:
            console.print(f"[bold yellow]Operation cancelled.[/bold yellow]")
            sys.exit(1)
        
    except get_filesystem_error_class() as e:
        logger.exception(f"Error moving file: {str(e)}")
        console.print(f"[bold red]Error:[/bold red] {str(e)}")
        sys.exit(1)


@app.command("write")
def write_file_command(
    path: str = typer.Argument(..., help="File to write to"),
    content: str = typer.Option(
        None, "--content", "-c", help="Content to write (if not provided, will prompt)"
    ),
    append: bool = typer.Option(
        False, "--append", "-a", help="Append to file instead of overwriting"
    ),
    dry_run: bool = typer.Option(
        False, "--dry-run", help="Show what would happen without making changes"
    ),
):
    """Write content to a file."""
    try:
        # If content is not provided, prompt for it
        if content is None:
            console.print(f"Enter content for {path} (press Ctrl+D on a new line to finish):")
            lines = []
            try:
                while True:
                    line = input()
                    lines.append(line)
            except EOFError:
                pass
            content = "\n".join(lines)
        
        # Run the operation
        write_file_func = get_write_file_func()
        success = asyncio.run(write_file_func(
            path, content, append=append, dry_run=dry_run
        ))
        
        if success:
            if dry_run:
                mode = "append to" if append else "write to"
                console.print(f"[bold blue]DRY RUN:[/bold blue] Would {mode} file: {path}")
            else:
                mode = "Appended to" if append else "Wrote to"
                console.print(f"[bold green]{mode} file:[/bold green] {path}")
        else:
            console.print(f"[bold yellow]Operation cancelled.[/bold yellow]")
            sys.exit(1)
        
    except get_filesystem_error_class() as e:
        logger.exception(f"Error writing file: {str(e)}")
        console.print(f"[bold red]Error:[/bold red] {str(e)}")
        sys.exit(1)


@app.command("find")
def find_files(
    pattern: str = typer.Argument(..., help="Pattern to search for"),
    path: str = typer.Option(
        ".", "--path", "-p", help="Directory to search in"
    ),
    include_hidden: bool = typer.Option(
        False, "--hidden", "-a", help="Include hidden files"
    ),
):
    """Find files matching a pattern."""
    try:
        context_manager = get_context_manager()
        base_dir = Path(path)
        if not base_dir.exists() or not base_dir.is_dir():
            console.print(f"[bold red]Error:[/bold red] Not a valid directory: {path}")
            sys.exit(1)
        
        # Find files matching the pattern
        matches = context_manager.find_files(
            pattern, base_dir=base_dir, include_hidden=include_hidden
        )
        
        if not matches:
            console.print(f"No files found matching pattern: {pattern}")
            return
        
        # Display results
        console.print(f"Found {len(matches)} files matching '{pattern}':")
        
        for match in matches:
            # Get file info
            file_info = context_manager.get_file_info(match)
            
            # Format the output
            if file_info.get("is_dir", False):
                console.print(f"[bold blue]{match}/[/bold blue]")
            elif file_info.get("language"):
                lang = file_info.get("language", "")
                console.print(f"[green]{match}[/green] - [magenta]{lang}[/magenta]")
            else:
                console.print(str(match))
        
    except Exception as e:
        logger.exception(f"Error finding files: {str(e)}")
        console.print(f"[bold red]Error:[/bold red] {str(e)}")
        sys.exit(1)


@app.command("rollback")
def rollback_command(
    list_only: bool = typer.Option(
        False, "--list", "-l", help="List recent operations without rolling back"
    ),
    operation_id: int = typer.Option(
        None, "--id", help="ID of the specific operation to roll back"
    ),
):
    """Roll back a previous file operation."""
    try:
        # Get recent operations
        rollback_manager = get_rollback_manager()
        operations = asyncio.run(rollback_manager.get_recent_operations())
        
        if not operations:
            console.print("No operations available for rollback.")
            return
        
        # If list_only is specified, just show the operations
        if list_only:
            table = Table(title="Recent Operations")
            table.add_column("ID", style="cyan")
            table.add_column("Timestamp", style="blue")
            table.add_column("Operation", style="green")
            table.add_column("Description")
            table.add_column("Can Rollback", style="magenta")
            
            for op in operations:
                can_rollback = "✓" if op["can_rollback"] else "✗"
                table.add_row(
                    str(op["id"]),
                    op["timestamp"],
                    op["operation_type"],
                    op["description"],
                    can_rollback
                )
            
            console.print(table)
            return
        
        # If no ID is provided, show the list and prompt for one
        if operation_id is None:
            table = Table(title="Recent Operations")
            table.add_column("ID", style="cyan")
            table.add_column("Timestamp", style="blue")
            table.add_column("Description")
            table.add_column("Can Rollback", style="magenta")
            
            for op in operations:
                can_rollback = "✓" if op["can_rollback"] else "✗"
                table.add_row(
                    str(op["id"]),
                    op["timestamp"],
                    op["description"],
                    can_rollback
                )
            
            console.print(table)
            
            # Prompt for the operation ID
            operation_id = int(Prompt.ask("Enter the ID of the operation to roll back"))
        
        # Check if the operation ID is valid
        valid_ids = [op["id"] for op in operations]
        if operation_id not in valid_ids:
            console.print(f"[bold red]Error:[/bold red] Invalid operation ID: {operation_id}")
            sys.exit(1)
        
        # Check if the operation can be rolled back
        operation = next(op for op in operations if op["id"] == operation_id)
        if not operation["can_rollback"]:
            console.print(f"[bold red]Error:[/bold red] Operation cannot be rolled back: {operation['description']}")
            sys.exit(1)
        
        # Confirm the rollback
        confirmed = Confirm.ask(f"Roll back operation: {operation['description']}?")
        if not confirmed:
            console.print("Rollback cancelled.")
            return
        
        # Perform the rollback
        success = asyncio.run(rollback_manager.rollback_operation(operation_id))
        
        if success:
            console.print(f"[bold green]Successfully rolled back:[/bold green] {operation['description']}")
        else:
            console.print(f"[bold red]Failed to roll back operation.[/bold red]")
            sys.exit(1)
        
    except Exception as e:
        logger.exception(f"Error during rollback: {str(e)}")
        console.print(f"[bold red]Error:[/bold red] {str(e)}")
        sys.exit(1)


@app.command("info")
def file_info(
    path: str = typer.Argument(
        None, help="File to get information about (defaults to current directory)"
    ),
    preview: bool = typer.Option(
        True, "--preview/--no-preview", help="Show file content preview"
    ),
):
    """Show detailed information about a file or directory."""
    try:
        context_manager = get_context_manager()
        file_path = Path(path) if path else context_manager.cwd
        
        if not file_path.exists():
            console.print(f"[bold red]Error:[/bold red] Path not found: {file_path}")
            sys.exit(1)
        
        # Get file information
        file_info = context_manager.get_file_info(file_path)
        
        # Display information
        console.print(Panel(
            f"[bold]Path:[/bold] {file_info['path']}\n"
            f"[bold]Type:[/bold] {file_info.get('type', 'unknown')}\n"
            + (f"[bold]Language:[/bold] {file_info.get('language', 'N/A')}\n" if file_info.get('language') else "")
            + (f"[bold]MIME Type:[/bold] {file_info.get('mime_type', 'N/A')}\n" if file_info.get('mime_type') else "")
            + (f"[bold]Size:[/bold] {format_size(file_info.get('size', 0))}\n" if 'size' in file_info else "")
            + (f"[bold]Binary:[/bold] {'Yes' if file_info.get('binary', False) else 'No'}\n" if not file_info.get('is_dir', False) else ""),
            title=f"Information for {file_path.name}",
            expand=False
        ))
        
        # Show content preview for files
        if preview and not file_info.get('is_dir', False) and not file_info.get('binary', False):
            content_preview = context_manager.get_file_preview(file_path)
            
            if content_preview:
                # Try to determine the language for syntax highlighting
                lang = "text"
                if file_info.get("language"):
                    lang_name = file_info.get("language", "").lower()
                    if "python" in lang_name:
                        lang = "python"
                    elif "javascript" in lang_name:
                        lang = "javascript"
                    elif "html" in lang_name:
                        lang = "html"
                    elif "css" in lang_name:
                        lang = "css"
                    elif "json" in lang_name:
                        lang = "json"
                    elif "yaml" in lang_name:
                        lang = "yaml"
                    elif "markdown" in lang_name:
                        lang = "markdown"
                    elif "bash" in lang_name or "shell" in lang_name:
                        lang = "bash"
                
                console.print(Panel(
                    Syntax(
                        content_preview,
                        lang,
                        theme="monokai",
                        line_numbers=True,
                        word_wrap=True
                    ),
                    title="Content Preview",
                    expand=False
                ))
        
    except Exception as e:
        logger.exception(f"Error getting file information: {str(e)}")
        console.print(f"[bold red]Error:[/bold red] {str(e)}")
        sys.exit(1)
</file>

<file path="components/cli/generation.py">
# angela/cli/generation.py
"""
CLI interface for code generation features in Angela CLI.
"""
import os
import asyncio
from pathlib import Path
from typing import List, Optional, Tuple, Dict, Any
import typer
from rich.console import Console
from rich.panel import Panel
from rich.syntax import Syntax
from rich.table import Table
from rich import print as rich_print
from rich.prompt import Prompt, Confirm
from rich.markdown import Markdown


from angela.utils.logging import get_logger
from angela.api.generation import (
    get_code_generation_engine,
    get_interactive_refiner,
    get_generation_context_manager,
    get_code_file_class
)
from angela.api.toolchain import (
    get_git_integration,
    get_package_manager_integration,
    get_test_framework_integration,
    get_ci_cd_integration
)
from angela.api.review import get_diff_manager, get_feedback_manager
from angela.api.context import (
    get_context_manager,
    get_context_enhancer,
    get_file_detector_func
)

app = typer.Typer(help="Code generation commands")
console = Console()
logger = get_logger(__name__)

@app.command("create-project")
def create_project(
    description: str = typer.Argument(..., help="Description of the project to generate"),
    output_dir: str = typer.Option(".", help="Directory where the project should be generated"),
    project_type: Optional[str] = typer.Option(None, help="Project type (python, node, etc.)"),
    git_init: bool = typer.Option(True, help="Initialize Git repository"),
    install_deps: bool = typer.Option(False, help="Install dependencies"),
    generate_tests: bool = typer.Option(False, help="Generate test files"),
    ci_platform: Optional[str] = typer.Option(None, help="Generate CI configuration (github, gitlab, etc.)"),
    dry_run: bool = typer.Option(False, help="Preview without creating files")
):
    """
    Generate a complete project from a description.
    """
    console.print(Panel(
        f"[bold green]Generating project from description:[/bold green]\n{description}",
        title="Project Generation",
        expand=False
    ))
    
    try:
        # Get current context
        context_manager = get_context_manager()
        context = context_manager.get_context_dict()
        
        # Get code generation engine
        code_generation_engine = get_code_generation_engine()
        
        # Generate project plan
        project_plan = asyncio.run(code_generation_engine.generate_project(
            description=description,
            output_dir=output_dir,
            project_type=project_type,
            context=context
        ))
        
        # Display project plan
        console.print("\n[bold blue]Project Plan:[/bold blue]")
        console.print(f"Name: [bold]{project_plan.name}[/bold]")
        console.print(f"Type: [bold]{project_plan.project_type}[/bold]")
        console.print(f"Files: [bold]{len(project_plan.files)}[/bold]")
        
        # Show the list of files
        table = Table(title="Files to Generate")
        table.add_column("Path", style="cyan")
        table.add_column("Purpose", style="green")
        
        for file in project_plan.files:
            table.add_row(file.path, file.purpose)
        
        console.print(table)
        
        # Create the files if not dry run
        if not dry_run:
            console.print("\n[bold]Creating project files...[/bold]")
            
            with console.status("[bold green]Creating files...[/bold green]"):
                result = asyncio.run(code_generation_engine.create_project_files(project_plan))
            
            console.print(f"[green]Created {result['file_count']} files[/green]")
            
            # Initialize Git repository if requested
            if git_init:
                console.print("\n[bold]Initializing Git repository...[/bold]")
                
                with console.status("[bold green]Initializing Git...[/bold green]"):
                    git_integration = get_git_integration()
                    git_result = asyncio.run(git_integration.init_repository(
                        path=output_dir,
                        initial_branch="main",
                        gitignore_template=project_plan.project_type
                    ))
                
                if git_result["success"]:
                    console.print("[green]Git repository initialized successfully[/green]")
                else:
                    console.print(f"[yellow]Failed to initialize Git repository: {git_result.get('error', 'Unknown error')}[/yellow]")
            
            # Install dependencies if requested
            if install_deps:
                console.print("\n[bold]Installing dependencies...[/bold]")
                
                with console.status("[bold green]Installing dependencies...[/bold green]"):
                    package_manager_integration = get_package_manager_integration()
                    deps_result = asyncio.run(package_manager_integration.install_dependencies(
                        path=output_dir,
                        dependencies=project_plan.dependencies.get("runtime", []),
                        dev_dependencies=project_plan.dependencies.get("development", []),
                        project_type=project_plan.project_type
                    ))
                
                if deps_result["success"]:
                    console.print("[green]Dependencies installed successfully[/green]")
                else:
                    console.print(f"[yellow]Failed to install dependencies: {deps_result.get('error', 'Unknown error')}[/yellow]")
            
            # Generate test files if requested
            if generate_tests:
                console.print("\n[bold]Generating test files...[/bold]")
                
                with console.status("[bold green]Generating tests...[/bold green]"):
                    test_framework_integration = get_test_framework_integration()
                    test_result = asyncio.run(test_framework_integration.generate_test_files(
                        src_files=project_plan.files,
                        project_type=project_plan.project_type,
                        root_dir=output_dir
                    ))
                
                if test_result["success"]:
                    console.print(f"[green]Generated {test_result['file_count']} test files[/green]")
                else:
                    console.print(f"[yellow]Failed to generate test files: {test_result.get('error', 'Unknown error')}[/yellow]")
            
            # Generate CI/CD configuration if requested
            if ci_platform:
                console.print("\n[bold]Generating CI/CD configuration...[/bold]")
                
                with console.status("[bold green]Generating {ci_platform} configuration...[/bold green]"):
                    ci_cd_integration = get_ci_cd_integration()
                    ci_result = asyncio.run(ci_cd_integration.generate_ci_configuration(
                        path=output_dir,
                        platform=ci_platform,
                        project_type=project_plan.project_type
                    ))
                
                if ci_result["success"]:
                    console.print(f"[green]Generated {ci_platform} configuration successfully[/green]")
                else:
                    console.print(f"[yellow]Failed to generate CI/CD configuration: {ci_result.get('error', 'Unknown error')}[/yellow]")
            
            # Create initial commit if Git was initialized
            if git_init:
                console.print("\n[bold]Creating initial commit...[/bold]")
                
                with console.status("[bold green]Creating commit...[/bold green]"):
                    git_integration = get_git_integration()
                    commit_result = asyncio.run(git_integration.commit_changes(
                        path=output_dir,
                        message="Initial project generation",
                        auto_stage=True
                    ))
                
                if commit_result["success"]:
                    console.print("[green]Created initial commit successfully[/green]")
                else:
                    console.print(f"[yellow]Failed to create initial commit: {commit_result.get('error', 'Unknown error')}[/yellow]")
            
            console.print(f"\n[bold green]Project generated successfully in: {output_dir}[/bold green]")
        else:
            console.print("\n[bold yellow]Dry run - no files were created[/bold yellow]")
    
    except Exception as e:
        logger.exception("Error generating project")
        console.print(f"[bold red]Error generating project:[/bold red] {str(e)}")


@app.command("add-feature")
async def add_feature(
    description: str = typer.Argument(..., help="Description of the feature to add"),
    project_dir: str = typer.Option(".", help="Project directory"),
    branch: Optional[str] = typer.Option(None, help="Create a feature branch"),
    generate_tests: bool = typer.Option(False, help="Generate test files"),
    install_deps: bool = typer.Option(False, help="Install new dependencies"),
    dry_run: bool = typer.Option(False, help="Preview without creating files"),
    auto_commit: bool = typer.Option(False, help="Commit changes automatically")
):
    """
    Add a new feature to an existing project.
    """
    console.print(Panel(
        f"[bold green]Adding feature to project:[/bold green]\n{description}",
        title="Feature Addition",
        expand=False
    ))
    
    try:
        # Get current context
        context_manager = get_context_manager()
        context = context_manager.get_context_dict()
        context_enhancer = get_context_enhancer()
        context = await context_enhancer.enrich_context(context)
        
        # Check if project directory exists
        project_path = Path(project_dir)
        if not project_path.exists() or not project_path.is_dir():
            console.print(f"[bold red]Project directory does not exist: {project_dir}[/bold red]")
            return
        
        # Create a feature branch if requested
        if branch:
            console.print(f"\n[bold]Creating feature branch: {branch}[/bold]")
            
            if not dry_run:
                with console.status("[bold green]Creating branch...[/bold green]"):
                    git_integration = get_git_integration()
                    branch_result = await git_integration.create_branch(
                        path=project_dir,
                        branch_name=branch,
                        checkout=True
                    )
                
                if branch_result["success"]:
                    console.print(f"[green]Created and checked out branch: {branch}[/green]")
                else:
                    console.print(f"[yellow]Failed to create branch: {branch_result.get('error', 'Unknown error')}[/yellow]")
        
        # Get project info
        with console.status("[bold green]Analyzing project...[/bold green]"):
            # Detect project type
            ci_cd_integration = get_ci_cd_integration()
            project_type_result = await ci_cd_integration.detect_project_type(project_dir)
            project_type = project_type_result.get("project_type")
            
            if not project_type:
                console.print("[yellow]Could not detect project type. Proceeding anyway...[/yellow]")
            else:
                console.print(f"[green]Detected project type: {project_type}[/green]")
        
        # Generate feature implementation
        console.print("\n[bold]Generating feature implementation...[/bold]")
        
        with console.status("[bold green]Generating feature implementation...[/bold green]"):
            # Use the new feature addition method
            code_generation_engine = get_code_generation_engine()
            result = await code_generation_engine.add_feature_to_project(
                description=description,
                project_dir=project_dir,
                context=context
            )
        
        if result["success"]:
            # Display results
            console.print(f"[green]Successfully added feature to project![/green]")
            
            if result.get("new_files"):
                console.print("\n[bold]Created Files:[/bold]")
                for file_path in result["new_files"]:
                    console.print(f"  ✅ {file_path}")
            
            if result.get("modified_files"):
                console.print("\n[bold]Modified Files:[/bold]")
                for file_path in result["modified_files"]:
                    console.print(f"  ✏️ {file_path}")
            
            # Generate tests if requested
            if generate_tests and not dry_run:
                console.print("\n[bold]Generating tests for new feature...[/bold]")
                
                with console.status("[bold green]Generating tests...[/bold green]"):
                    # Create a list of new files with CodeFile objects
                    src_files = []
                    CodeFile = get_code_file_class()
                    for file_path in result.get("new_files", []):
                        try:
                            with open(file_path, 'r', encoding='utf-8', errors='replace') as f:
                                content = f.read()
                                
                            rel_path = str(Path(file_path).relative_to(project_dir))
                            src_files.append(CodeFile(
                                path=rel_path,
                                content=content,
                                purpose=f"New feature: {description}",
                                dependencies=[],
                                language=project_type
                            ))
                        except Exception as e:
                            console.print(f"[yellow]Error reading file {file_path}: {str(e)}[/yellow]")
                    
                    if src_files:
                        test_framework_integration = get_test_framework_integration()
                        test_result = await test_framework_integration.generate_test_files(
                            src_files=src_files,
                            project_type=project_type,
                            root_dir=project_dir
                        )
                        
                        if test_result["success"]:
                            console.print(f"[green]Generated {test_result['file_count']} test files[/green]")
                        else:
                            console.print(f"[yellow]Failed to generate test files: {test_result.get('error', 'Unknown error')}[/yellow]")
                    else:
                        console.print("[yellow]No source files available for test generation[/yellow]")
            
            # Install dependencies if requested
            if install_deps and not dry_run:
                console.print("\n[bold]Installing dependencies...[/bold]")
                
                with console.status("[bold green]Extracting and installing dependencies...[/bold green]"):
                    # Extract dependencies from the feature files
                    dependencies = await code_generation_engine._extract_dependencies_from_feature(
                        feature_files={
                            "new_files": [{"path": path, "content": open(path, 'r', encoding='utf-8', errors='replace').read()} 
                                          for path in result.get("new_files", []) if Path(path).exists()],
                            "modified_files": [{"path": path, 
                                               "original_content": "", # We don't need original content for dependency extraction
                                               "modified_content": open(path, 'r', encoding='utf-8', errors='replace').read()} 
                                              for path in result.get("modified_files", []) if Path(path).exists()]
                        },
                        project_type=project_type
                    )
                    
                    if not dependencies["runtime"] and not dependencies["development"]:
                        console.print("[yellow]No new dependencies detected in the feature.[/yellow]")
                    else:
                        # Install the detected dependencies
                        runtime_deps = dependencies.get("runtime", [])
                        dev_deps = dependencies.get("development", [])
                        
                        if runtime_deps:
                            console.print(f"[bold]Runtime dependencies to install:[/bold] {', '.join(runtime_deps)}")
                            package_manager_integration = get_package_manager_integration()
                            install_result = await package_manager_integration.install_dependencies(
                                path=project_dir,
                                dependencies=runtime_deps,
                                project_type=project_type
                            )
                            
                            if install_result["success"]:
                                console.print(f"[green]Successfully installed runtime dependencies[/green]")
                            else:
                                console.print(f"[yellow]Failed to install runtime dependencies: {install_result.get('error', 'Unknown error')}[/yellow]")
                        
                        if dev_deps:
                            console.print(f"[bold]Development dependencies to install:[/bold] {', '.join(dev_deps)}")
                            package_manager_integration = get_package_manager_integration()
                            dev_install_result = await package_manager_integration.install_dependencies(
                                path=project_dir,
                                dependencies=[],
                                dev_dependencies=dev_deps,
                                project_type=project_type
                            )
                            
                            if dev_install_result["success"]:
                                console.print(f"[green]Successfully installed development dependencies[/green]")
                            else:
                                console.print(f"[yellow]Failed to install development dependencies: {dev_install_result.get('error', 'Unknown error')}[/yellow]")
    
    except Exception as e:
        logger.exception("Error adding feature")
        console.print(f"[bold red]Error adding feature:[/bold red] {str(e)}")


@app.command("refine-code")
async def refine_code(
    feedback: str = typer.Argument(..., help="Feedback for code improvement"),
    file_path: str = typer.Argument(..., help="Path to the file to refine"),
    apply: bool = typer.Option(False, help="Apply the changes"),
    backup: bool = typer.Option(True, help="Create backup before applying changes")
):
    """
    Refine code based on feedback.
    """
    console.print(Panel(
        f"[bold green]Refining code based on feedback:[/bold green]\n{feedback}",
        title="Code Refinement",
        expand=False
    ))
    
    try:
        # Get current context
        context_manager = get_context_manager()
        context = context_manager.get_context_dict()
        
        # Check if file exists
        file = Path(file_path)
        if not file.exists() or not file.is_file():
            console.print(f"[bold red]File does not exist: {file_path}[/bold red]")
            return
        
        # Read file content
        with open(file, 'r', encoding='utf-8', errors='replace') as f:
            original_code = f.read()
        
        # Process feedback
        console.print("\n[bold]Processing feedback...[/bold]")
        
        with console.status("[bold green]Generating improvements...[/bold green]"):
            feedback_manager = get_feedback_manager()
            result = await feedback_manager.process_feedback(
                feedback=feedback,
                original_code=original_code,
                file_path=str(file),
                context=context
            )
        
        # Display diff
        console.print("\n[bold blue]Code changes:[/bold blue]")
        
        syntax = Syntax(
            result["diff"],
            "diff",
            theme="monokai",
            line_numbers=True,
            word_wrap=True
        )
        console.print(syntax)
        
        # Display explanation
        console.print("\n[bold blue]Explanation:[/bold blue]")
        console.print(result["explanation"])
        
        # Apply changes if requested
        if apply:
            console.print("\n[bold]Applying changes...[/bold]")
            
            refinements = {
                "project_dir": str(file.parent),
                "results": [
                    {
                        "file_path": file.name,
                        "has_changes": original_code != result["improved_code"],
                        "diff": result["diff"],
                        "explanation": result["explanation"]
                    }
                ]
            }
            
            with console.status("[bold green]Applying changes...[/bold green]"):
                feedback_manager = get_feedback_manager()
                apply_result = await feedback_manager.apply_refinements(
                    refinements=refinements,
                    backup=backup
                )
            
            if apply_result["files_changed"] > 0:
                console.print("[green]Changes applied successfully[/green]")
                if backup:
                    backup_file = apply_result["results"][0].get("backup")
                    if backup_file:
                        console.print(f"[blue]Backup created: {backup_file}[/blue]")
            else:
                console.print("[yellow]No changes were applied[/yellow]")
        else:
            console.print("\n[bold yellow]Changes were not applied. Use --apply to apply changes.[/bold yellow]")
    
    except Exception as e:
        logger.exception("Error refining code")
        console.print(f"[bold red]Error refining code:[/bold red] {str(e)}")


@app.command("refine-project")
def refine_project(
    feedback: str = typer.Argument(..., help="Feedback for project improvement"),
    project_dir: str = typer.Option(".", help="Project directory"),
    focus: Optional[List[str]] = typer.Option(None, help="Files to focus on (glob patterns supported)"),
    apply: bool = typer.Option(False, help="Apply the changes"),
    backup: bool = typer.Option(True, help="Create backup before applying changes")
):
    """
    Refine an entire project based on feedback.
    """
    console.print(Panel(
        f"[bold green]Refining project based on feedback:[/bold green]\n{feedback}",
        title="Project Refinement",
        expand=False
    ))
    
    try:
        # Get current context
        context_manager = get_context_manager()
        context = context_manager.get_context_dict()
        
        # Check if project directory exists
        project_path = Path(project_dir)
        if not project_path.exists() or not project_path.is_dir():
            console.print(f"[bold red]Project directory does not exist: {project_dir}[/bold red]")
            return
        
        # Process feedback
        console.print("\n[bold]Processing feedback...[/bold]")
        
        with console.status("[bold green]Analyzing project and generating improvements...[/bold green]"):
            feedback_manager = get_feedback_manager()
            result = asyncio.run(feedback_manager.refine_project(
                project_dir=project_path,
                feedback=feedback,
                focus_files=focus,
                context=context
            ))
        
        # Display results
        console.print(f"\n[bold blue]Files analyzed: {len(result['results'])}[/bold blue]")
        
        # Create a table to show the changes
        table = Table(title="Refinement Results")
        table.add_column("File", style="cyan")
        table.add_column("Status", style="green")
        table.add_column("Changes", style="yellow")
        
        for file_result in result["results"]:
            file_path = file_result["file_path"]
            
            if "error" in file_result:
                status = "[red]Error[/red]"
                changes = file_result["error"]
            elif not file_result.get("has_changes", False):
                status = "[blue]No changes[/blue]"
                changes = "No changes needed"
            else:
                status = "[green]Changes pending[/green]"
                diff_lines = file_result["diff"].splitlines()
                additions = sum(1 for line in diff_lines if line.startswith('+') and not line.startswith('+++'))
                deletions = sum(1 for line in diff_lines if line.startswith('-') and not line.startswith('---'))
                changes = f"+{additions} -{deletions}"
            
            table.add_row(file_path, status, changes)
        
        console.print(table)
        
        # Show details for files with changes
        changed_files = [r for r in result["results"] if r.get("has_changes", False)]
        if changed_files:
            console.print(f"\n[bold blue]Files with changes ({len(changed_files)}):[/bold blue]")
            
            for file_result in changed_files:
                console.print(f"\n[bold cyan]File: {file_result['file_path']}[/bold cyan]")
                
                # Display diff
                syntax = Syntax(
                    file_result["diff"],
                    "diff",
                    theme="monokai",
                    line_numbers=True,
                    word_wrap=True
                )
                console.print(syntax)
                
                # Display explanation
                if "explanation" in file_result:
                    console.print(f"[italic]{file_result['explanation']}[/italic]")
        
        # Apply changes if requested
        if apply:
            console.print("\n[bold]Applying changes...[/bold]")
            
            with console.status("[bold green]Applying changes...[/bold green]"):
                feedback_manager = get_feedback_manager()
                apply_result = asyncio.run(feedback_manager.apply_refinements(
                    refinements=result,
                    backup=backup
                ))
            
            if apply_result["files_changed"] > 0:
                console.print(f"[green]Changes applied to {apply_result['files_changed']} files[/green]")
                if backup:
                    console.print("[blue]Backups created for modified files[/blue]")
            else:
                console.print("[yellow]No changes were applied[/yellow]")
        else:
            console.print("\n[bold yellow]Changes were not applied. Use --apply to apply changes.[/bold yellow]")
    
    except Exception as e:
        logger.exception("Error refining project")
        console.print(f"[bold red]Error refining project:[/bold red] {str(e)}")


@app.command("generate-ci")
def generate_ci(
    platform: str = typer.Argument(..., help="CI platform (github_actions, gitlab_ci, jenkins, travis, circle_ci)"),
    project_dir: str = typer.Option(".", help="Project directory"),
    project_type: Optional[str] = typer.Option(None, help="Project type (python, node, etc.)")
):
    """
    Generate CI/CD configuration for a project.
    """
    console.print(Panel(
        f"[bold green]Generating CI/CD configuration for platform:[/bold green] {platform}",
        title="CI/CD Configuration",
        expand=False
    ))
    
    try:
        # Check if project directory exists
        project_path = Path(project_dir)
        if not project_path.exists() or not project_path.is_dir():
            console.print(f"[bold red]Project directory does not exist: {project_dir}[/bold red]")
            return
        
        # Detect project type if not provided
        if not project_type:
            console.print("\n[bold]Detecting project type...[/bold]")
            
            with console.status("[bold green]Analyzing project...[/bold green]"):
                ci_cd_integration = get_ci_cd_integration()
                detection_result = asyncio.run(ci_cd_integration.detect_project_type(project_dir))
                project_type = detection_result.get("project_type")
            
            if project_type:
                console.print(f"[green]Detected project type: {project_type}[/green]")
            else:
                console.print("[red]Could not detect project type[/red]")
                return
        
        # Generate CI configuration
        console.print(f"\n[bold]Generating {platform} configuration...[/bold]")
        
        with console.status(f"[bold green]Generating configuration...[/bold green]"):
            ci_cd_integration = get_ci_cd_integration()
            result = asyncio.run(ci_cd_integration.generate_ci_configuration(
                path=project_dir,
                platform=platform,
                project_type=project_type
            ))
        
        if result["success"]:
            console.print(f"[green]Generated {platform} configuration successfully[/green]")
            console.print(f"Configuration file: [bold]{result['config_file']}[/bold]")
        else:
            console.print(f"[red]Failed to generate configuration: {result.get('error', 'Unknown error')}[/red]")
    
    except Exception as e:
        logger.exception("Error generating CI/CD configuration")
        console.print(f"[bold red]Error generating CI/CD configuration:[/bold red] {str(e)}")

@app.command("create-complex-project")
def create_complex_project(
    description: str = typer.Argument(..., help="Description of the project to generate"),
    output_dir: str = typer.Option(".", help="Directory where the project should be generated"),
    project_type: Optional[str] = typer.Option(None, help="Project type (python, node, etc.)"),
    framework: Optional[str] = typer.Option(None, help="Framework to use (django, react, etc.)"),
    detailed_planning: bool = typer.Option(True, help="Use detailed architecture planning"),
    git_init: bool = typer.Option(True, help="Initialize Git repository"),
    install_deps: bool = typer.Option(False, help="Install dependencies"),
    generate_tests: bool = typer.Option(False, help="Generate test files"),
    ci_platform: Optional[str] = typer.Option(None, help="Generate CI configuration (github, gitlab, etc.)"),
    dry_run: bool = typer.Option(False, help="Preview without creating files")
):
    """
    Generate a complex multi-file project with detailed architecture planning.
    """
    console.print(Panel(
        f"[bold green]Generating complex project from description:[/bold green]\n{description}",
        title="Complex Project Generation",
        expand=False
    ))
    
    try:
        # Get current context
        context_manager = get_context_manager()
        context = context_manager.get_context_dict()
        
        # Generate complex project
        with console.status("[bold green]Generating project plan and architecture...[/bold green]"):
            code_generation_engine = get_code_generation_engine()
            project_plan = asyncio.run(code_generation_engine.generate_complex_project(
                description=description,
                output_dir=output_dir,
                project_type=project_type,
                framework=framework,
                use_detailed_planning=detailed_planning,
                context=context
            ))
        
        # Display project plan
        console.print("\n[bold blue]Project Plan:[/bold blue]")
        console.print(f"Name: [bold]{project_plan.name}[/bold]")
        console.print(f"Type: [bold]{project_plan.project_type}[/bold]")
        console.print(f"Files: [bold]{len(project_plan.files)}[/bold]")
        
        # Generate architecture visualization if available
        generation_context_manager = get_generation_context_manager()
        architecture = generation_context_manager.get_global_context("architecture")
        if architecture:
            console.print("\n[bold blue]Architecture Overview:[/bold blue]")
            console.print(f"Structure: [bold]{architecture.get('structure_type', 'layered')}[/bold]")
            console.print(f"Components: [bold]{len(architecture.get('components', []))}[/bold]")
            console.print(f"Layers: [bold]{', '.join(architecture.get('layers', []))}[/bold]")
            
            # Show design patterns
            if architecture.get("patterns"):
                console.print(f"Design Patterns: [bold]{', '.join(architecture.get('patterns', []))}[/bold]")
        
        # Show the list of files grouped by component/directory
        grouped_files = group_files_by_directory(project_plan.files)
        
        for group, files in grouped_files.items():
            table = Table(title=f"Files in {group}")
            table.add_column("Path", style="cyan")
            table.add_column("Purpose", style="green")
            
            for file in files:
                table.add_row(file.path, file.purpose)
            
            console.print(table)
        
        # Interactive refinement phase
        if not dry_run and Confirm.ask("\nWould you like to refine any aspect of the project before creation?"):
            feedback = Prompt.ask("\nPlease describe what you'd like to refine")
            
            with console.status("[bold green]Refining project based on feedback...[/bold green]"):
                interactive_refiner = get_interactive_refiner()
                refined_project, refinement_results = asyncio.run(interactive_refiner.process_refinement_feedback(
                    feedback=feedback,
                    project=project_plan
                ))
                
                # Update project plan with refinements
                project_plan = refined_project
            
            # Show refinement summary
            summary = asyncio.run(interactive_refiner.summarize_refinements(refinement_results))
            
            console.print(f"\n[green]Refined {summary['files_modified']} files based on feedback[/green]")
            
            if summary["file_summaries"]:
                for file_summary in summary["file_summaries"]:
                    console.print(f"- [cyan]{file_summary['file_path']}[/cyan]: {file_summary['lines_added']} lines added, {file_summary['lines_deleted']} lines removed")
        
        # Create the files if not dry run
        if not dry_run:
            console.print("\n[bold]Creating project files...[/bold]")
            
            with console.status("[bold green]Creating files...[/bold green]"):
                code_generation_engine = get_code_generation_engine()
                result = asyncio.run(code_generation_engine.create_project_files(project_plan))
            
            console.print(f"[green]Created {result['file_count']} files[/green]")
            
            # Initialize Git repository if requested
            if git_init:
                console.print("\n[bold]Initializing Git repository...[/bold]")
                
                with console.status("[bold green]Initializing Git...[/bold green]"):
                    git_integration = get_git_integration()
                    git_result = asyncio.run(git_integration.init_repository(
                        path=output_dir,
                        initial_branch="main",
                        gitignore_template=project_plan.project_type
                    ))
                
                if git_result["success"]:
                    console.print("[green]Git repository initialized successfully[/green]")
                else:
                    console.print(f"[yellow]Failed to initialize Git repository: {git_result.get('error', 'Unknown error')}[/yellow]")
            
            # Install dependencies if requested
            if install_deps:
                console.print("\n[bold]Installing dependencies...[/bold]")
                
                with console.status("[bold green]Installing dependencies...[/bold green]"):
                    package_manager_integration = get_package_manager_integration()
                    deps_result = asyncio.run(package_manager_integration.install_dependencies(
                        path=output_dir,
                        dependencies=project_plan.dependencies.get("runtime", []),
                        dev_dependencies=project_plan.dependencies.get("development", []),
                        project_type=project_plan.project_type
                    ))
                
                if deps_result["success"]:
                    console.print("[green]Dependencies installed successfully[/green]")
                else:
                    console.print(f"[yellow]Failed to install dependencies: {deps_result.get('error', 'Unknown error')}[/yellow]")
            
            # Generate test files if requested
            if generate_tests:
                console.print("\n[bold]Generating test files...[/bold]")
                
                with console.status("[bold green]Generating tests...[/bold green]"):
                    test_framework_integration = get_test_framework_integration()
                    test_result = asyncio.run(test_framework_integration.generate_test_files(
                        src_files=project_plan.files,
                        project_type=project_plan.project_type,
                        root_dir=output_dir
                    ))
                
                if test_result["success"]:
                    console.print(f"[green]Generated {test_result['file_count']} test files[/green]")
                else:
                    console.print(f"[yellow]Failed to generate test files: {test_result.get('error', 'Unknown error')}[/yellow]")
            
            # Generate CI/CD configuration if requested
            if ci_platform:
                console.print("\n[bold]Generating CI/CD configuration...[/bold]")
                
                with console.status(f"[bold green]Generating {ci_platform} configuration...[/bold green]"):
                    ci_cd_integration = get_ci_cd_integration()
                    ci_result = asyncio.run(ci_cd_integration.generate_ci_configuration(
                        path=output_dir,
                        platform=ci_platform,
                        project_type=project_plan.project_type
                    ))
                
                if ci_result["success"]:
                    console.print(f"[green]Generated {ci_platform} configuration successfully[/green]")
                else:
                    console.print(f"[yellow]Failed to generate CI/CD configuration: {ci_result.get('error', 'Unknown error')}[/yellow]")
            
            # Create initial commit if Git was initialized
            if git_init:
                console.print("\n[bold]Creating initial commit...[/bold]")
                
                with console.status("[bold green]Creating commit...[/bold green]"):
                    git_integration = get_git_integration()
                    commit_result = asyncio.run(git_integration.commit_changes(
                        path=output_dir,
                        message="Initial project generation",
                        auto_stage=True
                    ))
                
                if commit_result["success"]:
                    console.print("[green]Created initial commit successfully[/green]")
                else:
                    console.print(f"[yellow]Failed to create initial commit: {commit_result.get('error', 'Unknown error')}[/yellow]")
            
            console.print(f"\n[bold green]Project generated successfully in: {output_dir}[/bold green]")
        else:
            console.print("\n[bold yellow]Dry run - no files were created[/bold yellow]")
    
    except Exception as e:
        logger.exception("Error generating complex project")
        console.print(f"[bold red]Error generating complex project:[/bold red] {str(e)}")

@app.command("create-framework-project")
def create_framework_project(
    framework: str = typer.Argument(..., help="Framework to generate (e.g., react, django)"),
    description: str = typer.Argument(..., help="Description of the project to generate"),
    output_dir: str = typer.Option(".", help="Directory where the project should be generated"),
    variant: Optional[str] = typer.Option(None, help="Framework variant (e.g., nextjs for React)"),
    typescript: Optional[bool] = typer.Option(None, help="Use TypeScript (for JS frameworks)"),
    with_auth: Optional[bool] = typer.Option(None, help="Include authentication"),
    enhanced: bool = typer.Option(True, help="Use enhanced project structure"),
    install_deps: bool = typer.Option(False, help="Install dependencies"),
    git_init: bool = typer.Option(True, help="Initialize Git repository"),
    dry_run: bool = typer.Option(False, help="Preview without creating files")
):
    """
    Generate a project for a specific framework with best practices.
    """
    console.print(Panel(
        f"[bold green]Generating {framework} project:[/bold green]\n{description}",
        title="Framework Project Generation",
        expand=False
    ))
    
    try:
        # Get current context
        context_manager = get_context_manager()
        context = context_manager.get_context_dict()
        
        # Prepare options
        options = {
            "variant": variant,
            "typescript": typescript,
            "authentication": with_auth
        }
        
        # Remove None values
        options = {k: v for k, v in options.items() if v is not None}
        
        # Generate framework project
        from angela.generation.frameworks import framework_generator
        
        with console.status(f"[bold green]Generating {framework} project...[/bold green]"):
            if enhanced:
                result = asyncio.run(framework_generator.generate_standard_project_structure(
                    framework=framework,
                    description=description,
                    output_dir=output_dir,
                    options=options
                ))
            else:
                result = asyncio.run(framework_generator.generate_framework_structure(
                    framework=framework,
                    description=description,
                    output_dir=output_dir,
                    options=options
                ))
        
        if not result["success"]:
            console.print(f"[bold red]Error generating {framework} project:[/bold red] {result.get('error', 'Unknown error')}")
            return
        
        # Display result information
        console.print("\n[bold blue]Framework Project:[/bold blue]")
        console.print(f"Framework: [bold]{result['framework']}[/bold]")
        
        if result.get("variant"):
            console.print(f"Variant: [bold]{result['variant']}[/bold]")
            
        console.print(f"Files: [bold]{len(result['files'])}[/bold]")
        
        # Group files by directory
        grouped_files = {}
        for file in result["files"]:
            directory = Path(file.path).parent.as_posix()
            if directory == ".":
                directory = "Root"
                
            if directory not in grouped_files:
                grouped_files[directory] = []
                
            grouped_files[directory].append(file)
        
        # Display files by directory
        for directory, files in grouped_files.items():
            table = Table(title=f"Files in {directory}")
            table.add_column("Path", style="cyan")
            table.add_column("Purpose", style="green")
            
            for file in files:
                table.add_row(file.path, file.purpose)
            
            console.print(table)
        
        # Create project if not dry run
        if not dry_run:
            # Create CodeProject object
            from angela.generation.engine import CodeProject
            
            project = CodeProject(
                name=f"{framework}_project",
                description=description,
                root_dir=output_dir,
                files=result["files"],
                dependencies={"runtime": [], "development": []},
                project_type=result["project_type"],
                structure_explanation=f"Standard {framework} project structure"
            )
            
            # Create files
            console.print("\n[bold]Creating project files...[/bold]")
            
            with console.status("[bold green]Creating files...[/bold green]"):
                code_generation_engine = get_code_generation_engine()
                creation_result = asyncio.run(code_generation_engine.create_project_files(project))
            
            console.print(f"[green]Created {creation_result['file_count']} files[/green]")
            
            # Initialize Git repository if requested
            if git_init:
                console.print("\n[bold]Initializing Git repository...[/bold]")
                
                with console.status("[bold green]Initializing Git...[/bold green]"):
                    git_integration = get_git_integration()
                    git_result = asyncio.run(git_integration.init_repository(
                        path=output_dir,
                        initial_branch="main",
                        gitignore_template=result["project_type"]
                    ))
                
                if git_result["success"]:
                    console.print("[green]Git repository initialized successfully[/green]")
                else:
                    console.print(f"[yellow]Failed to initialize Git repository: {git_result.get('error', 'Unknown error')}[/yellow]")
            
            # Install dependencies if requested
            if install_deps:
                console.print("\n[bold]Installing dependencies...[/bold]")
                
                with console.status("[bold green]Installing dependencies...[/bold green]"):
                    # Use package manager based on framework
                    project_type = result["project_type"]
                    
                    package_manager_integration = get_package_manager_integration()
                    deps_result = asyncio.run(package_manager_integration.install_dependencies(
                        path=output_dir,
                        project_type=project_type
                    ))
                
                if deps_result["success"]:
                    console.print("[green]Dependencies installed successfully[/green]")
                else:
                    console.print(f"[yellow]Failed to install dependencies: {deps_result.get('error', 'Unknown error')}[/yellow]")
            
            console.print(f"\n[bold green]Framework project generated successfully in: {output_dir}[/bold green]")
        else:
            console.print("\n[bold yellow]Dry run - no files were created[/bold yellow]")
    
    except Exception as e:
        logger.exception(f"Error generating {framework} project")
        console.print(f"[bold red]Error generating {framework} project:[/bold red] {str(e)}")


@app.command("refine-generated-project")
async def refine_generated_project(
    project_dir: str = typer.Argument(..., help="Directory of the generated project"),
    feedback: str = typer.Argument(..., help="Feedback for project improvement"),
    focus: Optional[List[str]] = typer.Option(None, help="Files to focus on (glob patterns supported)"),
    apply: bool = typer.Option(True, help="Apply the changes"),
    backup: bool = typer.Option(True, help="Create backup before applying changes")
):
    """
    Refine a generated project based on natural language feedback.
    """
    console.print(Panel(
        f"[bold green]Refining project based on feedback:[/bold green]\n{feedback}",
        title="Project Refinement",
        expand=False
    ))
    
    try:
        # Get current context
        context_manager = get_context_manager()
        context = context_manager.get_context_dict()
        
        # Check if project directory exists
        project_path = Path(project_dir)
        if not project_path.exists() or not project_path.is_dir():
            console.print(f"[bold red]Project directory does not exist: {project_dir}[/bold red]")
            return
        
        # Load project files
        with console.status("[bold green]Loading project files...[/bold green]"):
            project_files = []
            
            for root, _, files in os.walk(project_path):
                for file in files:
                    # Skip common non-source directories
                    if any(excluded in root for excluded in ['.git', 'node_modules', '__pycache__', '.venv']):
                        continue
                    
                    file_path = Path(root) / file
                    rel_path = file_path.relative_to(project_path)
                    
                    # Skip binary files
                    if any(file.endswith(ext) for ext in ['.jpg', '.png', '.gif', '.pdf', '.zip', '.pyc']):
                        continue
                    
                    try:
                        with open(file_path, 'r', encoding='utf-8', errors='replace') as f:
                            content = f.read()
                        
                        # Get file info
                        file_detector_func = get_file_detector_func()
                        file_type = file_detector_func(file_path)
                        
                        CodeFile = get_code_file_class()
                        project_files.append(CodeFile(
                            path=str(rel_path),
                            content=content,
                            purpose="",  # We don't know the purpose
                            dependencies=[],
                            language=file_type.get("language")
                        ))
                    except Exception as e:
                        console.print(f"[yellow]Error reading file {file_path}: {str(e)}[/yellow]")
        
        console.print(f"[green]Loaded {len(project_files)} project files[/green]")
        
        # Create a project object
        from angela.generation.engine import CodeProject
        
        project = CodeProject(
            name=project_path.name,
            description="",  # We don't know the description
            root_dir=str(project_path),
            files=project_files,
            dependencies={"runtime": [], "development": []},
            project_type="unknown",  # We'll try to infer this
            structure_explanation=""
        )
        
        # Try to infer project type
        with console.status("[bold green]Analyzing project...[/bold green]"):
            # Try to detect project type
            ci_cd_integration = get_ci_cd_integration()
            detection_result = await ci_cd_integration.detect_project_type(project_dir)
            
            if detection_result["project_type"]:
                project.project_type = detection_result["project_type"]
                console.print(f"[green]Detected project type: {project.project_type}[/green]")
            
            # Analyze code relationships
            generation_context_manager = get_generation_context_manager()
            analysis_result = await generation_context_manager.analyze_code_relationships(project.files)
            
            console.print(f"[green]Analyzed {analysis_result.get('entity_count', 0)} entities and {analysis_result.get('dependency_count', 0)} dependencies[/green]")
            
            # Detect architecture patterns
            if analysis_result.get("architecture_patterns"):
                console.print(f"[green]Detected architecture patterns: {', '.join(analysis_result.get('architecture_patterns', []))}[/green]")
        
        # Process feedback
        console.print("\n[bold]Processing feedback...[/bold]")
        
        with console.status("[bold green]Generating improvements based on feedback...[/bold green]"):
            interactive_refiner = get_interactive_refiner()
            refined_project, refinement_results = await interactive_refiner.process_refinement_feedback(
                feedback=feedback,
                project=project,
                focus_files=focus
            )
        
        # Display results
        console.print(f"\n[bold blue]Files analyzed: {len(refinement_results['results'])}[/bold blue]")
        
        # Create a table to show the changes
        table = Table(title="Refinement Results")
        table.add_column("File", style="cyan")
        table.add_column("Status", style="green")
        table.add_column("Changes", style="yellow")
        
        for file_result in refinement_results["results"]:
            file_path = file_result["file_path"]
            
            if "error" in file_result:
                status = "[red]Error[/red]"
                changes = file_result["error"]
            elif not file_result.get("has_changes", False):
                status = "[blue]No changes[/blue]"
                changes = "No changes needed"
            else:
                status = "[green]Changes pending[/green]"
                diff_lines = file_result["diff"].splitlines()
                additions = sum(1 for line in diff_lines if line.startswith('+') and not line.startswith('+++'))
                deletions = sum(1 for line in diff_lines if line.startswith('-') and not line.startswith('---'))
                changes = f"+{additions} -{deletions}"
            
            table.add_row(file_path, status, changes)
        
        console.print(table)
        
        # Show details for files with changes
        changed_files = [r for r in refinement_results["results"] if r.get("has_changes", False)]
        if changed_files:
            console.print(f"\n[bold blue]Files with changes ({len(changed_files)}):[/bold blue]")
            
            for file_result in changed_files:
                console.print(f"\n[bold cyan]File: {file_result['file_path']}[/bold cyan]")
                
                # Display diff
                syntax = Syntax(
                    file_result["diff"],
                    "diff",
                    theme="monokai",
                    line_numbers=True,
                    word_wrap=True
                )
                console.print(syntax)
                
                # Display explanation
                if "explanation" in file_result:
                    explanation_md = Markdown(file_result["explanation"])
                    console.print(explanation_md)
        
        # Apply changes if requested
        if apply:
            console.print("\n[bold]Applying changes...[/bold]")
            
            with console.status("[bold green]Applying changes...[/bold green]"):
                feedback_manager = get_feedback_manager()
                apply_result = await feedback_manager.apply_refinements(
                    refinements=refinement_results,
                    backup=backup
                )
            
            if apply_result["files_changed"] > 0:
                console.print(f"[green]Changes applied to {apply_result['files_changed']} files[/green]")
                if backup:
                    console.print("[blue]Backups created for modified files[/blue]")
            else:
                console.print("[yellow]No changes were applied[/yellow]")
        else:
            console.print("\n[bold yellow]Changes were not applied. Use --apply to apply changes.[/bold yellow]")
    
    except Exception as e:
        logger.exception("Error refining project")
        console.print(f"[bold red]Error refining project:[/bold red] {str(e)}")


def group_files_by_directory(files: List['CodeFile']) -> Dict[str, List['CodeFile']]:
    """
    Group files by their directory.
    
    Args:
        files: List of CodeFile objects
        
    Returns:
        Dictionary mapping directory names to lists of files
    """
    grouped = {}
    
    for file in files:
        directory = Path(file.path).parent.as_posix()
        if directory == ".":
            directory = "Root"
            
        if directory not in grouped:
            grouped[directory] = []
            
        grouped[directory].append(file)
    
    return grouped


@app.command("generate-tests")
def generate_tests(
    project_dir: str = typer.Option(".", help="Project directory"),
    test_framework: Optional[str] = typer.Option(None, help="Test framework to use"),
    focus: Optional[List[str]] = typer.Option(None, help="Files to focus on (glob patterns supported)")
):
    """
    Generate test files for a project.
    """
    console.print(Panel(
        "[bold green]Generating test files for project[/bold green]",
        title="Test Generation",
        expand=False
    ))
    
    try:
        # Check if project directory exists
        project_path = Path(project_dir)
        if not project_path.exists() or not project_path.is_dir():
            console.print(f"[bold red]Project directory does not exist: {project_dir}[/bold red]")
            return
        
        # Detect test framework if not provided
        if not test_framework:
            console.print("\n[bold]Detecting test framework...[/bold]")
            
            with console.status("[bold green]Analyzing project...[/bold green]"):
                test_framework_integration = get_test_framework_integration()
                detection_result = asyncio.run(test_framework_integration.detect_test_framework(project_dir))
                test_framework = detection_result.get("test_framework")
            
            if test_framework:
                console.print(f"[green]Detected test framework: {test_framework}[/green]")
            else:
                console.print("[yellow]Could not detect test framework. Using default for project type...[/yellow]")
        
        # Find source files
        console.print("\n[bold]Finding source files...[/bold]")
        
        src_files = []
        with console.status("[bold green]Scanning project...[/bold green]"):
            # Get project type
            ci_cd_integration = get_ci_cd_integration()
            project_type_result = asyncio.run(ci_cd_integration.detect_project_type(project_dir))
            project_type = project_type_result.get("project_type")
            
            # Map of project types to file extensions
            extensions = {
                "python": [".py"],
                "node": [".js", ".jsx", ".ts", ".tsx"],
                "java": [".java"],
                "go": [".go"],
                "rust": [".rs"],
                "ruby": [".rb"]
            }
            
            # Get relevant file extensions
            file_exts = extensions.get(project_type, [".py", ".js", ".java", ".go", ".rs", ".rb"])
            
            # Find all source files
            for root, _, files in os.walk(project_path):
                # Skip common test directories and non-source directories
                if any(excluded in root for excluded in ["test", "tests", "__pycache__", "node_modules", ".git"]):
                    continue
                
                for file in files:
                    _, ext = os.path.splitext(file)
                    if ext in file_exts:
                        # If focus is specified, check if file matches any pattern
                        if focus:
                            file_path = Path(root) / file
                            rel_path = file_path.relative_to(project_path)
                            
                            matched = False
                            for pattern in focus:
                                if Path(pattern).name == file or rel_path.match(pattern):
                                    matched = True
                                    break
                            
                            if not matched:
                                continue
                        
                        # Read file content
                        try:
                            with open(Path(root) / file, 'r', encoding='utf-8', errors='replace') as f:
                                content = f.read()
                            
                            # Create CodeFile object
                            CodeFile = get_code_file_class()
                            file_path = Path(root) / file
                            rel_path = file_path.relative_to(project_path)
                            
                            src_files.append(CodeFile(
                                path=str(rel_path),
                                content=content,
                                purpose=f"Source file: {file}",
                                dependencies=[],
                                language=project_type
                            ))
                        except Exception as e:
                            console.print(f"[yellow]Error reading file {file}: {str(e)}[/yellow]")
        
        console.print(f"[green]Found {len(src_files)} source files[/green]")
        
        # Generate test files
        console.print("\n[bold]Generating test files...[/bold]")
        
        with console.status("[bold green]Generating tests...[/bold green]"):
            test_framework_integration = get_test_framework_integration()
            result = asyncio.run(test_framework_integration.generate_test_files(
                src_files=src_files,
                test_framework=test_framework,
                project_type=project_type,
                root_dir=project_dir
            ))
        
        if result["success"]:
            console.print(f"[green]Generated {result['file_count']} test files[/green]")
            
            # Show generated files
            if result["generated_files"]:
                console.print("\n[bold blue]Generated test files:[/bold blue]")
                for file in result["generated_files"]:
                    console.print(f"- {file}")
        else:
            console.print(f"[red]Failed to generate test files: {result.get('error', 'Unknown error')}[/red]")
    
    except Exception as e:
        logger.exception("Error generating test files")
        console.print(f"[bold red]Error generating test files:[/bold red] {str(e)}")

if __name__ == "__main__":
    app()
</file>

<file path="components/cli/main.py">
# angela/cli/main.py
"""
Main command-line interface for Angela CLI.
"""
import sys
import asyncio
from typing import List, Optional

import typer
from rich.console import Console
from rich.panel import Panel
from rich.syntax import Syntax
from rich import print as rich_print

from angela import __version__
from angela.config import config_manager
from angela.api.context import get_context_manager
from angela.orchestrator import orchestrator            
from angela.api.execution import get_execution_engine
from angela.utils.logging import setup_logging, get_logger
from angela.api.shell import get_terminal_formatter, get_output_type_enum
from angela.api.ai import get_error_analyzer
from angela.api.context import get_session_manager

context_manager = get_context_manager()
execution_engine = get_execution_engine()
terminal_formatter = get_terminal_formatter()
OutputType = get_output_type_enum()
error_analyzer = get_error_analyzer()
session_manager = get_session_manager()

# Create the app
app = typer.Typer(help="Angela: AI-powered command-line assistant")
logger = get_logger(__name__)
console = Console()


def version_callback(value: bool):
    """Display version information and exit."""
    if value:
        console.print(f"Angela CLI version: {__version__}")
        sys.exit(0)


@app.callback()
def main(
    debug: bool = typer.Option(
        False, "--debug", "-d", help="Enable debug mode"
    ),
    version: bool = typer.Option(
        False, "--version", "-v", callback=version_callback, help="Show version and exit"
    ),
    monitor: bool = typer.Option(
        False, "--monitor", "-m", help="Enable background monitoring for proactive assistance"
    ),

):
    """
    Angela: AI-powered command-line assistant
    
    Angela helps you perform tasks through natural language commands.
    Simply tell Angela what you want to do in plain English!
    
    Examples:
      angela request "create a new directory called my_project"
      angela request "find all python files in the current directory"
      angela request "help me write a bash script that renames files"
    
    Use 'angela --help' to see available commands, or 'angela COMMAND --help'
    for more information about a specific command.
    """
    
    # Set debug mode
    config_manager.config.debug = debug
    
    # Configure logging
    setup_logging(debug=debug)
    
    # Start background monitoring if requested
    if monitor:
        # Import here to avoid circular imports
        from angela.monitoring.background import background_monitor
        background_monitor.start_monitoring()


@app.command()
def request(
    request_text: List[str] = typer.Argument(
        ..., help="The natural language request for Angela."
    ),
    suggest_only: bool = typer.Option(
        False, "--suggest-only", "-s", help="Only suggest commands without executing."
    ),
    dry_run: bool = typer.Option(
        False, "--dry-run", help="Preview command execution without making changes."
    ),
    force: bool = typer.Option(
        False, "--force", "-f", help="Execute without confirmation, even for risky operations."
    ),
    # The 'help' parameter is GONE
):
    """
    Send a natural language request to Angela.
    
    This is the main way to interact with Angela. Just type your request
    in plain English, and Angela will understand and take action.
    
    Examples:
      angela request "list all files in the current directory"
      angela request "show me the disk usage"
      angela request "create a file called todo.txt with a shopping list"
      angela request "help me debug my Python script"
    
    Use --suggest-only to see what command would be run without executing it.
    Use --dry-run to simulate execution without making changes.
    Use --force to skip confirmation prompts (use with caution for risky operations).
    """
    # The 'if help:' block is GONE
    
    # Combine all arguments into a single request string
    full_request = " ".join(request_text)
    
    try:
        # If forcing execution, set this in the session
        if force:
            session_manager.add_entity("force_execution", "preference", "true")
        
        # Process the request - note execute=True is now the default
        # Only switch to false if suggest_only is True
        execute = not suggest_only
        
        # Call the orchestrator to process the request
        result = asyncio.run(orchestrator.process_request(
            full_request, execute=execute, dry_run=dry_run
        ))
        
        
        # In debug mode, show context information
        if config_manager.config.debug:
            context_text = "\n".join([f"{k}: {v}" for k, v in result["context"].items()])
            rich_print("[bold blue]Context:[/bold blue]")
            rich_print(context_text)
            
    except Exception as e:
        logger.exception("Error processing request")
        console.print(f"[bold red]Error:[/bold red] {str(e)}")
        if config_manager.config.debug:
            import traceback
            console.print(traceback.format_exc())
        sys.exit(1)


@app.command()
def init():
    """Initialize Angela CLI with configuration."""
    console.print("Initializing Angela CLI...")
    
    # Check if API key is already set
    if config_manager.config.api.gemini_api_key:
        console.print("[green]API key already configured.[/green]")
    else:
        console.print("Google Gemini API key is required for Angela to function.")
        api_key = typer.prompt("Enter your Gemini API key", hide_input=True)
        config_manager.config.api.gemini_api_key = api_key
    
    # Configure safety options
    confirm_all = typer.confirm("Require confirmation for all operations?", default=False)
    config_manager.config.user.confirm_all_actions = confirm_all
    
    # Configure project root
    set_project_root = typer.confirm("Set a default project root?", default=False)
    if set_project_root:
        project_root = typer.prompt("Enter the path to your default project root")
        config_manager.config.user.default_project_root = project_root
    
    # Save the configuration
    config_manager.save_config()
    
    console.print("[green]Configuration saved successfully![/green]")
    console.print("\nAngela CLI is now initialized. You can use the following commands:")
    console.print("  [blue]angela <your request>[/blue] - Process a natural language request")
    console.print("  [blue]angela files <command>[/blue] - Perform file operations")
    console.print("  [blue]angela --help[/blue] - Show help")


@app.command("status")
def show_status():
    """Show the status of Angela CLI features and components."""
    from rich.table import Table
    from angela import __version__
    from angela.constants import APP_NAME, APP_DESCRIPTION
    
    # Display general status
    console.print(Panel(
        f"Angela CLI v{__version__}\n"
        f"{APP_DESCRIPTION}",
        title="Status",
        expand=False
    ))
    
    # Display configuration status
    config = config_manager.config
    config_status = Table(title="Configuration Status")
    config_status.add_column("Setting", style="cyan")
    config_status.add_column("Status", style="green")
    
    # Check API key status
    api_key_status = "[green]Configured[/green]" if config.api.gemini_api_key else "[red]Not configured[/red]"
    config_status.add_row("API Key", api_key_status)
    
    # Check configuration directory
    config_dir_status = "[green]Exists[/green]" if config_manager.CONFIG_DIR.exists() else "[red]Not found[/red]"
    config_status.add_row("Config Directory", config_dir_status)
    
    # Check user preferences
    confirm_actions = "Enabled" if config.user.confirm_all_actions else "Standard risk-based"
    config_status.add_row("Confirmations", confirm_actions)
    
    # Check debug mode
    debug_mode = "Enabled" if config.debug else "Disabled"
    config_status.add_row("Debug Mode", debug_mode)
    
    console.print(config_status)
    
    # Display project information if available
    project_type = context_manager.project_type
    project_root = context_manager.project_root
    
    if project_root:
        # Project detected
        project_info = Table(title="Project Information")
        project_info.add_column("Property", style="cyan")
        project_info.add_column("Value", style="green")
        
        project_info.add_row("Project Type", project_type or "Unknown")
        project_info.add_row("Project Root", str(project_root))
        
        # Count files in project
        try:
            file_count = sum(1 for _ in Path(project_root).glob('**/*') if _.is_file())
            project_info.add_row("Files", str(file_count))
        except Exception:
            project_info.add_row("Files", "Error counting")
        
        # Detect source directories
        try:
            source_dirs = []
            common_src_dirs = ['src', 'lib', 'app', 'angela']
            for dir_name in common_src_dirs:
                if (Path(project_root) / dir_name).is_dir():
                    source_dirs.append(dir_name)
            
            if source_dirs:
                project_info.add_row("Source Directories", ", ".join(source_dirs))
        except Exception:
            pass
        
        console.print(project_info)
    else:
        # No project detected
        console.print("[yellow]No project detected in the current directory.[/yellow]")
    
    # Display available components and services
    try:
        from angela.core.registry import registry
        
        services = registry.list_services()
        if services:
            service_table = Table(title="Registered Services")
            service_table.add_column("Service", style="cyan")
            service_table.add_column("Type", style="green")
            
            # Get a subset of interesting services to display
            interesting_services = {
                k: v for k, v in services.items() 
                if not k.startswith('_') and k not in ('app', 'registry')
            }
            
            # Limit to at most 10 services to avoid cluttering the display
            display_count = min(10, len(interesting_services))
            
            for i, (name, service_type) in enumerate(interesting_services.items()):
                if i >= display_count:
                    service_table.add_row(f"... and {len(interesting_services) - display_count} more", "")
                    break
                service_table.add_row(name, service_type.__name__)
            
            console.print(service_table)
    except Exception as e:
        if config.debug:
            console.print(f"[red]Error getting service information: {str(e)}[/red]")
    
    # Display system information
    console.print("\n[bold]System Information:[/bold]")
    console.print(f"• Current Directory: {context_manager.cwd}")
    if project_root:
        console.print(f"• Project Root: {project_root}")
    
    # Show Python version
    import sys
    console.print(f"• Python Version: {sys.version.split()[0]}")
    
    # Show platform
    import platform
    console.print(f"• Platform: {platform.system()} {platform.release()}")



@app.command("--notify", hidden=True)
def notify(
    notification_type: str = typer.Argument(..., help="Type of notification"),
    args: List[str] = typer.Argument(None, help="Additional arguments")
):
    """
    Handle notifications from shell hooks.
    This is an internal command not meant to be called directly by users.
    """
    # Import here to avoid circular imports
    from angela.monitoring.notification_handler import notification_handler
    
    try:
        # Run the notification handler asynchronously
        asyncio.run(notification_handler.handle_notification(notification_type, *args))
    except Exception as e:
        logger.exception(f"Error handling notification: {str(e)}")
        # Swallow the exception to avoid disrupting the shell
        pass
    
    # Always exit cleanly - we don't want to disrupt the shell
    return

@app.command("--completions", hidden=True)
def completions(
    args: List[str] = typer.Argument(None, help="Current command line arguments")
):
    """
    Generate completions for the angela command.
    This is an internal command used by shell completion.
    """
    try:
        # Import here to avoid circular imports
        from angela.shell.completion import completion_handler
        
        # Get the completions
        result = asyncio.run(completion_handler.get_completions(args))
        
        # Print the completions directly to stdout for shell consumption
        print(" ".join(result))
    except Exception as e:
        logger.exception(f"Error generating completions: {str(e)}")
        # Return empty result on error
        print("")
    
    return



@app.command()
def shell():
    """Launch an interactive shell with Angela."""
    from prompt_toolkit import PromptSession
    from prompt_toolkit.history import FileHistory
    from prompt_toolkit.auto_suggest import AutoSuggestFromHistory
    from prompt_toolkit.completion import WordCompleter
    
    # Create a history file
    history_file = config_manager.CONFIG_DIR / "shell_history.txt"
    history_file.parent.mkdir(parents=True, exist_ok=True)
    
    # Create session with history
    session = PromptSession(
        history=FileHistory(str(history_file)),
        auto_suggest=AutoSuggestFromHistory(),
    )
    
    console.print(Panel(
        "Welcome to Angela's interactive shell!\n"
        "Type your requests directly and press Enter.\n"
        "Type 'exit' or press Ctrl+D to exit.",
        title="Angela Interactive Shell",
        expand=False
    ))
    
    # Main interaction loop
    while True:
        try:
            # Get input from the user
            text = session.prompt("angela> ")
            
            # Check for exit command
            if text.lower() in ("exit", "quit", "bye"):
                break
            
            # Skip empty input
            if not text.strip():
                continue
            
            # Process the request
            result = asyncio.run(orchestrator.process_request(text))
            
            # Display the response
            if "suggestion" in result:
                suggestion = result["suggestion"]
                
                # Show the command suggestion
                console.print("[bold]I suggest:[/bold]")
                command_syntax = Syntax(suggestion.command, "bash", theme="monokai")
                console.print(Panel(command_syntax, title="Command", expand=False))
                
                # Show explanation
                console.print(suggestion.explanation)
                
                # Ask if the user wants to execute the command
                execute_command = typer.confirm("Execute this command?", default=False)
                if execute_command:
                    # Execute the command
                    stdout, stderr, return_code = asyncio.run(
                        execution_engine.execute_command(suggestion.command)
                    )
                    
                    # Display the results
                    if return_code == 0:
                        if stdout:
                            console.print(Panel(stdout, title="Output", expand=False))
                        else:
                            console.print("[green]Command executed successfully with no output.[/green]")
                    else:
                        console.print("[bold red]Command failed:[/bold red]")
                        if stderr:
                            console.print(Panel(stderr, title="Error", expand=False))
            
            else:
                # Fall back to simple response
                console.print(result.get("response", "I couldn't process that request."))
            
            # Add a separator between interactions
            console.print("─" * console.width)
            
        except KeyboardInterrupt:
            # Handle Ctrl+C gracefully
            continue
        except EOFError:
            # Handle Ctrl+D (exit)
            break
        except Exception as e:
            logger.exception("Error in interactive shell")
            console.print(f"[bold red]Error:[/bold red] {str(e)}")
    
    console.print("Goodbye!")
</file>

<file path="components/cli/workflows.py">
# angela/cli/workflows.py
"""
Workflow management commands for Angela CLI.

This module provides CLI commands for creating, running, and managing workflows.
"""
import sys
import asyncio
from pathlib import Path
from typing import List, Optional, Dict, Any

import typer
from rich.console import Console
from rich.table import Table
from rich.panel import Panel
from rich.syntax import Syntax
from rich.prompt import Prompt, Confirm

from angela.api.workflows import get_workflow_manager
from angela.api.context import get_context_manager
from angela.api.shell import get_terminal_formatter
from angela.utils.logging import get_logger

logger = get_logger(__name__)
console = Console()

# Create the workflow commands app
app = typer.Typer(help="Manage Angela workflows")


# Define this at module level to replace await_func
def run_async(coro):
    """Run an async function from a synchronous context.
    
    This function handles getting the appropriate event loop
    rather than creating a new one with asyncio.run(), which causes
    problems when called from an async context.
    """
    try:
        loop = asyncio.get_event_loop()
    except RuntimeError:
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
    return loop.run_until_complete(coro)


@app.command("list")
def list_workflows(
    tag: Optional[str] = typer.Option(
        None, "--tag", "-t", help="Filter workflows by tag"
    ),
    verbose: bool = typer.Option(
        False, "--verbose", "-v", help="Show detailed information"
    ),
):
    """List available workflows."""
    try:
        # Get workflows
        workflow_manager = get_workflow_manager()
        workflows = workflow_manager.list_workflows(tag)
        
        if not workflows:
            if tag:
                console.print(f"No workflows found with tag: {tag}")
            else:
                console.print("No workflows defined yet. Use 'angela workflows create' to define one.")
            return
        
        # Create table for workflows
        table = Table(title="Available Workflows")
        table.add_column("Name", style="cyan")
        table.add_column("Description", style="white")
        table.add_column("Steps", style="magenta", justify="right")
        
        if verbose:
            table.add_column("Tags", style="blue")
            table.add_column("Created", style="green")
        
        # Add workflows to table
        for workflow in workflows:
            if verbose:
                tags = ", ".join(workflow.tags) if workflow.tags else ""
                created = workflow.created.strftime("%Y-%m-%d %H:%M")
                table.add_row(
                    workflow.name,
                    workflow.description,
                    str(len(workflow.steps)),
                    tags,
                    created
                )
            else:
                table.add_row(
                    workflow.name,
                    workflow.description,
                    str(len(workflow.steps))
                )
        
        # Display the table
        console.print(table)
        
    except Exception as e:
        logger.exception(f"Error listing workflows: {str(e)}")
        console.print(f"[bold red]Error:[/bold red] {str(e)}")
        sys.exit(1)


@app.command("export")
def export_workflow(
    name: str = typer.Argument(
        ..., help="Name of the workflow to export"
    ),
    output: Optional[Path] = typer.Option(
        None, "--output", "-o", help="Output path for the exported workflow"
    ),
):
    """Export a workflow to a shareable package."""
    try:
        # Get the workflow sharing manager
        from angela.api.workflows import get_workflow_sharing_manager
        
        # Export the workflow using run_async instead of asyncio.run()
        workflow_sharing_manager = get_workflow_sharing_manager()
        result = run_async(workflow_sharing_manager.export_workflow(
            workflow_name=name,
            output_path=output
        ))
        
        if result["success"]:
            console.print(f"[bold green]Workflow '{name}' exported successfully![/bold green]")
            console.print(f"Output path: {result['output_path']}")
        else:
            console.print(f"[bold red]Error:[/bold red] {result.get('error', 'Unknown error')}")
            sys.exit(1)
        
    except Exception as e:
        logger.exception(f"Error exporting workflow: {str(e)}")
        console.print(f"[bold red]Error:[/bold red] {str(e)}")
        sys.exit(1)

@app.command("import")
def import_workflow(
    path: Path = typer.Argument(
        ..., help="Path to the workflow package to import"
    ),
    rename: Optional[str] = typer.Option(
        None, "--rename", "-r", help="New name for the imported workflow"
    ),
    replace: bool = typer.Option(
        False, "--replace", help="Replace existing workflow with same name"
    ),
):
    """Import a workflow from a package."""
    try:
        # Get the workflow sharing manager
        from angela.workflows.sharing import workflow_sharing_manager
        
        # Import the workflow using run_async instead of asyncio.run()
        result = run_async(workflow_sharing_manager.import_workflow(
            workflow_path=path,
            rename=rename,
            replace_existing=replace
        ))
        
        if result["success"]:
            console.print(f"[bold green]Workflow imported successfully as '{result['workflow']}'![/bold green]")
        else:
            console.print(f"[bold red]Error:[/bold red] {result.get('error', 'Unknown error')}")
            sys.exit(1)
        
    except Exception as e:
        logger.exception(f"Error importing workflow: {str(e)}")
        console.print(f"[bold red]Error:[/bold red] {str(e)}")
        sys.exit(1)


@app.command("create")
def create_workflow(
    name: str = typer.Argument(..., help="Name for the workflow"),
    description: Optional[str] = typer.Option(
        None, "--description", "-d", help="Description of the workflow"
    ),
    from_file: Optional[Path] = typer.Option(
        None, "--file", "-f", help="Load workflow definition from a file"
    ),
):
    """Create a new workflow."""
    try:
        # Get description if not provided
        if not description:
            description = Prompt.ask("Enter workflow description")
        
        steps = []
        
        if from_file:
            # Load workflow definition from file
            if not from_file.exists():
                console.print(f"[bold red]Error:[/bold red] File not found: {from_file}")
                sys.exit(1)
            
            # Read the file
            with open(from_file, "r") as f:
                workflow_text = f.read()
            
            # Get context
            context = context_manager.get_context_dict()
            
            # Define workflow from file content using run_async instead of asyncio.run()
            workflow = run_async(workflow_manager.define_workflow_from_natural_language(
                name=name,
                description=description,
                natural_language=workflow_text,
                context=context
            ))
        else:
            # Interactive workflow creation
            console.print("Enter the steps for your workflow. Each step should be a shell command.")
            console.print("Press [bold cyan]Enter[/bold cyan] on an empty line when finished.")
            
            step_num = 1
            while True:
                command = Prompt.ask(f"Step {step_num} command", default="")
                if not command:
                    break
                
                explanation = Prompt.ask(f"Step {step_num} explanation", default="")
                optional = Confirm.ask(f"Is step {step_num} optional?", default=False)
                requires_confirmation = Confirm.ask(f"Does step {step_num} require confirmation?", default=False)
                
                steps.append({
                    "command": command,
                    "explanation": explanation,
                    "optional": optional,
                    "requires_confirmation": requires_confirmation
                })
                
                step_num += 1
            
            # Need at least one step
            if not steps:
                console.print("[bold red]Error:[/bold red] Workflow must have at least one step.")
                sys.exit(1)
            
            # Define the workflow using run_async instead of asyncio.run()
            workflow = run_async(workflow_manager.define_workflow(
                name=name,
                description=description,
                steps=steps
            ))
        
        # Display the created workflow using run_async instead of await_func
        run_async(terminal_formatter.display_workflow(workflow))
        
        console.print(f"[bold green]Workflow '{name}' created successfully![/bold green]")
        console.print(f"Run it with: [bold cyan]angela workflows run {name}[/bold cyan]")
        
    except Exception as e:
        logger.exception(f"Error creating workflow: {str(e)}")
        console.print(f"[bold red]Error:[/bold red] {str(e)}")
        sys.exit(1)


@app.command("run")
def run_workflow(
    name: str = typer.Argument(..., help="Name of the workflow to run"),
    variable: List[str] = typer.Option(
        [], "--var", "-v", help="Variable value in format NAME=VALUE"
    ),
    dry_run: bool = typer.Option(
        False, "--dry-run", help="Show what would happen without executing"
    ),
):
    """Run a workflow."""
    try:
        # Check if workflow exists
        workflow = workflow_manager.get_workflow(name)
        if not workflow:
            console.print(f"[bold red]Error:[/bold red] Workflow '{name}' not found.")
            
            # Show available workflows
            available = workflow_manager.list_workflows()
            if available:
                console.print("\nAvailable workflows:")
                for w in available:
                    console.print(f"  - {w.name}")
            else:
                console.print("\nNo workflows defined yet. Use 'angela workflows create' to define one.")
                
            sys.exit(1)
        
        # Parse variables
        variables = {}
        for var in variable:
            if "=" in var:
                key, value = var.split("=", 1)
                variables[key] = value
            else:
                console.print(f"[bold yellow]Warning:[/bold yellow] Ignoring invalid variable format: {var}")
        
        # Get context
        context = context_manager.get_context_dict()
        
        # Display the workflow using run_async instead of await_func
        run_async(terminal_formatter.display_workflow(workflow, variables))
        
        # Confirm execution
        if not dry_run:
            if not Confirm.ask("Execute this workflow?", default=True):
                console.print("Workflow execution cancelled.")
                return
        
        # Execute the workflow using run_async instead of asyncio.run()
        result = run_async(workflow_manager.execute_workflow(
            workflow_name=name,
            variables=variables,
            context=context,
            dry_run=dry_run
        ))
        
        # Display results
        if result["success"]:
            status = "[bold green]Workflow executed successfully![/bold green]"
            if dry_run:
                status = "[bold blue]Dry run completed successfully![/bold blue]"
            console.print(status)
        else:
            console.print("[bold red]Workflow execution failed.[/bold red]")
            
            if result.get("error"):
                console.print(f"Error: {result['error']}")
        
    except Exception as e:
        logger.exception(f"Error running workflow: {str(e)}")
        console.print(f"[bold red]Error:[/bold red] {str(e)}")
        sys.exit(1)


@app.command("delete")
def delete_workflow(
    name: str = typer.Argument(..., help="Name of the workflow to delete"),
    force: bool = typer.Option(
        False, "--force", "-f", help="Delete without confirmation"
    ),
):
    """Delete a workflow."""
    try:
        # Check if workflow exists
        workflow = workflow_manager.get_workflow(name)
        if not workflow:
            console.print(f"[bold red]Error:[/bold red] Workflow '{name}' not found.")
            sys.exit(1)
        
        # Confirm deletion
        if not force:
            if not Confirm.ask(f"Are you sure you want to delete workflow '{name}'?", default=False):
                console.print("Deletion cancelled.")
                return
        
        # Delete the workflow
        workflow_manager.delete_workflow(name)
        
        console.print(f"[bold green]Workflow '{name}' deleted successfully.[/bold green]")
        
    except Exception as e:
        logger.exception(f"Error deleting workflow: {str(e)}")
        console.print(f"[bold red]Error:[/bold red] {str(e)}")
        sys.exit(1)


@app.command("show")
def show_workflow(
    name: str = typer.Argument(..., help="Name of the workflow to show"),
    verbose: bool = typer.Option(
        False, "--verbose", "-v", help="Show detailed information"
    ),
):
    """Show details of a workflow."""
    try:
        # Get the workflow
        workflow = workflow_manager.get_workflow(name)
        if not workflow:
            console.print(f"[bold red]Error:[/bold red] Workflow '{name}' not found.")
            sys.exit(1)
        
        # Display the workflow using run_async instead of await_func
        run_async(terminal_formatter.display_workflow(workflow))
        
        # Show additional details if verbose
        if verbose:
            console.print("\n[bold]Additional Details:[/bold]")
            console.print(f"Created: {workflow.created.strftime('%Y-%m-%d %H:%M:%S')}")
            console.print(f"Last Modified: {workflow.modified.strftime('%Y-%m-%d %H:%M:%S')}")
            
            if workflow.tags:
                console.print(f"Tags: {', '.join(workflow.tags)}")
            
            if workflow.author:
                console.print(f"Author: {workflow.author}")
            
            if workflow.variables:
                console.print("\n[bold]Variables:[/bold]")
                for var_name, var_desc in workflow.variables.items():
                    console.print(f"  {var_name}: {var_desc}")
        
    except Exception as e:
        logger.exception(f"Error showing workflow: {str(e)}")
        console.print(f"[bold red]Error:[/bold red] {str(e)}")
        sys.exit(1)
</file>

<file path="components/context/__init__.py">
# angela/context/__init__.py
"""Context management package for Angela CLI.

This package provides context awareness and tracking for files, projects,
sessions, and user preferences to enhance AI operations with relevant
environmental information.
"""

# Define initialization function instead of running at import time
def initialize_project_inference():
    """Initialize project inference for the current project in background."""
    # Forward to the API implementation to ensure consistent behavior
    from angela.api.context import initialize_project_inference as api_initialize
    api_initialize()

# Export the initialization function only
__all__ = ['initialize_project_inference']
</file>

<file path="components/context/enhanced_file_activity.py">
# angela/context/enhanced_file_activity.py
"""
Enhanced file activity tracking for Angela CLI.

This module extends the basic file activity tracking to include fine-grained
tracking of specific code entities (functions, classes, methods) being modified,
providing deeper contextual awareness.
"""
import os
import re
import time
import difflib
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, List, Tuple, Optional, Set, Union
from dataclasses import dataclass
from enum import Enum

from angela.utils.logging import get_logger
from angela.api.context import get_file_activity_tracker, get_activity_type
from angela.api.ai import get_semantic_analyzer, get_module_class, get_function_class, get_class_class

logger = get_logger(__name__)

class EntityType(str, Enum):
    """Types of code entities that can be tracked."""
    FUNCTION = "function"
    METHOD = "method"
    CLASS = "class"
    VARIABLE = "variable"
    IMPORT = "import"
    DOCSTRING = "docstring"
    PARAMETER = "parameter"
    UNKNOWN = "unknown"

@dataclass
class EntityActivity:
    """Represents an activity on a specific code entity."""
    entity_name: str
    entity_type: EntityType
    activity_type: Any  # Will be initialized from get_activity_type()
    file_path: Path
    timestamp: float
    line_start: int
    line_end: int
    details: Dict[str, Any]
    before_content: Optional[str] = None
    after_content: Optional[str] = None

    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for serialization."""
        return {
            "entity_name": self.entity_name,
            "entity_type": self.entity_type,
            "activity_type": self.activity_type,
            "file_path": str(self.file_path),
            "timestamp": self.timestamp,
            "datetime": datetime.fromtimestamp(self.timestamp).isoformat(),
            "line_start": self.line_start,
            "line_end": self.line_end,
            "details": self.details,
            "has_content_diff": self.before_content is not None and self.after_content is not None
        }

class EnhancedFileActivityTracker:
    """
    Enhanced tracker for file activities that includes code entity tracking.
    
    This class extends the basic file activity tracking to include:
    1. Function/method modification tracking
    2. Class structure changes
    3. Import statement changes
    4. Semantic diffs between versions
    """
    
    def __init__(self):
        """Initialize the enhanced file activity tracker."""
        self._logger = logger
        self._entity_activities: List[EntityActivity] = []
        self._max_activities = 100
        self._file_snapshots: Dict[str, Dict[str, Any]] = {}
        
        # Keep track of the last analyzed version of each file
        self._last_analyzed_modules: Dict[str, Any] = {}
        
        # Regular expressions for quick entity detection
        self._function_pattern = re.compile(r'(?:async\s+)?(?:def|function)\s+(\w+)\s*\(')
        self._class_pattern = re.compile(r'class\s+(\w+)\s*(?:\(|:)')
        self._import_pattern = re.compile(r'(?:import|from)\s+(\w+)')
    
    async def track_entity_changes(
        self, 
        file_path: Union[str, Path], 
        new_content: str = None,
        activity_type: Optional[Any] = None,
        details: Dict[str, Any] = None
    ) -> List[EntityActivity]:
        """
        Track changes to specific entities within a file.
        
        Args:
            file_path: Path to the file being modified
            new_content: New content of the file (if None, will read from disk)
            activity_type: Type of activity
            details: Additional details about the activity
            
        Returns:
            List of entity activities detected
        """
        path_obj = Path(file_path)
        
        # Get the ActivityType enum
        ActivityType = get_activity_type()
        
        # Use MODIFIED as default activity type if not provided
        if activity_type is None:
            activity_type = ActivityType.MODIFIED
            
        # Skip if file doesn't exist (or is being created)
        if not path_obj.exists() and activity_type != ActivityType.CREATED:
            return []
        
        # Skip binary files
        if self._is_binary_file(path_obj):
            return []
        
        # Get new content
        if new_content is None:
            try:
                with open(path_obj, 'r', encoding='utf-8', errors='replace') as f:
                    new_content = f.read()
            except Exception as e:
                self._logger.error(f"Error reading file {path_obj}: {str(e)}")
                return []
        
        # Get old content from snapshot or disk
        old_content = self._get_previous_content(path_obj)
        
        # If file is new or we don't have previous content
        if old_content is None:
            # This is a new file or we don't have previous content
            # Analyze it as a whole
            entity_activities = await self._analyze_new_file(path_obj, new_content, activity_type, details or {})
            
            # Update the snapshot
            self._update_file_snapshot(path_obj, new_content)
            
            return entity_activities
        
        # Skip if content hasn't changed
        if old_content == new_content:
            return []
        
        # Detect changed entities
        entity_activities = await self._detect_entity_changes(path_obj, old_content, new_content, details or {})
        
        # Update the snapshot
        self._update_file_snapshot(path_obj, new_content)
        
        return entity_activities
    
    def _is_binary_file(self, file_path: Path) -> bool:
        """Check if a file is binary."""
        try:
            with open(file_path, 'rb') as f:
                chunk = f.read(1024)
                return b'\0' in chunk
        except Exception:
            return False
    
    def _get_previous_content(self, file_path: Path) -> Optional[str]:
        """Get the previous content of a file from snapshot."""
        path_str = str(file_path)
        
        if path_str in self._file_snapshots:
            return self._file_snapshots[path_str].get('content')
        
        return None
    
    def _update_file_snapshot(self, file_path: Path, content: str) -> None:
        """Update the snapshot of a file."""
        path_str = str(file_path)
        
        self._file_snapshots[path_str] = {
            'content': content,
            'timestamp': time.time()
        }
    
    async def _analyze_new_file(
        self,
        file_path: Path,
        content: str,
        activity_type: Any,
        details: Dict[str, Any]
    ) -> List[EntityActivity]:
        """
        Analyze a new file to detect all entities.
        
        Args:
            file_path: Path to the file
            content: Content of the file
            activity_type: Type of activity
            details: Additional details
            
        Returns:
            List of entity activities
        """
        # Use semantic analyzer to extract entities
        try:
            semantic_analyzer = get_semantic_analyzer()
            module = await semantic_analyzer.analyze_file(file_path)
            
            if not module:
                return []
            
            # Store module for later comparisons
            self._last_analyzed_modules[str(file_path)] = module
            
            entity_activities = []
            
            # Track classes
            for class_name, class_obj in module.classes.items():
                entity_activities.append(EntityActivity(
                    entity_name=class_name,
                    entity_type=EntityType.CLASS,
                    activity_type=activity_type,
                    file_path=file_path,
                    timestamp=time.time(),
                    line_start=class_obj.line_start,
                    line_end=class_obj.line_end,
                    details={
                        **details,
                        'methods_count': len(class_obj.methods),
                        'attributes_count': len(class_obj.attributes),
                        'base_classes': class_obj.base_classes
                    },
                    before_content=None,
                    after_content=self._extract_entity_content(content, class_obj.line_start, class_obj.line_end)
                ))
                
                # Track methods
                for method_name, method_obj in class_obj.methods.items():
                    entity_activities.append(EntityActivity(
                        entity_name=f"{class_name}.{method_name}",
                        entity_type=EntityType.METHOD,
                        activity_type=activity_type,
                        file_path=file_path,
                        timestamp=time.time(),
                        line_start=method_obj.line_start,
                        line_end=method_obj.line_end,
                        details={
                            **details,
                            'params': method_obj.params,
                            'class_name': class_name,
                            'return_type': method_obj.return_type
                        },
                        before_content=None,
                        after_content=self._extract_entity_content(content, method_obj.line_start, method_obj.line_end)
                    ))
            
            # Track functions
            for func_name, func_obj in module.functions.items():
                entity_activities.append(EntityActivity(
                    entity_name=func_name,
                    entity_type=EntityType.FUNCTION,
                    activity_type=activity_type,
                    file_path=file_path,
                    timestamp=time.time(),
                    line_start=func_obj.line_start,
                    line_end=func_obj.line_end,
                    details={
                        **details,
                        'params': func_obj.params,
                        'return_type': func_obj.return_type
                    },
                    before_content=None,
                    after_content=self._extract_entity_content(content, func_obj.line_start, func_obj.line_end)
                ))
            
            # Track imports
            for import_name, import_obj in module.imports.items():
                entity_activities.append(EntityActivity(
                    entity_name=import_name,
                    entity_type=EntityType.IMPORT,
                    activity_type=activity_type,
                    file_path=file_path,
                    timestamp=time.time(),
                    line_start=import_obj.line_start,
                    line_end=import_obj.line_end,
                    details={
                        **details,
                        'import_path': import_obj.import_path,
                        'is_from': import_obj.is_from
                    },
                    before_content=None,
                    after_content=self._extract_entity_content(content, import_obj.line_start, import_obj.line_end)
                ))
            
            # Store entity activities
            self._store_entity_activities(entity_activities)
            
            return entity_activities
            
        except Exception as e:
            self._logger.error(f"Error analyzing new file {file_path}: {str(e)}")
            return []
    
    async def _detect_entity_changes(
        self,
        file_path: Path,
        old_content: str,
        new_content: str,
        details: Dict[str, Any]
    ) -> List[EntityActivity]:
        """
        Detect changes to specific entities between file versions.
        
        Args:
            file_path: Path to the file
            old_content: Previous content of the file
            new_content: New content of the file
            details: Additional details
            
        Returns:
            List of entity activities
        """
        # Get the ActivityType enum
        ActivityType = get_activity_type()
        
        entity_activities = []
        
        # Use semantic analyzer to extract entities from both versions
        try:
            semantic_analyzer = get_semantic_analyzer()
            
            # Create a temporary file for the old content
            old_temp_path = file_path.with_suffix(f"{file_path.suffix}.old")
            with open(old_temp_path, 'w', encoding='utf-8') as f:
                f.write(old_content)
            
            # Analyze old and new versions
            old_module = await semantic_analyzer.analyze_file(old_temp_path)
            
            # Remove temporary file
            os.unlink(old_temp_path)
            
            # If we already have the old module analyzed, use it
            path_str = str(file_path)
            if path_str in self._last_analyzed_modules:
                old_module = self._last_analyzed_modules[path_str]
            
            # Analyze new version
            new_module = await semantic_analyzer.analyze_file(file_path)
            
            if not old_module or not new_module:
                # Fall back to diff-based detection
                return await self._detect_changes_by_diff(file_path, old_content, new_content, details)
            
            # Store new module for later comparisons
            self._last_analyzed_modules[path_str] = new_module
            
            # Compare classes
            entity_activities.extend(self._compare_classes(
                file_path, old_module, new_module, old_content, new_content, details
            ))
            
            # Compare standalone functions
            entity_activities.extend(self._compare_functions(
                file_path, old_module, new_module, old_content, new_content, details
            ))
            
            # Compare imports
            entity_activities.extend(self._compare_imports(
                file_path, old_module, new_module, old_content, new_content, details
            ))
            
            # Compare docstring
            if old_module.docstring != new_module.docstring:
                entity_activities.append(EntityActivity(
                    entity_name="module_docstring",
                    entity_type=EntityType.DOCSTRING,
                    activity_type=ActivityType.MODIFIED,
                    file_path=file_path,
                    timestamp=time.time(),
                    line_start=1,
                    line_end=1 + (new_module.docstring or "").count('\n'),
                    details=details,
                    before_content=old_module.docstring,
                    after_content=new_module.docstring
                ))
            
            # Fall back to diff-based detection if no entities were detected
            if not entity_activities:
                entity_activities = await self._detect_changes_by_diff(file_path, old_content, new_content, details)
            
            # Store entity activities
            self._store_entity_activities(entity_activities)
            
            return entity_activities
            
        except Exception as e:
            self._logger.error(f"Error analyzing entity changes in {file_path}: {str(e)}")
            
            # Fall back to diff-based detection
            return await self._detect_changes_by_diff(file_path, old_content, new_content, details)
    
    def _compare_classes(
        self,
        file_path: Path,
        old_module: Any,
        new_module: Any,
        old_content: str,
        new_content: str,
        details: Dict[str, Any]
    ) -> List[EntityActivity]:
        """
        Compare classes between module versions.
        
        Args:
            file_path: Path to the file
            old_module: Previous module
            new_module: New module
            old_content: Previous content
            new_content: New content
            details: Additional details
            
        Returns:
            List of entity activities
        """
        # Get the ActivityType enum
        ActivityType = get_activity_type()
        
        entity_activities = []
        
        # Check for added classes
        for class_name, new_class in new_module.classes.items():
            if class_name not in old_module.classes:
                # Class was added
                entity_activities.append(EntityActivity(
                    entity_name=class_name,
                    entity_type=EntityType.CLASS,
                    activity_type=ActivityType.CREATED,
                    file_path=file_path,
                    timestamp=time.time(),
                    line_start=new_class.line_start,
                    line_end=new_class.line_end,
                    details={
                        **details,
                        'methods_count': len(new_class.methods),
                        'attributes_count': len(new_class.attributes),
                        'base_classes': new_class.base_classes
                    },
                    before_content=None,
                    after_content=self._extract_entity_content(new_content, new_class.line_start, new_class.line_end)
                ))
                continue
            
            # Class exists in both - check for changes
            old_class = old_module.classes[class_name]
            
            # Check if class definition changed
            class_changed = (
                old_class.base_classes != new_class.base_classes or
                old_class.docstring != new_class.docstring or
                old_class.decorators != new_class.decorators
            )
            
            if class_changed:
                entity_activities.append(EntityActivity(
                    entity_name=class_name,
                    entity_type=EntityType.CLASS,
                    activity_type=ActivityType.MODIFIED,
                    file_path=file_path,
                    timestamp=time.time(),
                    line_start=new_class.line_start,
                    line_end=new_class.line_end,
                    details={
                        **details,
                        'methods_count': len(new_class.methods),
                        'attributes_count': len(new_class.attributes),
                        'old_base_classes': old_class.base_classes,
                        'new_base_classes': new_class.base_classes
                    },
                    before_content=self._extract_entity_content(old_content, old_class.line_start, old_class.line_end),
                    after_content=self._extract_entity_content(new_content, new_class.line_start, new_class.line_end)
                ))
            
            # Check for added/modified methods
            for method_name, new_method in new_class.methods.items():
                if method_name not in old_class.methods:
                    # Method was added
                    entity_activities.append(EntityActivity(
                        entity_name=f"{class_name}.{method_name}",
                        entity_type=EntityType.METHOD,
                        activity_type=ActivityType.CREATED,
                        file_path=file_path,
                        timestamp=time.time(),
                        line_start=new_method.line_start,
                        line_end=new_method.line_end,
                        details={
                            **details,
                            'params': new_method.params,
                            'class_name': class_name,
                            'return_type': new_method.return_type
                        },
                        before_content=None,
                        after_content=self._extract_entity_content(new_content, new_method.line_start, new_method.line_end)
                    ))
                    continue
                
                # Method exists in both - check for changes
                old_method = old_class.methods[method_name]
                method_changed = (
                    old_method.params != new_method.params or
                    old_method.return_type != new_method.return_type or
                    old_method.docstring != new_method.docstring or
                    self._extract_entity_content(old_content, old_method.line_start, old_method.line_end) !=
                    self._extract_entity_content(new_content, new_method.line_start, new_method.line_end)
                )
                
                if method_changed:
                    entity_activities.append(EntityActivity(
                        entity_name=f"{class_name}.{method_name}",
                        entity_type=EntityType.METHOD,
                        activity_type=ActivityType.MODIFIED,
                        file_path=file_path,
                        timestamp=time.time(),
                        line_start=new_method.line_start,
                        line_end=new_method.line_end,
                        details={
                            **details,
                            'old_params': old_method.params,
                            'new_params': new_method.params,
                            'class_name': class_name,
                            'old_return_type': old_method.return_type,
                            'new_return_type': new_method.return_type
                        },
                        before_content=self._extract_entity_content(old_content, old_method.line_start, old_method.line_end),
                        after_content=self._extract_entity_content(new_content, new_method.line_start, new_method.line_end)
                    ))
            
            # Check for removed methods
            for method_name in old_class.methods:
                if method_name not in new_class.methods:
                    # Method was removed
                    old_method = old_class.methods[method_name]
                    entity_activities.append(EntityActivity(
                        entity_name=f"{class_name}.{method_name}",
                        entity_type=EntityType.METHOD,
                        activity_type=ActivityType.DELETED,
                        file_path=file_path,
                        timestamp=time.time(),
                        line_start=old_method.line_start,
                        line_end=old_method.line_end,
                        details={
                            **details,
                            'params': old_method.params,
                            'class_name': class_name,
                            'return_type': old_method.return_type
                        },
                        before_content=self._extract_entity_content(old_content, old_method.line_start, old_method.line_end),
                        after_content=None
                    ))
        
        # Check for removed classes
        for class_name, old_class in old_module.classes.items():
            if class_name not in new_module.classes:
                # Class was removed
                entity_activities.append(EntityActivity(
                    entity_name=class_name,
                    entity_type=EntityType.CLASS,
                    activity_type=ActivityType.DELETED,
                    file_path=file_path,
                    timestamp=time.time(),
                    line_start=old_class.line_start,
                    line_end=old_class.line_end,
                    details={
                        **details,
                        'methods_count': len(old_class.methods),
                        'attributes_count': len(old_class.attributes),
                        'base_classes': old_class.base_classes
                    },
                    before_content=self._extract_entity_content(old_content, old_class.line_start, old_class.line_end),
                    after_content=None
                ))
        
        return entity_activities
    
    def _compare_functions(
        self,
        file_path: Path,
        old_module: Any,
        new_module: Any,
        old_content: str,
        new_content: str,
        details: Dict[str, Any]
    ) -> List[EntityActivity]:
        """
        Compare standalone functions between module versions.
        
        Args:
            file_path: Path to the file
            old_module: Previous module
            new_module: New module
            old_content: Previous content
            new_content: New content
            details: Additional details
            
        Returns:
            List of entity activities
        """
        # Get the ActivityType enum
        ActivityType = get_activity_type()
        
        entity_activities = []
        
        # Check for added functions
        for func_name, new_func in new_module.functions.items():
            if func_name not in old_module.functions:
                # Function was added
                entity_activities.append(EntityActivity(
                    entity_name=func_name,
                    entity_type=EntityType.FUNCTION,
                    activity_type=ActivityType.CREATED,
                    file_path=file_path,
                    timestamp=time.time(),
                    line_start=new_func.line_start,
                    line_end=new_func.line_end,
                    details={
                        **details,
                        'params': new_func.params,
                        'return_type': new_func.return_type
                    },
                    before_content=None,
                    after_content=self._extract_entity_content(new_content, new_func.line_start, new_func.line_end)
                ))
                continue
            
            # Function exists in both - check for changes
            old_func = old_module.functions[func_name]
            func_changed = (
                old_func.params != new_func.params or
                old_func.return_type != new_func.return_type or
                old_func.docstring != new_func.docstring or
                self._extract_entity_content(old_content, old_func.line_start, old_func.line_end) !=
                self._extract_entity_content(new_content, new_func.line_start, new_func.line_end)
            )
            
            if func_changed:
                entity_activities.append(EntityActivity(
                    entity_name=func_name,
                    entity_type=EntityType.FUNCTION,
                    activity_type=ActivityType.MODIFIED,
                    file_path=file_path,
                    timestamp=time.time(),
                    line_start=new_func.line_start,
                    line_end=new_func.line_end,
                    details={
                        **details,
                        'old_params': old_func.params,
                        'new_params': new_func.params,
                        'old_return_type': old_func.return_type,
                        'new_return_type': new_func.return_type
                    },
                    before_content=self._extract_entity_content(old_content, old_func.line_start, old_func.line_end),
                    after_content=self._extract_entity_content(new_content, new_func.line_start, new_func.line_end)
                ))
        
        # Check for removed functions
        for func_name, old_func in old_module.functions.items():
            if func_name not in new_module.functions:
                # Function was removed
                entity_activities.append(EntityActivity(
                    entity_name=func_name,
                    entity_type=EntityType.FUNCTION,
                    activity_type=ActivityType.DELETED,
                    file_path=file_path,
                    timestamp=time.time(),
                    line_start=old_func.line_start,
                    line_end=old_func.line_end,
                    details={
                        **details,
                        'params': old_func.params,
                        'return_type': old_func.return_type
                    },
                    before_content=self._extract_entity_content(old_content, old_func.line_start, old_func.line_end),
                    after_content=None
                ))
        
        return entity_activities
    
    def _compare_imports(
        self,
        file_path: Path,
        old_module: Any,
        new_module: Any,
        old_content: str,
        new_content: str,
        details: Dict[str, Any]
    ) -> List[EntityActivity]:
        """
        Compare imports between module versions.
        
        Args:
            file_path: Path to the file
            old_module: Previous module
            new_module: New module
            old_content: Previous content
            new_content: New content
            details: Additional details
            
        Returns:
            List of entity activities
        """
        # Get the ActivityType enum
        ActivityType = get_activity_type()
        
        entity_activities = []
        
        # Check for added imports
        for import_name, new_import in new_module.imports.items():
            if import_name not in old_module.imports:
                # Import was added
                entity_activities.append(EntityActivity(
                    entity_name=import_name,
                    entity_type=EntityType.IMPORT,
                    activity_type=ActivityType.CREATED,
                    file_path=file_path,
                    timestamp=time.time(),
                    line_start=new_import.line_start,
                    line_end=new_import.line_end,
                    details={
                        **details,
                        'import_path': new_import.import_path,
                        'is_from': new_import.is_from
                    },
                    before_content=None,
                    after_content=self._extract_entity_content(new_content, new_import.line_start, new_import.line_end)
                ))
                continue
            
            # Import exists in both - check for changes
            old_import = old_module.imports[import_name]
            import_changed = (
                old_import.import_path != new_import.import_path or
                old_import.is_from != new_import.is_from or
                old_import.alias != new_import.alias
            )
            
            if import_changed:
                entity_activities.append(EntityActivity(
                    entity_name=import_name,
                    entity_type=EntityType.IMPORT,
                    activity_type=ActivityType.MODIFIED,
                    file_path=file_path,
                    timestamp=time.time(),
                    line_start=new_import.line_start,
                    line_end=new_import.line_end,
                    details={
                        **details,
                        'old_import_path': old_import.import_path,
                        'new_import_path': new_import.import_path,
                        'old_is_from': old_import.is_from,
                        'new_is_from': new_import.is_from
                    },
                    before_content=self._extract_entity_content(old_content, old_import.line_start, old_import.line_end),
                    after_content=self._extract_entity_content(new_content, new_import.line_start, new_import.line_end)
                ))
        
        # Check for removed imports
        for import_name, old_import in old_module.imports.items():
            if import_name not in new_module.imports:
                # Import was removed
                entity_activities.append(EntityActivity(
                    entity_name=import_name,
                    entity_type=EntityType.IMPORT,
                    activity_type=ActivityType.DELETED,
                    file_path=file_path,
                    timestamp=time.time(),
                    line_start=old_import.line_start,
                    line_end=old_import.line_end,
                    details={
                        **details,
                        'import_path': old_import.import_path,
                        'is_from': old_import.is_from
                    },
                    before_content=self._extract_entity_content(old_content, old_import.line_start, old_import.line_end),
                    after_content=None
                ))
        
        return entity_activities
    
    async def _detect_changes_by_diff(
        self,
        file_path: Path,
        old_content: str,
        new_content: str,
        details: Dict[str, Any]
    ) -> List[EntityActivity]:
        """
        Use diff to detect changes when semantic analysis fails.
        
        Args:
            file_path: Path to the file
            old_content: Previous content
            new_content: New content
            details: Additional details
            
        Returns:
            List of entity activities
        """
        # Get the ActivityType enum
        ActivityType = get_activity_type()
        
        if not old_content or not new_content:
            return []
        
        entity_activities = []
        
        # Generate diff
        diff = list(difflib.unified_diff(
            old_content.splitlines(),
            new_content.splitlines(),
            n=3  # Context lines
        ))
        
        if not diff:
            return []
        
        # Extract chunks of changes
        current_chunk = []
        chunks = []
        
        for line in diff:
            if line.startswith('@@'):
                if current_chunk:
                    chunks.append(current_chunk)
                current_chunk = [line]
            elif current_chunk:
                current_chunk.append(line)
        
        if current_chunk:
            chunks.append(current_chunk)
        
        # Process each chunk to detect entity changes
        for chunk in chunks:
            if not chunk:
                continue
            
            # Parse chunk header to get line numbers
            header = chunk[0]
            match = re.search(r'@@ -(\d+),(\d+) \+(\d+),(\d+) @@', header)
            if not match:
                continue
            
            old_start, old_count = int(match.group(1)), int(match.group(2))
            new_start, new_count = int(match.group(3)), int(match.group(4))
            
            # Get the chunk content
            chunk_content = chunk[1:]
            
            # Try to identify entities in this chunk
            entity_activity = self._identify_entity_in_diff_chunk(
                file_path, old_content, new_content, old_start, old_count, new_start, new_count, chunk_content, details
            )
            
            if entity_activity:
                entity_activities.append(entity_activity)
            else:
                # Fall back to a generic line range activity
                entity_activities.append(EntityActivity(
                    entity_name=f"line_range_{new_start}_{new_start + new_count}",
                    entity_type=EntityType.UNKNOWN,
                    activity_type=ActivityType.MODIFIED,
                    file_path=file_path,
                    timestamp=time.time(),
                    line_start=new_start,
                    line_end=new_start + new_count,
                    details=details,
                    before_content="\n".join(old_content.splitlines()[old_start-1:old_start+old_count-1]),
                    after_content="\n".join(new_content.splitlines()[new_start-1:new_start+new_count-1])
                ))
        
        # Store entity activities
        self._store_entity_activities(entity_activities)
        
        return entity_activities
    
    def _identify_entity_in_diff_chunk(
        self,
        file_path: Path,
        old_content: str,
        new_content: str,
        old_start: int,
        old_count: int,
        new_start: int,
        new_count: int,
        chunk_content: List[str],
        details: Dict[str, Any]
    ) -> Optional[EntityActivity]:
        """
        Try to identify what entity was modified in a diff chunk.
        
        Args:
            file_path: Path to the file
            old_content: Previous content
            new_content: New content
            old_start: Starting line in old content (1-based)
            old_count: Number of lines in old content
            new_start: Starting line in new content (1-based)
            new_count: Number of lines in new content
            chunk_content: Content of the diff chunk
            details: Additional details
            
        Returns:
            EntityActivity if identified, None otherwise
        """
        # Get the ActivityType enum
        ActivityType = get_activity_type()
        
        # Get the surrounding context from old and new content
        old_context_start = max(1, old_start - 10)
        old_context_end = min(len(old_content.splitlines()), old_start + old_count + 10)
        old_context = old_content.splitlines()[old_context_start-1:old_context_end]
        
        new_context_start = max(1, new_start - 10)
        new_context_end = min(len(new_content.splitlines()), new_start + new_count + 10)
        new_context = new_content.splitlines()[new_context_start-1:new_context_end]
        
        # Check for function definitions
        for i, line in enumerate(old_context):
            match = self._function_pattern.search(line)
            if match:
                func_start = old_context_start + i
                if old_start <= func_start <= old_start + old_count:
                    # Function was modified or deleted
                    func_name = match.group(1)
                    
                    # Look for the same function in new content
                    found_in_new = False
                    new_func_start = 0
                    
                    for j, new_line in enumerate(new_context):
                        new_match = self._function_pattern.search(new_line)
                        if new_match and new_match.group(1) == func_name:
                            found_in_new = True
                            new_func_start = new_context_start + j
                            break
                    
                    activity_type = ActivityType.MODIFIED if found_in_new else ActivityType.DELETED
                    
                    # Estimate function end
                    old_func_end = self._estimate_entity_end(old_content, func_start)
                    new_func_end = self._estimate_entity_end(new_content, new_func_start) if found_in_new else 0
                    
                    return EntityActivity(
                        entity_name=func_name,
                        entity_type=EntityType.FUNCTION,
                        activity_type=activity_type,
                        file_path=file_path,
                        timestamp=time.time(),
                        line_start=new_func_start if found_in_new else func_start,
                        line_end=new_func_end if found_in_new else old_func_end,
                        details=details,
                        before_content=self._extract_entity_content(old_content, func_start, old_func_end),
                        after_content=self._extract_entity_content(new_content, new_func_start, new_func_end) if found_in_new else None
                    )
        
        # Check for new functions in the new content
        for i, line in enumerate(new_context):
            match = self._function_pattern.search(line)
            if match:
                func_start = new_context_start + i
                if new_start <= func_start <= new_start + new_count:
                    # New function was added
                    func_name = match.group(1)
                    
                    # Check if this function exists in old content
                    found_in_old = False
                    for old_line in old_context:
                        old_match = self._function_pattern.search(old_line)
                        if old_match and old_match.group(1) == func_name:
                            found_in_old = True
                            break
                    
                    if not found_in_old:
                        # Estimate function end
                        func_end = self._estimate_entity_end(new_content, func_start)
                        
                        return EntityActivity(
                            entity_name=func_name,
                            entity_type=EntityType.FUNCTION,
                            activity_type=ActivityType.CREATED,
                            file_path=file_path,
                            timestamp=time.time(),
                            line_start=func_start,
                            line_end=func_end,
                            details=details,
                            before_content=None,
                            after_content=self._extract_entity_content(new_content, func_start, func_end)
                        )
        
        # Check for class definitions
        for i, line in enumerate(old_context):
            match = self._class_pattern.search(line)
            if match:
                class_start = old_context_start + i
                if old_start <= class_start <= old_start + old_count:
                    # Class was modified or deleted
                    class_name = match.group(1)
                    
                    # Look for the same class in new content
                    found_in_new = False
                    new_class_start = 0
                    
                    for j, new_line in enumerate(new_context):
                        new_match = self._class_pattern.search(new_line)
                        if new_match and new_match.group(1) == class_name:
                            found_in_new = True
                            new_class_start = new_context_start + j
                            break
                    
                    activity_type = ActivityType.MODIFIED if found_in_new else ActivityType.DELETED
                    
                    # Estimate class end
                    old_class_end = self._estimate_entity_end(old_content, class_start)
                    new_class_end = self._estimate_entity_end(new_content, new_class_start) if found_in_new else 0
                    
                    return EntityActivity(
                        entity_name=class_name,
                        entity_type=EntityType.CLASS,
                        activity_type=activity_type,
                        file_path=file_path,
                        timestamp=time.time(),
                        line_start=new_class_start if found_in_new else class_start,
                        line_end=new_class_end if found_in_new else old_class_end,
                        details=details,
                        before_content=self._extract_entity_content(old_content, class_start, old_class_end),
                        after_content=self._extract_entity_content(new_content, new_class_start, new_class_end) if found_in_new else None
                    )
        
        # Check for new classes in the new content
        for i, line in enumerate(new_context):
            match = self._class_pattern.search(line)
            if match:
                class_start = new_context_start + i
                if new_start <= class_start <= new_start + new_count:
                    # New class was added
                    class_name = match.group(1)
                    
                    # Check if this class exists in old content
                    found_in_old = False
                    for old_line in old_context:
                        old_match = self._class_pattern.search(old_line)
                        if old_match and old_match.group(1) == class_name:
                            found_in_old = True
                            break
                    
                    if not found_in_old:
                        # Estimate class end
                        class_end = self._estimate_entity_end(new_content, class_start)
                        
                        return EntityActivity(
                            entity_name=class_name,
                            entity_type=EntityType.CLASS,
                            activity_type=ActivityType.CREATED,
                            file_path=file_path,
                            timestamp=time.time(),
                            line_start=class_start,
                            line_end=class_end,
                            details=details,
                            before_content=None,
                            after_content=self._extract_entity_content(new_content, class_start, class_end)
                        )
        
        # Check for import statements
        for i, line in enumerate(old_context):
            match = self._import_pattern.search(line)
            if match:
                import_start = old_context_start + i
                if old_start <= import_start <= old_start + old_count:
                    # Import was modified or deleted
                    import_name = match.group(1)
                    
                    # Look for the same import in new content
                    found_in_new = False
                    new_import_start = 0
                    
                    for j, new_line in enumerate(new_context):
                        new_match = self._import_pattern.search(new_line)
                        if new_match and new_match.group(1) == import_name:
                            found_in_new = True
                            new_import_start = new_context_start + j
                            break
                    
                    activity_type = ActivityType.MODIFIED if found_in_new else ActivityType.DELETED
                    
                    return EntityActivity(
                        entity_name=import_name,
                        entity_type=EntityType.IMPORT,
                        activity_type=activity_type,
                        file_path=file_path,
                        timestamp=time.time(),
                        line_start=new_import_start if found_in_new else import_start,
                        line_end=new_import_start if found_in_new else import_start,
                        details=details,
                        before_content=line.strip(),
                        after_content=new_context[new_import_start - new_context_start].strip() if found_in_new else None
                    )
        
        # Check for new imports in the new content
        for i, line in enumerate(new_context):
            match = self._import_pattern.search(line)
            if match:
                import_start = new_context_start + i
                if new_start <= import_start <= new_start + new_count:
                    # New import was added
                    import_name = match.group(1)
                    
                    # Check if this import exists in old content
                    found_in_old = False
                    for old_line in old_context:
                        old_match = self._import_pattern.search(old_line)
                        if old_match and old_match.group(1) == import_name:
                            found_in_old = True
                            break
                    
                    if not found_in_old:
                        return EntityActivity(
                            entity_name=import_name,
                            entity_type=EntityType.IMPORT,
                            activity_type=ActivityType.CREATED,
                            file_path=file_path,
                            timestamp=time.time(),
                            line_start=import_start,
                            line_end=import_start,
                            details=details,
                            before_content=None,
                            after_content=line.strip()
                        )
        
        return None
    
    def _extract_entity_content(self, content: str, start_line: int, end_line: int) -> str:
        """
        Extract entity content from a file.
        
        Args:
            content: File content
            start_line: Starting line number (1-based)
            end_line: Ending line number (1-based)
            
        Returns:
            Extracted content
        """
        if not content:
            return ""
        
        lines = content.splitlines()
        
        # Adjust line numbers to be within bounds
        start_idx = max(0, start_line - 1)
        end_idx = min(len(lines), end_line)
        
        return "\n".join(lines[start_idx:end_idx])
    
    def _estimate_entity_end(self, content: str, start_line: int) -> int:
        """
        Estimate the ending line number of an entity.
        
        Args:
            content: File content
            start_line: Starting line number (1-based)
            
        Returns:
            Estimated ending line number (1-based)
        """
        lines = content.splitlines()
        
        # Adjust line number to be within bounds
        start_idx = max(0, start_line - 1)
        
        if start_idx >= len(lines):
            return start_line
        
        # Count indentation of the entity definition line
        first_line = lines[start_idx]
        indent_match = re.match(r'^(\s*)', first_line)
        base_indent = len(indent_match.group(1)) if indent_match else 0
        
        # Find the first line with the same or less indentation
        for i in range(start_idx + 1, len(lines)):
            line = lines[i]
            
            # Skip empty lines
            if not line.strip():
                continue
            
            # Check indentation
            indent_match = re.match(r'^(\s*)', line)
            indent = len(indent_match.group(1)) if indent_match else 0
            
            if indent <= base_indent:
                return i + 1  # 1-based line number
        
        # If we reach the end of the file, return the last line number
        return len(lines)
    
    def _store_entity_activities(self, entity_activities: List[EntityActivity]) -> None:
        """
        Store entity activities and log them to the basic file activity tracker.
        
        Args:
            entity_activities: List of entity activities to store
        """
        # Add to the entity activities list
        self._entity_activities.extend(entity_activities)
        
        # Trim if needed
        if len(self._entity_activities) > self._max_activities:
            self._entity_activities = self._entity_activities[-self._max_activities:]
        
        # Log to the basic file activity tracker
        file_activity_tracker = get_file_activity_tracker()
        ActivityType = get_activity_type()
        
        for activity in entity_activities:
            entity_str = f"{activity.entity_type.value}:{activity.entity_name}"
            
            # Track in the basic file activity tracker
            file_activity_tracker.track_activity(
                path=activity.file_path,
                activity_type=activity.activity_type,
                details={
                    "entity_name": activity.entity_name,
                    "entity_type": activity.entity_type.value,
                    "line_start": activity.line_start,
                    "line_end": activity.line_end,
                    **activity.details
                }
            )
            
            # Log the activity
            self._logger.debug(
                f"Tracked {activity.activity_type.value} of {entity_str} "
                f"in {activity.file_path.name}:{activity.line_start}-{activity.line_end}"
            )
    
    def get_recent_entity_activities(
        self,
        limit: int = 10,
        entity_types: Optional[List[EntityType]] = None,
        activity_types: Optional[List[Any]] = None
    ) -> List[Dict[str, Any]]:
        """
        Get recent entity activities.
        
        Args:
            limit: Maximum number of activities to return
            entity_types: Optional filter for entity types
            activity_types: Optional filter for activity types
            
        Returns:
            List of entity activities as dictionaries
        """
        # Apply filters
        filtered = self._entity_activities
        
        if entity_types:
            filtered = [a for a in filtered if a.entity_type in entity_types]
        
        if activity_types:
            filtered = [a for a in filtered if a.activity_type in activity_types]
        
        # Sort by timestamp (newest first)
        sorted_activities = sorted(filtered, key=lambda a: a.timestamp, reverse=True)
        
        # Convert to dictionaries
        return [a.to_dict() for a in sorted_activities[:limit]]
    
    def get_entity_activities_by_name(
        self,
        entity_name: str,
        limit: int = 10
    ) -> List[Dict[str, Any]]:
        """
        Get activities for a specific entity.
        
        Args:
            entity_name: Name of the entity
            limit: Maximum number of activities to return
            
        Returns:
            List of entity activities as dictionaries
        """
        # Filter by entity name
        filtered = [a for a in self._entity_activities if a.entity_name == entity_name]
        
        # Sort by timestamp (newest first)
        sorted_activities = sorted(filtered, key=lambda a: a.timestamp, reverse=True)
        
        # Convert to dictionaries
        return [a.to_dict() for a in sorted_activities[:limit]]
    
    def get_most_active_entities(self, limit: int = 5) -> List[Dict[str, Any]]:
        """
        Get the most actively modified entities.
        
        Args:
            limit: Maximum number of entities to return
            
        Returns:
            List of entities with activity counts
        """
        # Count activities by entity
        entity_counts = {}
        
        for activity in self._entity_activities:
            key = f"{activity.entity_type.value}:{activity.entity_name}"
            
            if key not in entity_counts:
                entity_counts[key] = {
                    "entity_name": activity.entity_name,
                    "entity_type": activity.entity_type.value,
                    "count": 0,
                    "last_activity": None,
                    "activity_types": set()
                }
            
            entity_counts[key]["count"] += 1
            entity_counts[key]["activity_types"].add(activity.activity_type.value)
            
            # Update last activity if newer
            if entity_counts[key]["last_activity"] is None or activity.timestamp > entity_counts[key]["last_activity"]:
                entity_counts[key]["last_activity"] = activity.timestamp
                entity_counts[key]["file_path"] = str(activity.file_path)
                entity_counts[key]["line_start"] = activity.line_start
                entity_counts[key]["line_end"] = activity.line_end
        
        # Convert to list and sort by count
        result = []
        for key, entity_info in entity_counts.items():
            # Convert activity types set to list
            entity_info["activity_types"] = list(entity_info["activity_types"])
            
            # Add formatted timestamp
            if entity_info["last_activity"]:
                entity_info["last_activity_time"] = datetime.fromtimestamp(entity_info["last_activity"]).isoformat()
            
            result.append(entity_info)
        
        result.sort(key=lambda x: x["count"], reverse=True)
        
        return result[:limit]
    
    def clear_activities(self) -> None:
        """Clear all tracked activities."""
        self._entity_activities.clear()
        self._file_snapshots.clear()
        self._last_analyzed_modules.clear()
        self._logger.debug("Cleared all entity activities and snapshots")
    
    def get_entity_history(self, entity_name: str, entity_type: Optional[EntityType] = None) -> List[Dict[str, Any]]:
        """
        Get the complete history of changes for a specific entity.
        
        Args:
            entity_name: Name of the entity
            entity_type: Optional entity type filter
            
        Returns:
            List of activities for the entity, ordered by time
        """
        # Filter activities
        filtered = [a for a in self._entity_activities if a.entity_name == entity_name]
        
        if entity_type:
            filtered = [a for a in filtered if a.entity_type == entity_type]
        
        # Sort by timestamp (oldest first for history)
        sorted_activities = sorted(filtered, key=lambda a: a.timestamp)
        
        # Convert to dictionaries
        return [a.to_dict() for a in sorted_activities]

# Global enhanced file activity tracker instance
enhanced_file_activity_tracker = EnhancedFileActivityTracker()
</file>

<file path="components/context/enhancer.py">
# angela/context/enhancer.py
"""
Enhanced project context management for Angela CLI.

This module provides advanced context enrichment by integrating project inference,
dependency detection, and file activity tracking to provide a richer context
for AI interactions.
"""
import asyncio
from typing import Dict, Any, Optional, List, Set, Callable, Awaitable
from pathlib import Path

# Non-circular imports can remain at the top level
from angela.config import config_manager
from angela.utils.logging import get_logger
from angela.api.context import get_project_inference, get_file_activity_tracker, get_file_resolver

logger = get_logger(__name__)

class ContextEnhancer:
    """
    Enhanced context manager that enriches context with project information,
    dependency detection, and recent activity tracking.
    """
    
    def __init__(self):
        """Initialize the context enhancer."""
        self._logger = logger
        self._project_info_cache = {}  # Cache project info by path
        self._file_activity_cache = {}  # Cache recent file activity
        self._enhancers: List[Callable[[Dict[str, Any]], Awaitable[Dict[str, Any]]]] = []
    
    async def enrich_context(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """
        Enrich the context with additional information.
        
        Args:
            context: The base context to enrich
            
        Returns:
            The enriched context
        """
        self._logger.debug("Enriching context with additional information")
        
        # Start with a copy of the original context
        enriched = dict(context)
        
        try:
            # Add enhanced project information if in a project
            if context.get("project_root"):
                await self._add_project_info(enriched, context["project_root"])
            
            # Add recent file activity
            await self._add_recent_file_activity(enriched)
            
            # Add file reference context
            await self._add_file_reference_context(enriched)
            
            # Add enhanced file activity information 
            file_activity_tracker = get_file_activity_tracker()
            
            try:
                # Get ActivityType enum
                from angela.api.context import get_activity_type
                ActivityType = get_activity_type()
                
                # Get recent file activities by types
                viewed_activities = file_activity_tracker.get_recent_activities(
                    limit=5, 
                    activity_types=[ActivityType.VIEWED]
                )
                modified_activities = file_activity_tracker.get_recent_activities(
                    limit=5, 
                    activity_types=[ActivityType.MODIFIED]
                )
                created_activities = file_activity_tracker.get_recent_activities(
                    limit=5, 
                    activity_types=[ActivityType.CREATED]
                )
                
                # Extract paths from activities
                viewed_files = [activity.get('path', '') for activity in viewed_activities if 'path' in activity]
                modified_files = [activity.get('path', '') for activity in modified_activities if 'path' in activity]
                created_files = [activity.get('path', '') for activity in created_activities if 'path' in activity]
                
                # Update or create recent_files
                if "recent_files" not in enriched:
                    enriched["recent_files"] = {}
                    
                enriched["recent_files"].update({
                    "accessed": viewed_files,
                    "modified": modified_files,
                    "created": created_files,
                })
                
                # Get most active files
                most_active = file_activity_tracker.get_most_active_files(limit=5)
                enriched["active_files"] = most_active
            except Exception as e:
                self._logger.warning(f"Error getting file activity: {str(e)}")
            
            # Add file resolver information if available
            if "requested_file" in context:
                file_resolver = get_file_resolver()
                
                try:
                    resolved_files = await file_resolver.resolve_file_references(
                        context.get("cwd", ""),
                        context.get("project_root", ""),
                        [context["requested_file"]]
                    )
                    
                    if resolved_files:
                        enriched["resolved_files"] = resolved_files
                except Exception as e:
                    self._logger.warning(f"Error resolving file references: {str(e)}")
            
            # Run all registered enhancers
            for enhancer in self._enhancers:
                try:
                    result = await enhancer(enriched)
                    if result:
                        enriched.update(result)
                except Exception as e:
                    self._logger.error(f"Error in enhancer {getattr(enhancer, '__name__', 'anonymous')}: {str(e)}")
            
            self._logger.debug(f"Context enriched with {len(enriched) - len(context)} additional keys")
        except Exception as e:
            self._logger.error(f"Error enriching context: {str(e)}")
            # Return what we have so far
        
        return enriched
    
    async def _add_project_info(self, context: Dict[str, Any], project_root: str) -> None:
        """
        Add enhanced project information to the context.
        
        Args:
            context: The context to enrich
            project_root: The path to the project root
        """
        # Get project_inference from API
        project_inference = get_project_inference()
        
        self._logger.debug(f"Adding project info for {project_root}")
        
        try:
            # Check cache first
            if project_root in self._project_info_cache:
                project_info = self._project_info_cache[project_root]
                self._logger.debug(f"Using cached project info for {project_root}")
            else:
                # Get project info from project_inference
                project_info = await project_inference.infer_project_info(Path(project_root))
                # Cache the result
                self._project_info_cache[project_root] = project_info
                self._logger.debug(f"Inferred project info for {project_root}")
            
            # Add project info to context
            context["enhanced_project"] = {
                "type": project_info.get("project_type", "unknown"),
                "frameworks": project_info.get("detected_frameworks", {}),
                "dependencies": self._format_dependencies(project_info.get("dependencies", [])),
                "important_files": self._format_important_files(project_info.get("detected_files", [])),
                "structure": self._summarize_structure(project_info.get("structure", {}))
            }
            
            self._logger.debug(f"Added enhanced project info to context: {context['enhanced_project']['type']}")
        except Exception as e:
            self._logger.error(f"Error adding project info: {str(e)}")
    
    def _format_dependencies(self, dependencies: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        Format dependencies for inclusion in context.
        
        Args:
            dependencies: List of dependency information
            
        Returns:
            Formatted dependency information
        """
        # Group dependencies by type
        dependency_types = {}
        
        for dep in dependencies:
            dep_type = dep.get("type", "unknown")
            if dep_type not in dependency_types:
                dependency_types[dep_type] = []
            
            dependency_types[dep_type].append({
                "name": dep.get("name", "unknown"),
                "version": dep.get("version_spec", "")
            })
        
        # Return summary with counts
        return {
            "types": list(dependency_types.keys()),
            "counts": {t: len(deps) for t, deps in dependency_types.items()},
            "total": len(dependencies),
            "top_dependencies": [d.get("name", "unknown") for d in dependencies[:10]]
        }
    
    def _format_important_files(self, files: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        Format important files for inclusion in context.
        
        Args:
            files: List of important file information
            
        Returns:
            Formatted important file information
        """
        # Group files by type
        file_types = {}
        paths = []
        
        for file in files:
            file_type = file.get("type", "unknown")
            if file_type not in file_types:
                file_types[file_type] = []
            
            file_types[file_type].append(file.get("path", "unknown"))
            paths.append(file.get("path", "unknown"))
        
        # Return summary
        return {
            "types": list(file_types.keys()),
            "counts": {t: len(files) for t, files in file_types.items()},
            "total": len(files),
            "paths": paths
        }
    
    def _summarize_structure(self, structure: Dict[str, Any]) -> Dict[str, Any]:
        """
        Summarize project structure for inclusion in context.
        
        Args:
            structure: Project structure information
            
        Returns:
            Summarized structure information
        """
        # Extract key information from structure
        if not structure:
            return {}
        
        # Handle case where main_directories is a list of strings instead of dictionaries
        main_directories = structure.get("main_directories", [])
        
        # Convert to proper format if needed
        main_dirs_list = []
        for directory in main_directories:
            if isinstance(directory, dict):
                main_dirs_list.append(directory.get("name", ""))
            elif isinstance(directory, str):
                main_dirs_list.append(directory)
        
        # Return summary
        return {
            "file_counts": structure.get("file_counts", {}),
            "total_files": structure.get("total_files", 0),
            "main_directories": main_dirs_list
        }
    
    async def _add_recent_file_activity(self, context: Dict[str, Any]) -> None:
        """
        Add recent file activity to the context.
        
        Args:
            context: The context to enrich
        """
        # Get session_manager from API
        from angela.api.context import get_session_manager
        session_manager = get_session_manager()
        
        self._logger.debug("Adding recent file activity to context")
        
        try:
            # Get recent file activities from session
            session = session_manager.get_context()
            entities = session.get("entities", {})
            
            # Filter for file-related entities
            file_entities = {
                name: entity for name, entity in entities.items()
                if entity.get("type") in ["file", "directory", "recent_file"]
            }
            
            # Extract accessed files from entities
            accessed_files = []
            for name, entity in file_entities.items():
                if "value" in entity and entity["value"]:
                    accessed_files.append(entity["value"])
            
            # Always ensure we have at least an empty list
            recent_activities = []
            
            # Format and add to context
            context["recent_files"] = {
                "accessed": accessed_files,
                "activities": recent_activities,
                "count": len(file_entities)
            }
            
            self._logger.debug(f"Added {len(file_entities)} recent files to context")
        except Exception as e:
            self._logger.error(f"Error adding recent file activity: {str(e)}")
            # IMPORTANT: Add an empty recent_files object to context even on failure
            context["recent_files"] = {
                "accessed": [],
                "activities": [],
                "count": 0
            }
    
    async def _add_file_reference_context(self, context: Dict[str, Any]) -> None:
        """
        Add file reference context information.
        
        Args:
            context: The context to enrich
        """
        self._logger.debug("Adding file reference context")
        
        try:
            # Get current working directory
            cwd = context.get("cwd", "")
            if not cwd:
                return
            
            # List files in the current directory
            cwd_path = Path(cwd)
            files = list(cwd_path.glob("*"))
            
            # Format file information
            file_info = {
                "files": [f.name for f in files if f.is_file()],
                "directories": [f.name for f in files if f.is_dir()],
                "total": len(files)
            }
            
            # Add to context
            context["file_reference"] = file_info
            
            self._logger.debug(f"Added file reference context with {len(files)} files")
        except Exception as e:
            self._logger.error(f"Error adding file reference context: {str(e)}")
    
    def clear_cache(self) -> None:
        """Clear the context enhancer cache."""
        self._logger.debug("Clearing context enhancer cache")
        self._project_info_cache.clear()
        self._file_activity_cache.clear()

    def register_enhancer(self, enhancer_func: Callable[[Dict[str, Any]], Awaitable[Dict[str, Any]]]) -> None:
        """
        Register a context enhancer function.
        
        Args:
            enhancer_func: Async function that takes a context dict and returns an enhanced context dict
        """
        self._logger.debug(f"Registering context enhancer: {enhancer_func.__name__ if hasattr(enhancer_func, '__name__') else 'anonymous'}")
        self._enhancers.append(enhancer_func)


# Global context enhancer instance
try:
    # Initialize the ContextEnhancer instance
    context_enhancer = ContextEnhancer()
    logger.debug(f"context_enhancer initialized: {context_enhancer}")
except Exception as e:
    logger.error(f"Failed to initialize context_enhancer: {e}")
    # Create a new instance with better error handling
    try:
        logger.debug("Attempting to create a new instance of ContextEnhancer")
        context_enhancer = ContextEnhancer()
        logger.debug(f"Successfully created a new instance: {context_enhancer}")
    except Exception as inner_e:
        logger.critical(f"Critical failure initializing context_enhancer: {inner_e}")
        # Create minimally functional instance to avoid further errors
        context_enhancer = ContextEnhancer()
</file>

<file path="components/context/file_activity.py">
# angela/context/file_activity.py
"""
File activity tracking for Angela CLI.

This module provides functionality to track file activities such as access,
modification, and creation, and to maintain a history of these activities.
"""
import os
import time
from datetime import datetime
from enum import Enum
from pathlib import Path
from typing import Dict, Any, Optional, List, Set, Union

from angela.api.context import get_session_manager
from angela.utils.logging import get_logger

logger = get_logger(__name__)

class ActivityType(str, Enum):
    """Types of file activities."""
    CREATED = "created"
    MODIFIED = "modified"
    DELETED = "deleted"
    VIEWED = "viewed"
    EXECUTED = "executed"
    ANALYZED = "analyzed"
    OTHER = "other"

class FileActivity:
    """Represents a file activity with related metadata."""
    
    def __init__(
        self,
        path: Union[str, Path],
        activity_type: ActivityType,
        timestamp: Optional[float] = None,
        command: Optional[str] = None,
        details: Optional[Dict[str, Any]] = None
    ):
        """
        Initialize a file activity.
        
        Args:
            path: Path to the file/directory
            activity_type: Type of activity
            timestamp: Optional timestamp (defaults to current time)
            command: Optional command that triggered the activity
            details: Optional additional details
        """
        self.path = Path(path) if isinstance(path, str) else path
        self.activity_type = activity_type
        self.timestamp = timestamp or time.time()
        self.command = command
        self.details = details or {}
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for serialization."""
        return {
            "path": str(self.path),
            "name": self.path.name,
            "activity_type": self.activity_type.value,
            "timestamp": self.timestamp,
            "datetime": datetime.fromtimestamp(self.timestamp).isoformat(),
            "command": self.command,
            "details": self.details
        }
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'FileActivity':
        """Create from dictionary."""
        return cls(
            path=data["path"],
            activity_type=ActivityType(data["activity_type"]),
            timestamp=data["timestamp"],
            command=data.get("command"),
            details=data.get("details", {})
        )


class FileActivityTracker:
    """
    Tracker for file activities with session integration.
    
    Provides methods to:
    1. Track file activities (creation, modification, etc.)
    2. Retrieve recent file activities
    3. Integrate with session management
    """
    
    def __init__(self, max_activities: int = 100):
        """
        Initialize the file activity tracker.
        
        Args:
            max_activities: Maximum number of activities to track
        """
        self._logger = logger
        self._activities: List[FileActivity] = []
        self._max_activities = max_activities
    
    def track_activity(
        self,
        path: Union[str, Path],
        activity_type: ActivityType,
        command: Optional[str] = None,
        details: Optional[Dict[str, Any]] = None
    ) -> None:
        """
        Track a file activity.
        
        Args:
            path: Path to the file/directory
            activity_type: Type of activity
            command: Optional command that triggered the activity
            details: Optional additional details
        """
        # Create a new activity
        activity = FileActivity(
            path=path,
            activity_type=activity_type,
            command=command,
            details=details
        )
        
        # Add to the activity list
        self._activities.append(activity)
        
        # Trim if needed
        if len(self._activities) > self._max_activities:
            self._activities = self._activities[-self._max_activities:]
        
        # Update session
        self._update_session(activity)
        
        self._logger.debug(f"Tracked {activity_type.value} activity for {path}")
    
    def track_file_creation(
        self,
        path: Union[str, Path],
        command: Optional[str] = None,
        details: Optional[Dict[str, Any]] = None
    ) -> None:
        """
        Track file creation.
        
        Args:
            path: Path to the created file
            command: Optional command that triggered the creation
            details: Optional additional details
        """
        self.track_activity(
            path=path,
            activity_type=ActivityType.CREATED,
            command=command,
            details=details
        )
    
    def track_file_modification(
        self,
        path: Union[str, Path],
        command: Optional[str] = None,
        details: Optional[Dict[str, Any]] = None
    ) -> None:
        """
        Track file modification.
        
        Args:
            path: Path to the modified file
            command: Optional command that triggered the modification
            details: Optional additional details
        """
        self.track_activity(
            path=path,
            activity_type=ActivityType.MODIFIED,
            command=command,
            details=details
        )
    
    def track_file_deletion(
        self,
        path: Union[str, Path],
        command: Optional[str] = None,
        details: Optional[Dict[str, Any]] = None
    ) -> None:
        """
        Track file deletion.
        
        Args:
            path: Path to the deleted file
            command: Optional command that triggered the deletion
            details: Optional additional details
        """
        self.track_activity(
            path=path,
            activity_type=ActivityType.DELETED,
            command=command,
            details=details
        )
    
    def track_file_viewing(
        self,
        path: Union[str, Path],
        command: Optional[str] = None,
        details: Optional[Dict[str, Any]] = None
    ) -> None:
        """
        Track file viewing.
        
        Args:
            path: Path to the viewed file
            command: Optional command that triggered the viewing
            details: Optional additional details
        """
        self.track_activity(
            path=path,
            activity_type=ActivityType.VIEWED,
            command=command,
            details=details
        )
    
    def get_recent_activities(
        self,
        limit: int = 10,
        activity_types: Optional[List[ActivityType]] = None
    ) -> List[Dict[str, Any]]:
        """
        Get recent file activities.
        
        Args:
            limit: Maximum number of activities to return
            activity_types: Optional filter for activity types
            
        Returns:
            List of activities as dictionaries
        """
        # Apply filters
        filtered = self._activities
        if activity_types:
            filtered = [a for a in filtered if a.activity_type in activity_types]
        
        # Sort by timestamp (newest first)
        sorted_activities = sorted(filtered, key=lambda a: a.timestamp, reverse=True)
        
        # Convert to dictionaries
        return [a.to_dict() for a in sorted_activities[:limit]]
    
    def get_activities_for_path(
        self,
        path: Union[str, Path],
        limit: int = 10
    ) -> List[Dict[str, Any]]:
        """
        Get activities for a specific path.
        
        Args:
            path: Path to get activities for
            limit: Maximum number of activities to return
            
        Returns:
            List of activities as dictionaries
        """
        path_obj = Path(path) if isinstance(path, str) else path
        
        # Filter by path
        path_activities = [a for a in self._activities if a.path == path_obj]
        
        # Sort by timestamp (newest first)
        sorted_activities = sorted(path_activities, key=lambda a: a.timestamp, reverse=True)
        
        # Convert to dictionaries
        return [a.to_dict() for a in sorted_activities[:limit]]
    
    def get_most_active_files(self, limit: int = 5) -> List[Dict[str, Any]]:
        """
        Get the most actively used files.
        
        Args:
            limit: Maximum number of files to return
            
        Returns:
            List of files with activity counts
        """
        # Count activities by path
        path_counts = {}
        for activity in self._activities:
            path_str = str(activity.path)
            if path_str not in path_counts:
                path_counts[path_str] = {
                    "path": path_str,
                    "name": activity.path.name,
                    "count": 0,
                    "last_activity": None,
                    "activities": set()
                }
            
            path_counts[path_str]["count"] += 1
            path_counts[path_str]["activities"].add(activity.activity_type.value)
            
            # Update last activity if newer
            if path_counts[path_str]["last_activity"] is None or \
               activity.timestamp > path_counts[path_str]["last_activity"]:
                path_counts[path_str]["last_activity"] = activity.timestamp
        
        # Convert to list and sort by count (highest first)
        result = []
        for path_info in path_counts.values():
            # Convert activities set to list
            path_info["activities"] = list(path_info["activities"])
            result.append(path_info)
        
        result.sort(key=lambda x: x["count"], reverse=True)
        
        return result[:limit]
    
    def clear_activities(self) -> None:
        """Clear all tracked activities."""
        self._activities.clear()
        self._logger.debug("Cleared all file activities")
    
    def _update_session(self, activity: FileActivity) -> None:
        """
        Update session with file activity.
        
        Args:
            activity: The file activity to add to the session
        """
        try:
            session_manager = get_session_manager()
            
            # Add to session as an entity
            path_name = activity.path.name
            entity_name = f"file:{path_name}"
            
            session_manager.add_entity(
                name=entity_name,
                entity_type="file",
                value=str(activity.path)
            )
            
            # Also add with activity type
            activity_entity_name = f"{activity.activity_type.value}_file:{path_name}"
            session_manager.add_entity(
                name=activity_entity_name,
                entity_type=f"{activity.activity_type.value}_file",
                value=str(activity.path)
            )
        except Exception as e:
            self._logger.error(f"Error updating session with file activity: {str(e)}")

# Global file activity tracker instance
file_activity_tracker = FileActivityTracker()
</file>

<file path="components/context/file_detector.py">
# angela/context/file_detector.py
"""
File type detection for Angela CLI.

This module provides functionality to detect file types and languages
to enhance context awareness for operations.
"""
import re
import os
import mimetypes
from pathlib import Path
from typing import Dict, Any, Optional, List, Tuple

from angela.utils.logging import get_logger

logger = get_logger(__name__)

# Initialize mimetypes
mimetypes.init()

# Map of file extensions to programming languages
LANGUAGE_EXTENSIONS = {
    # Web
    '.html': 'HTML',
    '.htm': 'HTML',
    '.css': 'CSS',
    '.js': 'JavaScript',
    '.jsx': 'JavaScript (React)',
    '.ts': 'TypeScript',
    '.tsx': 'TypeScript (React)',
    
    # Python
    '.py': 'Python',
    '.pyi': 'Python Interface',
    '.pyx': 'Cython',
    '.ipynb': 'Jupyter Notebook',
    
    # Ruby
    '.rb': 'Ruby',
    '.erb': 'Ruby (ERB)',
    '.rake': 'Ruby (Rake)',
    
    # Java/JVM
    '.java': 'Java',
    '.kt': 'Kotlin',
    '.groovy': 'Groovy',
    '.scala': 'Scala',
    
    # C/C++
    '.c': 'C',
    '.h': 'C Header',
    '.cpp': 'C++',
    '.cc': 'C++',
    '.hpp': 'C++ Header',
    
    # C#
    '.cs': 'C#',
    
    # Go
    '.go': 'Go',
    
    # Rust
    '.rs': 'Rust',
    
    # Swift
    '.swift': 'Swift',
    
    # PHP
    '.php': 'PHP',
    
    # Shell
    '.sh': 'Shell (Bash)',
    '.bash': 'Bash',
    '.zsh': 'Zsh',
    '.fish': 'Fish',
    
    # Configuration
    '.json': 'JSON',
    '.yaml': 'YAML',
    '.yml': 'YAML',
    '.toml': 'TOML',
    '.ini': 'INI',
    '.cfg': 'Config',
    '.conf': 'Config',
    
    # Markup
    '.md': 'Markdown',
    '.rst': 'reStructuredText',
    '.xml': 'XML',
    '.svg': 'SVG',
    
    # Data
    '.csv': 'CSV',
    '.tsv': 'TSV',
    '.txt': 'Text',
    '.log': 'Log',
    
    # Documents
    '.pdf': 'PDF',
    '.doc': 'MS Word',
    '.docx': 'MS Word',
    '.xls': 'MS Excel',
    '.xlsx': 'MS Excel',
    '.ppt': 'MS PowerPoint',
    '.pptx': 'MS PowerPoint',
    
    # Images
    '.jpg': 'JPEG Image',
    '.jpeg': 'JPEG Image',
    '.png': 'PNG Image',
    '.gif': 'GIF Image',
    '.bmp': 'BMP Image',
    '.webp': 'WebP Image',
    
    # Audio
    '.mp3': 'MP3 Audio',
    '.wav': 'WAV Audio',
    '.ogg': 'OGG Audio',
    '.flac': 'FLAC Audio',
    
    # Video
    '.mp4': 'MP4 Video',
    '.avi': 'AVI Video',
    '.mkv': 'MKV Video',
    '.mov': 'MOV Video',
    
    # Archives
    '.zip': 'ZIP Archive',
    '.tar': 'TAR Archive',
    '.gz': 'GZIP Archive',
    '.bz2': 'BZIP2 Archive',
    '.xz': 'XZ Archive',
    '.7z': '7-Zip Archive',
    '.rar': 'RAR Archive',
    
    # Executables
    '.exe': 'Windows Executable',
    '.dll': 'Windows Library',
    '.so': 'Shared Object',
    '.dylib': 'macOS Library',
    
    # Other
    '.sql': 'SQL',
    '.db': 'Database',
    '.sqlite': 'SQLite Database',
}

# Mapping of file names to types
FILENAME_MAPPING = {
    'Dockerfile': 'Docker',
    'docker-compose.yml': 'Docker Compose',
    'docker-compose.yaml': 'Docker Compose',
    '.dockerignore': 'Docker',
    'Makefile': 'Makefile',
    'CMakeLists.txt': 'CMake',
    'package.json': 'Node.js',
    'package-lock.json': 'Node.js',
    'yarn.lock': 'Yarn',
    'requirements.txt': 'Python',
    'setup.py': 'Python',
    'pyproject.toml': 'Python',
    'Pipfile': 'Python (Pipenv)',
    'Pipfile.lock': 'Python (Pipenv)',
    'Gemfile': 'Ruby',
    'Gemfile.lock': 'Ruby',
    'build.gradle': 'Gradle',
    'build.gradle.kts': 'Gradle (Kotlin)',
    'pom.xml': 'Maven',
    'Cargo.toml': 'Rust',
    'Cargo.lock': 'Rust',
    '.gitignore': 'Git',
    '.gitattributes': 'Git',
    '.gitlab-ci.yml': 'GitLab CI',
    '.travis.yml': 'Travis CI',
    'Jenkinsfile': 'Jenkins',
    '.editorconfig': 'EditorConfig',
    '.eslintrc': 'ESLint',
    '.eslintrc.js': 'ESLint',
    '.eslintrc.json': 'ESLint',
    '.prettierrc': 'Prettier',
    '.prettierrc.js': 'Prettier',
    '.prettierrc.json': 'Prettier',
    'tsconfig.json': 'TypeScript',
    'tslint.json': 'TSLint',
    '.babelrc': 'Babel',
    'babel.config.js': 'Babel',
    'webpack.config.js': 'Webpack',
    'rollup.config.js': 'Rollup',
    'vite.config.js': 'Vite',
    'jest.config.js': 'Jest',
    '.env': 'Environment Variables',
    '.env.example': 'Environment Variables',
    'README.md': 'Documentation',
    'LICENSE': 'License',
    'CHANGELOG.md': 'Changelog',
    'CONTRIBUTING.md': 'Documentation',
    'CODE_OF_CONDUCT.md': 'Documentation',
}

# Language-specific shebang patterns
SHEBANG_PATTERNS = [
    (r'^#!/bin/bash', 'Bash'),
    (r'^#!/usr/bin/env\s+bash', 'Bash'),
    (r'^#!/bin/sh', 'Shell'),
    (r'^#!/usr/bin/env\s+sh', 'Shell'),
    (r'^#!/usr/bin/python', 'Python'),
    (r'^#!/usr/bin/env\s+python', 'Python'),
    (r'^#!/usr/bin/node', 'JavaScript'),
    (r'^#!/usr/bin/env\s+node', 'JavaScript'),
    (r'^#!/usr/bin/ruby', 'Ruby'),
    (r'^#!/usr/bin/env\s+ruby', 'Ruby'),
    (r'^#!/usr/bin/perl', 'Perl'),
    (r'^#!/usr/bin/env\s+perl', 'Perl'),
    (r'^#!/usr/bin/php', 'PHP'),
    (r'^#!/usr/bin/env\s+php', 'PHP'),
]

# Mapping of MIME type prefixes to general types
MIME_TYPE_MAPPING = {
    'image/': 'image',
    'audio/': 'audio',
    'video/': 'video',
    'text/': 'text',
    'application/pdf': 'document',
    'application/msword': 'document',
    'application/vnd.openxmlformats-officedocument': 'document',
    'application/zip': 'archive',
    'application/x-tar': 'archive',
    'application/x-gzip': 'archive',
    'application/x-bzip2': 'archive',
    'application/x-xz': 'archive',
    'application/x-7z-compressed': 'archive',
    'application/x-rar-compressed': 'archive',
}


# In angela/context/file_detector.py - update the detect_file_type function

def detect_file_type(path: Path) -> Dict[str, Any]:
    """
    Detect the type of a file based on extension, content, and other heuristics.
    
    Args:
        path: The path to the file.
        
    Returns:
        A dictionary with file type information.
    """
    result = {
        'type': 'unknown',
        'language': None,
        'mime_type': None,
        'binary': False,
        'encoding': None,
    }
    
    try:
        if not path.exists():
            return result
        
        # Check if it's a directory
        if path.is_dir():
            result['type'] = 'directory'
            return result
        
        # Get file name and extension
        name = path.name
        extension = path.suffix.lower()
        
        # Check if it's a known file by name
        if name in FILENAME_MAPPING:
            result['type'] = FILENAME_MAPPING[name]
            
        # Get MIME type
        mime_type, encoding = mimetypes.guess_type(str(path))
        if mime_type:
            result['mime_type'] = mime_type
            result['encoding'] = encoding
            
            # Get general type from MIME
            main_type = mime_type.split('/')[0]
            result['type'] = main_type
        
        # Detect language based on extension
        if extension in LANGUAGE_EXTENSIONS:
            result['language'] = LANGUAGE_EXTENSIONS[extension]
            result['type'] = 'source_code'  # Set type to source_code when a language is detected
        
        # Special case for known project files
        if name == "requirements.txt":
            result['type'] = "Python"  # Force correct type for requirements.txt
            
        # For text files without a clear type, check for shebangs
        if extension in ['.txt', ''] or not result['language']:
            try:
                # Read the first line of the file
                with open(path, 'r', errors='ignore') as f:
                    first_line = f.readline().strip()
                
                # Check for shebang patterns
                for pattern, language in SHEBANG_PATTERNS:
                    if re.match(pattern, first_line):
                        result['language'] = language
                        result['type'] = 'source_code'  # Set the type for scripting files with shebangs
                        break
            except UnicodeDecodeError:
                # File is likely binary
                result['binary'] = True
                result['type'] = 'binary'
        
        # Check if the file is binary
        if not result['binary'] and not result['type'] == 'directory':
            try:
                with open(path, 'rb') as f:
                    chunk = f.read(4096)
                    # Check for null bytes (common in binary files)
                    if b'\0' in chunk:
                        result['binary'] = True
                        if not result['type'] or result['type'] == 'unknown':
                            result['type'] = 'binary'
            except IOError:
                pass
        
        return result
    
    except Exception as e:
        logger.exception(f"Error detecting file type for {path}: {str(e)}")
        return result


def get_content_preview(path: Path, max_lines: int = 10, max_chars: int = 1000) -> Optional[str]:
    """
    Get a preview of a file's content.
    
    Args:
        path: The path to the file.
        max_lines: Maximum number of lines to preview.
        max_chars: Maximum number of characters to preview.
        
    Returns:
        A string with the file preview, or None if the file is not readable.
    """
    try:
        if not path.exists() or not path.is_file():
            return None
        
        # Check file type
        file_info = detect_file_type(path)
        if file_info['binary']:
            return "[Binary file]"
        
        # Read the file content
        with open(path, 'r', errors='replace') as f:
            lines = []
            total_chars = 0
            
            for i, line in enumerate(f):
                if i >= max_lines:
                    lines.append("...")
                    break
                
                if total_chars + len(line) > max_chars:
                    # Truncate the line if it would exceed max_chars
                    available_chars = max_chars - total_chars
                    if available_chars > 3:
                        lines.append(line[:available_chars - 3] + "...")
                    break
                
                lines.append(line.rstrip('\n'))
                total_chars += len(line)
        
        return '\n'.join(lines)
    
    except Exception as e:
        logger.exception(f"Error getting content preview for {path}: {str(e)}")
        return None
</file>

<file path="components/context/file_resolver.py">
# angela/context/file_resolver.py

import os
import re
import difflib
from pathlib import Path
from typing import Dict, Any, Optional, List, Tuple, Union, Set

from angela.api.context import get_context_manager, get_session_manager
from angela.utils.logging import get_logger

logger = get_logger(__name__)

class FileResolver:
    """
    Resolver for file references in natural language queries.
    
    Provides multiple strategies for resolving file references:
    1. Exact path matching
    2. Fuzzy name matching
    3. Pattern matching
    4. Context-aware resolution (recent files, project files)
    5. Special references (current file, last modified file)
    """
    
    def __init__(self):
        """Initialize the file resolver."""
        self._logger = logger
        self._threshold = 0.6  # Threshold for fuzzy matching
    
    async def resolve_reference(
        self, 
        reference: str, 
        context: Dict[str, Any],
        search_scope: Optional[str] = None
    ) -> Optional[Path]:
        """
        Resolve a file reference to an actual file path.
        
        Args:
            reference: The file reference to resolve
            context: Context information
            search_scope: Optional scope for the search (project, directory, recent)
            
        Returns:
            Path object if found, None otherwise
        """
        self._logger.info(f"Resolving file reference: '{reference}'")
        
        # Strip quotes if present
        reference = reference.strip('\'"')
        
        # If reference is empty or None, return None
        if not reference:
            return None
        
        # Try resolving with each strategy in order
        resolved = await self._resolve_exact_path(reference, context)
        if resolved:
            self._logger.debug(f"Resolved via exact path: {resolved}")
            self._record_resolution(reference, resolved, "exact_path")
            return resolved
        
        resolved = await self._resolve_special_reference(reference, context)
        if resolved:
            self._logger.debug(f"Resolved via special reference: {resolved}")
            self._record_resolution(reference, resolved, "special_reference")
            return resolved
        
        resolved = await self._resolve_recent_file(reference, context)
        if resolved:
            self._logger.debug(f"Resolved via recent file: {resolved}")
            self._record_resolution(reference, resolved, "recent_file")
            return resolved
        
        resolved = await self._resolve_fuzzy_match(reference, context, search_scope)
        if resolved:
            self._logger.debug(f"Resolved via fuzzy match: {resolved}")
            self._record_resolution(reference, resolved, "fuzzy_match")
            return resolved
        
        resolved = await self._resolve_pattern_match(reference, context, search_scope)
        if resolved:
            self._logger.debug(f"Resolved via pattern match: {resolved}")
            self._record_resolution(reference, resolved, "pattern_match")
            return resolved
        
        # If all strategies fail, log and return None
        self._logger.warning(f"Could not resolve file reference: '{reference}'")
        return None
    
    async def extract_references(
        self, 
        text: str,
        context: Dict[str, Any]
    ) -> List[Tuple[str, Optional[Path]]]:
        """
        Extract and resolve file references from text.
        
        Args:
            text: The text to extract references from
            context: Context information
            
        Returns:
            List of (reference, resolved_path) tuples
        """
        self._logger.info(f"Extracting file references from: '{text}'")
        
        # Define common words that should not be treated as file references
        common_words = set([
            "that", "this", "those", "these", "the", "it", "which", "what", 
            "inside", "called", "named", "from", "with", "using", "into", 
            "as", "for", "about", "like", "than", "then", "when", "where",
            "how", "why", "who", "whom", "whose", "my", "your", "our", "their"
        ])
        
        # Define minimum token length for potential file references
        MIN_TOKEN_LENGTH = 3
        
        # Define more specific and detailed patterns for finding file references
        patterns = [
            # Quoted paths with extensions - high confidence
            r'["\']([a-zA-Z0-9](?:[a-zA-Z0-9_\-\.]+)/(?:[a-zA-Z0-9_\-\.]+/)*[a-zA-Z0-9_\-\.]+\.[a-zA-Z0-9]{1,10})["\']',
            
            # Quoted files with extensions - high confidence
            r'["\']([a-zA-Z0-9][a-zA-Z0-9_\-\.]{1,50}\.[a-zA-Z0-9]{1,10})["\']',
            
            # Unquoted but clear file paths with extensions - medium confidence
            r'\b([a-zA-Z0-9](?:[a-zA-Z0-9_\-\.]+)/(?:[a-zA-Z0-9_\-\.]+/)*[a-zA-Z0-9_\-\.]+\.[a-zA-Z0-9]{1,10})\b',
            
            # Unquoted files with extensions and minimal length - medium confidence
            r'\b([a-zA-Z0-9][a-zA-Z0-9_\-\.]{2,}\.[a-zA-Z0-9]{1,10})\b',
            
            # Very specific references with operation keywords - high confidence
            r'(?:edit|open|read|cat|view|show|display|modify|update|check)\s+(?:file|script|module|config)?\s*["\']?([a-zA-Z0-9][a-zA-Z0-9_\-\.]{2,}(?:\.[a-zA-Z0-9]{1,10})?)["\']?',
            
            # Specific file operations with clear file reference - high confidence
            r'(?:append\s+to|write\s+to|delete|remove)\s+(?:file|script)?\s*["\']?([a-zA-Z0-9][a-zA-Z0-9_\-\.]{2,}(?:\.[a-zA-Z0-9]{1,10})?)["\']?',
            
            # File references with clear context - medium confidence
            r'(?:in|from|to)\s+(?:file|directory|folder)\s+["\']?([a-zA-Z0-9][a-zA-Z0-9_\-\.]{2,}(?:\.[a-zA-Z0-9]{1,10})?)["\']?',
            
            # References with operations that specifically mention "file" - medium-high confidence
            r'(?:the|this|that|my)\s+file\s+["\']?([a-zA-Z0-9][a-zA-Z0-9_\-\.]{2,}(?:\.[a-zA-Z0-9]{1,10})?)["\']?',
        ]
        
        # Special patterns for creation targets that we won't try to resolve as existing files
        creation_patterns = [
            # "save as X" pattern
            r'save\s+(?:it\s+)?(?:as|to)\s+["\']?([a-zA-Z0-9][a-zA-Z0-9_\-\.]{2,}(?:\.[a-zA-Z0-9]{1,10})?)["\']?',
            
            # "create file X" pattern
            r'create\s+(?:a\s+)?(?:new\s+)?(?:file|script)\s+["\']?([a-zA-Z0-9][a-zA-Z0-9_\-\.]{2,}(?:\.[a-zA-Z0-9]{1,10})?)["\']?',
            
            # "generate X" where X has a file extension
            r'generate\s+(?:a\s+)?(?:new\s+)?(?:file|script|code)?\s*["\']?([a-zA-Z0-9][a-zA-Z0-9_\-\.]{2,}\.[a-zA-Z0-9]{1,10})["\']?',
        ]
        
        references = []
        creation_targets = []  # Track references that are meant for file creation
        
        # First identify creation targets that we should NOT try to resolve
        for pattern in creation_patterns:
            for match in re.finditer(pattern, text, re.IGNORECASE):
                reference = match.group(1)
                
                # Skip references that are too short or common words
                if len(reference) < MIN_TOKEN_LENGTH or reference.lower() in common_words:
                    continue
                    
                # Skip if it's just a number
                if reference.isdigit():
                    continue
                    
                # Skip if we've already seen this reference
                if reference in creation_targets:
                    continue
                    
                # This is a creation target, not an existing file to resolve
                creation_targets.append(reference)
                self._logger.debug(f"Identified creation target: {reference}")
        
        # Now extract references that should be resolved as existing files
        for pattern in patterns:
            for match in re.finditer(pattern, text, re.IGNORECASE):
                reference = match.group(1)
                
                # Skip references that are too short or common words
                if len(reference) < MIN_TOKEN_LENGTH or reference.lower() in common_words:
                    continue
                    
                # Skip if it's just a number
                if reference.isdigit():
                    continue
                    
                # Skip if this reference is a creation target or already in our list
                if reference in creation_targets or any(ref == reference for ref, _ in references):
                    continue
                
                # Try to resolve the reference
                resolved = await self.resolve_reference(reference, context)
                references.append((reference, resolved))
        
        self._logger.debug(f"Extracted {len(references)} file references and skipped {len(creation_targets)} creation targets")
        return references
    
    async def _resolve_exact_path(
        self, 
        reference: str, 
        context: Dict[str, Any]
    ) -> Optional[Path]:
        """
        Resolve a reference as an exact path.
        
        Args:
            reference: The reference to resolve
            context: Context information
            
        Returns:
            Path object if found, None otherwise
        """
        # Try as absolute path
        path = Path(reference)
        if path.is_absolute() and path.exists():
            return path
        
        # Try relative to current directory
        cwd_path = Path(context["cwd"]) / reference
        if cwd_path.exists():
            return cwd_path
        
        # Try relative to project root if available
        if context.get("project_root"):
            proj_path = Path(context["project_root"]) / reference
            if proj_path.exists():
                return proj_path
        
        return None
    
    async def _resolve_special_reference(
        self, 
        reference: str, 
        context: Dict[str, Any]
    ) -> Optional[Path]:
        """
        Resolve special references like "current file", "last modified", etc.
        
        Args:
            reference: The reference to resolve
            context: Context information
            
        Returns:
            Path object if found, None otherwise
        """
        # Handle various special references
        lowercase_ref = reference.lower()
        
        # Current file
        if lowercase_ref in ["current file", "this file", "current"]:
            if context.get("current_file") and "path" in context["current_file"]:
                return Path(context["current_file"]["path"])
        
        # Last modified file (via session)
        if lowercase_ref in ["last file", "last modified", "previous file"]:
            session_manager = get_session_manager()
            session = session_manager.get_context()
            entities = session.get("entities", {})
            
            # Look for the most recent file entity
            last_file = None
            last_time = None
            
            for name, entity in entities.items():
                if entity.get("type") in ["file", "recent_file"]:
                    entity_time = entity.get("created")
                    if entity_time and (not last_time or entity_time > last_time):
                        last_time = entity_time
                        last_file = entity.get("value")
            
            if last_file:
                return Path(last_file)
        
        return None
    
    async def _resolve_recent_file(
        self, 
        reference: str, 
        context: Dict[str, Any]
    ) -> Optional[Path]:
        """
        Resolve a reference against recently used files.
        
        Args:
            reference: The reference to resolve
            context: Context information
            
        Returns:
            Path object if found, None otherwise
        """
        # Get recent files from session
        session_manager = get_session_manager()
        session = session_manager.get_context()
        entities = session.get("entities", {})
        
        # Filter for file entities
        file_entities = {
            name: entity for name, entity in entities.items()
            if entity.get("type") in ["file", "directory", "recent_file"]
        }
        
        # Look for exact matches first
        for name, entity in file_entities.items():
            path = Path(entity.get("value", ""))
            if path.name.lower() == reference.lower():
                return path
        
        # Then try fuzzy matches
        for name, entity in file_entities.items():
            path = Path(entity.get("value", ""))
            similarity = difflib.SequenceMatcher(None, path.name.lower(), reference.lower()).ratio()
            if similarity >= self._threshold:
                return path
        
        return None
    
    async def _resolve_fuzzy_match(
        self, 
        reference: str, 
        context: Dict[str, Any],
        search_scope: Optional[str] = None
    ) -> Optional[Path]:
        """
        Resolve a reference using fuzzy matching.
        
        Args:
            reference: The reference to resolve
            context: Context information
            search_scope: Optional scope for the search
            
        Returns:
            Path object if found, None otherwise
        """
        paths_to_check = []
        
        # Get paths based on search scope
        if search_scope == "project" and context.get("project_root"):
            # Get all files in the project
            project_path = Path(context["project_root"])
            paths_to_check.extend(project_path.glob("**/*"))
        elif search_scope == "directory":
            # Get all files in the current directory
            cwd_path = Path(context["cwd"])
            paths_to_check.extend(cwd_path.glob("*"))
        else:
            # Default: check both current directory and project root
            cwd_path = Path(context["cwd"])
            paths_to_check.extend(cwd_path.glob("*"))
            
            if context.get("project_root"):
                project_path = Path(context["project_root"])
                
                # Only search project root if different from current directory
                if project_path != cwd_path:
                    # Get direct children of project root
                    paths_to_check.extend(project_path.glob("*"))
                    
                    # Add common directories like src, lib, test
                    common_dirs = ["src", "lib", "test", "tests", "docs", "app", "bin"]
                    for dirname in common_dirs:
                        dir_path = project_path / dirname
                        if dir_path.exists() and dir_path.is_dir():
                            paths_to_check.extend(dir_path.glob("*"))
        
        # Deduplicate paths
        paths_to_check = list(set(paths_to_check))
        
        # Sort paths by the similarity of their name to the reference
        matches = []
        for path in paths_to_check:
            similarity = difflib.SequenceMatcher(None, path.name.lower(), reference.lower()).ratio()
            if similarity >= self._threshold:
                matches.append((path, similarity))
        
        # Sort by similarity (highest first)
        matches.sort(key=lambda x: x[1], reverse=True)
        
        # Return the best match if any
        if matches:
            return matches[0][0]
        
        return None
    
    async def _resolve_pattern_match(
        self, 
        reference: str, 
        context: Dict[str, Any],
        search_scope: Optional[str] = None
    ) -> Optional[Path]:
        """
        Resolve a reference using pattern matching.
        
        Args:
            reference: The reference to resolve
            context: Context information
            search_scope: Optional scope for the search
            
        Returns:
            Path object if found, None otherwise
        """
        # Try to interpret the reference as a glob pattern
        base_paths = []
        
        # Determine base paths based on search scope
        if search_scope == "project" and context.get("project_root"):
            base_paths.append(Path(context["project_root"]))
        elif search_scope == "directory":
            base_paths.append(Path(context["cwd"]))
        else:
            # Default: check both current directory and project root
            base_paths.append(Path(context["cwd"]))
            if context.get("project_root"):
                project_path = Path(context["project_root"])
                if project_path != Path(context["cwd"]):
                    base_paths.append(project_path)
        
        # Try each base path
        for base_path in base_paths:
            # Try with wildcard prefix/suffix if needed
            patterns_to_try = [
                reference,  # As-is
                f"*{reference}*",  # Wildcard prefix and suffix
                f"*{reference}",  # Wildcard prefix
                f"{reference}*"  # Wildcard suffix
            ]
            
            for pattern in patterns_to_try:
                try:
                    # Use glob to find matching files
                    matches = list(base_path.glob(pattern))
                    if matches:
                        return matches[0]  # Return the first match
                except Exception:
                    # Invalid pattern, try the next one
                    continue
        
        return None
    
    def _record_resolution(
        self, 
        reference: str, 
        resolved_path: Path, 
        method: str
    ) -> None:
        """
        Record a successful resolution for learning.
        
        Args:
            reference: The original reference
            resolved_path: The resolved path
            method: The method used for resolution
        """
        # Store in session for future reference
        try:
            session_manager = get_session_manager()
            session_manager.add_entity(
                name=f"file_ref:{reference}",
                entity_type="file_reference",
                value=str(resolved_path)
            )
            
            # Also store as a recent file
            session_manager.add_entity(
                name=f"recent_file:{resolved_path.name}",
                entity_type="recent_file",
                value=str(resolved_path)
            )
        except Exception as e:
            self._logger.error(f"Error recording resolution: {str(e)}")

# Global file resolver instance
file_resolver = FileResolver()
</file>

<file path="components/context/history.py">
# angela/context/history.py

import json
import os
import re
from pathlib import Path
from typing import Dict, Any, Optional, List, Tuple
from datetime import datetime, timedelta
from collections import Counter, defaultdict

from angela.config import config_manager
from angela.utils.logging import get_logger
from angela.api.context import get_preferences_manager

logger = get_logger(__name__)

class CommandRecord:
    """Record of a command execution."""
    
    def __init__(
        self,
        command: str,
        natural_request: str,
        success: bool,
        timestamp: Optional[datetime] = None,
        output: Optional[str] = None,
        error: Optional[str] = None,
        risk_level: int = 0
    ):
        self.command = command
        self.natural_request = natural_request
        self.success = success
        self.timestamp = timestamp or datetime.now()
        self.output = output
        self.error = error
        self.risk_level = risk_level
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert the record to a dictionary for storage."""
        return {
            "command": self.command,
            "natural_request": self.natural_request,
            "success": self.success,
            "timestamp": self.timestamp.isoformat(),
            "output": self.output,
            "error": self.error,
            "risk_level": self.risk_level
        }
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'CommandRecord':
        """Create a record from a dictionary."""
        return cls(
            command=data["command"],
            natural_request=data["natural_request"],
            success=data["success"],
            timestamp=datetime.fromisoformat(data["timestamp"]),
            output=data.get("output"),
            error=data.get("error"),
            risk_level=data.get("risk_level", 0)
        )


class CommandPattern:
    """Pattern of commands executed by the user."""
    
    def __init__(self, base_command: str, count: int = 1, success_rate: float = 1.0):
        self.base_command = base_command
        self.count = count
        self.success_rate = success_rate
        self.last_used = datetime.now()
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert the pattern to a dictionary for storage."""
        return {
            "base_command": self.base_command,
            "count": self.count,
            "success_rate": self.success_rate,
            "last_used": self.last_used.isoformat()
        }
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'CommandPattern':
        """Create a pattern from a dictionary."""
        pattern = cls(
            base_command=data["base_command"],
            count=data["count"],
            success_rate=data["success_rate"]
        )
        pattern.last_used = datetime.fromisoformat(data["last_used"])
        return pattern


class HistoryManager:
    """Manager for command history and pattern analysis."""
    
    def __init__(self):
        """Initialize the history manager."""
        self._history_file = config_manager.CONFIG_DIR / "command_history.json"
        self._patterns_file = config_manager.CONFIG_DIR / "command_patterns.json"
        self._history: List[CommandRecord] = []
        self._patterns: Dict[str, CommandPattern] = {}
        self._load_history()
        self._load_patterns()
    
    def _load_history(self) -> None:
        """Load history from file."""
        try:
            if self._history_file.exists():
                with open(self._history_file, "r") as f:
                    data = json.load(f)
                    self._history = [CommandRecord.from_dict(item) for item in data]
                logger.debug(f"Loaded {len(self._history)} history items")
                
                # Trim history if needed
                preferences_manager = get_preferences_manager()
                max_items = preferences_manager.preferences.context.max_history_items
                if len(self._history) > max_items:
                    self._history = self._history[-max_items:]
                    self._save_history()  # Save the trimmed history
            else:
                logger.debug("No history file found")
        except Exception as e:
            logger.error(f"Error loading history: {e}")
            self._history = []
    
    def _load_patterns(self) -> None:
        """Load command patterns from file."""
        try:
            if self._patterns_file.exists():
                with open(self._patterns_file, "r") as f:
                    data = json.load(f)
                    self._patterns = {k: CommandPattern.from_dict(v) for k, v in data.items()}
                logger.debug(f"Loaded {len(self._patterns)} command patterns")
            else:
                logger.debug("No patterns file found")
        except Exception as e:
            logger.error(f"Error loading patterns: {e}")
            self._patterns = {}
    
    def _save_history(self) -> None:
        """Save history to file."""
        try:
            self._history_file.parent.mkdir(parents=True, exist_ok=True)
            with open(self._history_file, "w") as f:
                json.dump([item.to_dict() for item in self._history], f, indent=2)
            logger.debug(f"Saved history with {len(self._history)} items")
        except Exception as e:
            logger.error(f"Error saving history: {e}")
    
    def _save_patterns(self) -> None:
        """Save command patterns to file."""
        try:
            self._patterns_file.parent.mkdir(parents=True, exist_ok=True)
            with open(self._patterns_file, "w") as f:
                json.dump({k: v.to_dict() for k, v in self._patterns.items()}, f, indent=2)
            logger.debug(f"Saved {len(self._patterns)} command patterns")
        except Exception as e:
            logger.error(f"Error saving patterns: {e}")
    
    def add_command(
        self, 
        command: str, 
        natural_request: str, 
        success: bool,
        output: Optional[str] = None,
        error: Optional[str] = None,
        risk_level: int = 0
    ) -> None:
        """
        Add a command to the history.
        
        Args:
            command: The shell command executed
            natural_request: The natural language request
            success: Whether the command executed successfully
            output: Command output (if any)
            error: Command error (if any)
            risk_level: Risk level of the command
        """
        # Create and add the record
        record = CommandRecord(
            command=command,
            natural_request=natural_request,
            success=success,
            output=output,
            error=error,
            risk_level=risk_level
        )
        self._history.append(record)
        
        # Save the updated history
        self._save_history()
        
        # Update patterns if enabled
        preferences_manager = get_preferences_manager()
        if preferences_manager.preferences.context.auto_learn_patterns:
            self._update_patterns(record)
    
    def _extract_base_command(self, command: str) -> str:
        """
        Extract the base command without arguments.
        
        Args:
            command: The full command string
            
        Returns:
            The base command
        """
        # Extract the first word (command name)
        base = command.strip().split()[0]
        
        # For some commands, include the first argument if it's an operation
        if base in ["git", "docker", "npm", "pip", "apt", "apt-get"]:
            parts = command.strip().split()
            if len(parts) > 1 and not parts[1].startswith("-"):
                base = f"{base} {parts[1]}"
        
        return base
    
    def _update_patterns(self, record: CommandRecord) -> None:
        """
        Update command patterns based on a new record.
        
        Args:
            record: The command record
        """
        base_command = self._extract_base_command(record.command)
        
        if base_command in self._patterns:
            # Update existing pattern
            pattern = self._patterns[base_command]
            pattern.count += 1
            pattern.last_used = record.timestamp
            
            # Update success rate
            success_weight = 1.0 / pattern.count  # Weight of the new record
            pattern.success_rate = (
                (pattern.success_rate * (1 - success_weight)) + 
                (1.0 if record.success else 0.0) * success_weight
            )
        else:
            # Create new pattern
            self._patterns[base_command] = CommandPattern(
                base_command=base_command,
                count=1,
                success_rate=1.0 if record.success else 0.0
            )
        
        # Save updated patterns
        self._save_patterns()
    
    def get_recent_commands(self, limit: int = 10) -> List[CommandRecord]:
        """
        Get the most recent commands.
        
        Args:
            limit: Maximum number of commands to return
            
        Returns:
            List of recent CommandRecord objects
        """
        return self._history[-limit:]
    
    def get_command_frequency(self, command: str) -> int:
        """
        Get the frequency of a command.
        
        Args:
            command: The command to check
            
        Returns:
            The number of times the command has been executed
        """
        base_command = self._extract_base_command(command)
        pattern = self._patterns.get(base_command)
        return pattern.count if pattern else 0
    
    def get_command_success_rate(self, command: str) -> float:
        """
        Get the success rate of a command.
        
        Args:
            command: The command to check
            
        Returns:
            The success rate (0.0-1.0) or 0.0 if command not found
        """
        base_command = self._extract_base_command(command)
        pattern = self._patterns.get(base_command)
        return pattern.success_rate if pattern else 0.0
    
    def search_similar_command(self, request: str) -> Optional[str]:
        """
        Search for a similar command in history based on natural language request.
        
        Args:
            request: The natural language request
            
        Returns:
            A similar command if found, None otherwise
        """
        # Simple similarity: lowercase and remove punctuation
        request = re.sub(r'[^\w\s]', '', request.lower())
        
        for record in reversed(self._history):  # Start from most recent
            historical_request = re.sub(r'[^\w\s]', '', record.natural_request.lower())
            
            # Check for significant overlap in words
            request_words = set(request.split())
            historical_words = set(historical_request.split())
            
            # Calculate Jaccard similarity
            if request_words and historical_words:
                intersection = request_words.intersection(historical_words)
                union = request_words.union(historical_words)
                similarity = len(intersection) / len(union)
                
                # If similarity is high enough, return this command
                if similarity > 0.6:
                    return record.command
        
        return None
    
    def find_error_patterns(self, error: str) -> List[Tuple[str, str]]:
        """
        Find patterns in error messages and corresponding fixes.
        
        Args:
            error: The error message
            
        Returns:
            List of (failed_command, successful_fix) tuples
        """
        error_patterns = []
        
        # Find failed commands with this error
        for i, record in enumerate(self._history):
            if not record.success and record.error and error in record.error:
                # Look ahead for successful fixes
                for j in range(i+1, min(i+5, len(self._history))):
                    if self._history[j].success:
                        error_patterns.append((record.command, self._history[j].command))
                        break
        
        return error_patterns
    
    def get_common_command_contexts(self) -> Dict[str, List[str]]:
        """
        Get common command sequences or contexts.
        
        Returns:
            Dict mapping commands to commonly following commands
        """
        context_map = defaultdict(Counter)
        
        # Analyze command sequences
        for i in range(1, len(self._history)):
            prev_cmd = self._extract_base_command(self._history[i-1].command)
            curr_cmd = self._extract_base_command(self._history[i].command)
            context_map[prev_cmd][curr_cmd] += 1
        
        # Convert to more usable format
        result = {}
        for cmd, followers in context_map.items():
            # Get the most common followers
            result[cmd] = [cmd for cmd, count in followers.most_common(3)]
        
        return result

# Global history manager instance
history_manager = HistoryManager()
</file>

<file path="components/context/manager.py">
# angela/context/manager.py
"""
Context management for Angela CLI.
"""
import os
from pathlib import Path
from typing import Optional, Dict, Any, List, Set

from angela.constants import PROJECT_MARKERS
from angela.utils.logging import get_logger
from angela.api.context import get_file_detector

logger = get_logger(__name__)


class ContextManager:
    """
    Manages context information about the current environment.
    
    The context includes:
    - Current working directory
    - Project root (if detected)
    - Project type (if detected)
    - File details for current or specified path
    """
    
    def __init__(self):
        self._cwd: Path = Path.cwd()
        self._project_root: Optional[Path] = None
        self._project_type: Optional[str] = None
        self._current_file: Optional[Path] = None
        self._file_cache: Dict[str, Dict[str, Any]] = {}
        
        # Initialize context
        self.refresh_context()
    
    def refresh_context(self) -> None:
        """Refresh all context information."""
        self._update_cwd()
        self._detect_project_root()
        logger.debug(f"Context refreshed: cwd={self._cwd}, project_root={self._project_root}")
    
    def _update_cwd(self) -> None:
        """Update the current working directory."""
        self._cwd = Path.cwd()
    
    def _detect_project_root(self) -> None:
        """
        Detect the project root by looking for marker files.
        
        Traverses up from the current directory until a marker is found or
        the filesystem root is reached.
        """
        self._project_root = None
        self._project_type = None
        
        # Start from current directory
        current_dir = self._cwd
        
        # Walk up the directory tree
        while current_dir != current_dir.parent:  # Stop at filesystem root
            # Check for project markers
            for marker in PROJECT_MARKERS:
                marker_path = current_dir / marker
                if marker_path.exists():
                    self._project_root = current_dir
                    self._project_type = self._determine_project_type(marker)
                    logger.debug(f"Project detected: {self._project_type} at {self._project_root}")
                    return
            
            # Move up to parent directory
            current_dir = current_dir.parent
    
    def _determine_project_type(self, marker: str) -> str:
        """
        Determine the project type based on the marker file.
        
        Args:
            marker: The marker file that was found.
            
        Returns:
            A string representing the project type.
        """
        marker_to_type = {
            ".git": "git",
            "package.json": "node",
            "requirements.txt": "python",
            "Cargo.toml": "rust",
            "pom.xml": "maven",
            "build.gradle": "gradle",
            "Dockerfile": "docker",
            "docker-compose.yml": "docker-compose",
            "CMakeLists.txt": "cmake",
            "Makefile": "make",
        }
        
        return marker_to_type.get(marker, "unknown")
    
    def set_current_file(self, file_path: Path) -> None:
        """
        Set the current file being worked on.
        
        Args:
            file_path: The path to the current file.
        """
        self._current_file = file_path
    
    def get_file_info(self, path: Optional[Path] = None) -> Dict[str, Any]:
        """
        Get information about a file or the current file.
        
        Args:
            path: The path to get information about, or None to use the current file.
            
        Returns:
            A dictionary with file information, or an empty dict if no file is available.
        """
        file_path = path or self._current_file
        if not file_path:
            return {}
        
        # Check if we have cached information
        cache_key = str(file_path)
        if cache_key in self._file_cache:
            return self._file_cache[cache_key]
        
        # If file doesn't exist, return minimal info
        if not file_path.exists():
            return {
                "path": str(file_path),
                "exists": False,
                "name": file_path.name,
                "extension": file_path.suffix,
            }
        
        # Get basic file info
        stat = file_path.stat()
        
        # Get detailed file type info
        file_detector = get_file_detector()
        type_info = file_detector.detect_file_type(file_path)
        
        # Create the result
        result = {
            "path": str(file_path),
            "exists": True,
            "name": file_path.name,
            "extension": file_path.suffix,
            "size": stat.st_size,
            "modified": stat.st_mtime,
            "is_dir": file_path.is_dir(),
            "type": type_info["type"],
            "language": type_info["language"],
            "mime_type": type_info["mime_type"],
            "binary": type_info["binary"],
        }
        
        # Cache the result
        self._file_cache[cache_key] = result
        
        return result
    
    def get_directory_contents(self, path: Optional[Path] = None, include_hidden: bool = False) -> List[Dict[str, Any]]:
        """
        Get information about the contents of a directory.
        
        Args:
            path: The directory path to examine, or None to use the current directory.
            include_hidden: Whether to include hidden files (starting with .).
            
        Returns:
            A list of dictionaries with information about each item in the directory.
        """
        dir_path = path or self._cwd
        if not dir_path.is_dir():
            return []
        
        result = []
        
        try:
            for item in dir_path.iterdir():
                # Skip hidden files unless requested
                if not include_hidden and item.name.startswith('.'):
                    continue
                
                # Get information about this item
                item_info = self.get_file_info(item)
                result.append(item_info)
            
            # Sort by directories first, then by name
            result.sort(key=lambda x: (not x["is_dir"], x["name"].lower()))
            
            return result
        
        except Exception as e:
            logger.exception(f"Error getting directory contents for {dir_path}: {str(e)}")
            return []
    
    def get_file_preview(self, path: Optional[Path] = None, max_lines: int = 10) -> Optional[str]:
        """
        Get a preview of a file's contents.
        
        Args:
            path: The file path to preview, or None to use the current file.
            max_lines: Maximum number of lines to preview.
            
        Returns:
            A string with a preview of the file's contents, or None if not available.
        """
        file_path = path or self._current_file
        if not file_path or not file_path.is_file():
            return None
        
        file_detector = get_file_detector()
        return file_detector.get_content_preview(file_path, max_lines=max_lines)
    
    def find_files(
        self, 
        pattern: str, 
        base_dir: Optional[Path] = None, 
        max_depth: int = 10,
        include_hidden: bool = False
    ) -> List[Path]:
        """
        Find files matching a pattern.
        
        Args:
            pattern: The glob pattern to match.
            base_dir: The directory to start from, or None to use the current directory.
            max_depth: Maximum directory depth to search.
            include_hidden: Whether to include hidden files (starting with .).
            
        Returns:
            A list of paths matching the pattern.
        """
        start_dir = base_dir or self._cwd
        if not start_dir.is_dir():
            return []
        
        result = []
        
        try:
            # Use Path.glob for pattern matching
            for path in start_dir.glob(pattern):
                # Skip hidden files unless requested
                if not include_hidden and any(part.startswith('.') for part in path.parts):
                    continue
                
                result.append(path)
            
            return result
        
        except Exception as e:
            logger.exception(f"Error finding files with pattern {pattern}: {str(e)}")
            return []
    
    @property
    def cwd(self) -> Path:
        """Get the current working directory."""
        return self._cwd
    
    @property
    def project_root(self) -> Optional[Path]:
        """Get the detected project root."""
        return self._project_root
    
    @property
    def project_type(self) -> Optional[str]:
        """Get the detected project type."""
        return self._project_type
    
    @property
    def is_in_project(self) -> bool:
        """Check if the current directory is within a project."""
        return self._project_root is not None
    
    @property
    def relative_path(self) -> Optional[Path]:
        """Get the path relative to the project root."""
        if not self._project_root:
            return None
        
        return self._cwd.relative_to(self._project_root)
    
    @property
    def current_file(self) -> Optional[Path]:
        """Get the current file being worked on."""
        return self._current_file
    
    def get_context_dict(self) -> Dict[str, Any]:
        """
        Get a dictionary representation of the current context.
        
        Returns:
            A dictionary with context information.
        """
        context = {
            "cwd": str(self._cwd),
            "project_root": str(self._project_root) if self._project_root else None,
            "project_type": self._project_type,
            "is_in_project": self.is_in_project,
            "relative_path": str(self.relative_path) if self.relative_path else None,
        }
        
        # Add information about the current file if available
        if self._current_file:
            context["current_file"] = self.get_file_info(self._current_file)
        
        return context


# Global context manager instance
context_manager = ContextManager()
</file>

<file path="components/context/preferences.py">
# angela/context/preferences.py

import json
import os
from pathlib import Path
from typing import Dict, Any, Optional, List
from pydantic import BaseModel, Field

from angela.config import config_manager
from angela.constants import RISK_LEVELS
from angela.utils.logging import get_logger

logger = get_logger(__name__)

# Update in angela/components/context/preferences.py
class TrustPreferences(BaseModel):
    """Model for user trust preferences."""
    default_trust_level: int = Field(4, description="Default trust level (0-4)")
    auto_execute_safe: bool = Field(True, description="Auto-execute SAFE operations")
    auto_execute_low: bool = Field(True, description="Auto-execute LOW risk operations")
    auto_execute_medium: bool = Field(False, description="Auto-execute MEDIUM risk operations")
    auto_execute_high: bool = Field(False, description="Auto-execute HIGH risk operations")
    auto_execute_critical: bool = Field(False, description="Auto-execute CRITICAL risk operations")
    trusted_commands: List[str] = Field(default_factory=list, description="Commands that are always trusted")
    untrusted_commands: List[str] = Field(default_factory=list, description="Commands that require confirmation")
    command_rejections: Dict[str, int] = Field(default_factory=dict, description="Count of rejections for commands")

class UIPreferences(BaseModel):
    """Model for UI preferences."""
    show_command_preview: bool = Field(True, description="Show command preview before execution")
    show_impact_analysis: bool = Field(True, description="Show impact analysis for commands")
    use_rich_output: bool = Field(True, description="Use rich formatted output")
    verbose_feedback: bool = Field(True, description="Show detailed execution feedback")
    use_spinners: bool = Field(True, description="Show spinners for long-running operations")

class ContextPreferences(BaseModel):
    """Model for context preferences."""
    remember_session_context: bool = Field(True, description="Maintain context between commands")
    max_history_items: int = Field(50, description="Maximum number of history items to remember")
    auto_learn_patterns: bool = Field(True, description="Automatically learn command patterns")

class UserPreferences(BaseModel):
    """User preferences model."""
    trust: TrustPreferences = Field(default_factory=TrustPreferences, description="Trust settings")
    ui: UIPreferences = Field(default_factory=UIPreferences, description="UI settings")
    context: ContextPreferences = Field(default_factory=ContextPreferences, description="Context settings")

class PreferencesManager:
    """Manager for user preferences."""
    
    def __init__(self):
        """Initialize the preferences manager."""
        self._prefs = UserPreferences()
        self._prefs_file = config_manager.CONFIG_DIR / "preferences.json"
        self._load_preferences()
    
    def _load_preferences(self) -> None:
        """Load preferences from file."""
        try:
            if self._prefs_file.exists():
                with open(self._prefs_file, "r") as f:
                    data = json.load(f)
                    self._prefs = UserPreferences.parse_obj(data)
                logger.debug(f"Loaded preferences from {self._prefs_file}")
            else:
                logger.debug("No preferences file found, using defaults")
                self._save_preferences()  # Create the file with defaults
        except Exception as e:
            logger.error(f"Error loading preferences: {e}")
    
    def _save_preferences(self) -> None:
        """Save preferences to file."""
        try:
            with open(self._prefs_file, "w") as f:
                json.dump(self._prefs.dict(), f, indent=2)
            logger.debug(f"Saved preferences to {self._prefs_file}")
        except Exception as e:
            logger.error(f"Error saving preferences: {e}")
    
    def update_preferences(self, **kwargs) -> None:
        """Update preferences with provided values."""
        if not kwargs:
            return
            
        # Handle nested preferences
        for section in ["trust", "ui", "context"]:
            section_data = kwargs.pop(section, None)
            if section_data and isinstance(section_data, dict):
                for k, v in section_data.items():
                    setattr(getattr(self._prefs, section), k, v)
        
        # Handle top-level preferences
        for k, v in kwargs.items():
            setattr(self._prefs, k, v)
        
        self._save_preferences()
    
    def should_auto_execute(self, risk_level: int, command: str) -> bool:
        """
        Determine if a command should be auto-executed based on risk level
        and user preferences.
        
        Args:
            risk_level: The risk level of the command
            command: The command string
            
        Returns:
            True if should auto-execute, False if confirmation needed
        """
        # Check if the command is explicitly trusted or untrusted
        if command in self._prefs.trust.trusted_commands:
            return True
        if command in self._prefs.trust.untrusted_commands:
            return False
        
        # Check based on risk level
        if risk_level == RISK_LEVELS["SAFE"]:
            return self._prefs.trust.auto_execute_safe
        elif risk_level == RISK_LEVELS["LOW"]:
            return self._prefs.trust.auto_execute_low
        elif risk_level == RISK_LEVELS["MEDIUM"]:
            return self._prefs.trust.auto_execute_medium
        elif risk_level == RISK_LEVELS["HIGH"]:
            return self._prefs.trust.auto_execute_high
        elif risk_level == RISK_LEVELS["CRITICAL"]:
            return self._prefs.trust.auto_execute_critical
        
        # Default to require confirmation
        return False
    
    def add_trusted_command(self, command: str) -> None:
        """Add a command to the trusted commands list."""
        if command not in self._prefs.trust.trusted_commands:
            self._prefs.trust.trusted_commands.append(command)
            # Remove from untrusted if present
            if command in self._prefs.trust.untrusted_commands:
                self._prefs.trust.untrusted_commands.remove(command)
            self._save_preferences()
    
    def add_untrusted_command(self, command: str) -> None:
        """Add a command to the untrusted commands list."""
        if command not in self._prefs.trust.untrusted_commands:
            self._prefs.trust.untrusted_commands.append(command)
            # Remove from trusted if present
            if command in self._prefs.trust.trusted_commands:
                self._prefs.trust.trusted_commands.remove(command)
            self._save_preferences()

    def get_command_rejection_count(self, command: str) -> int:
        """
        Get the number of times a user has rejected auto-execution for a command.
        
        Args:
            command: The command to check
            
        Returns:
            Number of rejections
        """
        if not hasattr(self._prefs.trust, 'command_rejections'):
            self._prefs.trust.command_rejections = {}
        
        # Extract base command for more general tracking
        base_command = command.split()[0] if command else ""
        return self._prefs.trust.command_rejections.get(base_command, 0)
    
    def increment_command_rejection_count(self, command: str) -> None:
        """
        Increment the rejection count for a command.
        
        Args:
            command: The command to increment rejection count for
        """
        if not hasattr(self._prefs.trust, 'command_rejections'):
            self._prefs.trust.command_rejections = {}
        
        # Extract base command for more general tracking
        base_command = command.split()[0] if command else ""
        current_count = self._prefs.trust.command_rejections.get(base_command, 0)
        self._prefs.trust.command_rejections[base_command] = current_count + 1
        self._save_preferences()

    
    @property
    def preferences(self) -> UserPreferences:
        """Get the current preferences."""
        return self._prefs

# Create a global instance of the preferences manager
preferences_manager = PreferencesManager()
</file>

<file path="components/context/project_inference.py">
# angela/context/project_inference.py

import os
import glob
import json
import re
import asyncio
from pathlib import Path
from typing import Dict, Any, List, Set, Optional, Tuple

from angela.utils.logging import get_logger

logger = get_logger(__name__)

class ProjectInference:
    """
    Advanced project type and structure inference.
    
    This class provides:
    1. Detection of project type based on files and structure
    2. Inference of project dependencies
    3. Identification of important project files
    4. Framework and technology detection
    """
    
    # Project type signatures
    PROJECT_SIGNATURES = {
        "python": {
            "files": ["requirements.txt", "setup.py", "pyproject.toml", "Pipfile"],
            "directories": ["venv", ".venv", "env", ".env"],
            "extensions": [".py", ".pyi", ".pyx"]
        },
        "node": {
            "files": ["package.json", "package-lock.json", "yarn.lock", "node_modules"],
            "extensions": [".js", ".jsx", ".ts", ".tsx"]
        },
        "rust": {
            "files": ["Cargo.toml", "Cargo.lock"],
            "directories": ["src", "target"],
            "extensions": [".rs"]
        },
        "go": {
            "files": ["go.mod", "go.sum"],
            "directories": ["pkg", "cmd"],
            "extensions": [".go"]
        },
        "java": {
            "files": ["pom.xml", "build.gradle", "gradlew", "settings.gradle"],
            "directories": ["src/main/java", "target", "build"],
            "extensions": [".java", ".class", ".jar"]
        },
        "dotnet": {
            "files": [".csproj", ".sln", "packages.config"],
            "directories": ["bin", "obj"],
            "extensions": [".cs", ".vb", ".fs"]
        },
        "php": {
            "files": ["composer.json", "composer.lock"],
            "directories": ["vendor"],
            "extensions": [".php"]
        },
        "ruby": {
            "files": ["Gemfile", "Gemfile.lock", "Rakefile"],
            "directories": ["lib", "bin"],
            "extensions": [".rb"]
        },
        "flutter": {
            "files": ["pubspec.yaml", "pubspec.lock"],
            "directories": ["lib", "android", "ios"],
            "extensions": [".dart"]
        },
        "docker": {
            "files": ["Dockerfile", "docker-compose.yml", "docker-compose.yaml"],
            "extensions": []
        },
        "web": {
            "files": ["index.html", "style.css", "script.js"],
            "extensions": [".html", ".htm", ".css", ".js"]
        }
    }
    
    # Framework signatures
    FRAMEWORK_SIGNATURES = {
        "python": {
            "django": ["manage.py", "settings.py", "wsgi.py", "asgi.py"],
            "flask": ["app.py", "wsgi.py", "requirements.txt"],
            "fastapi": ["main.py", "app.py", "api.py"],
            "tornado": ["server.py", "app.py"],
            "sqlalchemy": ["models.py", "database.py"],
            "pytest": ["conftest.py", "test_*.py", "pytest.ini"],
            "jupyter": [".ipynb"],
            "pandas": ["*.csv", "*.xlsx"],
            "tensorflow": ["model.h5", "keras"]
        },
        "node": {
            "react": ["react", "jsx", "tsx", "components"],
            "vue": ["vue.config.js", "Vue", "components"],
            "angular": ["angular.json", "app.module.ts"],
            "express": ["app.js", "routes", "middleware"],
            "nextjs": ["next.config.js", "pages", "public"],
            "gatsby": ["gatsby-config.js", "gatsby-node.js"],
            "electron": ["electron", "main.js", "renderer.js"]
        },
        "web": {
            "bootstrap": ["bootstrap"],
            "tailwind": ["tailwind.config.js", "tailwindcss"],
            "jquery": ["jquery"]
        }
    }
    
    def __init__(self):
        """Initialize the project inference system."""
        self._logger = logger
        self._cache = {}  # Cache inference results
    
    async def infer_project_info(self, project_root: Path) -> Dict[str, Any]:
        """
        Infer detailed information about a project.
        
        Args:
            project_root: The project root directory
            
        Returns:
            Dictionary with project information
        """
        # Check cache first
        cache_key = str(project_root)
        if cache_key in self._cache:
            return self._cache[cache_key]
        
        self._logger.info(f"Inferring project information for {project_root}")
        
        # Basic project type detection
        project_type = await self._detect_project_type(project_root)
        
        # Get more detailed information
        result = {
            "project_root": str(project_root),
            "project_type": project_type,
            "detected_files": await self._list_important_files(project_root, project_type),
            "detected_frameworks": await self._detect_frameworks(project_root, project_type),
            "dependencies": await self._detect_dependencies(project_root, project_type),
            "structure": await self._analyze_project_structure(project_root, project_type)
        }
        
        # Cache the result
        self._cache[cache_key] = result
        
        return result
    
    async def _detect_project_type(self, project_root: Path) -> str:
        """
        Detect the primary type of a project.
        
        Args:
            project_root: The project root directory
            
        Returns:
            Project type string
        """
        # Count signature matches for each project type
        scores = {}
        
        for project_type, signature in self.PROJECT_SIGNATURES.items():
            score = 0
            
            # Check for signature files
            for file_pattern in signature.get("files", []):
                # Handle glob patterns
                if "*" in file_pattern:
                    matches = list(project_root.glob(file_pattern))
                    score += len(matches)
                else:
                    if (project_root / file_pattern).exists():
                        score += 3  # Higher weight for exact file matches
                        
            # Check for signature directories
            for dir_pattern in signature.get("directories", []):
                # Handle glob patterns
                if "*" in dir_pattern:
                    matches = list(project_root.glob(dir_pattern))
                    score += len(matches)
                else:
                    if (project_root / dir_pattern).exists() and (project_root / dir_pattern).is_dir():
                        score += 2  # Medium weight for directory matches
            
            # Check for file extensions
            for ext in signature.get("extensions", []):
                # Count files with this extension
                count = len(list(project_root.glob(f"**/*{ext}")))
                score += min(count, 10)  # Cap at 10 to avoid skewing
            
            scores[project_type] = score
        
        # Get the project type with the highest score
        if not scores:
            return "unknown"
        
        # If multiple project types have similar scores, handle mixed projects
        max_score = max(scores.values())
        candidates = [pt for pt, score in scores.items() if score >= max_score * 0.7]
        
        if len(candidates) > 1:
            # Special case: For web + node, prefer node as it's more specific
            if "web" in candidates and "node" in candidates:
                return "node"
            
            # Return composite project type for truly mixed projects
            return "+".join(candidates)
        
        # Return the highest scoring project type
        return max(scores.items(), key=lambda x: x[1])[0]
    
    async def _list_important_files(self, project_root: Path, project_type: str) -> List[Dict[str, Any]]:
        """
        List important files in the project.
        
        Args:
            project_root: The project root directory
            project_type: The detected project type
            
        Returns:
            List of important file information
        """
        important_files = []
        
        # Handle composite project types
        if "+" in project_type:
            types = project_type.split("+")
            for pt in types:
                important_files.extend(await self._list_important_files(project_root, pt))
            return important_files
        
        # Get signatures for this project type
        signature = self.PROJECT_SIGNATURES.get(project_type, {})
        
        # Check for signature files
        for file_pattern in signature.get("files", []):
            # Handle glob patterns
            if "*" in file_pattern:
                for file_path in project_root.glob(file_pattern):
                    if file_path.is_file():
                        important_files.append({
                            "path": str(file_path.relative_to(project_root)),
                            "type": "signature_file",
                            "project_type": project_type
                        })
            else:
                file_path = project_root / file_pattern
                if file_path.exists() and file_path.is_file():
                    important_files.append({
                        "path": str(file_path.relative_to(project_root)),
                        "type": "signature_file",
                        "project_type": project_type
                    })
        
        # Add common important files for any project
        common_files = ["README.md", "LICENSE", ".gitignore", "CHANGELOG.md"]
        for file_name in common_files:
            file_path = project_root / file_name
            if file_path.exists() and file_path.is_file():
                important_files.append({
                    "path": file_name,
                    "type": "documentation"
                })
        
        # Add project-specific logic
        if project_type == "python":
            # Look for main Python modules
            for file_path in project_root.glob("**/*.py"):
                if file_path.name == "__main__.py" or file_path.name == "main.py":
                    important_files.append({
                        "path": str(file_path.relative_to(project_root)),
                        "type": "entry_point"
                    })
        
        elif project_type == "node":
            # Look for main JavaScript/TypeScript files
            for pattern in ["index.js", "main.js", "server.js", "app.js", "index.ts", "main.ts"]:
                for file_path in project_root.glob(f"**/{pattern}"):
                    # Skip node_modules
                    if "node_modules" not in str(file_path):
                        important_files.append({
                            "path": str(file_path.relative_to(project_root)),
                            "type": "entry_point"
                        })
        
        # Add more project-specific logic as needed
        
        return important_files
    
    async def _detect_frameworks(self, project_root: Path, project_type: str) -> Dict[str, float]:
        """
        Detect frameworks and technologies used in the project.
        
        Args:
            project_root: The project root directory
            project_type: The detected project type
            
        Returns:
            Dictionary of framework names and confidence scores
        """
        frameworks = {}
        
        # Handle composite project types
        if "+" in project_type:
            types = project_type.split("+")
            for pt in types:
                frameworks.update(await self._detect_frameworks(project_root, pt))
            return frameworks
        
        # Get framework signatures for this project type
        if project_type in self.FRAMEWORK_SIGNATURES:
            for framework, patterns in self.FRAMEWORK_SIGNATURES[project_type].items():
                matches = 0
                total_patterns = len(patterns)
                
                for pattern in patterns:
                    # Handle glob patterns
                    if "*" in pattern:
                        files = list(project_root.glob(f"**/{pattern}"))
                        if files:
                            matches += 1
                    else:
                        # Check for exact file match
                        for file_path in project_root.glob("**/*"):
                            if pattern in file_path.name or pattern in str(file_path):
                                matches += 1
                                break
                
                # Calculate confidence score
                if total_patterns > 0 and matches > 0:
                    confidence = min(matches / total_patterns, 1.0)
                    if confidence >= 0.3:  # Threshold for reporting
                        frameworks[framework] = confidence
        
        # Check dependencies if we have appropriate files
        if project_type == "python":
            requirements_path = project_root / "requirements.txt"
            if requirements_path.exists():
                frameworks.update(await self._analyze_python_requirements(requirements_path))
            
        elif project_type == "node":
            package_json_path = project_root / "package.json"
            if package_json_path.exists():
                frameworks.update(await self._analyze_package_json(package_json_path))
        
        return frameworks
    
    async def _detect_dependencies(self, project_root: Path, project_type: str) -> List[Dict[str, Any]]:
        """
        Detect dependencies of the project.
        
        Args:
            project_root: The project root directory
            project_type: The detected project type
            
        Returns:
            List of dependencies with metadata
        """
        dependencies = []
        
        # Handle composite project types
        if "+" in project_type:
            types = project_type.split("+")
            for pt in types:
                dependencies.extend(await self._detect_dependencies(project_root, pt))
            return dependencies
        
        # Extract dependencies based on project type
        if project_type == "python":
            # Check requirements.txt
            requirements_path = project_root / "requirements.txt"
            if requirements_path.exists():
                dependencies.extend(await self._extract_python_requirements(requirements_path))
            
            # Check setup.py
            setup_py_path = project_root / "setup.py"
            if setup_py_path.exists():
                dependencies.extend(await self._extract_python_setup_dependencies(setup_py_path))
                
            # Check pyproject.toml
            pyproject_path = project_root / "pyproject.toml"
            if pyproject_path.exists():
                dependencies.extend(await self._extract_pyproject_dependencies(pyproject_path))
                
        elif project_type == "node":
            # Check package.json
            package_json_path = project_root / "package.json"
            if package_json_path.exists():
                dependencies.extend(await self._extract_node_dependencies(package_json_path))
                
        # Add more project types as needed
        
        return dependencies
    
    async def _analyze_project_structure(self, project_root: Path, project_type: str) -> Dict[str, Any]:
        """
        Analyze the structure of the project.
        
        Args:
            project_root: The project root directory
            project_type: The detected project type
            
        Returns:
            Dictionary with structure information
        """
        # Count files by type
        file_counts = {}
        
        # Walk the directory tree
        for root, dirs, files in os.walk(project_root):
            # Skip hidden directories and common exclude patterns
            dirs[:] = [d for d in dirs if not d.startswith(".") and d not in ["node_modules", "venv", "__pycache__", "build", "dist"]]
            
            for file in files:
                # Get file extension
                _, ext = os.path.splitext(file)
                if ext:
                    if ext not in file_counts:
                        file_counts[ext] = 0
                    file_counts[ext] += 1
        
        # Identify main directories
        main_dirs = []
        for item in project_root.iterdir():
            if item.is_dir() and not item.name.startswith(".") and item.name not in ["node_modules", "venv", "__pycache__"]:
                main_dirs.append({
                    "name": item.name,
                    "path": str(item.relative_to(project_root)),
                    "file_count": sum(1 for _ in item.glob("**/*") if _.is_file())
                })
        
        # Sort by file count
        main_dirs.sort(key=lambda x: x["file_count"], reverse=True)
        
        return {
            "file_counts": file_counts,
            "main_directories": main_dirs[:5],  # Top 5 directories
            "total_files": sum(file_counts.values()),
            "directory_structure": await self._generate_directory_structure(project_root)
        }
    
    async def _generate_directory_structure(self, project_root: Path, max_depth: int = 3) -> Dict[str, Any]:
        """
        Generate a hierarchical representation of the directory structure.
        
        Args:
            project_root: The project root directory
            max_depth: Maximum depth to traverse
            
        Returns:
            Dictionary representing the directory structure
        """
        def _build_tree(path: Path, current_depth: int) -> Dict[str, Any]:
            if current_depth > max_depth:
                return {"type": "directory", "name": path.name, "truncated": True}
            
            result = {"type": "directory", "name": path.name, "children": []}
            
            try:
                # List directory contents
                items = list(path.iterdir())
                
                # Skip large directories
                if len(items) > 50:
                    result["children"].append({"type": "info", "name": f"{len(items)} items (too many to show)"})
                    return result
                
                # Add directories first
                for item in sorted([i for i in items if i.is_dir()], key=lambda x: x.name):
                    # Skip hidden directories and common excludes
                    if item.name.startswith(".") or item.name in ["node_modules", "venv", "__pycache__", "build", "dist"]:
                        continue
                    
                    child = _build_tree(item, current_depth + 1)
                    result["children"].append(child)
                
                # Then add files
                for item in sorted([i for i in items if i.is_file()], key=lambda x: x.name):
                    # Skip hidden files
                    if item.name.startswith("."):
                        continue
                    
                    result["children"].append({"type": "file", "name": item.name})
                
                return result
            except PermissionError:
                result["children"].append({"type": "error", "name": "Permission denied"})
                return result
        
        return _build_tree(project_root, 0)
    
    async def _analyze_python_requirements(self, requirements_path: Path) -> Dict[str, float]:
        """
        Analyze Python requirements.txt for frameworks.
        
        Args:
            requirements_path: Path to requirements.txt
            
        Returns:
            Dictionary of framework names and confidence scores
        """
        frameworks = {}
        
        # Framework indicators in requirements
        framework_indicators = {
            "django": "django",
            "flask": "flask",
            "fastapi": "fastapi",
            "tornado": "tornado",
            "sqlalchemy": "sqlalchemy",
            "pytest": "pytest",
            "pandas": "pandas",
            "numpy": "numpy",
            "tensorflow": "tensorflow",
            "pytorch": "torch",
            "jupyter": "jupyter"
        }
        
        try:
            with open(requirements_path, "r") as f:
                requirements = f.read()
                
            for framework, indicator in framework_indicators.items():
                pattern = rf"\b{re.escape(indicator)}[>=<~!]"
                if re.search(pattern, requirements, re.IGNORECASE):
                    frameworks[framework] = 1.0  # High confidence for direct dependencies
        except Exception as e:
            self._logger.error(f"Error analyzing requirements.txt: {str(e)}")
        
        return frameworks
    
    async def _analyze_package_json(self, package_json_path: Path) -> Dict[str, float]:
        """
        Analyze package.json for frameworks.
        
        Args:
            package_json_path: Path to package.json
            
        Returns:
            Dictionary of framework names and confidence scores
        """
        frameworks = {}
        
        # Framework indicators in package.json
        framework_indicators = {
            "react": ["react", "react-dom"],
            "vue": ["vue"],
            "angular": ["@angular/core"],
            "express": ["express"],
            "nextjs": ["next"],
            "gatsby": ["gatsby"],
            "electron": ["electron"]
        }
        
        try:
            with open(package_json_path, "r") as f:
                package_data = json.load(f)
            
            # Check dependencies and devDependencies
            all_deps = {}
            all_deps.update(package_data.get("dependencies", {}))
            all_deps.update(package_data.get("devDependencies", {}))
            
            for framework, indicators in framework_indicators.items():
                if any(dep in all_deps for dep in indicators):
                    frameworks[framework] = 1.0  # High confidence for direct dependencies
        except Exception as e:
            self._logger.error(f"Error analyzing package.json: {str(e)}")
        
        return frameworks
    
    async def _extract_python_requirements(self, requirements_path: Path) -> List[Dict[str, Any]]:
        """
        Extract dependencies from requirements.txt.
        
        Args:
            requirements_path: Path to requirements.txt
            
        Returns:
            List of dependencies
        """
        dependencies = []
        
        try:
            with open(requirements_path, "r") as f:
                for line in f:
                    line = line.strip()
                    
                    # Skip comments and empty lines
                    if not line or line.startswith("#"):
                        continue
                    
                    # Parse requirement
                    parts = re.split(r"[>=<~!]", line, 1)
                    name = parts[0].strip()
                    version_spec = line[len(name):].strip() if len(parts) > 1 else ""
                    
                    dependencies.append({
                        "name": name,
                        "version_spec": version_spec,
                        "type": "python",
                        "source": "requirements.txt"
                    })
        except Exception as e:
            self._logger.error(f"Error extracting Python requirements: {str(e)}")
        
        return dependencies
    
    async def _extract_python_setup_dependencies(self, setup_py_path: Path) -> List[Dict[str, Any]]:
        """
        Extract dependencies from setup.py.
        
        Args:
            setup_py_path: Path to setup.py
            
        Returns:
            List of dependencies
        """
        dependencies = []
        
        try:
            with open(setup_py_path, "r") as f:
                setup_content = f.read()
            
            # Look for install_requires
            install_requires_match = re.search(r"install_requires\s*=\s*\[(.*?)\]", setup_content, re.DOTALL)
            if install_requires_match:
                requires_text = install_requires_match.group(1)
                
                # Extract individual requirements
                for req_match in re.finditer(r"[\"']([^\"']+)[\"']", requires_text):
                    req = req_match.group(1)
                    
                    # Parse requirement
                    parts = re.split(r"[>=<~!]", req, 1)
                    name = parts[0].strip()
                    version_spec = req[len(name):].strip() if len(parts) > 1 else ""
                    
                    dependencies.append({
                        "name": name,
                        "version_spec": version_spec,
                        "type": "python",
                        "source": "setup.py"
                    })
        except Exception as e:
            self._logger.error(f"Error extracting setup.py dependencies: {str(e)}")
        
        return dependencies
    
    async def _extract_pyproject_dependencies(self, pyproject_path: Path) -> List[Dict[str, Any]]:
        """
        Extract dependencies from pyproject.toml.
        
        Args:
            pyproject_path: Path to pyproject.toml
            
        Returns:
            List of dependencies
        """
        dependencies = []
        
        try:
            # Simple parsing of dependencies from pyproject.toml
            with open(pyproject_path, "r") as f:
                content = f.read()
            
            # Look for dependencies section
            deps_match = re.search(r"dependencies\s*=\s*\[(.*?)\]", content, re.DOTALL)
            if deps_match:
                deps_text = deps_match.group(1)
                
                # Extract individual dependencies
                for dep_match in re.finditer(r"[\"']([^\"']+)[\"']", deps_text):
                    dep = dep_match.group(1)
                    
                    # Parse requirement
                    parts = re.split(r"[>=<~!]", dep, 1)
                    name = parts[0].strip()
                    version_spec = dep[len(name):].strip() if len(parts) > 1 else ""
                    
                    dependencies.append({
                        "name": name,
                        "version_spec": version_spec,
                        "type": "python",
                        "source": "pyproject.toml"
                    })
        except Exception as e:
            self._logger.error(f"Error extracting pyproject.toml dependencies: {str(e)}")
        
        return dependencies
    
    async def _extract_node_dependencies(self, package_json_path: Path) -> List[Dict[str, Any]]:
        """
        Extract dependencies from package.json.
        
        Args:
            package_json_path: Path to package.json
            
        Returns:
            List of dependencies
        """
        dependencies = []
        
        try:
            with open(package_json_path, "r") as f:
                package_data = json.load(f)
            
            # Process dependencies
            for dep_type in ["dependencies", "devDependencies"]:
                deps = package_data.get(dep_type, {})
                for name, version in deps.items():
                    dependencies.append({
                        "name": name,
                        "version_spec": version,
                        "type": "node",
                        "dev": dep_type == "devDependencies",
                        "source": "package.json"
                    })
        except Exception as e:
            self._logger.error(f"Error extracting Node.js dependencies: {str(e)}")
        
        return dependencies

# Global project inference instance
project_inference = ProjectInference()
</file>

<file path="components/context/project_state_analyzer.py">
# angela/context/project_state_analyzer.py
"""
Project state analysis for Angela CLI.

This module extends Angela's context awareness by providing detailed information
about the current state of the project, including Git status, pending migrations,
test coverage, and build health.
"""
import os
import re
import json
import asyncio
import subprocess
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple, Set, Union
from datetime import datetime

from angela.utils.logging import get_logger
from angela.api.context import get_project_inference
from angela.api.execution import get_execution_engine

logger = get_logger(__name__)

class ProjectStateAnalyzer:
    """
    Project state analyzer that provides detailed information about the
    current state of the project beyond basic type detection.
    
    This class tracks:
    1. Git state (current branch, pending changes, stashes)
    2. Test coverage status
    3. Build health
    4. Pending migrations
    5. Dependency health (outdated packages, vulnerabilities)
    6. Linting/code quality issues
    7. TODO/FIXME comments
    """
    
    def __init__(self):
        """Initialize the project state analyzer."""
        self._logger = logger
        self._cache = {}  # Cache for state information
        self._cache_valid_time = 60  # Seconds before cache is invalid
        self._last_analysis_time = {}  # Timestamp of last analysis per project
    
    async def get_project_state(self, project_root: Union[str, Path]) -> Dict[str, Any]:
        """
        Get detailed information about the current state of the project.
        
        Args:
            project_root: Path to the project root directory
            
        Returns:
            Dictionary with project state information
        """
        path_obj = Path(project_root)
        path_str = str(path_obj)
        
        # Check if we have recent cached information
        if path_str in self._cache:
            # Check if the cache is still valid
            cache_age = datetime.now().timestamp() - self._last_analysis_time.get(path_str, 0)
            if cache_age < self._cache_valid_time:
                self._logger.debug(f"Using cached project state for {path_str} (age: {cache_age:.1f}s)")
                return self._cache[path_str]
        
        self._logger.info(f"Analyzing project state for {path_str}")
        
        # First, get basic project information
        try:
            project_inference = get_project_inference()
            basic_info = await project_inference.infer_project_info(path_obj)
        except Exception as e:
            self._logger.error(f"Error getting basic project info: {str(e)}")
            basic_info = {"project_type": "unknown"}
        
        # Initialize state information
        state_info = {
            "project_root": path_str,
            "project_type": basic_info.get("project_type", "unknown"),
            "analysis_time": datetime.now().isoformat(),
            "git_state": {},
            "test_status": {},
            "build_status": {},
            "migrations": {},
            "dependencies": {},
            "code_quality": {},
            "todo_items": []
        }
        
        # Analyze different aspects of the project state in parallel
        tasks = [
            self._analyze_git_state(path_obj),
            self._analyze_test_status(path_obj, basic_info.get("project_type", "unknown")),
            self._analyze_build_status(path_obj, basic_info.get("project_type", "unknown")),
            self._analyze_migrations(path_obj, basic_info.get("project_type", "unknown")),
            self._analyze_dependencies(path_obj, basic_info.get("project_type", "unknown")),
            self._analyze_code_quality(path_obj, basic_info.get("project_type", "unknown")),
            self._find_todo_items(path_obj)
        ]
        
        # Execute all tasks
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Process results
        if not isinstance(results[0], Exception):
            state_info["git_state"] = results[0] or {}
        if not isinstance(results[1], Exception):
            state_info["test_status"] = results[1] or {}
        if not isinstance(results[2], Exception):
            state_info["build_status"] = results[2] or {}
        if not isinstance(results[3], Exception):
            state_info["migrations"] = results[3] or {}
        if not isinstance(results[4], Exception):
            state_info["dependencies"] = results[4] or {}
        if not isinstance(results[5], Exception):
            state_info["code_quality"] = results[5] or {}
        if not isinstance(results[6], Exception):
            state_info["todo_items"] = results[6] or []
        
        # Cache the result
        self._cache[path_str] = state_info
        self._last_analysis_time[path_str] = datetime.now().timestamp()
        
        return state_info
    
    async def _analyze_git_state(self, project_root: Path) -> Dict[str, Any]:
        """
        Analyze the Git state of the project.
        
        Args:
            project_root: Path to the project root
            
        Returns:
            Dictionary with Git state information
        """
        result = {
            "is_git_repo": False,
            "current_branch": None,
            "has_changes": False,
            "untracked_files": [],
            "modified_files": [],
            "staged_files": [],
            "stashes": [],
            "remote_state": {},
            "recent_commits": []
        }
        
        # Check if this is a Git repository
        git_dir = project_root / ".git"
        if not git_dir.exists():
            return result
        
        result["is_git_repo"] = True
        
        try:
            execution_engine = get_execution_engine()
            
            # Get current branch
            stdout, stderr, return_code = await execution_engine.execute_command(
                command=f"cd {project_root} && git branch --show-current",
                check_safety=False
            )
            
            if return_code == 0 and stdout.strip():
                result["current_branch"] = stdout.strip()
            
            # Get git status
            stdout, stderr, return_code = await execution_engine.execute_command(
                command=f"cd {project_root} && git status --porcelain",
                check_safety=False
            )
            
            if return_code == 0:
                result["has_changes"] = bool(stdout.strip())
                
                # Parse status output
                for line in stdout.splitlines():
                    if not line.strip():
                        continue
                    
                    # Parse status code and filename
                    status_code = line[:2]
                    file_path = line[3:].strip()
                    
                    if status_code.startswith('??'):
                        # Untracked file
                        result["untracked_files"].append(file_path)
                    elif status_code.startswith('M'):
                        # Modified file
                        result["modified_files"].append(file_path)
                    elif status_code.startswith('A') or status_code.startswith('R'):
                        # Staged file (added or renamed)
                        result["staged_files"].append(file_path)
            
            # Get stash list
            stdout, stderr, return_code = await execution_engine.execute_command(
                command=f"cd {project_root} && git stash list",
                check_safety=False
            )
            
            if return_code == 0 and stdout.strip():
                # Parse stash list
                for line in stdout.splitlines():
                    match = re.match(r'stash@{(\d+)}: (.*)', line)
                    if match:
                        stash_id = match.group(1)
                        stash_description = match.group(2)
                        result["stashes"].append({
                            "id": stash_id,
                            "description": stash_description
                        })
            
            # Check remote state
            stdout, stderr, return_code = await execution_engine.execute_command(
                command=f"cd {project_root} && git status -sb",
                check_safety=False
            )
            
            if return_code == 0 and stdout.strip():
                # Parse first line for branch and tracking info
                first_line = stdout.splitlines()[0]
                
                # Check for ahead/behind information
                ahead_match = re.search(r'ahead (\d+)', first_line)
                behind_match = re.search(r'behind (\d+)', first_line)
                
                result["remote_state"] = {
                    "tracking": "origin" in first_line,
                    "ahead": int(ahead_match.group(1)) if ahead_match else 0,
                    "behind": int(behind_match.group(1)) if behind_match else 0
                }
            
            # Get recent commits
            stdout, stderr, return_code = await execution_engine.execute_command(
                command=f"cd {project_root} && git log -n 5 --pretty=format:'%h|%an|%s|%cr'",
                check_safety=False
            )
            
            if return_code == 0 and stdout.strip():
                for line in stdout.splitlines():
                    parts = line.split('|', 3)
                    if len(parts) == 4:
                        commit_hash, author, message, time = parts
                        result["recent_commits"].append({
                            "hash": commit_hash,
                            "author": author,
                            "message": message,
                            "time": time
                        })
            
            return result
            
        except Exception as e:
            self._logger.error(f"Error analyzing Git state: {str(e)}")
            return result
    
    async def _analyze_test_status(self, project_root: Path, project_type: str) -> Dict[str, Any]:
        """
        Analyze the test status of the project.
        
        Args:
            project_root: Path to the project root
            project_type: Type of the project
            
        Returns:
            Dictionary with test status information
        """
        result = {
            "test_framework_detected": False,
            "framework": None,
            "test_files_count": 0,
            "last_run": None,
            "coverage": None,
            "failing_tests": [],
            "performance_issues": []
        }
        
        # Look for test frameworks based on project type
        if "python" in project_type:
            # Check for pytest
            pytest_file = project_root / "pytest.ini"
            conftest_file = project_root / "conftest.py"
            
            if pytest_file.exists() or conftest_file.exists() or list(project_root.glob("**/test_*.py")):
                result["test_framework_detected"] = True
                result["framework"] = "pytest"
                
                # Count test files
                test_files = list(project_root.glob("**/test_*.py"))
                result["test_files_count"] = len(test_files)
                
                # Look for coverage file
                coverage_file = project_root / ".coverage"
                coverage_xml = project_root / "coverage.xml"
                
                if coverage_file.exists() or coverage_xml.exists():
                    # Try to get coverage info
                    if coverage_xml.exists():
                        try:
                            import xml.etree.ElementTree as ET
                            tree = ET.parse(coverage_xml)
                            root = tree.getroot()
                            
                            # Get coverage percentage
                            coverage_attr = root.get('line-rate')
                            if coverage_attr:
                                coverage_percentage = float(coverage_attr) * 100
                                result["coverage"] = {
                                    "percentage": coverage_percentage,
                                    "report_path": str(coverage_xml.relative_to(project_root))
                                }
                        except Exception as e:
                            self._logger.error(f"Error parsing coverage XML: {str(e)}")
            
            # Check for unittest
            elif list(project_root.glob("**/test*.py")):
                result["test_framework_detected"] = True
                result["framework"] = "unittest"
                
                # Count test files
                test_files = list(project_root.glob("**/test*.py"))
                result["test_files_count"] = len(test_files)
        
        elif "node" in project_type or "javascript" in project_type or "typescript" in project_type:
            # Check for Jest
            jest_config = project_root / "jest.config.js"
            package_json = project_root / "package.json"
            
            if jest_config.exists() or (package_json.exists() and "jest" in open(package_json).read()):
                result["test_framework_detected"] = True
                result["framework"] = "jest"
                
                # Count test files
                test_files = list(project_root.glob("**/*.test.js")) + list(project_root.glob("**/*.test.ts"))
                result["test_files_count"] = len(test_files)
                
                # Look for coverage directory
                coverage_dir = project_root / "coverage"
                if coverage_dir.exists():
                    try:
                        coverage_summary = coverage_dir / "coverage-summary.json"
                        if coverage_summary.exists():
                            with open(coverage_summary) as f:
                                coverage_data = json.load(f)
                                
                                if "total" in coverage_data and "lines" in coverage_data["total"]:
                                    coverage_percentage = coverage_data["total"]["lines"]["pct"]
                                    result["coverage"] = {
                                        "percentage": coverage_percentage,
                                        "report_path": str(coverage_summary.relative_to(project_root))
                                    }
                    except Exception as e:
                        self._logger.error(f"Error parsing Jest coverage: {str(e)}")
            
            # Check for Mocha
            elif package_json.exists() and "mocha" in open(package_json).read():
                result["test_framework_detected"] = True
                result["framework"] = "mocha"
                
                # Count test files
                test_files = list(project_root.glob("**/test/**/*.js")) + list(project_root.glob("**/test/**/*.ts"))
                result["test_files_count"] = len(test_files)
        
        elif "java" in project_type:
            # Check for JUnit
            if list(project_root.glob("**/src/test/**/*.java")):
                result["test_framework_detected"] = True
                result["framework"] = "junit"
                
                # Count test files
                test_files = list(project_root.glob("**/src/test/**/*.java"))
                result["test_files_count"] = len(test_files)
        
        return result
    
    async def _analyze_build_status(self, project_root: Path, project_type: str) -> Dict[str, Any]:
        """
        Analyze the build status of the project.
        
        Args:
            project_root: Path to the project root
            project_type: Type of the project
            
        Returns:
            Dictionary with build status information
        """
        result = {
            "build_system_detected": False,
            "system": None,
            "last_build": None,
            "artifacts": [],
            "problems": []
        }
        
        if "python" in project_type:
            # Check for setuptools
            setup_py = project_root / "setup.py"
            pyproject_toml = project_root / "pyproject.toml"
            
            if setup_py.exists():
                result["build_system_detected"] = True
                result["system"] = "setuptools"
                
                # Check for dist directory
                dist_dir = project_root / "dist"
                if dist_dir.exists():
                    # Get artifact files
                    artifacts = list(dist_dir.glob("*.whl")) + list(dist_dir.glob("*.tar.gz"))
                    
                    if artifacts:
                        result["artifacts"] = [str(a.relative_to(project_root)) for a in artifacts]
                        
                        # Get the most recent artifact's timestamp
                        most_recent = max(artifacts, key=lambda p: p.stat().st_mtime)
                        result["last_build"] = datetime.fromtimestamp(most_recent.stat().st_mtime).isoformat()
            
            elif pyproject_toml.exists():
                result["build_system_detected"] = True
                
                # Determine build system from pyproject.toml
                try:
                    import tomli
                    with open(pyproject_toml, "rb") as f:
                        pyproject_data = tomli.load(f)
                    
                    if "build-system" in pyproject_data:
                        build_backend = pyproject_data["build-system"].get("build-backend", "")
                        
                        if "setuptools" in build_backend:
                            result["system"] = "setuptools"
                        elif "poetry" in build_backend:
                            result["system"] = "poetry"
                        elif "flit" in build_backend:
                            result["system"] = "flit"
                        elif "hatchling" in build_backend:
                            result["system"] = "hatch"
                        else:
                            result["system"] = build_backend
                except Exception as e:
                    self._logger.error(f"Error parsing pyproject.toml: {str(e)}")
                    result["system"] = "unknown-pyproject"
                
                # Check for dist directory
                dist_dir = project_root / "dist"
                if dist_dir.exists():
                    # Get artifact files
                    artifacts = list(dist_dir.glob("*.whl")) + list(dist_dir.glob("*.tar.gz"))
                    
                    if artifacts:
                        result["artifacts"] = [str(a.relative_to(project_root)) for a in artifacts]
                        
                        # Get the most recent artifact's timestamp
                        most_recent = max(artifacts, key=lambda p: p.stat().st_mtime)
                        result["last_build"] = datetime.fromtimestamp(most_recent.stat().st_mtime).isoformat()
        
        elif "node" in project_type or "javascript" in project_type or "typescript" in project_type:
            # Check for various build systems
            package_json = project_root / "package.json"
            
            if package_json.exists():
                result["build_system_detected"] = True
                
                # Determine build system from package.json
                try:
                    with open(package_json) as f:
                        package_data = json.load(f)
                    
                    if "scripts" in package_data:
                        scripts = package_data["scripts"]
                        
                        if "build" in scripts:
                            build_script = scripts["build"]
                            
                            if "webpack" in build_script:
                                result["system"] = "webpack"
                            elif "rollup" in build_script:
                                result["system"] = "rollup"
                            elif "parcel" in build_script:
                                result["system"] = "parcel"
                            elif "tsc" in build_script:
                                result["system"] = "typescript"
                            elif "next build" in build_script:
                                result["system"] = "next.js"
                            elif "vue-cli-service build" in build_script:
                                result["system"] = "vue-cli"
                            elif "ng build" in build_script:
                                result["system"] = "angular-cli"
                            else:
                                result["system"] = "npm-script"
                except Exception as e:
                    self._logger.error(f"Error parsing package.json: {str(e)}")
                    result["system"] = "npm"
                
                # Check for build artifacts
                build_dir = project_root / "build"
                dist_dir = project_root / "dist"
                
                if build_dir.exists():
                    artifacts = list(build_dir.glob("**/*.*"))
                    if artifacts:
                        result["artifacts"] = [str(a.relative_to(project_root)) for a in artifacts[:5]]  # Limit to 5
                        
                        # Get the most recent artifact's timestamp
                        most_recent = max(artifacts, key=lambda p: p.stat().st_mtime)
                        result["last_build"] = datetime.fromtimestamp(most_recent.stat().st_mtime).isoformat()
                
                elif dist_dir.exists():
                    artifacts = list(dist_dir.glob("**/*.*"))
                    if artifacts:
                        result["artifacts"] = [str(a.relative_to(project_root)) for a in artifacts[:5]]  # Limit to 5
                        
                        # Get the most recent artifact's timestamp
                        most_recent = max(artifacts, key=lambda p: p.stat().st_mtime)
                        result["last_build"] = datetime.fromtimestamp(most_recent.stat().st_mtime).isoformat()
        
        elif "java" in project_type:
            # Check for Maven and Gradle
            pom_xml = project_root / "pom.xml"
            gradle_build = project_root / "build.gradle"
            
            if pom_xml.exists():
                result["build_system_detected"] = True
                result["system"] = "maven"
                
                # Check for target directory
                target_dir = project_root / "target"
                if target_dir.exists():
                    jar_files = list(target_dir.glob("*.jar"))
                    war_files = list(target_dir.glob("*.war"))
                    
                    artifacts = jar_files + war_files
                    if artifacts:
                        result["artifacts"] = [str(a.relative_to(project_root)) for a in artifacts]
                        
                        # Get the most recent artifact's timestamp
                        most_recent = max(artifacts, key=lambda p: p.stat().st_mtime)
                        result["last_build"] = datetime.fromtimestamp(most_recent.stat().st_mtime).isoformat()
            
            elif gradle_build.exists():
                result["build_system_detected"] = True
                result["system"] = "gradle"
                
                # Check for build directory
                build_dir = project_root / "build"
                if build_dir.exists():
                    jar_files = list(build_dir.glob("**/*.jar"))
                    war_files = list(build_dir.glob("**/*.war"))
                    
                    artifacts = jar_files + war_files
                    if artifacts:
                        result["artifacts"] = [str(a.relative_to(project_root)) for a in artifacts]
                        
                        # Get the most recent artifact's timestamp
                        most_recent = max(artifacts, key=lambda p: p.stat().st_mtime)
                        result["last_build"] = datetime.fromtimestamp(most_recent.stat().st_mtime).isoformat()
        
        return result
    
    async def _analyze_migrations(self, project_root: Path, project_type: str) -> Dict[str, Any]:
        """
        Analyze the migration status of the project.
        
        Args:
            project_root: Path to the project root
            project_type: Type of the project
            
        Returns:
            Dictionary with migration status information
        """
        result = {
            "has_migrations": False,
            "framework": None,
            "migration_files": [],
            "pending_migrations": []
        }
        
        if "python" in project_type:
            # Check for Django migrations
            migrations_dirs = list(project_root.glob("**/migrations"))
            
            if migrations_dirs:
                result["has_migrations"] = True
                result["framework"] = "django"
                
                # Get migration files
                migration_files = []
                for migrations_dir in migrations_dirs:
                    files = list(migrations_dir.glob("*.py"))
                    migration_files.extend([str(f.relative_to(project_root)) for f in files if f.name != "__init__.py"])
                
                result["migration_files"] = migration_files
                
                # Try to detect pending migrations using Django
                manage_py = project_root / "manage.py"
                if manage_py.exists():
                    try:
                        execution_engine = get_execution_engine()
                        stdout, stderr, return_code = await execution_engine.execute_command(
                            command=f"cd {project_root} && python manage.py showmigrations",
                            check_safety=False
                        )
                        
                        if return_code == 0:
                            # Parse output to find pending migrations
                            pending = []
                            current_app = None
                            
                            for line in stdout.splitlines():
                                if not line.strip():
                                    continue
                                
                                if not line.startswith(' '):
                                    # This is an app name
                                    current_app = line.strip()
                                elif line.strip().startswith('[ ]'):
                                    # This is a pending migration
                                    migration_name = line.strip()[4:].strip()
                                    if current_app:
                                        pending.append(f"{current_app}/{migration_name}")
                            
                            result["pending_migrations"] = pending
                    except Exception as e:
                        self._logger.error(f"Error detecting Django pending migrations: {str(e)}")
            
            # Check for Alembic migrations (SQLAlchemy)
            alembic_ini = project_root / "alembic.ini"
            if alembic_ini.exists():
                alembic_dir = None
                
                # Try to find migrations directory from alembic.ini
                try:
                    with open(alembic_ini) as f:
                        for line in f:
                            if line.startswith('script_location = '):
                                alembic_dir = line.split('=')[1].strip()
                                break
                except Exception:
                    pass
                
                if alembic_dir:
                    alembic_path = project_root / alembic_dir / "versions"
                    if alembic_path.exists():
                        result["has_migrations"] = True
                        result["framework"] = "alembic"
                        
                        # Get migration files
                        migration_files = list(alembic_path.glob("*.py"))
                        result["migration_files"] = [str(f.relative_to(project_root)) for f in migration_files]
                        
                        # Try to detect pending migrations
                        try:
                            execution_engine = get_execution_engine()
                            stdout, stderr, return_code = await execution_engine.execute_command(
                                command=f"cd {project_root} && alembic current",
                                check_safety=False
                            )
                            
                            if return_code == 0:
                                # Get current revision
                                current_revision = None
                                if stdout.strip():
                                    current_revision = stdout.strip().split(' ')[0]
                                
                                # Get available revisions
                                available_revisions = []
                                for file in migration_files:
                                    # Extract revision ID from filename
                                    revision_match = re.match(r'(\w+)_', file.stem)
                                    if revision_match:
                                        available_revisions.append(revision_match.group(1))
                                
                                # If we have a current revision, find pending ones
                                if current_revision and current_revision in available_revisions:
                                    current_index = available_revisions.index(current_revision)
                                    pending_revisions = available_revisions[current_index+1:]
                                    
                                    result["pending_migrations"] = [
                                        str((alembic_path / f"{rev}_something.py").relative_to(project_root))
                                        for rev in pending_revisions
                                    ]
                        except Exception as e:
                            self._logger.error(f"Error detecting Alembic pending migrations: {str(e)}")
        
        elif "node" in project_type or "javascript" in project_type:
            # Check for Sequelize migrations
            migrations_dir = project_root / "migrations"
            if migrations_dir.exists() and migrations_dir.is_dir():
                # Look for a Sequelize config file
                config_file = project_root / "config" / "config.json"
                sequelize_rc = project_root / ".sequelizerc"
                
                if config_file.exists() or sequelize_rc.exists():
                    result["has_migrations"] = True
                    result["framework"] = "sequelize"
                    
                    # Get migration files
                    migration_files = list(migrations_dir.glob("*.js"))
                    result["migration_files"] = [str(f.relative_to(project_root)) for f in migration_files]
        
        elif "ruby" in project_type or "rails" in project_type:
            # Check for Rails migrations
            migrations_dir = project_root / "db" / "migrate"
            if migrations_dir.exists() and migrations_dir.is_dir():
                result["has_migrations"] = True
                result["framework"] = "rails"
                
                # Get migration files
                migration_files = list(migrations_dir.glob("*.rb"))
                result["migration_files"] = [str(f.relative_to(project_root)) for f in migration_files]
                
                # Try to detect pending migrations
                try:
                    execution_engine = get_execution_engine()
                    stdout, stderr, return_code = await execution_engine.execute_command(
                        command=f"cd {project_root} && rake db:migrate:status",
                        check_safety=False
                    )
                    
                    if return_code == 0:
                        # Parse output to find pending migrations
                        pending = []
                        
                        for line in stdout.splitlines():
                            if line.strip().startswith('down '):
                                # This is a pending migration
                                migration_info = line.strip()[5:].strip()
                                migration_match = re.search(r'(\d+)_([a-z_]+)\.rb', migration_info)
                                if migration_match:
                                    pending.append(f"db/migrate/{migration_match.group(0)}")
                        
                        result["pending_migrations"] = pending
                except Exception as e:
                    self._logger.error(f"Error detecting Rails pending migrations: {str(e)}")
        
        return result
    
    async def _analyze_dependencies(self, project_root: Path, project_type: str) -> Dict[str, Any]:
        """
        Analyze the dependency health of the project.
        
        Args:
            project_root: Path to the project root
            project_type: Type of the project
            
        Returns:
            Dictionary with dependency health information
        """
        result = {
            "has_dependencies": False,
            "dependency_file": None,
            "package_manager": None,
            "dependencies_count": 0,
            "dev_dependencies_count": 0,
            "outdated_packages": [],
            "vulnerable_packages": []
        }
        
        if "python" in project_type:
            # Check for pip requirements
            requirements_txt = project_root / "requirements.txt"
            pipfile = project_root / "Pipfile"
            pyproject_toml = project_root / "pyproject.toml"
            
            if requirements_txt.exists():
                result["has_dependencies"] = True
                result["dependency_file"] = str(requirements_txt.relative_to(project_root))
                result["package_manager"] = "pip"
                
                # Count dependencies
                try:
                    with open(requirements_txt) as f:
                        deps = [line.strip() for line in f if line.strip() and not line.strip().startswith('#')]
                        result["dependencies_count"] = len(deps)
                except Exception as e:
                    self._logger.error(f"Error reading requirements.txt: {str(e)}")
                
                # Try to find outdated packages
                try:
                    execution_engine = get_execution_engine()
                    stdout, stderr, return_code = await execution_engine.execute_command(
                        command=f"cd {project_root} && pip list --outdated --format=json",
                        check_safety=False
                    )
                    
                    if return_code == 0 and stdout.strip():
                        try:
                            outdated = json.loads(stdout)
                            result["outdated_packages"] = [
                                {
                                    "name": pkg["name"],
                                    "current_version": pkg["version"],
                                    "latest_version": pkg["latest_version"]
                                }
                                for pkg in outdated
                            ]
                        except json.JSONDecodeError:
                            pass
                except Exception as e:
                    self._logger.error(f"Error checking for outdated packages: {str(e)}")
            
            elif pipfile.exists():
                result["has_dependencies"] = True
                result["dependency_file"] = str(pipfile.relative_to(project_root))
                result["package_manager"] = "pipenv"
                
                # Try to get dependency info from Pipfile.lock
                pipfile_lock = project_root / "Pipfile.lock"
                if pipfile_lock.exists():
                    try:
                        with open(pipfile_lock) as f:
                            lock_data = json.load(f)
                            
                            if "default" in lock_data:
                                result["dependencies_count"] = len(lock_data["default"])
                            
                            if "develop" in lock_data:
                                result["dev_dependencies_count"] = len(lock_data["develop"])
                    except Exception as e:
                        self._logger.error(f"Error reading Pipfile.lock: {str(e)}")
            
            elif pyproject_toml.exists():
                result["has_dependencies"] = True
                result["dependency_file"] = str(pyproject_toml.relative_to(project_root))
                
                # Try to determine the package manager
                try:
                    import tomli
                    with open(pyproject_toml, "rb") as f:
                        pyproject_data = tomli.load(f)
                    
                    if "build-system" in pyproject_data:
                        build_backend = pyproject_data["build-system"].get("build-backend", "")
                        
                        if "poetry" in build_backend:
                            result["package_manager"] = "poetry"
                        else:
                            result["package_manager"] = "pip"
                    
                    # Count dependencies
                    if "project" in pyproject_data and "dependencies" in pyproject_data["project"]:
                        if isinstance(pyproject_data["project"]["dependencies"], list):
                            result["dependencies_count"] = len(pyproject_data["project"]["dependencies"])
                    
                    # Count dev dependencies
                    if "project" in pyproject_data and "optional-dependencies" in pyproject_data["project"]:
                        if "dev" in pyproject_data["project"]["optional-dependencies"]:
                            result["dev_dependencies_count"] = len(pyproject_data["project"]["optional-dependencies"]["dev"])
                
                except Exception as e:
                    self._logger.error(f"Error reading pyproject.toml: {str(e)}")
        
        elif "node" in project_type or "javascript" in project_type or "typescript" in project_type:
            # Check for NPM/Yarn dependencies
            package_json = project_root / "package.json"
            
            if package_json.exists():
                result["has_dependencies"] = True
                result["dependency_file"] = str(package_json.relative_to(project_root))
                
                # Determine package manager
                yarn_lock = project_root / "yarn.lock"
                package_lock = project_root / "package-lock.json"
                
                if yarn_lock.exists():
                    result["package_manager"] = "yarn"
                else:
                    result["package_manager"] = "npm"
                
                # Count dependencies
                try:
                    with open(package_json) as f:
                        package_data = json.load(f)
                        
                        if "dependencies" in package_data:
                            result["dependencies_count"] = len(package_data["dependencies"])
                        
                        if "devDependencies" in package_data:
                            result["dev_dependencies_count"] = len(package_data["devDependencies"])
                except Exception as e:
                    self._logger.error(f"Error reading package.json: {str(e)}")
                
                # Try to find outdated packages
                npm_cmd = "npm" if result["package_manager"] == "npm" else "yarn"
                try:
                    execution_engine = get_execution_engine()
                    stdout, stderr, return_code = await execution_engine.execute_command(
                        command=f"cd {project_root} && {npm_cmd} outdated --json",
                        check_safety=False
                    )
                    
                    if return_code == 0 and stdout.strip():
                        try:
                            outdated = json.loads(stdout)
                            
                            if isinstance(outdated, dict):
                                result["outdated_packages"] = [
                                    {
                                        "name": pkg_name,
                                        "current_version": pkg_info.get("current", "unknown"),
                                        "latest_version": pkg_info.get("latest", "unknown")
                                    }
                                    for pkg_name, pkg_info in outdated.items()
                                ]
                        except json.JSONDecodeError:
                            pass
                except Exception as e:
                    self._logger.error(f"Error checking for outdated packages: {str(e)}")
        
        elif "java" in project_type:
            # Check for Maven dependencies
            pom_xml = project_root / "pom.xml"
            gradle_build = project_root / "build.gradle"
            
            if pom_xml.exists():
                result["has_dependencies"] = True
                result["dependency_file"] = str(pom_xml.relative_to(project_root))
                result["package_manager"] = "maven"
                
                # Try to count dependencies using basic XML parsing
                try:
                    import xml.etree.ElementTree as ET
                    tree = ET.parse(pom_xml)
                    root = tree.getroot()
                    
                    # Add namespace to XML tags
                    ns = {"mvn": "http://maven.apache.org/POM/4.0.0"}
                    
                    # Count dependencies
                    dependencies = root.findall(".//mvn:dependencies/mvn:dependency", ns)
                    result["dependencies_count"] = len(dependencies)
                except Exception as e:
                    self._logger.error(f"Error parsing pom.xml: {str(e)}")
            
            elif gradle_build.exists():
                result["has_dependencies"] = True
                result["dependency_file"] = str(gradle_build.relative_to(project_root))
                result["package_manager"] = "gradle"
                
                # Count dependencies using a simple regex
                try:
                    with open(gradle_build) as f:
                        content = f.read()
                        
                        # Count dependencies
                        dependency_matches = re.findall(r"implementation ['\"]([^'\"]+?)['\"]", content)
                        result["dependencies_count"] = len(dependency_matches)
                        
                        # Count dev dependencies
                        test_matches = re.findall(r"testImplementation ['\"]([^'\"]+?)['\"]", content)
                        result["dev_dependencies_count"] = len(test_matches)
                except Exception as e:
                    self._logger.error(f"Error reading build.gradle: {str(e)}")
        
        return result
    
    async def _analyze_code_quality(self, project_root: Path, project_type: str) -> Dict[str, Any]:
        """
        Analyze the code quality of the project.
        
        Args:
            project_root: Path to the project root
            project_type: Type of the project
            
        Returns:
            Dictionary with code quality information
        """
        result = {
            "linting_setup_detected": False,
            "linter": None,
            "formatter": None,
            "issues_count": 0,
            "issues_by_type": {},
            "high_priority_issues": []
        }
        
        if "python" in project_type:
            # Check for Python linters
            flake8_config = project_root / ".flake8"
            pylintrc = project_root / ".pylintrc"
            mypy_ini = project_root / "mypy.ini"
            
            if flake8_config.exists():
                result["linting_setup_detected"] = True
                result["linter"] = "flake8"
                
                # Try to run flake8 to get issue count
                try:
                    execution_engine = get_execution_engine()
                    stdout, stderr, return_code = await execution_engine.execute_command(
                        command=f"cd {project_root} && flake8 --max-line-length=120 --count",
                        check_safety=False
                    )
                    
                    if return_code == 0 and stdout.strip():
                        # Last line contains the issue count
                        result["issues_count"] = int(stdout.strip().splitlines()[-1])
                except Exception as e:
                    self._logger.error(f"Error running flake8: {str(e)}")
            
            elif pylintrc.exists():
                result["linting_setup_detected"] = True
                result["linter"] = "pylint"
            
            # Check for Python formatters
            black_config = project_root / "pyproject.toml"
            if black_config.exists():
                try:
                    import tomli
                    with open(black_config, "rb") as f:
                        config_data = tomli.load(f)
                    
                    if "tool" in config_data and "black" in config_data["tool"]:
                        result["formatter"] = "black"
                except Exception:
                    pass
            
            isort_config = project_root / ".isort.cfg"
            if isort_config.exists():
                if result["formatter"]:
                    result["formatter"] += "+isort"
                else:
                    result["formatter"] = "isort"
        
        elif "node" in project_type or "javascript" in project_type or "typescript" in project_type:
            # Check for JS/TS linters
            eslintrc = any(
                (project_root / f).exists() 
                for f in [".eslintrc", ".eslintrc.js", ".eslintrc.json", ".eslintrc.yml"]
            )
            
            if eslintrc:
                result["linting_setup_detected"] = True
                result["linter"] = "eslint"
                
                # Try to run eslint to get issue count
                try:
                    execution_engine = get_execution_engine()
                    stdout, stderr, return_code = await execution_engine.execute_command(
                        command=f"cd {project_root} && npx eslint . --max-warnings=9999 -f json",
                        check_safety=False
                    )
                    
                    if return_code == 0 and stdout.strip():
                        try:
                            lint_results = json.loads(stdout)
                            
                            # Count issues
                            total_issues = sum(len(file_result.get("messages", [])) for file_result in lint_results)
                            result["issues_count"] = total_issues
                            
                            # Count by severity
                            severity_counts = {"error": 0, "warning": 0, "info": 0}
                            
                            for file_result in lint_results:
                                for msg in file_result.get("messages", []):
                                    severity = msg.get("severity")
                                    if severity == 2:
                                        severity_counts["error"] += 1
                                    elif severity == 1:
                                        severity_counts["warning"] += 1
                                    else:
                                        severity_counts["info"] += 1
                            
                            result["issues_by_type"] = severity_counts
                            
                            # Collect high-priority issues
                            for file_result in lint_results:
                                for msg in file_result.get("messages", []):
                                    if msg.get("severity") == 2:  # Error
                                        result["high_priority_issues"].append({
                                            "file": file_result.get("filePath", "unknown"),
                                            "line": msg.get("line", 0),
                                            "column": msg.get("column", 0),
                                            "message": msg.get("message", "Unknown error"),
                                            "rule": msg.get("ruleId", "unknown")
                                        })
                                        
                                        # Limit to 10 issues
                                        if len(result["high_priority_issues"]) >= 10:
                                            break
                                
                                if len(result["high_priority_issues"]) >= 10:
                                    break
                        except json.JSONDecodeError:
                            pass
                except Exception as e:
                    self._logger.error(f"Error running eslint: {str(e)}")
            
            # Check for tslint
            tslint_json = project_root / "tslint.json"
            if tslint_json.exists():
                result["linting_setup_detected"] = True
                result["linter"] = "tslint"
            
            # Check for formatters
            prettier_config = any(
                (project_root / f).exists() 
                for f in [".prettierrc", ".prettierrc.js", ".prettierrc.json", ".prettier.config.js"]
            )
            
            if prettier_config:
                result["formatter"] = "prettier"
        
        elif "java" in project_type:
            # Check for Java linters
            checkstyle_xml = project_root / "checkstyle.xml"
            pmd_xml = project_root / "pmd.xml"
            
            if checkstyle_xml.exists():
                result["linting_setup_detected"] = True
                result["linter"] = "checkstyle"
            
            elif pmd_xml.exists():
                result["linting_setup_detected"] = True
                result["linter"] = "pmd"
        
        return result
    
    async def _find_todo_items(self, project_root: Path) -> List[Dict[str, Any]]:
        """
        Find TODO and FIXME comments in the project.
        
        Args:
            project_root: Path to the project root
            
        Returns:
            List of dictionaries with todo items
        """
        # List of file extensions to search
        extensions = [
            ".py", ".js", ".jsx", ".ts", ".tsx", ".java", ".c", ".cpp", ".h", ".cs", 
            ".rb", ".php", ".go", ".rs", ".swift", ".kt", ".scala", ".html", ".css", 
            ".scss", ".less", ".md", ".txt", ".sh", ".bat", ".ps1"
        ]
        
        # Patterns to search for
        todo_patterns = [
            r'(?://|#|<!--|;|/\*)\s*TODO\s*(?:\(([^)]+)\)\s*)?:?\s*(.*?)(?:\*/|-->)?$',
            r'(?://|#|<!--|;|/\*)\s*FIXME\s*(?:\(([^)]+)\)\s*)?:?\s*(.*?)(?:\*/|-->)?$',
            r'(?://|#|<!--|;|/\*)\s*HACK\s*(?:\(([^)]+)\)\s*)?:?\s*(.*?)(?:\*/|-->)?$',
            r'(?://|#|<!--|;|/\*)\s*BUG\s*(?:\(([^)]+)\)\s*)?:?\s*(.*?)(?:\*/|-->)?$',
            r'(?://|#|<!--|;|/\*)\s*NOTE\s*(?:\(([^)]+)\)\s*)?:?\s*(.*?)(?:\*/|-->)?$'
        ]
        
        # Exclude patterns
        exclude_patterns = [
            "node_modules", "__pycache__", ".git", "venv", ".venv", "env", 
            "build", "dist", "target", "bin", "obj", ".pytest_cache"
        ]
        
        # Result list
        todo_items = []
        
        # Find files to search
        for ext in extensions:
            files = []
            for file in project_root.glob(f"**/*{ext}"):
                # Skip excluded directories
                if any(excl in str(file) for excl in exclude_patterns):
                    continue
                files.append(file)
            
            # Limit to 1000 files to avoid excessive processing
            if len(files) > 1000:
                files = files[:1000]
            
            # Search each file
            for file in files:
                try:
                    with open(file, 'r', encoding='utf-8', errors='ignore') as f:
                        for i, line in enumerate(f, 1):
                            for pattern in todo_patterns:
                                matches = re.search(pattern, line)
                                if matches:
                                    # Extract todo info
                                    assignee = matches.group(1) if matches.lastindex >= 1 else None
                                    text = matches.group(2) if matches.lastindex >= 2 else matches.group(0)
                                    
                                    # Determine todo type
                                    todo_type = None
                                    if "TODO" in line:
                                        todo_type = "TODO"
                                    elif "FIXME" in line:
                                        todo_type = "FIXME"
                                    elif "HACK" in line:
                                        todo_type = "HACK"
                                    elif "BUG" in line:
                                        todo_type = "BUG"
                                    elif "NOTE" in line:
                                        todo_type = "NOTE"
                                    
                                    # Add to result
                                    todo_items.append({
                                        "type": todo_type,
                                        "text": text.strip(),
                                        "file": str(file.relative_to(project_root)),
                                        "line": i,
                                        "assignee": assignee.strip() if assignee else None
                                    })
                except Exception as e:
                    self._logger.error(f"Error searching for todos in {file}: {str(e)}")
        
        # Sort by file and line number
        todo_items.sort(key=lambda item: (item["file"], item["line"]))
        
        # Limit to 100 items to avoid excessive data
        return todo_items[:100]
    
    async def get_detailed_git_status(self, project_root: Union[str, Path]) -> Dict[str, Any]:
        """
        Get detailed information about the Git status of the project.
        
        Args:
            project_root: Path to the project root
            
        Returns:
            Dictionary with detailed Git information
        """
        path_obj = Path(project_root)
        
        # Get basic project state
        project_state = await self.get_project_state(path_obj)
        git_state = project_state.get("git_state", {})
        
        if not git_state.get("is_git_repo", False):
            return {"is_git_repo": False}
        
        # Add more detailed Git information
        result = dict(git_state)
        
        try:
            execution_engine = get_execution_engine()
            
            # Get the git log graph
            stdout, stderr, return_code = await execution_engine.execute_command(
                command=f"cd {path_obj} && git log --graph --oneline --decorate -n 10",
                check_safety=False
            )
            
            if return_code == 0 and stdout.strip():
                result["log_graph"] = stdout.strip()
            
            # Get branch info
            stdout, stderr, return_code = await execution_engine.execute_command(
                command=f"cd {path_obj} && git branch -vv",
                check_safety=False
            )
            
            if return_code == 0 and stdout.strip():
                branches = []
                for line in stdout.splitlines():
                    if not line.strip():
                        continue
                    
                    # Parse branch line
                    is_current = line.startswith('*')
                    branch_line = line[2:].strip()
                    
                    # Extract branch name and info
                    parts = branch_line.split(' ', 1)
                    if len(parts) == 2:
                        branch_name = parts[0]
                        branch_info = parts[1].strip()
                        
                        # Extract tracking info
                        tracking_match = re.search(r'\[(.*?)\]', branch_info)
                        tracking_info = tracking_match.group(1) if tracking_match else None
                        
                        branches.append({
                            "name": branch_name,
                            "is_current": is_current,
                            "tracking_info": tracking_info,
                            "info": branch_info
                        })
                
                result["branches"] = branches
            
            # Get remote info
            stdout, stderr, return_code = await execution_engine.execute_command(
                command=f"cd {path_obj} && git remote -v",
                check_safety=False
            )
            
            if return_code == 0 and stdout.strip():
                remotes = {}
                for line in stdout.splitlines():
                    if not line.strip():
                        continue
                    
                    # Parse remote line
                    parts = line.split()
                    if len(parts) >= 2:
                        remote_name = parts[0]
                        remote_url = parts[1]
                        remote_type = parts[2][1:-1] if len(parts) >= 3 else "fetch"
                        
                        if remote_name not in remotes:
                            remotes[remote_name] = {}
                        
                        remotes[remote_name][remote_type] = remote_url
                
                result["remotes"] = remotes
            
            # Get git config for the repo
            stdout, stderr, return_code = await execution_engine.execute_command(
                command=f"cd {path_obj} && git config --local --list",
                check_safety=False
            )
            
            if return_code == 0 and stdout.strip():
                git_config = {}
                for line in stdout.splitlines():
                    if not line.strip() or '=' not in line:
                        continue
                    
                    key, value = line.split('=', 1)
                    git_config[key.strip()] = value.strip()
                
                # Extract useful config values
                config_extract = {}
                
                # User info
                if "user.name" in git_config:
                    config_extract["user.name"] = git_config["user.name"]
                if "user.email" in git_config:
                    config_extract["user.email"] = git_config["user.email"]
                
                # Branch default
                if "init.defaultBranch" in git_config:
                    config_extract["default_branch"] = git_config["init.defaultBranch"]
                
                # Pull strategy
                if "pull.rebase" in git_config:
                    config_extract["pull_strategy"] = "rebase" if git_config["pull.rebase"] == "true" else "merge"
                
                result["config"] = config_extract
            
            return result
            
        except Exception as e:
            self._logger.error(f"Error getting detailed Git status: {str(e)}")
            return git_state
    
    async def get_project_tasks(self, project_root: Union[str, Path]) -> Dict[str, Any]:
        """
        Get a list of tasks (todos, issues, pending features, etc.) for the project.
        
        Args:
            project_root: Path to the project root
            
        Returns:
            Dictionary with project tasks
        """
        path_obj = Path(project_root)
        
        # Get the project state
        project_state = await self.get_project_state(path_obj)
        
        # Extract todo items
        todo_items = project_state.get("todo_items", [])
        
        # Look for task files
        tasks_files = []
        for file in ["TODO.md", "TODO.txt", "TASKS.md", "ROADMAP.md"]:
            task_file = path_obj / file
            if task_file.exists():
                tasks_files.append(str(task_file.relative_to(path_obj)))
        
        # Look for GitHub/GitLab issue templates
        issue_templates = []
        for template_dir in [".github/ISSUE_TEMPLATE", ".gitlab/issue_templates"]:
            template_path = path_obj / template_dir
            if template_path.exists() and template_path.is_dir():
                for template_file in template_path.glob("*.md"):
                    issue_templates.append(str(template_file.relative_to(path_obj)))
        
        # Combine everything into a tasks summary
        return {
            "todo_items": todo_items,
            "tasks_files": tasks_files,
            "issue_templates": issue_templates,
            "pending_migrations": project_state.get("migrations", {}).get("pending_migrations", []),
            "failing_tests": project_state.get("test_status", {}).get("failing_tests", []),
            "high_priority_issues": project_state.get("code_quality", {}).get("high_priority_issues", [])
        }

# Global project state analyzer instance
project_state_analyzer = ProjectStateAnalyzer()
</file>

<file path="components/context/semantic_context_manager.py">
# angela/context/semantic_context_manager.py
"""
Semantic context management for Angela CLI.

This module integrates and manages all semantic understanding components,
providing a unified interface for accessing rich contextual information
about code, project state, and user intentions.
"""
import os
import asyncio
import traceback
from pathlib import Path
from typing import Dict, Any, List, Optional, Set, Union
from datetime import datetime

from angela.utils.logging import get_logger
from angela.api.context import get_context_manager, get_file_activity_tracker
from angela.api.context import get_project_state_analyzer 
from angela.api.ai import get_semantic_analyzer
from angela.core.registry import registry

logger = get_logger(__name__)

class SemanticContextManager:
    """
    Central manager for all semantic context information.
    
    This class integrates:
    1. Code semantic analysis (functions, classes, relationships)
    2. Project state information (git status, migrations, dependencies)
    3. File activity tracking (down to function/class level)
    4. User intention mapping based on history and current context
    
    It provides a unified interface for AI components to access rich
    contextual information for more informed responses.
    """
    
    def __init__(self):
        """Initialize the semantic context manager."""
        self._logger = logger
        self._analysis_cache = {}  # Cache of semantic analysis results
        self._last_analysis_time = {}  # Timestamp of last analysis
        self._active_analyses = set()  # Currently running analyses
        self._analysis_valid_time = 300  # Seconds before a cached analysis is invalid
        
        # Project module cache - maps project root to module info
        self._project_modules = {}
        
        # Map of file paths to functions and classes
        self._entity_map = {}  # Maps "function_name" -> file_path
        self._recent_entity_usages = []  # List of recently used entities
        
        # Register this service
        registry.register("semantic_context_manager", self)
        
        # Try to get and register with context enhancer - this ensures we're integrated
        try:
            from angela.api.context import get_context_enhancer
            context_enhancer = get_context_enhancer()
            if context_enhancer:
                # Register our enhancer function
                context_enhancer.register_enhancer(self.get_enriched_context)
                self._logger.info("Registered semantic context enhancer function")
        except Exception as e:
            self._logger.error(f"Failed to register with context_enhancer: {str(e)}")
    
    async def refresh_context(self, force: bool = False) -> None:
        """
        Refresh the semantic context for the current project.
        
        Args:
            force: Whether to force a refresh even if the cache is valid
        """
        # Get the current project root
        context_manager = get_context_manager()
        project_root = context_manager.project_root
        if not project_root:
            self._logger.debug("No project root detected, skipping semantic context refresh")
            return
        
        # Check if we need to refresh
        if not force and project_root in self._last_analysis_time:
            last_time = self._last_analysis_time[project_root]
            age = datetime.now().timestamp() - last_time
            if age < self._analysis_valid_time:
                self._logger.debug(f"Using cached semantic analysis for {project_root} (age: {age:.1f}s)")
                return
        
        # Don't start multiple analyses for the same project
        if project_root in self._active_analyses:
            self._logger.debug(f"Analysis already in progress for {project_root}")
            return
            
        self._active_analyses.add(project_root)
        
        try:
            self._logger.info(f"Refreshing semantic context for {project_root}")
            
            # Get project state analyzer
            project_state_analyzer = get_project_state_analyzer()
            
            # Get semantic analyzer
            semantic_analyzer = get_semantic_analyzer()
            
            # Get project state asynchronously
            project_state_task = asyncio.create_task(
                project_state_analyzer.get_project_state(project_root)
            )
            
            # Start semantic analysis of key files asynchronously
            semantic_analysis_task = asyncio.create_task(
                self._analyze_key_files(project_root)
            )
            
            # Wait for both tasks to complete
            project_state, semantic_analysis = await asyncio.gather(
                project_state_task, 
                semantic_analysis_task
            )
            
            # Store the results
            self._analysis_cache[str(project_root)] = {
                "project_state": project_state,
                "semantic_analysis": semantic_analysis,
                "timestamp": datetime.now().isoformat()
            }
            
            self._last_analysis_time[str(project_root)] = datetime.now().timestamp()
            self._logger.info(f"Semantic context refresh completed for {project_root}")
            
        except Exception as e:
            self._logger.exception(f"Error refreshing semantic context: {str(e)}")
        finally:
            self._active_analyses.remove(project_root)
    
    async def _analyze_key_files(self, project_root: Path) -> Dict[str, Any]:
        """
        Analyze the key files in the project.
        
        Args:
            project_root: The project root path
            
        Returns:
            Dictionary with semantic analysis information
        """
        # Get key files to analyze
        key_files = await self._identify_key_files(project_root)
        
        # Get semantic analyzer
        semantic_analyzer = get_semantic_analyzer()
        
        # Perform semantic analysis
        modules = {}
        for file_path in key_files:
            try:
                module = await semantic_analyzer.analyze_file(file_path)
                if module:
                    modules[str(file_path)] = module
            except Exception as e:
                self._logger.error(f"Error analyzing file {file_path}: {str(e)}")
        
        # Update the entity map
        self._update_entity_map(modules)
        
        # Store the modules for this project
        self._project_modules[str(project_root)] = modules
        
        # Calculate project-wide metrics
        metrics = semantic_analyzer.calculate_project_metrics(modules)
        
        # Return summary
        return {
            "analyzed_files_count": len(modules),
            "entities": {
                "functions": sum(len(module.functions) for module in modules.values()),
                "classes": sum(len(module.classes) for module in modules.values()),
                "variables": sum(len(module.variables) for module in modules.values())
            },
            "metrics": metrics,
            "key_files": [str(f) for f in key_files[:10]]  # Include only the first 10 for brevity
        }
    
    def _update_entity_map(self, modules: Dict[str, Any]) -> None:
        """
        Update the entity map with the analyzed modules.
        
        Args:
            modules: Dictionary of modules
        """
        for file_path, module in modules.items():
            # Add functions
            for func_name, func in module.functions.items():
                self._entity_map[func_name] = file_path
            
            # Add classes
            for class_name, cls in module.classes.items():
                self._entity_map[class_name] = file_path
                
                # Add methods with class prefix
                for method_name in cls.methods:
                    qualified_name = f"{class_name}.{method_name}"
                    self._entity_map[qualified_name] = file_path
    
    async def _identify_key_files(self, project_root: Path) -> List[Path]:
        """
        Identify the key files in the project for analysis.
        
        Prioritizes:
        1. Recently accessed files
        2. Files with the most activity
        3. Entry point files (main.py, index.js, etc.)
        4. Config and initialization files
        
        Args:
            project_root: The project root path
            
        Returns:
            List of file paths
        """
        key_files = set()
        
        # Get file activity tracker
        file_activity_tracker = get_file_activity_tracker()
        
        # Get recently accessed files
        recent_activities = file_activity_tracker.get_recent_activities(limit=20)
        for activity in recent_activities:
            file_path = Path(activity["path"])
            if file_path.exists() and file_path.is_file():
                key_files.add(file_path)
        
        # Get most active files
        active_files = file_activity_tracker.get_most_active_files(limit=20)
        for file_info in active_files:
            file_path = Path(file_info["path"])
            if file_path.exists() and file_path.is_file():
                key_files.add(file_path)
        
        # Find entry point files
        entry_point_patterns = [
            "main.py", "__main__.py", "app.py", "index.js", "server.js",
            "index.ts", "App.tsx", "App.jsx", "Main.java", "Program.cs"
        ]
        
        for pattern in entry_point_patterns:
            for file_path in project_root.glob(f"**/{pattern}"):
                if file_path.exists() and file_path.is_file():
                    key_files.add(file_path)
        
        # Find config and initialization files
        config_patterns = [
            "config.py", "settings.py", "constants.py", "__init__.py",
            "package.json", "tsconfig.json", ".eslintrc.js", "webpack.config.js",
            "Dockerfile", "docker-compose.yml", "requirements.txt", "pyproject.toml"
        ]
        
        for pattern in config_patterns:
            for file_path in project_root.glob(f"**/{pattern}"):
                if file_path.exists() and file_path.is_file():
                    key_files.add(file_path)
        
        # Limit to 100 files to avoid excessive analysis
        return list(key_files)[:100]
    
    async def get_enriched_context(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """
        Get an enriched context dictionary with semantic information.
        This function is registered as a context enhancer.
        
        Args:
            context: Base context dictionary
            
        Returns:
            Dictionary with enriched context information
        """
        # Get context manager
        context_manager = get_context_manager()
        
        project_root = context_manager.project_root
        if not project_root:
            return context  # Return original context if no project root

        # Ensure we have up-to-date analysis
        await self.refresh_context()
        
        if str(project_root) not in self._analysis_cache:
            return context  # Return original context if no analysis available
        
        # Get the analysis results
        analysis = self._analysis_cache[str(project_root)]
        
        # Get the current file
        current_file = context_manager.current_file
        current_file_entities = None
        
        if current_file and str(current_file) in self._project_modules.get(str(project_root), {}):
            module = self._project_modules[str(project_root)][str(current_file)]
            
            # Extract information about entities in the current file
            current_file_entities = {
                "functions": list(module.functions.keys()),
                "classes": list(module.classes.keys()),
                "imports": list(module.imports.keys()),
                "docstring": module.docstring
            }
        
        # Build the enriched context
        enhanced_context = dict(context)  # Copy the original context
        
        enhanced_context["semantic_context"] = {
            "available": True,
            "project_info": {
                "analyzed_files": analysis["semantic_analysis"]["analyzed_files_count"],
                "entity_counts": analysis["semantic_analysis"]["entities"],
                "key_metrics": {
                    "total_lines": analysis["semantic_analysis"]["metrics"].get("total_lines", 0),
                    "function_count": analysis["semantic_analysis"]["metrics"].get("function_count", 0),
                    "class_count": analysis["semantic_analysis"]["metrics"].get("class_count", 0),
                    "average_function_complexity": analysis["semantic_analysis"]["metrics"].get("average_function_complexity", 0)
                }
            },
            "project_state": {
                "git": {
                    "current_branch": analysis["project_state"]["git_state"].get("current_branch"),
                    "has_changes": analysis["project_state"]["git_state"].get("has_changes", False),
                    "modified_files_count": len(analysis["project_state"]["git_state"].get("modified_files", [])),
                    "untracked_files_count": len(analysis["project_state"]["git_state"].get("untracked_files", []))
                },
                "tests": {
                    "framework": analysis["project_state"]["test_status"].get("framework"),
                    "test_files_count": analysis["project_state"]["test_status"].get("test_files_count", 0),
                    "coverage": analysis["project_state"]["test_status"].get("coverage", {}).get("percentage")
                },
                "dependencies": {
                    "count": analysis["project_state"]["dependencies"].get("dependencies_count", 0),
                    "outdated_count": len(analysis["project_state"]["dependencies"].get("outdated_packages", [])),
                    "package_manager": analysis["project_state"]["dependencies"].get("package_manager")
                },
                "code_quality": {
                    "linter": analysis["project_state"]["code_quality"].get("linter"),
                    "issues_count": analysis["project_state"]["code_quality"].get("issues_count", 0)
                },
                "todos_count": len(analysis["project_state"]["todo_items"])
            },
            "current_file": current_file_entities
        }
        
        return enhanced_context
    
    async def track_entity_access(self, entity_name: str, file_path: Optional[Path] = None) -> None:
        """
        Track access to a code entity (function, class, etc.).
        
        Args:
            entity_name: Name of the entity being accessed
            file_path: Optional path to the file containing the entity
        """
        if not entity_name:
            return
        
        # If file_path is not provided, try to find it from the entity map
        if not file_path and entity_name in self._entity_map:
            file_path = Path(self._entity_map[entity_name])
        
        if not file_path:
            return
        
        # Add to recent entity usages
        timestamp = datetime.now().timestamp()
        self._recent_entity_usages.append({
            "entity_name": entity_name,
            "file_path": str(file_path),
            "timestamp": timestamp
        })
        
        # Keep only the most recent 100 usages
        if len(self._recent_entity_usages) > 100:
            self._recent_entity_usages = self._recent_entity_usages[-100:]
        
        # Also track file access at the file level
        file_activity_tracker = get_file_activity_tracker()
        file_activity_tracker.track_file_viewing(file_path, None, {
            "entity_name": entity_name,
            "entity_type": "code_entity",
            "timestamp": timestamp
        })
    
    async def get_entity_info(self, entity_name: str) -> Optional[Dict[str, Any]]:
        """
        Get detailed information about a code entity.
        
        Args:
            entity_name: Name of the entity to look up
            
        Returns:
            Dictionary with entity information or None if not found
        """
        # Get context manager
        context_manager = get_context_manager()
        
        project_root = context_manager.project_root
        if not project_root:
            return None
        
        # Ensure the context is refreshed
        await self.refresh_context()
        
        # Check if the entity exists in our map
        if entity_name not in self._entity_map:
            return None
        
        file_path = self._entity_map[entity_name]
        
        # Track this entity access
        await self.track_entity_access(entity_name, Path(file_path))
        
        # Get the module
        if str(project_root) not in self._project_modules or file_path not in self._project_modules[str(project_root)]:
            return None
        
        module = self._project_modules[str(project_root)][file_path]
        
        # Check if it's a function, class, or class method
        if "." in entity_name:
            # Class method
            class_name, method_name = entity_name.split(".", 1)
            
            if class_name in module.classes and method_name in module.classes[class_name].methods:
                method = module.classes[class_name].methods[method_name]
                return {
                    "type": "method",
                    "name": method_name,
                    "class_name": class_name,
                    "file_path": file_path,
                    "line_start": method.line_start,
                    "line_end": method.line_end,
                    "params": method.params,
                    "docstring": method.docstring,
                    "return_type": method.return_type,
                    "complexity": method.complexity
                }
        
        elif entity_name in module.functions:
            # Function
            function = module.functions[entity_name]
            return {
                "type": "function",
                "name": entity_name,
                "file_path": file_path,
                "line_start": function.line_start,
                "line_end": function.line_end,
                "params": function.params,
                "docstring": function.docstring,
                "return_type": function.return_type,
                "complexity": function.complexity,
                "called_functions": function.called_functions
            }
        
        elif entity_name in module.classes:
            # Class
            cls = module.classes[entity_name]
            return {
                "type": "class",
                "name": entity_name,
                "file_path": file_path,
                "line_start": cls.line_start,
                "line_end": cls.line_end,
                "docstring": cls.docstring,
                "base_classes": cls.base_classes,
                "methods": list(cls.methods.keys()),
                "attributes": list(cls.attributes.keys())
            }
        
        return None
    
    async def find_related_code(self, query: str, limit: int = 5) -> List[Dict[str, Any]]:
        """
        Find code entities related to a natural language query.
        
        Args:
            query: Natural language query describing code functionality
            limit: Maximum number of results to return
            
        Returns:
            List of entity information dictionaries
        """
        # Get context manager
        context_manager = get_context_manager()
        
        project_root = context_manager.project_root
        if not project_root:
            return []
        
        # Ensure the context is refreshed
        await self.refresh_context()
        
        if str(project_root) not in self._project_modules:
            return []
        
        # Simple keyword matching for now
        # In a real implementation, you might use embedding-based similarity
        keywords = query.lower().split()
        matches = []
        
        # Search through all modules
        for file_path, module in self._project_modules[str(project_root)].items():
            # Search functions
            for func_name, func in module.functions.items():
                score = self._calculate_match_score(func, keywords)
                if score > 0:
                    matches.append({
                        "entity_name": func_name,
                        "type": "function",
                        "file_path": file_path,
                        "score": score,
                        "preview": func.docstring[:100] + "..." if func.docstring and len(func.docstring) > 100 else func.docstring,
                        "line": func.line_start
                    })
            
            # Search classes
            for class_name, cls in module.classes.items():
                score = self._calculate_match_score(cls, keywords)
                if score > 0:
                    matches.append({
                        "entity_name": class_name,
                        "type": "class",
                        "file_path": file_path,
                        "score": score,
                        "preview": cls.docstring[:100] + "..." if cls.docstring and len(cls.docstring) > 100 else cls.docstring,
                        "line": cls.line_start
                    })
                
                # Search class methods
                for method_name, method in cls.methods.items():
                    score = self._calculate_match_score(method, keywords)
                    if score > 0:
                        matches.append({
                            "entity_name": f"{class_name}.{method_name}",
                            "type": "method",
                            "file_path": file_path,
                            "score": score,
                            "preview": method.docstring[:100] + "..." if method.docstring and len(method.docstring) > 100 else method.docstring,
                            "line": method.line_start
                        })
        
        # Sort by score
        matches.sort(key=lambda x: x["score"], reverse=True)
        
        # Return top results
        return matches[:limit]
    
    def _calculate_match_score(self, entity: Any, keywords: List[str]) -> float:
        """
        Calculate a match score between an entity and keywords.
        
        Args:
            entity: Code entity (function, class, etc.)
            keywords: List of keywords to match
            
        Returns:
            Score value where higher is better
        """
        score = 0.0
        
        # Check entity name
        name_words = entity.name.lower().split('_')
        for word in name_words:
            for keyword in keywords:
                if keyword in word:
                    score += 1.0 if keyword == word else 0.5
        
        # Check docstring
        if entity.docstring:
            for keyword in keywords:
                if keyword in entity.docstring.lower():
                    score += 0.3
        
        return score
    
    async def get_recent_entity_usages(self, limit: int = 10) -> List[Dict[str, Any]]:
        """
        Get the most recently used code entities.
        
        Args:
            limit: Maximum number of results to return
            
        Returns:
            List of entity usage information
        """
        # Sort by timestamp (newest first)
        sorted_usages = sorted(
            self._recent_entity_usages, 
            key=lambda x: x["timestamp"], 
            reverse=True
        )
        
        # Take only the most recent usage of each entity
        unique_entities = {}
        for usage in sorted_usages:
            entity_name = usage["entity_name"]
            if entity_name not in unique_entities:
                unique_entities[entity_name] = usage
                
                # Add entity information
                try:
                    entity_info = await self.get_entity_info(entity_name)
                    if entity_info:
                        unique_entities[entity_name]["info"] = entity_info
                except Exception:
                    pass
        
        # Convert to list and limit
        return list(unique_entities.values())[:limit]
    
    async def get_code_summary(self, file_path: Union[str, Path]) -> Dict[str, Any]:
        """
        Get a summary of the code in a file.
        
        Args:
            file_path: Path to the file
            
        Returns:
            Dictionary with code summary information
        """
        path_obj = Path(file_path)
        
        # Get context manager
        context_manager = get_context_manager()
        project_root = context_manager.project_root
        
        if not project_root:
            return {"error": "No project root detected"}
        
        # Ensure the context is refreshed
        await self.refresh_context()
        
        if str(project_root) not in self._project_modules:
            return {"error": "Project not analyzed"}
        
        # Get the module
        file_str = str(path_obj)
        if file_str not in self._project_modules[str(project_root)]:
            # Try to analyze the file now
            try:
                semantic_analyzer = get_semantic_analyzer()
                module = await semantic_analyzer.analyze_file(path_obj)
                if module:
                    self._project_modules[str(project_root)][file_str] = module
                else:
                    return {"error": "Failed to analyze file"}
            except Exception as e:
                return {"error": f"Error analyzing file: {str(e)}"}
        
        module = self._project_modules[str(project_root)][file_str]
        
        # Create a summary
        return {
            "file_path": file_str,
            "language": module.language,
            "docstring": module.docstring,
            "functions": [
                {
                    "name": name,
                    "params": func.params,
                    "docstring": func.docstring[:100] + "..." if func.docstring and len(func.docstring) > 100 else func.docstring,
                    "complexity": func.complexity
                }
                for name, func in module.functions.items()
            ],
            "classes": [
                {
                    "name": name,
                    "docstring": cls.docstring[:100] + "..." if cls.docstring and len(cls.docstring) > 100 else cls.docstring,
                    "methods_count": len(cls.methods),
                    "attributes_count": len(cls.attributes),
                    "base_classes": cls.base_classes
                }
                for name, cls in module.classes.items()
            ],
            "imports": len(module.imports),
            "metrics": module.code_metrics
        }
    
    async def get_project_summary(self) -> Dict[str, Any]:
        """
        Get a summary of the current project.
        
        Returns:
            Dictionary with project summary information
        """
        # Get context manager
        context_manager = get_context_manager()
        project_root = context_manager.project_root
        
        if not project_root:
            return {"error": "No project root detected"}
        
        # Ensure the context is refreshed
        await self.refresh_context()
        
        if str(project_root) not in self._analysis_cache:
            return {"error": "Project not analyzed"}
        
        # Get the analysis results
        analysis = self._analysis_cache[str(project_root)]
        
        # Build a detailed project summary
        return {
            "project_root": str(project_root),
            "project_type": analysis["project_state"].get("project_type", "unknown"),
            "git_info": {
                "current_branch": analysis["project_state"]["git_state"].get("current_branch"),
                "is_git_repo": analysis["project_state"]["git_state"].get("is_git_repo", False),
                "has_changes": analysis["project_state"]["git_state"].get("has_changes", False),
                "modified_files": analysis["project_state"]["git_state"].get("modified_files", []),
                "untracked_files": analysis["project_state"]["git_state"].get("untracked_files", []),
                "recent_commits": analysis["project_state"]["git_state"].get("recent_commits", [])
            },
            "test_info": {
                "framework": analysis["project_state"]["test_status"].get("framework"),
                "test_files_count": analysis["project_state"]["test_status"].get("test_files_count", 0),
                "coverage": analysis["project_state"]["test_status"].get("coverage", {}).get("percentage")
            },
            "build_info": {
                "system": analysis["project_state"]["build_status"].get("system"),
                "last_build": analysis["project_state"]["build_status"].get("last_build"),
                "artifacts_count": len(analysis["project_state"]["build_status"].get("artifacts", []))
            },
            "dependency_info": {
                "package_manager": analysis["project_state"]["dependencies"].get("package_manager"),
                "dependencies_count": analysis["project_state"]["dependencies"].get("dependencies_count", 0),
                "dev_dependencies_count": analysis["project_state"]["dependencies"].get("dev_dependencies_count", 0),
                "outdated_packages": analysis["project_state"]["dependencies"].get("outdated_packages", [])
            },
            "code_quality": {
                "linter": analysis["project_state"]["code_quality"].get("linter"),
                "formatter": analysis["project_state"]["code_quality"].get("formatter"),
                "issues_count": analysis["project_state"]["code_quality"].get("issues_count", 0)
            },
            "todo_items": analysis["project_state"]["todo_items"][:5],  # Include only the first 5 for brevity
            "semantic_stats": {
                "analyzed_files": analysis["semantic_analysis"]["analyzed_files_count"],
                "functions": analysis["semantic_analysis"]["entities"]["functions"],
                "classes": analysis["semantic_analysis"]["entities"]["classes"],
                "total_lines": analysis["semantic_analysis"]["metrics"].get("total_lines", 0),
                "avg_func_complexity": analysis["semantic_analysis"]["metrics"].get("average_function_complexity", 0)
            }
        }

# Global semantic context manager instance
semantic_context_manager = SemanticContextManager()
</file>

<file path="components/context/session.py">
# angela/context/session.py

import json
import time
from typing import Dict, Any, Optional, List, Set
from datetime import datetime, timedelta
from dataclasses import dataclass, field, asdict

from angela.api.context import get_preferences_manager
from angela.utils.logging import get_logger

logger = get_logger(__name__)

@dataclass
class EntityReference:
    """A reference to an entity in the session context."""
    name: str  # Name or identifier of the entity
    type: str  # Type of entity (file, directory, command, result, etc.)
    value: str  # Actual value or path 
    created: datetime = field(default_factory=datetime.now)
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for serialization."""
        return {
            "name": self.name,
            "type": self.type,
            "value": self.value,
            "created": self.created.isoformat()
        }
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'EntityReference':
        """Create from dictionary."""
        return cls(
            name=data["name"],
            type=data["type"],
            value=data["value"],
            created=datetime.fromisoformat(data["created"])
        )


class SessionMemory:
    """Memory for a single conversation session."""
    
    def __init__(self):
        """Initialize session memory."""
        self.entities: Dict[str, EntityReference] = {}
        self.recent_commands: List[str] = []
        self.recent_results: List[str] = []
        self.created = datetime.now()
        self.last_accessed = datetime.now()
    
    def add_entity(self, name: str, entity_type: str, value: str) -> None:
        """
        Add an entity to the session memory.
        
        Args:
            name: The name or identifier of the entity
            entity_type: The type of entity
            value: The value or path of the entity
        """
        self.entities[name] = EntityReference(name, entity_type, value)
        self.last_accessed = datetime.now()
    
    def get_entity(self, name: str) -> Optional[EntityReference]:
        """
        Get an entity from the session memory.
        
        Args:
            name: The name or identifier of the entity
            
        Returns:
            The entity reference, or None if not found
        """
        self.last_accessed = datetime.now()
        return self.entities.get(name)
    
    def add_command(self, command: str) -> None:
        """
        Add a command to the recent commands list.
        
        Args:
            command: The command string
        """
        self.recent_commands.append(command)
        self.last_accessed = datetime.now()
        
        # Keep only the last 10 commands
        if len(self.recent_commands) > 10:
            self.recent_commands.pop(0)
    
    def add_result(self, result: str) -> None:
        """
        Add a result to the recent results list.
        
        Args:
            result: The result string
        """
        self.recent_results.append(result)
        self.last_accessed = datetime.now()
        
        # Keep only the last 5 results
        if len(self.recent_results) > 5:
            self.recent_results.pop(0)
    
    def get_context_dict(self) -> Dict[str, Any]:
        """
        Get the session memory as a dictionary.
        
        Returns:
            A dictionary representation of the session memory
        """
        return {
            "entities": {k: v.to_dict() for k, v in self.entities.items()},
            "recent_commands": self.recent_commands,
            "recent_results": self.recent_results,
            "created": self.created.isoformat(),
            "last_accessed": self.last_accessed.isoformat()
        }
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'SessionMemory':
        """
        Create a session memory from a dictionary.
        
        Args:
            data: The dictionary representation
            
        Returns:
            A new SessionMemory instance
        """
        session = cls()
        session.entities = {
            k: EntityReference.from_dict(v) 
            for k, v in data.get("entities", {}).items()
        }
        session.recent_commands = data.get("recent_commands", [])
        session.recent_results = data.get("recent_results", [])
        session.created = datetime.fromisoformat(data["created"])
        session.last_accessed = datetime.fromisoformat(data["last_accessed"])
        return session


class SessionManager:
    """Manager for conversation session memories."""
    
    def __init__(self):
        """Initialize the session manager."""
        self._current_session = SessionMemory()
        self._logger = logger
    
    def refresh_session(self) -> None:
        """Refresh the current session or create a new one if expired."""
        # Check if session is enabled in preferences
        preferences_manager = get_preferences_manager()
        if not preferences_manager.preferences.context.remember_session_context:
            self._current_session = SessionMemory()
            return
        
        # Check if the session has expired (2 hours of inactivity)
        now = datetime.now()
        session_timeout = timedelta(hours=2)
        
        if now - self._current_session.last_accessed > session_timeout:
            self._logger.debug("Session expired, creating new session")
            self._current_session = SessionMemory()
    
    def add_entity(self, name: str, entity_type: str, value: str) -> None:
        """
        Add an entity to the current session.
        
        Args:
            name: The name or identifier of the entity
            entity_type: The type of entity
            value: The value or path of the entity
        """
        self.refresh_session()
        self._current_session.add_entity(name, entity_type, value)
    
    def get_entity(self, name: str) -> Optional[EntityReference]:
        """
        Get an entity from the current session.
        
        Args:
            name: The name or identifier of the entity
            
        Returns:
            The entity reference, or None if not found
        """
        self.refresh_session()
        return self._current_session.get_entity(name)
    
    def add_command(self, command: str) -> None:
        """
        Add a command to the current session.
        
        Args:
            command: The command string
        """
        self.refresh_session()
        self._current_session.add_command(command)
    
    def add_result(self, result: str) -> None:
        """
        Add a result to the current session.
        
        Args:
            result: The result string
        """
        self.refresh_session()
        self._current_session.add_result(result)
    
    def get_context(self) -> Dict[str, Any]:
        """
        Get the current session context as a dictionary.
        
        Returns:
            A dictionary representation of the current session context
        """
        self.refresh_session()
        return self._current_session.get_context_dict()
    
    def clear_session(self) -> None:
        """Clear the current session."""
        self._current_session = SessionMemory()

# Global session manager instance
session_manager = SessionManager()
</file>

<file path="components/execution/__init__.py">
# angela/execution/__init__.py
"""
Execution components for Angela CLI.

This package provides functionality for executing commands, managing file operations,
handling errors during execution, and supporting rollback capabilities.
"""
# Export key components that other modules need
from .engine import execution_engine
from .adaptive_engine import adaptive_engine
from .rollback import rollback_manager
from .filesystem import (
    create_directory, delete_directory,
    create_file, read_file, write_file, delete_file,
    copy_file, move_file, FileSystemError
)
from .hooks import execution_hooks

__all__ = [
    'execution_engine', 
    'adaptive_engine',
    'rollback_manager',
    'execution_hooks',
    'create_directory', 'delete_directory',
    'create_file', 'read_file', 'write_file', 'delete_file',
    'copy_file', 'move_file', 'FileSystemError'
]
</file>

<file path="components/execution/adaptive_engine.py">
# angela/execution/adaptive_engine.py

import asyncio
import os
import sys
import signal
import time
from typing import Dict, Any, Optional, List, Tuple, Union
from pathlib import Path

from rich.console import Console
from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn, TimeElapsedColumn

# Import through API layer - fixed imports
from angela.api.safety import get_command_risk_classifier, get_adaptive_confirmation
from angela.api.execution import get_execution_engine
from angela.api.context import get_history_manager, get_preferences_manager, get_session_manager
from angela.utils.logging import get_logger

logger = get_logger(__name__)
console = Console()

class AdaptiveExecutionEngine:
    """
    Context-aware command execution engine.
    
    This engine adapts its behavior based on user history, preferences,
    and command characteristics.
    """
    
    def __init__(self):
        """Initialize the adaptive execution engine."""
        self._logger = logger
        self._error_analyzer = None # Initialize to None
    
    def _get_error_analyzer(self): # New helper method
        if self._error_analyzer is None:
            # Import through API layer
            from angela.api.ai import get_error_analyzer
            self._error_analyzer = get_error_analyzer()
        return self._error_analyzer
    
    async def execute_command(
        self, 
        command: str,
        natural_request: str,
        explanation: Optional[str] = None,
        dry_run: bool = False
    ) -> Dict[str, Any]:
        """
        Execute a command with adaptive behavior based on user context.
        
        Args:
            command: The command to execute
            natural_request: The original natural language request
            explanation: AI explanation of what the command does
            dry_run: Whether to simulate the command without execution
            
        Returns:
            Dictionary with execution results
        """
        self._logger.info(f"Preparing to execute command: {command}")
        
        # Analyze command risk and impact
        classifier = get_command_risk_classifier()
        risk_level, risk_reason = classifier.classify(command)
        impact = classifier.analyze_impact(command)
        
        # Add to session context
        session_manager = get_session_manager()
        session_manager.add_command(command)
        
        # Generate command preview if needed
        preferences_manager = get_preferences_manager()
        
        preview = None
        if preferences_manager.preferences.ui.show_command_preview:
            from angela.api.safety import get_command_preview_generator
            preview_generator = get_command_preview_generator()
            preview = await preview_generator.generate_preview(command)
        
        # Get confidence score if available
        confidence_score = None
        try:
            from angela.api.ai import get_confidence_scorer
            confidence_scorer = get_confidence_scorer()
            context = {"request": natural_request}
            confidence_score = confidence_scorer.score_command_confidence(natural_request, command, context)
        except Exception as e:
            self._logger.error(f"Error calculating confidence score: {str(e)}")
        
        # Prepare full information to display before confirmation
        command_info = {
            "command": command,
            "risk_level": risk_level,
            "risk_reason": risk_reason,
            "impact": impact,
            "preview": preview,
            "explanation": explanation,
            "confidence_score": confidence_score,
            "dry_run": dry_run
        }
        
        # Get adaptive confirmation based on risk level and user history
        confirmation_handler = get_adaptive_confirmation()
        confirmed = await confirmation_handler(
            command=command,
            risk_level=risk_level,
            risk_reason=risk_reason,
            impact=impact,
            preview=preview,
            explanation=explanation,
            natural_request=natural_request,
            dry_run=dry_run,
            confidence_score=confidence_score,  # Pass the confidence score
            command_info=command_info          # Pass the command info
        )
        
        if not confirmed and not dry_run:
            self._logger.info(f"Command execution cancelled by user: {command}")
            return {
                "command": command,
                "success": False,
                "cancelled": True,
                "stdout": "",
                "stderr": "Command execution cancelled by user",
                "return_code": 1,
                "dry_run": dry_run,
                "confidence": confidence_score  # Include confidence in result
            }
        
        # Execute the command
        result = await self._execute_with_feedback(command, dry_run)
        
        # Add confidence score to the result
        result["confidence"] = confidence_score
        
        # Add to history
        history_manager = get_history_manager()
        history_manager.add_command(
            command=command,
            natural_request=natural_request,
            success=result["success"],
            output=result.get("stdout", ""),
            error=result.get("stderr", ""),
            risk_level=risk_level
        )
        
        # If execution failed, analyze error and suggest fixes
        if not result["success"] and result.get("stderr"):
            error_analyzer = self._get_error_analyzer()
            result["error_analysis"] = error_analyzer.analyze_error(command, result["stderr"])
            result["fix_suggestions"] = error_analyzer.generate_fix_suggestions(command, result["stderr"])
        
        # Offer to learn from successful executions
        if result["success"] and risk_level > 0:
            from angela.api.safety import offer_command_learning
            if offer_command_learning:
                await offer_command_learning(command)
        
        return result
    
    async def _execute_with_feedback(self, command: str, dry_run: bool) -> Dict[str, Any]:
        """
        Execute a command with rich feedback.
        
        Args:
            command: The command to execute
            dry_run: Whether to simulate the command without execution
            
        Returns:
            Dictionary with execution results
        """
        preferences_manager = get_preferences_manager()
        use_spinners = preferences_manager.preferences.ui.use_spinners
        
        # For dry runs, return the preview directly
        if dry_run:
            # Get execution engine through API
            execution_engine = get_execution_engine()
            
            # Execute in dry-run mode
            stdout, stderr, return_code = await execution_engine.dry_run_command(command)
            
            return {
                "command": command,
                "success": True,
                "stdout": stdout,
                "stderr": stderr,
                "return_code": return_code,
                "dry_run": True
            }
        
        # Get execution engine through API
        execution_engine = get_execution_engine()
        
        # Show execution spinner if enabled
        from angela.api.shell import display_execution_timer
        stdout, stderr, return_code, execution_time = await display_execution_timer(
            command,
            with_philosophy=True
        )
        
        # Store result in session for reference
        session_manager = get_session_manager()
        if stdout.strip():
            session_manager.add_result(stdout.strip())
        
        return {
            "command": command,
            "success": return_code == 0,
            "stdout": stdout,
            "stderr": stderr,
            "return_code": return_code,
            "dry_run": False
        }

# Global adaptive execution engine instance
adaptive_engine = AdaptiveExecutionEngine()
</file>

<file path="components/execution/engine.py">
# angela/execution/engine.py 
"""
Engine for safely executing commands.
"""
import asyncio
import shlex
import subprocess
from typing import Dict, Any, List, Tuple, Optional, TYPE_CHECKING

# Import through API layer
from angela.utils.logging import get_logger
from angela.core.registry import registry  # Fixed import

if TYPE_CHECKING:
    from angela.intent.models import ActionPlan
    
logger = get_logger(__name__)

class ExecutionEngine:
    """Engine for safely executing commands."""
    
    def __init__(self):
        """Initialize the execution engine."""
        self._logger = logger
    
    def _get_safety_check_function(self):
        """Get the safety check function with lazy imports to avoid circular dependencies."""
        # Use direct import here as a fallback if registry approach fails
        try:
            # Try registry first
            check_func = registry.get("check_command_safety")
            
            if check_func:
                return check_func
                
            # Direct import as fallback (should not normally happen once initialized)
            from angela.components.safety import check_command_safety
            return check_command_safety
        except ImportError:
            self._logger.error("Could not import safety check function")
            return None
    
    async def execute_command(
        self, 
        command: str,
        check_safety: bool = True,
        dry_run: bool = False
    ) -> Tuple[str, str, int]:
        """
        Execute a shell command and return its output.
        
        Args:
            command: The shell command to execute.
            check_safety: Whether to perform safety checks before execution.
            dry_run: Whether to simulate the command without actual execution.
            
        Returns:
            A tuple of (stdout, stderr, return_code).
        """
        self._logger.info(f"Preparing to execute command: {command}")
        
        # If safety checks are requested, perform them
        if check_safety:
            # Get safety check function using our helper method
            check_command_safety_func = self._get_safety_check_function()

            if not check_command_safety_func:
                self._logger.error("Safety check function not available")
                return "", "Safety check function not configured", 1

            # Check if the command is safe to execute
            is_safe = await check_command_safety_func(command, dry_run)
            if not is_safe:
                self._logger.warning(f"Command execution cancelled due to safety concerns: {command}")
                return "", "Command execution cancelled due to safety concerns", 1

            # For dry runs, return simulated results
            if dry_run:
                self._logger.info(f"DRY RUN: Would execute command: {command}")
                return f"[DRY RUN] Would execute: {command}", "", 0
        
        # Execute the command
        try:
            # Split the command properly using shlex
            args = shlex.split(command)
            
            # Execute the command and capture output
            if command.startswith('cd ') and ' && ' in command:
                # Extract working directory and actual command
                cd_part, actual_command = command.split(' && ', 1)
                working_dir = cd_part[3:].strip()  # Remove the 'cd ' prefix
                
                # Execute the actual command with the correct working directory
                process = await asyncio.create_subprocess_exec(
                    *shlex.split(actual_command),
                    stdout=asyncio.subprocess.PIPE,
                    stderr=asyncio.subprocess.PIPE,
                    cwd=working_dir  # Set the working directory instead of using cd
                )
            else:
                # For regular commands without cd
                use_shell = '&&' in command or '|' in command or '>' in command or '<' in command
                
                if use_shell:
                    # Use shell mode for complex shell commands
                    process = await asyncio.create_subprocess_shell(
                        command,
                        stdout=asyncio.subprocess.PIPE,
                        stderr=asyncio.subprocess.PIPE,
                    )
                else:
                    # For simple commands, use exec
                    process = await asyncio.create_subprocess_exec(
                        *shlex.split(command),
                        stdout=asyncio.subprocess.PIPE,
                        stderr=asyncio.subprocess.PIPE,
                    )
            
            # Wait for the command to complete
            stdout_bytes, stderr_bytes = await process.communicate()
            stdout = stdout_bytes.decode('utf-8', errors='replace')
            stderr = stderr_bytes.decode('utf-8', errors='replace')
            
            self._logger.debug(f"Command completed with return code: {process.returncode}")
            self._logger.debug(f"stdout: {stdout[:100]}{'...' if len(stdout) > 100 else ''}")
            if stderr:
                self._logger.debug(f"stderr: {stderr}")
            
            # Record the operation for potential rollback
            if not dry_run and process.returncode == 0:
                # Get rollback_manager safely without circular imports
                try:
                    # Try getting from registry first 
                    rollback_manager_instance = registry.get("rollback_manager")
                    
                    if rollback_manager_instance:
                        await rollback_manager_instance.record_operation(
                            operation_type="execute_command",
                            params={"command": command},
                            backup_path=None  # Commands don't have direct file backups
                        )
                except Exception as e:
                    self._logger.warning(f"Could not record operation for rollback: {e}")
            
            return stdout, stderr, process.returncode
        
        except Exception as e:
            self._logger.exception(f"Error executing command '{command}': {str(e)}")
            return "", str(e), -1
    
    async def dry_run_command(self, command: str) -> Tuple[str, str, int]:
        """
        Simulate command execution for previewing without actually running the command.
        
        Args:
            command: The shell command to simulate
            
        Returns:
            A tuple of (stdout, stderr, return_code)
        """
        return f"[DRY RUN] Would execute: {command}", "", 0

# Global execution engine instance
execution_engine = ExecutionEngine()
</file>

<file path="components/execution/error_recovery.py">
# angela/execution/error_recovery.py

import os
import re
import asyncio
import tempfile
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple, Set, Union
from enum import Enum
from datetime import datetime

# Import through API layer
from angela.api.ai import get_gemini_client, get_gemini_request_class
from angela.api.ai import get_error_analyzer
from angela.api.context import get_context_manager
from angela.utils.logging import get_logger

logger = get_logger(__name__)

class RecoveryStrategy(Enum):
    """Types of error recovery strategies."""
    RETRY = "retry"                 # Simple retry
    MODIFY_COMMAND = "modify"       # Modify the command and retry
    ALTERNATIVE_COMMAND = "alternative"  # Try an alternative command
    PREPARE_ENV = "prepare"         # Prepare the environment and retry
    REVERT_CHANGES = "revert"       # Revert changes and retry
    SKIP = "skip"                   # Skip the step and continue
    ABORT = "abort"                 # Abort the plan execution

class ErrorRecoveryManager:
    """
    Manager for error recovery during multi-step execution.
    
    This class provides:
    1. Analysis of execution errors
    2. Automatic recovery strategies
    3. Guided recovery with user input
    4. Learning from successful recoveries
    """
    
    def __init__(self):
        """Initialize the error recovery manager."""
        self._logger = logger
        self._recovery_history = {}  
        self._success_patterns = {}
        
            
    async def handle_error(
        self, 
        step: Any, 
        error_result: Dict[str, Any], 
        context: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Handle an error during step execution with enhanced recovery learning.
        
        Args:
            step: The step that failed
            error_result: The execution result with error information
            context: Context information
            
        Returns:
            Updated execution result with recovery information
        """
        self._logger.info(f"Handling error for step {step.id if hasattr(step, 'id') else 'unknown'}")
        
        # Extract error information
        command = error_result.get("command") or getattr(step, "command", None)
        stderr = error_result.get("stderr", "")
        error_msg = error_result.get("error", "")
        
        if not command or (not stderr and not error_msg):
            # Not enough information to recover
            self._logger.warning("Insufficient information for error recovery")
            return error_result
        
        # Analyze the error
        analysis = await self._analyze_error(command, stderr or error_msg)
        
        # Get error pattern for historical matching
        error_pattern = self._extract_error_pattern(error_result)
        
        # Generate recovery strategies
        recovery_strategies = await self._generate_recovery_strategies(command, analysis, context)
        
        # Check if we have historical successes for this error pattern
        if error_pattern in self._success_patterns:
            self._logger.info(f"Found historical recovery strategies for error pattern: {error_pattern}")
            # Add strategies from successful historical recoveries
            for strategy_record in self._success_patterns[error_pattern]:
                historical_strategy = {
                    "type": strategy_record["type"],
                    "command": strategy_record["command"],
                    "description": f"Previously successful strategy (used {strategy_record['success_count']} times)",
                    "confidence": min(0.5 + (strategy_record["success_count"] * 0.1), 0.9),
                    "source": "history"
                }
                # Add to beginning of strategies list if not already present
                if not any(s["type"] == historical_strategy["type"] and s["command"] == historical_strategy["command"] 
                        for s in recovery_strategies):
                    recovery_strategies.insert(0, historical_strategy)
        
        # Prioritize strategies based on historical success
        prioritized_strategies = self._prioritize_strategies_by_history(recovery_strategies)
        
        # Record error analysis and strategies
        result = dict(error_result)
        result["error_analysis"] = analysis
        result["recovery_strategies"] = prioritized_strategies
        
        # Check if we can auto-recover
        if prioritized_strategies and self._can_auto_recover(prioritized_strategies[0]):
            # Attempt automatic recovery
            recovery_result = await self._execute_recovery_strategy(
                prioritized_strategies[0], step, error_result, context
            )
            
            # Update the result
            result["recovery_attempted"] = True
            result["recovery_strategy"] = prioritized_strategies[0]
            result["recovery_success"] = recovery_result.get("success", False)
            
            # Learn from this recovery attempt
            await self._learn_from_recovery_result(recovery_result, step, error_result, context)
            
            # If recovery succeeded, replace the result
            if recovery_result.get("success", False):
                self._logger.info(f"Automatic recovery succeeded for step {getattr(step, 'id', 'unknown')}")
                result.update(recovery_result)
        else:
            # Guided recovery with user input
            recovery_result = await self._guided_recovery(prioritized_strategies, step, error_result, context)
            
            # Update the result
            result["recovery_attempted"] = recovery_result is not None
            if recovery_result:
                result["recovery_strategy"] = recovery_result.get("strategy", {})
                result["recovery_success"] = recovery_result.get("success", False)
                
                # Learn from this recovery attempt
                await self._learn_from_recovery_result(recovery_result, step, error_result, context)
                
                # If recovery succeeded, replace the result
                if recovery_result.get("success", False):
                    self._logger.info(f"Guided recovery succeeded for step {getattr(step, 'id', 'unknown')}")
                    result.update(recovery_result)
        
        return result
    
    async def _learn_from_recovery_result(
        self, 
        recovery_result: Dict[str, Any], 
        step: Any, 
        error_result: Dict[str, Any], 
        context: Dict[str, Any]
    ) -> None:
        """
        Learn from recovery attempts to improve future strategies.
        
        Args:
            recovery_result: Result from the recovery attempt
            step: The step that failed
            error_result: Original error information
            context: Context information
        """
        if recovery_result.get("success", False):
            strategy = recovery_result.get("strategy", recovery_result.get("recovery_strategy", {}))
            strategy_type = strategy.get("type")
            error_pattern = self._extract_error_pattern(error_result)
            
            self._logger.debug(f"Learning from successful recovery of type {strategy_type} for pattern {error_pattern}")
            
            if strategy_type and error_pattern:
                # Update recovery history
                strategy_key = f"{strategy_type}:{strategy.get('command', '')}"
                if strategy_key not in self._recovery_history:
                    self._recovery_history[strategy_key] = {"success_count": 0}
                
                self._recovery_history[strategy_key]["success_count"] += 1
                self._recovery_history[strategy_key]["last_success"] = datetime.now().isoformat()
                
                # Record successful strategy for this error pattern
                if error_pattern not in self._success_patterns:
                    self._success_patterns[error_pattern] = []
                
                # Check if strategy already exists for this pattern
                existing_strategy = next(
                    (s for s in self._success_patterns[error_pattern] 
                     if s["type"] == strategy_type and s["command"] == strategy.get("command", "")),
                    None
                )
                
                if existing_strategy:
                    # Update existing strategy
                    existing_strategy["success_count"] += 1
                    existing_strategy["last_success"] = datetime.now().isoformat()
                else:
                    # Add new strategy record
                    strategy_record = {
                        "type": strategy_type,
                        "command": strategy.get("command", ""),
                        "success_count": 1,
                        "last_success": datetime.now().isoformat()
                    }
                    self._success_patterns[error_pattern].append(strategy_record)
                
                # Publish learning event if event_bus is available
                try:
                    if 'event_bus' in globals() or hasattr(self, 'event_bus'):
                        event_bus_obj = globals().get('event_bus', getattr(self, 'event_bus', None))
                        if event_bus_obj and hasattr(event_bus_obj, 'publish'):
                            await event_bus_obj.publish("recovery:learning", {
                                "error_pattern": error_pattern,
                                "successful_strategy": strategy_type,
                                "timestamp": datetime.now().isoformat()
                            })
                except Exception as e:
                    self._logger.error(f"Error publishing learning event: {str(e)}")
    
    def _extract_error_pattern(
        self, 
        error_result: Dict[str, Any]
    ) -> str:
        """
        Extract a pattern that identifies the type of error.
        
        Args:
            error_result: Error information
            
        Returns:
            String pattern identifying the error type
        """
        stderr = error_result.get("stderr", "")
        error_msg = error_result.get("error", "")
        
        # Use stderr or error message
        error_text = stderr or error_msg
        
        # Try to extract the most specific part of the error
        # This is a simplified approach - can be made more sophisticated
        
        # Check for common error patterns
        patterns = self._get_common_error_patterns()
        
        for pattern, pattern_name in patterns:
            if re.search(pattern, error_text, re.IGNORECASE):
                return pattern_name
        
        # If no specific pattern matches, return a more generic one
        # based on the first line of the error
        first_line = error_text.split('\n')[0][:50]
        if first_line:
            return f"generic:{first_line}"
        
        return "unknown_error"
    
    def _prioritize_strategies_by_history(
        self, 
        strategies: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """
        Prioritize recovery strategies based on historical success.
        
        Args:
            strategies: List of recovery strategies
            
        Returns:
            Prioritized list of strategies
        """
        # Create a copy of the strategies to avoid modifying the original
        prioritized = list(strategies)
        
        # Adjust confidence based on historical success
        for strategy in prioritized:
            strategy_key = f"{strategy.get('type')}:{strategy.get('command', '')}"
            
            # Check history for this strategy
            if strategy_key in self._recovery_history:
                history = self._recovery_history[strategy_key]
                success_count = history.get("success_count", 0)
                
                # Boost confidence based on success count
                # Original confidence is weighted at 60%, historical at 40%
                original_confidence = strategy.get("confidence", 0.5)
                history_confidence = min(0.3 + (success_count * 0.1), 0.9)
                
                # Combine confidences
                adjusted_confidence = (original_confidence * 0.6) + (history_confidence * 0.4)
                strategy["confidence"] = min(adjusted_confidence, 0.95)
                
                # Add metadata about historical success
                strategy["history"] = {
                    "success_count": success_count,
                    "last_success": history.get("last_success", "unknown")
                }
        
        # Sort by confidence (highest first)
        return sorted(prioritized, key=lambda s: s.get("confidence", 0), reverse=True)
    
    async def _analyze_error(self, command: str, error: str) -> Dict[str, Any]:
        """
        Analyze an error to determine its cause and possible fixes.
        
        Args:
            command: The command that failed
            error: The error message
            
        Returns:
            Error analysis result
        """
        # Use the error analyzer to analyze the error
        error_analyzer = get_error_analyzer()
        
        analysis = error_analyzer.analyze_error(command, error)
        
        # Generate fix suggestions
        suggestions = error_analyzer.generate_fix_suggestions(command, error)
        analysis["fix_suggestions"] = suggestions
        
        # Add error patterns
        error_patterns = analysis.get("error_patterns", [])
        if not error_patterns:
            # Try to match common error patterns
            for pattern in self._get_common_error_patterns():
                if re.search(pattern["pattern"], error, re.IGNORECASE):
                    error_patterns.append({
                        "pattern": pattern["pattern"],
                        "description": pattern["description"],
                        "fixes": pattern["fixes"]
                    })
        
        analysis["error_patterns"] = error_patterns
        
        return analysis
    
    async def _generate_recovery_strategies(
        self, 
        command: str, 
        analysis: Dict[str, Any], 
        context: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """
        Generate recovery strategies based on error analysis.
        
        Args:
            command: The command that failed
            analysis: Error analysis result
            context: Context information
            
        Returns:
            List of recovery strategies
        """
        strategies = []
        
        # Check if we have fix suggestions
        if analysis.get("fix_suggestions"):
            for suggestion in analysis["fix_suggestions"]:
                # Parse the suggestion to generate a recovery strategy
                strategy = self._parse_fix_suggestion(suggestion, command)
                if strategy:
                    strategies.append(strategy)
        
        # Check for error patterns
        if analysis.get("error_patterns"):
            for pattern in analysis["error_patterns"]:
                for fix in pattern.get("fixes", []):
                    strategy = self._create_strategy_from_pattern_fix(fix, command)
                    if strategy and not any(s["command"] == strategy["command"] for s in strategies):
                        strategies.append(strategy)
        
        # If no strategies yet, use AI to generate strategies
        if not strategies:
            ai_strategies = await self._generate_ai_recovery_strategies(command, analysis, context)
            strategies.extend(ai_strategies)
        
        # Always add retry and skip as fallback strategies
        if not any(s["type"] == RecoveryStrategy.RETRY.value for s in strategies):
            strategies.append({
                "type": RecoveryStrategy.RETRY.value,
                "command": command,
                "description": "Retry the command without changes",
                "confidence": 0.3
            })
        
        # Add skip strategy
        strategies.append({
            "type": RecoveryStrategy.SKIP.value,
            "command": None,
            "description": "Skip this step and continue with the plan",
            "confidence": 0.2
        })
        
        # Sort strategies by confidence
        strategies.sort(key=lambda s: s.get("confidence", 0), reverse=True)
        
        return strategies
    
    def _parse_fix_suggestion(self, suggestion: str, command: str) -> Optional[Dict[str, Any]]:
        """
        Parse a fix suggestion into a recovery strategy.
        
        Args:
            suggestion: The fix suggestion
            command: The original command
            
        Returns:
            Recovery strategy or None if parsing fails
        """
        # Check for suggested commands in the form of "Try: command"
        command_match = re.search(r'try:?\s*`?([^`]+)`?', suggestion, re.IGNORECASE)
        if command_match:
            suggested_command = command_match.group(1).strip()
            return {
                "type": RecoveryStrategy.MODIFY_COMMAND.value,
                "command": suggested_command,
                "description": suggestion,
                "confidence": 0.8
            }
        
        # Check for permission issues
        if "permission" in suggestion.lower():
            if "sudo" not in command.lower() and not command.strip().startswith("sudo "):
                # Add sudo to the command
                sudo_command = f"sudo {command}"
                return {
                    "type": RecoveryStrategy.MODIFY_COMMAND.value,
                    "command": sudo_command,
                    "description": "Add sudo to the command for elevated privileges",
                    "confidence": 0.7
                }
        
        # Check for missing file or directory suggestions
        if "file not found" in suggestion.lower() or "directory not found" in suggestion.lower():
            mkdir_match = re.search(r'mkdir\s+([^\s]+)', suggestion, re.IGNORECASE)
            if mkdir_match:
                dir_path = mkdir_match.group(1)
                return {
                    "type": RecoveryStrategy.PREPARE_ENV.value,
                    "command": f"mkdir -p {dir_path}",
                    "description": f"Create the directory {dir_path} and retry",
                    "confidence": 0.7,
                    "retry_original": True
                }
        
        # General suggestion without a specific command
        return {
            "type": RecoveryStrategy.ALTERNATIVE_COMMAND.value,
            "command": None,  # Will be filled in by AI
            "description": suggestion,
            "confidence": 0.5
        }
    
    def _create_strategy_from_pattern_fix(self, fix: str, command: str) -> Optional[Dict[str, Any]]:
        """
        Create a recovery strategy from a pattern fix.
        
        Args:
            fix: The fix description
            command: The original command
            
        Returns:
            Recovery strategy or None if parsing fails
        """
        # Check for command suggestions
        command_match = re.search(r'`([^`]+)`', fix)
        if command_match:
            suggested_command = command_match.group(1).strip()
            return {
                "type": RecoveryStrategy.ALTERNATIVE_COMMAND.value,
                "command": suggested_command,
                "description": fix,
                "confidence": 0.7
            }
        
        # Check for common actions
        if "install" in fix.lower():
            # Extract package name
            pkg_match = re.search(r'install\s+(\w+)', fix, re.IGNORECASE)
            if pkg_match:
                pkg_name = pkg_match.group(1)
                return {
                    "type": RecoveryStrategy.PREPARE_ENV.value,
                    "command": f"apt-get install -y {pkg_name}",
                    "description": f"Install missing package: {pkg_name}",
                    "confidence": 0.7,
                    "retry_original": True
                }
        
        return None
    
    async def _generate_ai_recovery_strategies(
        self, 
        command: str, 
        analysis: Dict[str, Any], 
        context: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """
        Generate recovery strategies using the AI.
        
        Args:
            command: The command that failed
            analysis: Error analysis result
            context: Context information
            
        Returns:
            List of AI-generated recovery strategies
        """
        # Build a prompt for strategy generation
        error_summary = analysis.get("error_summary", "Unknown error")
        possible_cause = analysis.get("possible_cause", "Unknown cause")
        
        prompt = f"""
Generate recovery strategies for a failed command execution.

Failed command: `{command}`
Error summary: {error_summary}
Possible cause: {possible_cause}

Generate 2-3 specific recovery strategies, each with:
1. A specific command to execute
2. A description of what the strategy does
3. A confidence level (0.0-1.0)

Format your response as JSON:
[
  {{
    "type": "modify",
    "command": "modified command",
    "description": "Description of the strategy",
    "confidence": 0.8
  }},
  {{
    "type": "prepare",
    "command": "preparation command",
    "description": "Prepare the environment",
    "confidence": 0.7,
    "retry_original": true
  }}
]

Valid strategy types:
- modify: Modify the original command
- alternative: Use an alternative command
- prepare: Prepare the environment and retry
- revert: Revert changes and retry
"""
        
        # Call the AI service
        gemini_client = get_gemini_client()
        GeminiRequest = get_gemini_request_class()
        
        api_request = GeminiRequest(prompt=prompt, max_tokens=1000)
        api_response = await gemini_client.generate_text(api_request)
        
        # Parse the response
        try:
            # Extract JSON from the response
            import re
            import json
            
            json_match = re.search(r'```(?:json)?\s*(.*?)\s*```', api_response.text, re.DOTALL)
            if json_match:
                json_str = json_match.group(1)
            else:
                json_str = api_response.text
            
            # Parse the JSON
            strategies = json.loads(json_str)
            
            # Validate and normalize strategies
            valid_strategies = []
            for strategy in strategies:
                if isinstance(strategy, dict) and "type" in strategy and "command" in strategy:
                    # Ensure type is valid
                    try:
                        RecoveryStrategy(strategy["type"])
                        valid_strategies.append(strategy)
                    except ValueError:
                        # Invalid strategy type, skip it
                        pass
            
            return valid_strategies
            
        except Exception as e:
            self._logger.error(f"Error parsing AI recovery strategies: {str(e)}")
            return []
    
    def _can_auto_recover(self, strategy: Dict[str, Any]) -> bool:
        """
        Determine if a strategy can be applied automatically.
        
        Args:
            strategy: The recovery strategy
            
        Returns:
            True if auto-recovery is possible, False otherwise
        """
        # High confidence strategies can be auto-applied
        if strategy.get("confidence", 0) >= 0.8:
            return True
        
        # Certain strategy types can always be auto-applied
        auto_types = [
            RecoveryStrategy.RETRY.value
        ]
        
        if strategy.get("type") in auto_types:
            return True
        
        # Check if the strategy has been successful in the past
        strategy_key = f"{strategy.get('type')}:{strategy.get('command')}"
        if self._recovery_history.get(strategy_key, {}).get("success_count", 0) > 0:
            return True
        
        return False
    
    async def _execute_recovery_strategy(
        self, 
        strategy: Dict[str, Any], 
        step: Any, 
        error_result: Dict[str, Any], 
        context: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Execute a recovery strategy.
        
        Args:
            strategy: The recovery strategy to execute
            step: The step that failed
            error_result: The original error result
            context: Context information
            
        Returns:
            Updated execution result
        """
        self._logger.info(f"Executing recovery strategy: {strategy.get('type')}")
        
        # Import through API layer
        from angela.api.execution import get_execution_engine
        execution_engine = get_execution_engine()
        
        result = {
            "strategy": strategy,
            "original_error": error_result.get("error"),
            "original_stderr": error_result.get("stderr")
        }
        
        try:
            # Execute based on strategy type
            strategy_type = strategy.get("type")
            
            if strategy_type == RecoveryStrategy.RETRY.value:
                # Simple retry of the original command
                command = getattr(step, "command", None) or error_result.get("command")
                if command:
                    stdout, stderr, return_code = await execution_engine.execute_command(
                        command,
                        check_safety=False  # Skip safety checks for retry
                    )
                    
                    result.update({
                        "command": command,
                        "stdout": stdout,
                        "stderr": stderr,
                        "return_code": return_code,
                        "success": return_code == 0
                    })
                else:
                    result.update({
                        "error": "No command available for retry",
                        "success": False
                    })
            
            elif strategy_type in [RecoveryStrategy.MODIFY_COMMAND.value, 
                                RecoveryStrategy.ALTERNATIVE_COMMAND.value]:
                # Execute modified or alternative command
                command = strategy.get("command")
                if command:
                    stdout, stderr, return_code = await execution_engine.execute_command(
                        command,
                        check_safety=True  # Safety checks for new commands
                    )
                    
                    result.update({
                        "command": command,
                        "stdout": stdout,
                        "stderr": stderr,
                        "return_code": return_code,
                        "success": return_code == 0
                    })
                    
                    # If successful and requested, retry the original command
                    if return_code == 0 and strategy.get("retry_original"):
                        original_command = getattr(step, "command", None) or error_result.get("command")
                        if original_command:
                            self._logger.info(f"Retrying original command: {original_command}")
                            stdout, stderr, return_code = await execution_engine.execute_command(
                                original_command,
                                check_safety=False
                            )
                            
                            result.update({
                                "original_retry": {
                                    "command": original_command,
                                    "stdout": stdout,
                                    "stderr": stderr,
                                    "return_code": return_code,
                                    "success": return_code == 0
                                }
                            })
                            
                            # Update overall success based on original command retry
                            result["success"] = return_code == 0
                else:
                    result.update({
                        "error": "No command specified in strategy",
                        "success": False
                    })
            
            elif strategy_type == RecoveryStrategy.PREPARE_ENV.value:
                # Execute preparation command
                command = strategy.get("command")
                if command:
                    stdout, stderr, return_code = await execution_engine.execute_command(
                        command,
                        check_safety=True
                    )
                    
                    result.update({
                        "preparation": {
                            "command": command,
                            "stdout": stdout,
                            "stderr": stderr,
                            "return_code": return_code,
                            "success": return_code == 0
                        }
                    })
                    
                    # If preparation succeeded and requested, retry the original command
                    if return_code == 0 and strategy.get("retry_original"):
                        original_command = getattr(step, "command", None) or error_result.get("command")
                        if original_command:
                            self._logger.info(f"Retrying original command after preparation: {original_command}")
                            stdout, stderr, return_code = await execution_engine.execute_command(
                                original_command,
                                check_safety=False
                            )
                            
                            result.update({
                                "command": original_command,
                                "stdout": stdout,
                                "stderr": stderr,
                                "return_code": return_code,
                                "success": return_code == 0
                            })
                        else:
                            result.update({
                                "error": "No original command available for retry",
                                "success": False
                            })
                    else:
                        # No retry requested or preparation failed
                        result["success"] = return_code == 0
                else:
                    result.update({
                        "error": "No preparation command specified in strategy",
                        "success": False
                    })
            
            elif strategy_type == RecoveryStrategy.REVERT_CHANGES.value:
                # Revert changes (simplified implementation)
                result.update({
                    "message": "Revert changes not implemented",
                    "success": False
                })
            
            elif strategy_type == RecoveryStrategy.SKIP.value:
                # Skip the step
                result.update({
                    "message": "Step skipped",
                    "success": True,
                    "skipped": True
                })
            
            else:
                result.update({
                    "error": f"Unknown strategy type: {strategy_type}",
                    "success": False
                })
            
            # Update recovery history
            if result.get("success"):
                strategy_key = f"{strategy.get('type')}:{strategy.get('command')}"
                if strategy_key in self._recovery_history:
                    self._recovery_history[strategy_key]["success_count"] += 1
                    self._recovery_history[strategy_key]["last_success"] = datetime.now()
                else:
                    self._recovery_history[strategy_key] = {
                        "success_count": 1,
                        "failure_count": 0,
                        "last_success": datetime.now()
                    }
            
            return result
            
        except Exception as e:
            self._logger.exception(f"Error executing recovery strategy: {str(e)}")
            return {
                "strategy": strategy,
                "error": str(e),
                "success": False
            }
    
    async def _guided_recovery(
        self, 
        strategies: List[Dict[str, Any]], 
        step: Any, 
        error_result: Dict[str, Any], 
        context: Dict[str, Any]
    ) -> Optional[Dict[str, Any]]:
        """
        Guide the user through recovery options.
        """
        # Import through API layer
        from angela.api.shell import get_terminal_formatter, get_output_type

        if not strategies:
            return None

        # Get terminal formatter from API layer
        terminal_formatter = get_terminal_formatter()
        OutputType = get_output_type()
        
        # Display recovery options
        terminal_formatter.print_output(
            "Command execution failed. The following recovery options are available:",
            OutputType.WARNING,
            title="Recovery Options"
        )
        
        # Display the error
        terminal_formatter.print_output(
            error_result.get("stderr", "") or error_result.get("error", "Unknown error"),
            OutputType.ERROR,
            title="Error"
        )
        
        # Show strategies
        for i, strategy in enumerate(strategies):
            description = strategy.get("description", "No description")
            command = strategy.get("command", "No command")
            
            if strategy.get("type") == RecoveryStrategy.SKIP.value:
                terminal_formatter.print_output(
                    f"Option {i+1}: [Skip] {description}",
                    OutputType.INFO
                )
            else:
                terminal_formatter.print_output(
                    f"Option {i+1}: {description}\n  Command: {command}",
                    OutputType.INFO
                )
        
        # Add abort option
        terminal_formatter.print_output(
            f"Option {len(strategies)+1}: [Abort] Abort execution",
            OutputType.WARNING
        )
        
        # Get user selection
        from prompt_toolkit.shortcuts import input_dialog
        selection = input_dialog(
            title="Select Recovery Option",
            text="Enter option number:",
        ).run()
        
        if not selection or not selection.isdigit():
            return None
        
        option = int(selection)
        
        # Handle abort option
        if option == len(strategies) + 1:
            return {
                "message": "Execution aborted by user",
                "success": False,
                "aborted": True
            }
        
        # Handle strategy selection
        if 1 <= option <= len(strategies):
            selected_strategy = strategies[option - 1]
            
            # Execute the selected strategy
            return await self._execute_recovery_strategy(
                selected_strategy, step, error_result, context
            )
        
        return None
    
    def _get_common_error_patterns(self) -> List[Dict[str, Any]]:
        """
        Get common error patterns and fix suggestions.
        
        Returns:
            List of error pattern dictionaries
        """
        return [
            {
                "pattern": r'permission denied|cannot access|operation not permitted',
                "description": "Permission denied error",
                "fixes": [
                    "Try running the command with sudo: `sudo {command}`",
                    "Check file permissions with `ls -l {path}`",
                    "Change file permissions with `chmod +x {path}`"
                ]
            },
            {
                "pattern": r'command not found|not installed|no such file or directory',
                "description": "Command or file not found",
                "fixes": [
                    "Install the package containing the command",
                    "Check if the path is correct",
                    "Use `which {command}` to check if the command is in PATH"
                ]
            },
            {
                "pattern": r'syntax error|invalid option|unrecognized option',
                "description": "Command syntax error",
                "fixes": [
                    "Check the command syntax with `man {command}`",
                    "Remove problematic options or flags",
                    "Ensure quotes and brackets are properly matched"
                ]
            },
            {
                "pattern": r'cannot connect|connection refused|network is unreachable',
                "description": "Network connection error",
                "fixes": [
                    "Check if the host is reachable with `ping {host}`",
                    "Verify network connectivity",
                    "Ensure the service is running with `systemctl status {service}`"
                ]
            },
            {
                "pattern": r'disk quota exceeded|no space left on device|file system is full',
                "description": "Disk space issue",
                "fixes": [
                    "Free up disk space with `df -h` to check and `rm` to remove files",
                    "Clean up temporary files with `apt-get clean` or `yum clean all`",
                    "Compress large files with `gzip {file}`"
                ]
            },
            {
                "pattern": r'resource temporarily unavailable|resource busy|device or resource busy',
                "description": "Resource busy error",
                "fixes": [
                    "Wait and try again later",
                    "Check what processes are using the resource with `lsof {path}`",
                    "Terminate competing processes with `kill {pid}`"
                ]
            }
        ]

error_recovery_manager = ErrorRecoveryManager()
</file>

<file path="components/execution/filesystem.py">
# angela/execution/filesystem.py
"""
File system operations for Angela CLI.

This module provides high-level file and directory operations with safety checks
and proper error handling.
"""
import os
import shutil
import tempfile
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, Optional, List, Tuple, Union, BinaryIO, TextIO

# Import through API layer
from angela.utils.logging import get_logger

logger = get_logger(__name__)

# Directory for storing backup files for rollback operations
BACKUP_DIR = Path(tempfile.gettempdir()) / "angela-backups"

def _get_operation_safety_checker():
    """Get operation safety function with lazy import to avoid circular dependencies."""
    from angela.core.registry import registry
    # First try to get from registry
    checker = registry.get("check_operation_safety")
    if checker:
        return checker
    
    # Fall back to direct import
    try:
        from angela.components.safety import check_operation_safety
        return check_operation_safety
    except ImportError:
        from angela.utils.logging import get_logger
        logger = get_logger(__name__)
        logger.error("Could not import operation safety checker")
        return None



class FileSystemError(Exception):
    """Exception raised for file system operation errors."""
    pass


# Ensure backup directory exists
def _ensure_backup_dir():
    """Create the backup directory if it doesn't exist."""
    BACKUP_DIR.mkdir(parents=True, exist_ok=True)


async def create_directory(
    path: Union[str, Path], 
    parents: bool = True,
    dry_run: bool = False
) -> bool:
    """
    Create a directory at the specified path.
    
    Args:
        path: The path where the directory should be created.
        parents: Whether to create parent directories if they don't exist.
        dry_run: Whether to simulate the operation without making changes.
        
    Returns:
        True if the operation was successful, False otherwise.
    """
    path_obj = Path(path)
    operation_params = {
        'path': str(path_obj),
        'parents': parents
    }
    
    try:
        # Check if the operation is safe
        safety_checker = get_operation_safety_checker()
        if not await safety_checker.check_operation_safety('create_directory', operation_params, dry_run):
            return False
        
        # If this is a dry run, stop here
        if dry_run:
            logger.info(f"DRY RUN: Would create directory at {path_obj}")
            return True
        
        # Create the directory
        if parents:
            path_obj.mkdir(parents=True, exist_ok=True)
        else:
            path_obj.mkdir(exist_ok=False)
        
        logger.info(f"Created directory at {path_obj}")
        return True
    
    except Exception as e:
        logger.exception(f"Error creating directory at {path_obj}: {str(e)}")
        raise FileSystemError(f"Failed to create directory: {str(e)}")


async def delete_directory(
    path: Union[str, Path], 
    recursive: bool = False,
    force: bool = False,
    dry_run: bool = False
) -> bool:
    """
    Delete a directory at the specified path.
    
    Args:
        path: The path of the directory to delete.
        recursive: Whether to recursively delete contents (rmdir vs rm -r).
        force: Whether to ignore errors if the directory doesn't exist.
        dry_run: Whether to simulate the operation without making changes.
        
    Returns:
        True if the operation was successful, False otherwise.
    """
    path_obj = Path(path)
    operation_params = {
        'path': str(path_obj),
        'recursive': recursive,
        'force': force
    }
    
    try:
        # Check if the directory exists
        if not path_obj.exists():
            if force:
                logger.info(f"Directory does not exist, but force=True: {path_obj}")
                return True
            else:
                raise FileSystemError(f"Directory does not exist: {path_obj}")
        
        # Verify it's actually a directory
        if not path_obj.is_dir():
            raise FileSystemError(f"Path is not a directory: {path_obj}")
        
        # Check if the operation is safe
        safety_checker = get_operation_safety_checker()
        if not await safety_checker.check_operation_safety('delete_directory', operation_params, dry_run):
            return False
        
        # Create a backup for rollback if needed
        if not dry_run and recursive:
            await _backup_directory(path_obj)
        
        # If this is a dry run, stop here
        if dry_run:
            logger.info(f"DRY RUN: Would delete directory at {path_obj}")
            return True
        
        # Delete the directory
        if recursive:
            shutil.rmtree(path_obj)
            logger.info(f"Recursively deleted directory at {path_obj}")
        else:
            path_obj.rmdir()
            logger.info(f"Deleted directory at {path_obj}")
        
        return True
    
    except Exception as e:
        logger.exception(f"Error deleting directory at {path_obj}: {str(e)}")
        raise FileSystemError(f"Failed to delete directory: {str(e)}")


async def create_file(
    path: Union[str, Path], 
    content: Optional[str] = None,
    dry_run: bool = False
) -> bool:
    """
    Create a file at the specified path with optional content.
    
    Args:
        path: The path where the file should be created.
        content: Optional content to write to the file (if None, like touch).
        dry_run: Whether to simulate the operation without making changes.
        
    Returns:
        True if the operation was successful, False otherwise.
    """
    path_obj = Path(path)
    operation_params = {
        'path': str(path_obj),
        'content': content is not None
    }
    
    try:
        # Check if the operation is safe
        safety_checker = get_operation_safety_checker()
        if not await safety_checker.check_operation_safety('create_file', operation_params, dry_run):
            return False
        
        # If this is a dry run, stop here
        if dry_run:
            if content:
                logger.info(f"DRY RUN: Would create file with content at {path_obj}")
            else:
                logger.info(f"DRY RUN: Would touch file at {path_obj}")
            return True
        
        # Make sure parent directory exists
        if not path_obj.parent.exists():
            await create_directory(path_obj.parent, dry_run=False)
        
        # Handle backup if the file already exists
        if path_obj.exists():
            await _backup_file(path_obj)
        
        # Create/write the file
        if content is not None:
            with open(path_obj, 'w') as f:
                f.write(content)
            logger.info(f"Created file with content at {path_obj}")
        else:
            path_obj.touch()
            logger.info(f"Touched file at {path_obj}")
        
        return True
    
    except Exception as e:
        logger.exception(f"Error creating file at {path_obj}: {str(e)}")
        raise FileSystemError(f"Failed to create file: {str(e)}")


async def read_file(
    path: Union[str, Path], 
    binary: bool = False
) -> Union[str, bytes]:
    """
    Read the content of a file.
    
    Args:
        path: The path of the file to read.
        binary: Whether to read the file in binary mode.
        
    Returns:
        The content of the file as a string or bytes.
    """
    path_obj = Path(path)
    operation_params = {
        'path': str(path_obj),
        'binary': binary
    }
    
    try:
        # Check if the file exists
        if not path_obj.exists():
            raise FileSystemError(f"File does not exist: {path_obj}")
        
        # Verify it's actually a file
        if not path_obj.is_file():
            raise FileSystemError(f"Path is not a file: {path_obj}")
        
        # Check if the operation is safe
        safety_checker = get_operation_safety_checker()
        if not await safety_checker.check_operation_safety('read_file', operation_params, False):
            raise FileSystemError("Operation not permitted due to safety constraints")
        
        # Read the file
        if binary:
            with open(path_obj, 'rb') as f:
                content = f.read()
        else:
            with open(path_obj, 'r', errors='replace') as f:
                content = f.read()
        
        logger.info(f"Read file at {path_obj}")
        return content
    
    except Exception as e:
        logger.exception(f"Error reading file at {path_obj}: {str(e)}")
        raise FileSystemError(f"Failed to read file: {str(e)}")


async def write_file(
    path: Union[str, Path], 
    content: Union[str, bytes],
    append: bool = False,
    dry_run: bool = False
) -> bool:
    """
    Write content to a file.
    
    Args:
        path: The path of the file to write.
        content: The content to write to the file.
        append: Whether to append to the file instead of overwriting.
        dry_run: Whether to simulate the operation without making changes.
        
    Returns:
        True if the operation was successful, False otherwise.
    """
    path_obj = Path(path)
    is_binary = isinstance(content, bytes)
    operation_params = {
        'path': str(path_obj),
        'append': append,
        'binary': is_binary
    }
    
    try:
        # Check if the operation is safe
        safety_checker = get_operation_safety_checker()
        if not await safety_checker.check_operation_safety('write_file', operation_params, dry_run):
            return False
        
        # If this is a dry run, stop here
        if dry_run:
            mode = "append to" if append else "write to"
            logger.info(f"DRY RUN: Would {mode} file at {path_obj}")
            return True
        
        # Make sure parent directory exists
        if not path_obj.parent.exists():
            await create_directory(path_obj.parent, dry_run=False)
        
        # Handle backup if the file already exists
        if path_obj.exists():
            await _backup_file(path_obj)
        
        # Write the file
        mode = 'ab' if append and is_binary else 'wb' if is_binary else 'a' if append else 'w'
        with open(path_obj, mode) as f:
            f.write(content)
        
        action = "Appended to" if append else "Wrote to"
        logger.info(f"{action} file at {path_obj}")
        return True
    
    except Exception as e:
        logger.exception(f"Error writing to file at {path_obj}: {str(e)}")
        raise FileSystemError(f"Failed to write file: {str(e)}")


async def delete_file(
    path: Union[str, Path], 
    force: bool = False,
    dry_run: bool = False
) -> bool:
    """
    Delete a file at the specified path.
    
    Args:
        path: The path of the file to delete.
        force: Whether to ignore errors if the file doesn't exist.
        dry_run: Whether to simulate the operation without making changes.
        
    Returns:
        True if the operation was successful, False otherwise.
    """
    path_obj = Path(path)
    operation_params = {
        'path': str(path_obj),
        'force': force
    }
    
    try:
        # Check if the file exists
        if not path_obj.exists():
            if force:
                logger.info(f"File does not exist, but force=True: {path_obj}")
                return True
            else:
                raise FileSystemError(f"File does not exist: {path_obj}")
        
        # Verify it's actually a file
        if not path_obj.is_file():
            raise FileSystemError(f"Path is not a file: {path_obj}")
        
        # Check if the operation is safe
        safety_checker = get_operation_safety_checker()
        if not await safety_checker.check_operation_safety('delete_file', operation_params, dry_run):
            return False
        
        # Create a backup for rollback if needed
        if not dry_run:
            await _backup_file(path_obj)
        
        # If this is a dry run, stop here
        if dry_run:
            logger.info(f"DRY RUN: Would delete file at {path_obj}")
            return True
        
        # Delete the file
        path_obj.unlink()
        logger.info(f"Deleted file at {path_obj}")
        
        return True
    
    except Exception as e:
        logger.exception(f"Error deleting file at {path_obj}: {str(e)}")
        raise FileSystemError(f"Failed to delete file: {str(e)}")


async def copy_file(
    source: Union[str, Path], 
    destination: Union[str, Path],
    overwrite: bool = False,
    dry_run: bool = False
) -> bool:
    """
    Copy a file from source to destination.
    
    Args:
        source: The path of the file to copy.
        destination: The destination path.
        overwrite: Whether to overwrite the destination if it exists.
        dry_run: Whether to simulate the operation without making changes.
        
    Returns:
        True if the operation was successful, False otherwise.
    """
    source_obj = Path(source)
    dest_obj = Path(destination)
    operation_params = {
        'source': str(source_obj),
        'destination': str(dest_obj),
        'overwrite': overwrite
    }
    
    try:
        # Check if the source file exists
        if not source_obj.exists():
            raise FileSystemError(f"Source file does not exist: {source_obj}")
        
        # Verify source is actually a file
        if not source_obj.is_file():
            raise FileSystemError(f"Source path is not a file: {source_obj}")
        
        # Check if the destination exists and handle overwrite
        if dest_obj.exists():
            if not overwrite:
                raise FileSystemError(f"Destination already exists: {dest_obj}")
            
            # Create a backup of the destination file
            if not dry_run:
                await _backup_file(dest_obj)
        
        # Check if the operation is safe
        safety_checker = get_operation_safety_checker()
        if not await safety_checker.check_operation_safety('copy_file', operation_params, dry_run):
            return False
        
        # If this is a dry run, stop here
        if dry_run:
            logger.info(f"DRY RUN: Would copy {source_obj} to {dest_obj}")
            return True
        
        # Make sure parent directory exists
        if not dest_obj.parent.exists():
            await create_directory(dest_obj.parent, dry_run=False)
        
        # Copy the file
        shutil.copy2(source_obj, dest_obj)
        logger.info(f"Copied {source_obj} to {dest_obj}")
        
        return True
    
    except Exception as e:
        logger.exception(f"Error copying {source_obj} to {dest_obj}: {str(e)}")
        raise FileSystemError(f"Failed to copy file: {str(e)}")


async def move_file(
    source: Union[str, Path], 
    destination: Union[str, Path],
    overwrite: bool = False,
    dry_run: bool = False
) -> bool:
    """
    Move a file from source to destination.
    
    Args:
        source: The path of the file to move.
        destination: The destination path.
        overwrite: Whether to overwrite the destination if it exists.
        dry_run: Whether to simulate the operation without making changes.
        
    Returns:
        True if the operation was successful, False otherwise.
    """
    source_obj = Path(source)
    dest_obj = Path(destination)
    operation_params = {
        'source': str(source_obj),
        'destination': str(dest_obj),
        'overwrite': overwrite
    }
    
    try:
        # Check if the source file exists
        if not source_obj.exists():
            raise FileSystemError(f"Source file does not exist: {source_obj}")
        
        # Verify source is actually a file
        if not source_obj.is_file():
            raise FileSystemError(f"Source path is not a file: {source_obj}")
        
        # Check if the destination exists and handle overwrite
        if dest_obj.exists():
            if not overwrite:
                raise FileSystemError(f"Destination already exists: {dest_obj}")
            
            # Create a backup of the destination file
            if not dry_run:
                await _backup_file(dest_obj)
        
        # Check if the operation is safe
        safety_checker = get_operation_safety_checker()
        if not await safety_checker.check_operation_safety('move_file', operation_params, dry_run):
            return False
        
        # Create a backup of the source file
        if not dry_run:
            await _backup_file(source_obj)
        
        # If this is a dry run, stop here
        if dry_run:
            logger.info(f"DRY RUN: Would move {source_obj} to {dest_obj}")
            return True
        
        # Make sure parent directory exists
        if not dest_obj.parent.exists():
            await create_directory(dest_obj.parent, dry_run=False)
        
        # Move the file
        shutil.move(str(source_obj), str(dest_obj))
        logger.info(f"Moved {source_obj} to {dest_obj}")
        
        return True
    
    except Exception as e:
        logger.exception(f"Error moving {source_obj} to {dest_obj}: {str(e)}")
        raise FileSystemError(f"Failed to move file: {str(e)}")


# --- Helper functions for backups and rollbacks ---

async def _backup_file(path: Path) -> Path:
    """
    Create a backup of a file for potential rollback.
    
    Args:
        path: The path of the file to back up.
        
    Returns:
        The path of the backup file.
    """
    try:
        _ensure_backup_dir()
        
        # Create a unique backup filename
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_name = f"{path.name}.{timestamp}.bak"
        backup_path = BACKUP_DIR / backup_name
        
        # Copy the file to the backup location
        shutil.copy2(path, backup_path)
        logger.debug(f"Created backup of {path} at {backup_path}")
        
        return backup_path
    
    except Exception as e:
        logger.warning(f"Failed to create backup of {path}: {str(e)}")
        # Not raising an exception here as this is a non-critical operation
        return None


async def _backup_directory(path: Path) -> Path:
    """
    Create a backup of a directory for potential rollback.
    
    Args:
        path: The path of the directory to back up.
        
    Returns:
        The path of the backup directory.
    """
    try:
        _ensure_backup_dir()
        
        # Create a unique backup directory name
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_name = f"{path.name}.{timestamp}.bak"
        backup_path = BACKUP_DIR / backup_name
        
        # Copy the directory to the backup location
        shutil.copytree(path, backup_path)
        logger.debug(f"Created backup of directory {path} at {backup_path}")
        
        return backup_path
    
    except Exception as e:
        logger.warning(f"Failed to create backup of directory {path}: {str(e)}")
        # Not raising an exception here as this is a non-critical operation
        return None
</file>

<file path="components/execution/hooks.py">
# angela/execution/hooks.py
"""
Execution hooks for file operations and command execution.

This module provides hooks for tracking file activities during execution
and enriching context in real-time.
"""
import os
import re
from pathlib import Path
from typing import Dict, Any, Optional, List, Tuple, Union

# Import through API layer
from angela.api.context import get_file_activity_tracker, get_activity_type
from angela.utils.logging import get_logger

logger = get_logger(__name__)

class ExecutionHooks:
    """
    Hooks for file operations and command execution.
    
    Provides hooks to track file activities during execution:
    1. Pre-execution hooks (before a command/operation is executed)
    2. Post-execution hooks (after a command/operation is executed)
    """
    
    def __init__(self):
        """Initialize the execution hooks."""
        self._logger = logger
        self._pre_execute_hooks = {}
        self._post_execute_hooks = {}


    def register_hook(self, hook_type: str, handler: callable) -> None:
        """
        Register a hook handler for a specific hook type.
        
        Args:
            hook_type: Type of hook (e.g., 'pre_execute_command', 'post_execute_command')
            handler: Callable to execute when the hook is triggered
        """
        self._logger.debug(f"Registering {hook_type} hook")
        
        if hook_type.startswith('pre_'):
            if hook_type not in self._pre_execute_hooks:
                self._pre_execute_hooks[hook_type] = []
            self._pre_execute_hooks[hook_type].append(handler)
        elif hook_type.startswith('post_'):
            if hook_type not in self._post_execute_hooks:
                self._post_execute_hooks[hook_type] = []
            self._post_execute_hooks[hook_type].append(handler)
        else:
            self._logger.warning(f"Unknown hook type: {hook_type}")
    
    def unregister_hook(self, hook_type: str, handler: callable) -> None:
        """
        Unregister a hook handler.
        
        Args:
            hook_type: Type of hook
            handler: Handler to unregister
        """
        self._logger.debug(f"Unregistering {hook_type} hook")
        
        if hook_type.startswith('pre_') and hook_type in self._pre_execute_hooks:
            if handler in self._pre_execute_hooks[hook_type]:
                self._pre_execute_hooks[hook_type].remove(handler)
        elif hook_type.startswith('post_') and hook_type in self._post_execute_hooks:
            if handler in self._post_execute_hooks[hook_type]:
                self._post_execute_hooks[hook_type].remove(handler)


    
    async def pre_execute_command(self, command: str, context: Dict[str, Any]) -> None:
        """
        Pre-execution hook for commands.
        
        Args:
            command: The command to be executed
            context: Context information
        """
        self._logger.debug(f"Pre-execute hook for command: {command}")
        
        # Extract potential file operations from the command
        await self._analyze_command_for_files(command, context)
        
        # Call registered pre-execute hooks
        hook_type = 'pre_execute_command'
        if hook_type in self._pre_execute_hooks:
            for handler in self._pre_execute_hooks[hook_type]:
                try:
                    await handler(command, context)
                except Exception as e:
                    self._logger.error(f"Error in {hook_type} hook: {str(e)}")
    
    async def post_execute_command(self, command: str, result: Dict[str, Any], context: Dict[str, Any]) -> None:
        """
        Post-execution hook for commands.
        
        Args:
            command: The executed command
            result: The execution result
            context: Context information
        """
        self._logger.debug(f"Post-execute hook for command: {command}")
        
        # Check if the command succeeded
        if not result.get("success", False):
            return
        
        # Analyze command output for file information
        await self._analyze_command_output(command, result.get("stdout", ""), context)
        
        # Track file activities based on the command type
        base_command = command.split()[0] if command else ""
        
        # Handle common file operation commands
        if base_command in ["cat", "less", "more", "head", "tail"]:
            await self._track_file_viewing(command, context)
        elif base_command in ["touch", "echo", "tee"]:
            await self._track_file_creation(command, context)
        elif base_command in ["rm", "rmdir"]:
            await self._track_file_deletion(command, context)
        elif base_command in ["cp", "mv", "rsync"]:
            await self._track_file_copy_move(command, context)
        elif base_command in ["sed", "awk", "perl", "nano", "vim", "emacs"]:
            await self._track_file_modification(command, context)
        
        # Call registered post-execute hooks
        hook_type = 'post_execute_command'
        if hook_type in self._post_execute_hooks:
            for handler in self._post_execute_hooks[hook_type]:
                try:
                    await handler(command, result, context)
                except Exception as e:
                    self._logger.error(f"Error in {hook_type} hook: {str(e)}")
    
    async def pre_execute_file_operation(
        self, 
        operation_type: str,
        parameters: Dict[str, Any],
        context: Dict[str, Any]
    ) -> None:
        """
        Pre-execution hook for file operations.
        
        Args:
            operation_type: The type of file operation
            parameters: The operation parameters
            context: Context information
        """
        self._logger.debug(f"Pre-execute hook for file operation: {operation_type}")
        
        # Nothing specific to do before file operations for now
        pass
    
    async def post_execute_file_operation(
        self, 
        operation_type: str,
        parameters: Dict[str, Any],
        result: Dict[str, Any],
        context: Dict[str, Any]
    ) -> None:
        """
        Post-execution hook for file operations.
        
        Args:
            operation_type: The type of file operation
            parameters: The operation parameters
            result: The operation result
            context: Context information
        """
        self._logger.debug(f"Post-execute hook for file operation: {operation_type}")
        
        # Check if the operation succeeded
        if not result.get("success", False):
            return
        
        # Get file activity tracker via API
        file_activity_tracker = get_file_activity_tracker()
        ActivityType = get_activity_type()
        
        # Track file activity based on operation type
        if operation_type == "create_file":
            file_path = parameters.get("path")
            if file_path:
                file_activity_tracker.track_file_creation(
                    Path(file_path),
                    None,  # No command
                    {"operation": operation_type}
                )
        elif operation_type == "write_file":
            file_path = parameters.get("path")
            if file_path:
                file_activity_tracker.track_file_modification(
                    Path(file_path),
                    None,  # No command
                    {"operation": operation_type, "append": parameters.get("append", False)}
                )
        elif operation_type == "delete_file":
            file_path = parameters.get("path")
            if file_path:
                file_activity_tracker.track_file_deletion(
                    Path(file_path),
                    None,  # No command
                    {"operation": operation_type}
                )
        elif operation_type == "read_file":
            file_path = parameters.get("path")
            if file_path:
                file_activity_tracker.track_file_viewing(
                    Path(file_path),
                    None,  # No command
                    {"operation": operation_type}
                )
        elif operation_type == "copy_file":
            source = parameters.get("source")
            destination = parameters.get("destination")
            if source and destination:
                # Track the source as read
                file_activity_tracker.track_file_viewing(
                    Path(source),
                    None,  # No command
                    {"operation": operation_type}
                )
                # Track the destination as created/modified
                file_activity_tracker.track_file_creation(
                    Path(destination),
                    None,  # No command
                    {"operation": operation_type, "source": source}
                )
        elif operation_type == "move_file":
            source = parameters.get("source")
            destination = parameters.get("destination")
            if source and destination:
                # Track the source as deleted
                file_activity_tracker.track_file_deletion(
                    Path(source),
                    None,  # No command
                    {"operation": operation_type}
                )
                # Track the destination as created
                file_activity_tracker.track_file_creation(
                    Path(destination),
                    None,  # No command
                    {"operation": operation_type, "source": source}
                )
    
    async def _analyze_command_for_files(
        self, 
        command: str,
        context: Dict[str, Any]
    ) -> None:
        """
        Analyze a command for potential file operations.
        
        Args:
            command: The command to analyze
            context: Context information
        """
        # Get file activity tracker via API
        file_activity_tracker = get_file_activity_tracker()
        
        # Split command into tokens
        tokens = command.split()
        
        if not tokens:
            return
        
        base_command = tokens[0]
        
        # Check for file paths in tokens
        paths = []
        
        for token in tokens[1:]:
            # Skip options
            if token.startswith('-'):
                continue
            
            # Skip redirection operators
            if token in ['>', '<', '>>', '2>', '&>']:
                continue
            
            # Check if token looks like a path
            if '/' in token or '.' in token:
                # Resolve relative to CWD
                path = Path(context.get("cwd", ".")) / token
                if path.exists():
                    paths.append(path)
        
        # Track potential file accesses
        for path in paths:
            if path.is_file():
                file_activity_tracker.track_file_viewing(
                    path,
                    command,
                    {"pre_execution": True}
                )
    
    async def _analyze_command_output(
        self, 
        command: str,
        output: str,
        context: Dict[str, Any]
    ) -> None:
        """
        Analyze command output for file information.
        
        Args:
            command: The executed command
            output: The command output
            context: Context information
        """
        # Get file activity tracker via API
        file_activity_tracker = get_file_activity_tracker()
        
        # Check for file paths in output
        paths = set()
        
        # Look for patterns that might be file paths
        path_patterns = [
            r'[\'"]([/\w\-\.]+\.\w+)[\'"]',  # Quoted paths with extension
            r'\b(/[/\w\-\.]+\.\w+)\b',       # Absolute paths with extension
            r'\b(\./[/\w\-\.]+\.\w+)\b',     # Relative paths with ./ prefix
        ]
        
        for pattern in path_patterns:
            for match in re.finditer(pattern, output):
                potential_path = match.group(1)
                
                # Resolve relative to CWD
                if not potential_path.startswith('/'):
                    potential_path = os.path.join(context.get("cwd", "."), potential_path)
                
                path = Path(potential_path)
                if path.exists() and path.is_file():
                    paths.add(path)
        
        # Track found files
        for path in paths:
            if path.is_file():
                file_activity_tracker.track_file_viewing(
                    path,
                    command,
                    {"from_output": True}
                )
    
    async def _track_file_viewing(
        self, 
        command: str,
        context: Dict[str, Any]
    ) -> None:
        """
        Track file viewing for commands like cat, less, more, etc.
        
        Args:
            command: The executed command
            context: Context information
        """
        # Get file activity tracker via API
        file_activity_tracker = get_file_activity_tracker()
        
        tokens = command.split()
        
        if len(tokens) < 2:
            return
        
        # Find potential file paths
        for token in tokens[1:]:
            # Skip options
            if token.startswith('-'):
                continue
            
            # Skip if it looks like a pipe or redirection
            if token in ['|', '>', '<', '>>', '2>', '&>']:
                break
            
            # Resolve path
            path = Path(token)
            if not path.is_absolute():
                path = Path(context.get("cwd", ".")) / token
            
            if path.exists() and path.is_file():
                file_activity_tracker.track_file_viewing(
                    path,
                    command,
                    {}
                )
    
    async def _track_file_creation(
        self, 
        command: str,
        context: Dict[str, Any]
    ) -> None:
        """
        Track file creation for commands like touch, etc.
        
        Args:
            command: The executed command
            context: Context information
        """
        # Get file activity tracker via API
        file_activity_tracker = get_file_activity_tracker()
        
        tokens = command.split()
        
        if len(tokens) < 2:
            return
        
        # Handle touch command
        if tokens[0] == 'touch':
            for token in tokens[1:]:
                # Skip options
                if token.startswith('-'):
                    continue
                
                # Resolve path
                path = Path(token)
                if not path.is_absolute():
                    path = Path(context.get("cwd", ".")) / token
                
                if path.exists() and path.is_file():
                    file_activity_tracker.track_file_creation(
                        path,
                        command,
                        {}
                    )
        
        # Handle echo/redirection
        elif tokens[0] == 'echo':
            # Look for redirection
            redirect_idx = -1
            for i, token in enumerate(tokens):
                if token in ['>', '>>']:
                    redirect_idx = i
                    break
            
            if redirect_idx > 0 and redirect_idx < len(tokens) - 1:
                file_path = tokens[redirect_idx + 1]
                
                # Resolve path
                path = Path(file_path)
                if not path.is_absolute():
                    path = Path(context.get("cwd", ".")) / file_path
                
                # Get ActivityType via API
                ActivityType = get_activity_type()
                operation = ActivityType.CREATED
                if tokens[redirect_idx] == '>>':
                    operation = ActivityType.MODIFIED
                
                if path.exists() and path.is_file():
                    if operation == ActivityType.CREATED:
                        file_activity_tracker.track_file_creation(
                            path,
                            command,
                            {"redirect": tokens[redirect_idx]}
                        )
                    else:
                        file_activity_tracker.track_file_modification(
                            path,
                            command,
                            {"redirect": tokens[redirect_idx]}
                        )
    
    async def _track_file_deletion(
        self, 
        command: str,
        context: Dict[str, Any]
    ) -> None:
        """
        Track file deletion for commands like rm, etc.
        
        Args:
            command: The executed command
            context: Context information
        """
        # Get file activity tracker via API
        file_activity_tracker = get_file_activity_tracker()
        
        tokens = command.split()
        
        if len(tokens) < 2:
            return
        
        # Skip arguments until we find a non-option
        for token in tokens[1:]:
            # Skip options
            if token.startswith('-'):
                continue
            
            # This is probably a path
            path = Path(token)
            if not path.is_absolute():
                path = Path(context.get("cwd", ".")) / token
            
            # We can't check if it exists since it was deleted
            file_activity_tracker.track_file_deletion(
                path,
                command,
                {}
            )
    
    async def _track_file_copy_move(
        self, 
        command: str,
        context: Dict[str, Any]
    ) -> None:
        """
        Track file copy/move for commands like cp, mv, etc.
        
        Args:
            command: The executed command
            context: Context information
        """
        # Get file activity tracker via API
        file_activity_tracker = get_file_activity_tracker()
        
        tokens = command.split()
        
        if len(tokens) < 3:
            return
        
        base_command = tokens[0]
        
        # Find the source and destination
        source_tokens = []
        dest_token = tokens[-1]  # Last token is destination
        
        # Collect all tokens that aren't options as source tokens
        for token in tokens[1:-1]:
            if not token.startswith('-'):
                source_tokens.append(token)
        
        # Resolve destination path
        dest_path = Path(dest_token)
        if not dest_path.is_absolute():
            dest_path = Path(context.get("cwd", ".")) / dest_token
        
        # Track each source
        for source_token in source_tokens:
            source_path = Path(source_token)
            if not source_path.is_absolute():
                source_path = Path(context.get("cwd", ".")) / source_token
            
            if base_command == 'cp':
                # For cp, source is viewed and destination is created
                file_activity_tracker.track_file_viewing(
                    source_path,
                    command,
                    {"operation": "copy", "destination": str(dest_path)}
                )
                
                # If destination is a directory, the file goes inside it
                if dest_path.is_dir():
                    actual_dest = dest_path / source_path.name
                else:
                    actual_dest = dest_path
                
                file_activity_tracker.track_file_creation(
                    actual_dest,
                    command,
                    {"operation": "copy", "source": str(source_path)}
                )
            elif base_command == 'mv':
                # For mv, source is deleted and destination is created
                file_activity_tracker.track_file_deletion(
                    source_path,
                    command,
                    {"operation": "move", "destination": str(dest_path)}
                )
                
                # If destination is a directory, the file goes inside it
                if dest_path.is_dir():
                    actual_dest = dest_path / source_path.name
                else:
                    actual_dest = dest_path
                
                file_activity_tracker.track_file_creation(
                    actual_dest,
                    command,
                    {"operation": "move", "source": str(source_path)}
                )
    
    async def _track_file_modification(
        self, 
        command: str,
        context: Dict[str, Any]
    ) -> None:
        """
        Track file modification for commands like sed, etc.
        
        Args:
            command: The executed command
            context: Context information
        """
        # Get file activity tracker via API
        file_activity_tracker = get_file_activity_tracker()
        
        tokens = command.split()
        
        if len(tokens) < 2:
            return
        
        base_command = tokens[0]
        
        # Handle sed command
        if base_command == 'sed':
            # Find the file token (usually the last one)
            file_token = None
            for i in range(len(tokens) - 1, 0, -1):
                if not tokens[i].startswith('-') and not tokens[i].startswith("'") and not tokens[i].startswith('"'):
                    file_token = tokens[i]
                    break
            
            if file_token:
                path = Path(file_token)
                if not path.is_absolute():
                    path = Path(context.get("cwd", ".")) / file_token
                
                if path.exists() and path.is_file():
                    file_activity_tracker.track_file_modification(
                        path,
                        command,
                        {}
                    )

# Global execution hooks instance
execution_hooks = ExecutionHooks()
</file>

<file path="components/execution/rollback_commands.py">
# angela/cli/rollback_commands.py
"""
CLI commands for enhanced rollback functionality.
"""
import asyncio
import typer
from pathlib import Path
from typing import Optional, List

from rich.console import Console
from rich.panel import Panel
from rich.table import Table
from rich import print as rich_print
from rich.prompt import Confirm
from rich.syntax import Syntax

from angela.api.execution import get_rollback_manager
from angela.utils.logging import get_logger

logger = get_logger(__name__)
console = Console()

app = typer.Typer(help="Rollback commands for undoing operations")

@app.command("list", help="List recent operations that can be rolled back")
def list_operations(
    limit: int = typer.Option(10, help="Maximum number of operations to show"),
    transactions: bool = typer.Option(False, help="Show transactions instead of individual operations")
):
    """List recent operations or transactions that can be rolled back."""
    # Get rollback_manager through API
    rollback_manager = get_rollback_manager()
    
    if transactions:
        # Show transactions
        transaction_list = asyncio.run(rollback_manager.get_recent_transactions(limit))
        
        if not transaction_list:
            console.print("[yellow]No transactions found.[/yellow]")
            return
        
        # Create a table for the transactions
        table = Table(title="Recent Transactions")
        table.add_column("ID", style="cyan")
        table.add_column("Timestamp", style="green")
        table.add_column("Description", style="white")
        table.add_column("Status", style="yellow")
        table.add_column("Operations", style="blue")
        table.add_column("Can Rollback", style="red")
        
        # Add rows for each transaction
        for transaction in transaction_list:
            table.add_row(
                transaction["id"],
                transaction["timestamp"],
                transaction["description"],
                transaction["status"],
                str(transaction["operation_count"]),
                "✓" if transaction["can_rollback"] else "✗"
            )
        
        # Display the table
        console.print(table)
        
        # Show usage hint
        console.print("\n[bold]Use the following command to roll back a transaction:[/bold]")
        console.print("  [blue]angela rollback transaction <ID>[/blue]")
    
    else:
        # Show individual operations
        operation_list = asyncio.run(rollback_manager.get_recent_operations(limit))
        
        if not operation_list:
            console.print("[yellow]No operations found.[/yellow]")
            return
        
        # Create a table for the operations
        table = Table(title="Recent Operations")
        table.add_column("ID", style="cyan")
        table.add_column("Timestamp", style="green")
        table.add_column("Type", style="white")
        table.add_column("Description", style="blue")
        table.add_column("Can Rollback", style="red")
        table.add_column("Transaction", style="yellow")
        
        # Add rows for each operation
        for operation in operation_list:
            transaction_info = operation.get("transaction")
            transaction_desc = f"{transaction_info['description']} ({transaction_info['status']})" if transaction_info else "None"
            
            table.add_row(
                str(operation["id"]),
                operation["timestamp"],
                operation["operation_type"],
                operation["description"],
                "✓" if operation["can_rollback"] else "✗",
                transaction_desc
            )
        
        # Display the table
        console.print(table)
        
        # Show usage hint
        console.print("\n[bold]Use the following command to roll back an operation:[/bold]")
        console.print("  [blue]angela rollback operation <ID>[/blue]")


@app.command("operation", help="Roll back a specific operation")
def rollback_operation(
    operation_id: int = typer.Argument(..., help="ID of the operation to roll back"),
    force: bool = typer.Option(False, help="Skip confirmation prompt")
):
    """Roll back a specific operation by ID."""
    # Get rollback_manager through API
    rollback_manager = get_rollback_manager()
    
    # Get operation details
    operation_list = asyncio.run(rollback_manager.get_recent_operations(100))
    
    # Find the operation
    operation = None
    for op in operation_list:
        if op["id"] == operation_id:
            operation = op
            break
    
    if not operation:
        console.print(f"[red]Operation with ID {operation_id} not found.[/red]")
        return
    
    # Check if the operation can be rolled back
    if not operation["can_rollback"]:
        console.print("[red]This operation cannot be rolled back.[/red]")
        return
    
    # Display operation details
    console.print(Panel(
        f"[bold]Type:[/bold] {operation['operation_type']}\n"
        f"[bold]Description:[/bold] {operation['description']}\n"
        f"[bold]Timestamp:[/bold] {operation['timestamp']}",
        title="Operation Details",
        expand=False
    ))
    
    # Get confirmation
    if not force and not Confirm.ask("Are you sure you want to roll back this operation?"):
        console.print("[yellow]Rollback cancelled.[/yellow]")
        return
    
    # Execute the rollback
    with console.status("[bold green]Rolling back operation...[/bold green]"):
        success = asyncio.run(rollback_manager.rollback_operation(operation_id))
    
    # Show the result
    if success:
        console.print("[green]Operation successfully rolled back.[/green]")
    else:
        console.print("[red]Failed to roll back operation.[/red]")


@app.command("transaction", help="Roll back an entire transaction")
def rollback_transaction(
    transaction_id: str = typer.Argument(..., help="ID of the transaction to roll back"),
    force: bool = typer.Option(False, help="Skip confirmation prompt")
):
    """Roll back all operations in a transaction."""
    # Get rollback_manager through API
    rollback_manager = get_rollback_manager()
    
    # Get transaction details
    transaction_list = asyncio.run(rollback_manager.get_recent_transactions(100))
    
    # Find the transaction
    transaction = None
    for tx in transaction_list:
        if tx["id"] == transaction_id:
            transaction = tx
            break
    
    if not transaction:
        console.print(f"[red]Transaction with ID {transaction_id} not found.[/red]")
        return
    
    # Check if the transaction can be rolled back
    if not transaction["can_rollback"]:
        console.print("[red]This transaction cannot be rolled back.[/red]")
        return
    
    # Display transaction details
    console.print(Panel(
        f"[bold]Description:[/bold] {transaction['description']}\n"
        f"[bold]Status:[/bold] {transaction['status']}\n"
        f"[bold]Timestamp:[/bold] {transaction['timestamp']}\n"
        f"[bold]Operation count:[/bold] {transaction['operation_count']}",
        title="Transaction Details",
        expand=False
    ))
    
    # Get confirmation
    if not force and not Confirm.ask("Are you sure you want to roll back this entire transaction?"):
        console.print("[yellow]Rollback cancelled.[/yellow]")
        return
    
    # Execute the rollback
    with console.status("[bold green]Rolling back transaction...[/bold green]"):
        result = asyncio.run(rollback_manager.rollback_transaction(transaction_id))
    
    # Show the result
    if result["success"]:
        console.print(f"[green]Transaction successfully rolled back. {result['rolled_back']} operations reverted.[/green]")
    else:
        console.print(f"[red]Transaction rollback failed or partially succeeded. "
                     f"{result['rolled_back']} operations reverted, {result['failed']} operations failed.[/red]")
        
        # Show details of failed operations
        if result["failed"] > 0 and "results" in result:
            failed_ops = [r for r in result["results"] if not r["success"]]
            
            if failed_ops:
                console.print("\n[bold]Failed operations:[/bold]")
                for op in failed_ops:
                    console.print(f"- ID {op['operation_id']}: {op['description']} - {op.get('error', 'Unknown error')}")


@app.command("last", help="Roll back the most recent operation or transaction")
def rollback_last(
    transaction: bool = typer.Option(False, help="Roll back the last transaction instead of the last operation"),
    force: bool = typer.Option(False, help="Skip confirmation prompt")
):
    """Roll back the most recent operation or transaction."""
    # Get rollback_manager through API
    rollback_manager = get_rollback_manager()
    
    if transaction:
        # Get the most recent transaction
        transactions = asyncio.run(rollback_manager.get_recent_transactions(1))
        
        if not transactions:
            console.print("[yellow]No transactions found.[/yellow]")
            return
        
        # Use the first (most recent) transaction
        transaction_id = transactions[0]["id"]
        
        # Call the rollback_transaction function
        rollback_transaction(transaction_id, force)
    else:
        # Get the most recent operation
        operations = asyncio.run(rollback_manager.get_recent_operations(1))
        
        if not operations:
            console.print("[yellow]No operations found.[/yellow]")
            return
        
        # Use the first (most recent) operation
        operation_id = operations[0]["id"]
        
        # Call the rollback_operation function
        rollback_operation(operation_id, force)
        
        
# To be used for integration with the main CLI
def register_commands(parent_app: typer.Typer):
    """Register rollback commands with a parent Typer app."""
    parent_app.add_typer(app, name="rollback", help="Commands for rolling back operations")
</file>

<file path="components/execution/rollback.py">
# angela/execution/rollback.py
"""
Enhanced rollback functionality for Angela CLI operations.

This module provides the ability to undo complex, multi-step operations
by tracking and reverting individual actions within a transaction.
It supports rolling back different types of operations including:
- File system operations (create, modify, delete files and directories)
- Content manipulations (AI-driven file changes)
- Command executions (with compensating actions)
"""
import os
import json
import shlex
import uuid
import shutil
import asyncio
import tempfile
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, Optional, List, Tuple, Union, Set


from angela.utils.logging import get_logger
from angela.api.review import get_diff_manager
from angela.api.execution import get_execution_engine, get_backup_dir

logger = get_logger(__name__)

# File to store operation history for rollback
BACKUP_DIR = get_backup_dir()
HISTORY_FILE = BACKUP_DIR / "operation_history.json"
TRANSACTION_DIR = BACKUP_DIR / "transactions"

# Operation types
OP_FILE_SYSTEM = "filesystem"     # File system operations (create, delete, etc.)
OP_CONTENT = "content"            # Content manipulation operations
OP_COMMAND = "command"            # Command execution operations
OP_PLAN = "plan"                  # Plan execution operations


class OperationRecord:
    """Record of an operation for rollback purposes."""
    
    def __init__(
        self,
        operation_type: str,
        params: Dict[str, Any],
        timestamp: Optional[datetime] = None,
        backup_path: Optional[str] = None,
        transaction_id: Optional[str] = None,
        step_id: Optional[str] = None,
        undo_info: Optional[Dict[str, Any]] = None
    ):
        self.operation_type = operation_type
        self.params = params
        self.timestamp = timestamp or datetime.now()
        self.backup_path = backup_path
        self.transaction_id = transaction_id
        self.step_id = step_id
        self.undo_info = undo_info or {}
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert the record to a dictionary for storage."""
        return {
            "operation_type": self.operation_type,
            "params": self.params,
            "timestamp": self.timestamp.isoformat(),
            "backup_path": str(self.backup_path) if self.backup_path else None,
            "transaction_id": self.transaction_id,
            "step_id": self.step_id,
            "undo_info": self.undo_info
        }
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'OperationRecord':
        """Create a record from a dictionary."""
        return cls(
            operation_type=data["operation_type"],
            params=data["params"],
            timestamp=datetime.fromisoformat(data["timestamp"]),
            backup_path=data["backup_path"],
            transaction_id=data.get("transaction_id"),
            step_id=data.get("step_id"),
            undo_info=data.get("undo_info", {})
        )


class Transaction:
    """A group of operations that form a single logical action."""
    
    def __init__(
        self,
        transaction_id: str,
        description: str,
        timestamp: Optional[datetime] = None,
        status: str = "started"  # started, completed, failed, rolled_back
    ):
        self.transaction_id = transaction_id
        self.description = description
        self.timestamp = timestamp or datetime.now()
        self.status = status
        self.operation_ids: List[int] = []
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert the transaction to a dictionary for storage."""
        return {
            "transaction_id": self.transaction_id,
            "description": self.description,
            "timestamp": self.timestamp.isoformat(),
            "status": self.status,
            "operation_ids": self.operation_ids
        }
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'Transaction':
        """Create a transaction from a dictionary."""
        transaction = cls(
            transaction_id=data["transaction_id"],
            description=data["description"],
            timestamp=datetime.fromisoformat(data["timestamp"]),
            status=data["status"]
        )
        transaction.operation_ids = data.get("operation_ids", [])
        return transaction


class RollbackManager:
    """Manager for operation history and rollback functionality."""
    
    def __init__(self):
        """Initialize the rollback manager."""
        self._ensure_directories()
        self._operations = self._load_history()
        self._transactions = self._load_transactions()
        self._active_transactions: Dict[str, Transaction] = {}
        self._command_compensations = self._load_command_compensations()
    
    def _ensure_directories(self):
        """Ensure the history file and transaction directory exist."""
        BACKUP_DIR.mkdir(parents=True, exist_ok=True)
        TRANSACTION_DIR.mkdir(parents=True, exist_ok=True)
        if not HISTORY_FILE.exists():
            self._save_history([])
    
    def _load_history(self) -> List[OperationRecord]:
        """Load operation history from the history file."""
        try:
            with open(HISTORY_FILE, 'r') as f:
                data = json.load(f)
            
            return [OperationRecord.from_dict(item) for item in data]
        
        except Exception as e:
            logger.error(f"Error loading operation history: {str(e)}")
            return []
    
    def _save_history(self, operations: List[OperationRecord]):
        """Save operation history to the history file."""
        try:
            with open(HISTORY_FILE, 'w') as f:
                json.dump([op.to_dict() for op in operations], f, indent=2)
        
        except Exception as e:
            logger.error(f"Error saving operation history: {str(e)}")
    
    def _load_transactions(self) -> Dict[str, Transaction]:
        """Load transactions from the transaction directory."""
        transactions = {}
        try:
            for file in TRANSACTION_DIR.glob("*.json"):
                try:
                    with open(file, 'r') as f:
                        data = json.load(f)
                        transaction = Transaction.from_dict(data)
                        transactions[transaction.transaction_id] = transaction
                except Exception as e:
                    logger.error(f"Error loading transaction {file}: {str(e)}")
        except Exception as e:
            logger.error(f"Error loading transactions: {str(e)}")
        
        return transactions
    
    def _save_transaction(self, transaction: Transaction):
        """Save a transaction to its file."""
        try:
            file_path = TRANSACTION_DIR / f"{transaction.transaction_id}.json"
            with open(file_path, 'w') as f:
                json.dump(transaction.to_dict(), f, indent=2)
        except Exception as e:
            logger.error(f"Error saving transaction {transaction.transaction_id}: {str(e)}")
    
    def _load_command_compensations(self) -> Dict[str, str]:
        """Load command compensation rules from a file."""
        compensations = {}
        
        # Define built-in command compensations
        built_in = {
            # Git compensations
            "git add": "git reset",               # Unstage files
            "git commit": "git reset --soft HEAD~1",  # Undo last commit
            "git push": "git push -f origin HEAD~1:${branch}",  # Force push previous commit
            "git branch": "git branch -D",        # Delete branch
            
            # Package manager compensations
            "npm install": "npm uninstall",       # Uninstall npm package
            "pip install": "pip uninstall -y",    # Uninstall pip package
            "apt-get install": "apt-get remove",  # Remove apt package
            
            # File operations (as fallbacks)
            "mkdir": "rmdir",                     # Remove directory
            "touch": "rm",                        # Remove file
        }
        
        # Add the built-in compensations
        compensations.update(built_in)
        
        # TODO: Load custom compensations from a file
        
        return compensations
    
    async def start_transaction(self, description: str) -> str:
        """
        Start a new transaction.
        
        Args:
            description: Description of the transaction
            
        Returns:
            Transaction ID
        """
        transaction_id = str(uuid.uuid4())
        transaction = Transaction(transaction_id, description)
        
        # Add to active transactions
        self._active_transactions[transaction_id] = transaction
        
        # Save the transaction
        self._save_transaction(transaction)
        
        logger.info(f"Started transaction {transaction_id}: {description}")
        return transaction_id
    
    async def end_transaction(self, transaction_id: str, status: str = "completed") -> bool:
        """
        End a transaction.
        
        Args:
            transaction_id: Transaction ID
            status: Transaction status ("completed" or "failed")
            
        Returns:
            True if successful, False otherwise
        """
        if transaction_id not in self._active_transactions:
            logger.error(f"Transaction {transaction_id} not found in active transactions")
            return False
        
        # Update the transaction
        transaction = self._active_transactions[transaction_id]
        transaction.status = status
        
        # Save the transaction
        self._save_transaction(transaction)
        
        # Add to transactions dict
        self._transactions[transaction_id] = transaction
        
        # Remove from active transactions
        del self._active_transactions[transaction_id]
        
        logger.info(f"Ended transaction {transaction_id} with status: {status}")
        return True
    
    async def record_operation(
        self,
        operation_type: str,
        params: Dict[str, Any],
        backup_path: Optional[Union[str, Path]] = None,
        transaction_id: Optional[str] = None,
        step_id: Optional[str] = None,
        undo_info: Optional[Dict[str, Any]] = None
    ) -> Optional[int]:
        """
        Record an operation for potential rollback.
        
        Args:
            operation_type: The type of operation.
            params: Parameters of the operation.
            backup_path: Path to the backup, if one was created.
            transaction_id: ID of the transaction this operation belongs to.
            step_id: ID of the step in the plan this operation belongs to.
            undo_info: Additional information needed for undoing the operation.
            
        Returns:
            Index of the operation in the history or None on error
        """
        try:
            # Create the operation record
            record = OperationRecord(
                operation_type=operation_type,
                params=params,
                backup_path=str(backup_path) if backup_path else None,
                transaction_id=transaction_id,
                step_id=step_id,
                undo_info=undo_info or {}
            )
            
            # Add to operations list
            operation_id = len(self._operations)
            self._operations.append(record)
            
            # Update transaction if provided
            if transaction_id:
                if transaction_id in self._active_transactions:
                    self._active_transactions[transaction_id].operation_ids.append(operation_id)
                    self._save_transaction(self._active_transactions[transaction_id])
                elif transaction_id in self._transactions:
                    self._transactions[transaction_id].operation_ids.append(operation_id)
                    self._save_transaction(self._transactions[transaction_id])
                else:
                    logger.warning(f"Transaction {transaction_id} not found when recording operation")
            
            # Save updated history
            self._save_history(self._operations)
            
            logger.debug(f"Recorded operation: {operation_type} (ID: {operation_id})")
            return operation_id
        
        except Exception as e:
            logger.error(f"Error recording operation: {str(e)}")
            return None
    
    async def record_file_operation(
        self,
        operation_type: str,
        params: Dict[str, Any],
        backup_path: Optional[Union[str, Path]] = None,
        transaction_id: Optional[str] = None,
        step_id: Optional[str] = None
    ) -> Optional[int]:
        """
        Record a file system operation for potential rollback.
        
        Args:
            operation_type: The type of file operation.
            params: Parameters of the operation.
            backup_path: Path to the backup, if one was created.
            transaction_id: ID of the transaction this operation belongs to.
            step_id: ID of the step in the plan this operation belongs to.
            
        Returns:
            Index of the operation in the history or None on error
        """
        return await self.record_operation(
            operation_type=OP_FILE_SYSTEM,
            params={
                "file_operation": operation_type,
                **params
            },
            backup_path=backup_path,
            transaction_id=transaction_id,
            step_id=step_id
        )
    
    async def record_content_manipulation(
        self,
        file_path: Union[str, Path],
        original_content: str,
        modified_content: str,
        instruction: Optional[str] = None,
        transaction_id: Optional[str] = None,
        step_id: Optional[str] = None
    ) -> Optional[int]:
        """
        Record a content manipulation operation for potential rollback.
        
        Args:
            file_path: Path to the file that was modified.
            original_content: Original content of the file.
            modified_content: Modified content of the file.
            instruction: The instruction that caused the modification.
            transaction_id: ID of the transaction this operation belongs to.
            step_id: ID of the step in the plan this operation belongs to.
            
        Returns:
            Index of the operation in the history or None on error
        """
        try:
            # Generate diff between original and modified content
            diff = diff_manager.generate_diff(original_content, modified_content)
            
            # Record the operation
            return await self.record_operation(
                operation_type=OP_CONTENT,
                params={
                    "file_path": str(file_path),
                    "instruction": instruction
                },
                transaction_id=transaction_id,
                step_id=step_id,
                undo_info={
                    "diff": diff,
                    "has_changes": original_content != modified_content
                }
            )
        
        except Exception as e:
            logger.error(f"Error recording content manipulation: {str(e)}")
            return None
    
    async def record_command_execution(
        self,
        command: str,
        return_code: int,
        stdout: str,
        stderr: str,
        cwd: Optional[str] = None,
        transaction_id: Optional[str] = None,
        step_id: Optional[str] = None
    ) -> Optional[int]:
        """
        Record a command execution for potential rollback.
        
        Args:
            command: The command that was executed.
            return_code: Return code of the command.
            stdout: Standard output of the command.
            stderr: Standard error of the command.
            cwd: Current working directory when the command was executed.
            transaction_id: ID of the transaction this operation belongs to.
            step_id: ID of the step in the plan this operation belongs to.
            
        Returns:
            Index of the operation in the history or None on error
        """
        try:
            # Determine the compensating action for this command
            compensating_action = await self._identify_compensating_action(
                command=command,
                stdout=stdout,
                stderr=stderr,
                cwd=cwd
            )
            
            # Record the operation
            return await self.record_operation(
                operation_type=OP_COMMAND,
                params={
                    "command": command,
                    "return_code": return_code,
                    "cwd": cwd or str(Path.cwd())
                },
                transaction_id=transaction_id,
                step_id=step_id,
                undo_info={
                    "compensating_action": compensating_action,
                    "stdout": stdout[:1000] if stdout else "",  # Truncate for storage
                    "stderr": stderr[:1000] if stderr else ""   # Truncate for storage
                }
            )
        
        except Exception as e:
            logger.error(f"Error recording command execution: {str(e)}")
            return None
    
    async def record_plan_execution(
        self,
        plan_id: str,
        goal: str,
        plan_data: Dict[str, Any],
        transaction_id: Optional[str] = None
    ) -> Optional[int]:
        """
        Record a plan execution for potential rollback.
        
        Args:
            plan_id: ID of the plan that was executed.
            goal: Goal of the plan.
            plan_data: Plan data structure.
            transaction_id: ID of the transaction this operation belongs to.
            
        Returns:
            Index of the operation in the history or None on error
        """
        try:
            # Record the operation
            return await self.record_operation(
                operation_type=OP_PLAN,
                params={
                    "plan_id": plan_id,
                    "goal": goal
                },
                transaction_id=transaction_id,
                undo_info={
                    "plan_data": plan_data
                }
            )
        
        except Exception as e:
            logger.error(f"Error recording plan execution: {str(e)}")
            return None
    
    async def get_recent_operations(self, limit: int = 10) -> List[Dict[str, Any]]:
        """
        Get a list of recent operations that can be rolled back.
        
        Args:
            limit: Maximum number of operations to return.
            
        Returns:
            A list of operation details.
        """
        try:
            # Get the most recent operations, up to the limit
            recent = self._operations[-limit:] if self._operations else []
            
            # Convert to a more user-friendly format
            result = []
            for i, op in enumerate(reversed(recent)):
                # Create a more readable description
                description = self._get_operation_description(op)
                
                # Determine if the operation can be rolled back
                can_rollback = bool(op.backup_path) or op.operation_type in [OP_CONTENT, OP_COMMAND]
                
                # Get transaction info if available
                transaction_info = None
                if op.transaction_id:
                    if op.transaction_id in self._transactions:
                        transaction = self._transactions[op.transaction_id]
                        transaction_info = {
                            "id": transaction.transaction_id,
                            "description": transaction.description,
                            "status": transaction.status
                        }
                    elif op.transaction_id in self._active_transactions:
                        transaction = self._active_transactions[op.transaction_id]
                        transaction_info = {
                            "id": transaction.transaction_id,
                            "description": transaction.description,
                            "status": transaction.status
                        }
                
                result.append({
                    "id": len(self._operations) - i - 1,  # Original index in the full list
                    "timestamp": op.timestamp.strftime("%Y-%m-%d %H:%M:%S"),
                    "operation_type": op.operation_type,
                    "description": description,
                    "can_rollback": can_rollback,
                    "transaction": transaction_info
                })
            
            return result
        
        except Exception as e:
            logger.error(f"Error getting recent operations: {str(e)}")
            return []
    
    async def get_recent_transactions(self, limit: int = 5) -> List[Dict[str, Any]]:
        """
        Get a list of recent transactions that can be rolled back.
        
        Args:
            limit: Maximum number of transactions to return.
            
        Returns:
            A list of transaction details.
        """
        try:
            # Combine active and completed transactions
            all_transactions = list(self._transactions.values()) + list(self._active_transactions.values())
            
            # Sort by timestamp (newest first)
            all_transactions.sort(key=lambda t: t.timestamp, reverse=True)
            
            # Take only the most recent up to the limit
            recent = all_transactions[:limit]
            
            # Convert to a more user-friendly format
            result = []
            for transaction in recent:
                # Count operations in this transaction
                operation_count = len(transaction.operation_ids)
                
                # Determine if the transaction can be rolled back
                can_rollback = transaction.status == "completed" and operation_count > 0
                
                result.append({
                    "id": transaction.transaction_id,
                    "timestamp": transaction.timestamp.strftime("%Y-%m-%d %H:%M:%S"),
                    "description": transaction.description,
                    "status": transaction.status,
                    "operation_count": operation_count,
                    "can_rollback": can_rollback
                })
            
            return result
        
        except Exception as e:
            logger.error(f"Error getting recent transactions: {str(e)}")
            return []
    
    def _get_operation_description(self, op: OperationRecord) -> str:
        """Generate a human-readable description of an operation."""
        try:
            if op.operation_type == OP_FILE_SYSTEM:
                file_operation = op.params.get("file_operation", "unknown")
                
                if file_operation == "create_file":
                    return f"Created file: {op.params.get('path', 'unknown')}"
                
                elif file_operation == "write_file":
                    return f"Wrote to file: {op.params.get('path', 'unknown')}"
                
                elif file_operation == "delete_file":
                    return f"Deleted file: {op.params.get('path', 'unknown')}"
                
                elif file_operation == "create_directory":
                    return f"Created directory: {op.params.get('path', 'unknown')}"
                
                elif file_operation == "delete_directory":
                    return f"Deleted directory: {op.params.get('path', 'unknown')}"
                
                elif file_operation == "copy_file":
                    return f"Copied file from {op.params.get('source', 'unknown')} to {op.params.get('destination', 'unknown')}"
                
                elif file_operation == "move_file":
                    return f"Moved file from {op.params.get('source', 'unknown')} to {op.params.get('destination', 'unknown')}"
                
                else:
                    return f"{file_operation}: {op.params}"
            
            elif op.operation_type == OP_CONTENT:
                file_path = op.params.get("file_path", "unknown")
                instruction = op.params.get("instruction", "Modified content")
                return f"Modified content of {file_path}: {instruction}"
            
            elif op.operation_type == OP_COMMAND:
                command = op.params.get("command", "unknown")
                return f"Executed command: {command}"
            
            elif op.operation_type == OP_PLAN:
                goal = op.params.get("goal", "unknown")
                return f"Executed plan: {goal}"
            
            else:
                return f"{op.operation_type}: {op.params}"
                
        except Exception as e:
            logger.error(f"Error generating operation description: {str(e)}")
            return "Unknown operation"
    
    async def _identify_compensating_action(
        self,
        command: str,
        stdout: str,
        stderr: str,
        cwd: Optional[str] = None
    ) -> Optional[str]:
        """
        Identify a compensating action for a command.
        
        Args:
            command: The command that was executed.
            stdout: Standard output of the command.
            stderr: Standard error of the command.
            cwd: Current working directory when the command was executed.
            
        Returns:
            Compensating action command or None if not available.
        """
        try:
            # Parse the command
            tokens = shlex.split(command)
            if not tokens:
                return None
            
            base_cmd = tokens[0]
            
            # Check for common commands with known compensations
            for cmd_pattern, compensation in self._command_compensations.items():
                if command.startswith(cmd_pattern):
                    # Extract arguments to construct the compensation
                    args = tokens[len(cmd_pattern.split()):]
                    if not args:
                        continue
                    
                    # Special handling for different command types
                    if cmd_pattern == "git add":
                        # git reset <files>
                        return f"{compensation} {' '.join(args)}"
                    
                    elif cmd_pattern == "git push":
                        # Extract branch from command or use current branch
                        branch = None
                        for i, arg in enumerate(tokens):
                            if i > 0 and arg not in ["-f", "--force", "-u", "--set-upstream"]:
                                branch = arg
                                break
                        
                        if not branch:
                            # Get current branch
                            branch = "$(git rev-parse --abbrev-ref HEAD)"
                        
                        # Substitute ${branch} in the compensation
                        return compensation.replace("${branch}", branch)
                    
                    elif cmd_pattern in ["npm install", "pip install", "apt-get install"]:
                        # Extract package names, skipping flags
                        packages = []
                        for arg in args:
                            if not arg.startswith("-"):
                                packages.append(arg)
                        
                        if packages:
                            return f"{compensation} {' '.join(packages)}"
                    
                    else:
                        # Generic compensation with all arguments
                        return f"{compensation} {' '.join(args)}"
            
            # No known compensation found
            return None
            
        except Exception as e:
            logger.error(f"Error identifying compensating action: {str(e)}")
            return None
    
    async def rollback_operation(self, operation_id: int) -> bool:
        """
        Roll back an operation by its ID.
        
        Args:
            operation_id: The ID of the operation to roll back.
            
        Returns:
            True if the rollback was successful, False otherwise.
        """
        try:
            # Validate the operation ID
            if operation_id < 0 or operation_id >= len(self._operations):
                logger.error(f"Invalid operation ID: {operation_id}")
                return False
            
            # Get the operation record
            op = self._operations[operation_id]
            
            # Roll back based on operation type
            if op.operation_type == OP_FILE_SYSTEM:
                success = await self._rollback_file_operation(op)
            elif op.operation_type == OP_CONTENT:
                success = await self._rollback_content_manipulation(op)
            elif op.operation_type == OP_COMMAND:
                success = await self._rollback_command_execution(op)
            elif op.operation_type == OP_PLAN:
                success = await self._rollback_plan_execution(op)
            else:
                logger.error(f"Unsupported operation type for rollback: {op.operation_type}")
                return False
            
            # If rollback was successful, update the operations list
            if success:
                # If this operation is part of a transaction, we don't remove it here
                # Instead, we'll handle it in rollback_transaction
                if not op.transaction_id:
                    self._operations = self._operations[:operation_id]
                    self._save_history(self._operations)
                
                logger.info(f"Successfully rolled back operation {operation_id}: {op.operation_type}")
                return True
            else:
                logger.error(f"Failed to roll back operation {operation_id}: {op.operation_type}")
                return False
                
        except Exception as e:
            logger.exception(f"Error rolling back operation {operation_id}: {str(e)}")
            return False
    
    async def _rollback_file_operation(self, op: OperationRecord) -> bool:
        """
        Roll back a file system operation.
        
        Args:
            op: The operation record.
            
        Returns:
            True if successful, False otherwise.
        """
        try:
            file_operation = op.params.get("file_operation")
            
            if file_operation == "create_file":
                # For file creation, delete the created file
                path = Path(op.params.get("path", ""))
                if path.exists() and path.is_file():
                    path.unlink()
                    logger.info(f"Rolled back file creation: {path}")
                    return True
                else:
                    logger.warning(f"File no longer exists: {path}")
                    return True  # Consider it success if file is already gone
            
            elif file_operation in ["write_file", "delete_file"]:
                # For file writing/deletion, restore from backup
                path = Path(op.params.get("path", ""))
                backup_path = op.backup_path
                
                if not backup_path:
                    logger.error(f"No backup path for {file_operation} operation")
                    return False
                
                backup_path_obj = Path(backup_path)
                if not backup_path_obj.exists():
                    logger.error(f"Backup file not found: {backup_path}")
                    return False
                
                # Create parent directory if it doesn't exist
                path.parent.mkdir(parents=True, exist_ok=True)
                
                # Restore the file
                shutil.copy2(backup_path_obj, path)
                logger.info(f"Restored file from backup: {path}")
                return True
            
            elif file_operation == "create_directory":
                # For directory creation, delete the created directory
                path = Path(op.params.get("path", ""))
                if path.exists() and path.is_dir():
                    # Use rmtree to handle non-empty directories
                    shutil.rmtree(path)
                    logger.info(f"Rolled back directory creation: {path}")
                    return True
                else:
                    logger.warning(f"Directory no longer exists: {path}")
                    return True  # Consider it success if directory is already gone
            
            elif file_operation == "delete_directory":
                # For directory deletion, restore from backup
                path = Path(op.params.get("path", ""))
                backup_path = op.backup_path
                
                if not backup_path:
                    logger.error(f"No backup path for {file_operation} operation")
                    return False
                
                backup_path_obj = Path(backup_path)
                if not backup_path_obj.exists():
                    logger.error(f"Backup directory not found: {backup_path}")
                    return False
                
                # Create parent directory if it doesn't exist
                path.parent.mkdir(parents=True, exist_ok=True)
                
                # Restore the directory
                if path.exists():
                    shutil.rmtree(path)  # Remove existing directory first
                shutil.copytree(backup_path_obj, path)
                logger.info(f"Restored directory from backup: {path}")
                return True
            
            elif file_operation in ["copy_file", "move_file"]:
                # For copy/move, multiple files may need to be restored
                destination = Path(op.params.get("destination", ""))
                backup_path = op.backup_path
                
                if not backup_path:
                    logger.error(f"No backup path for {file_operation} operation")
                    return False
                
                # Restore the destination if it was overwritten
                if destination.exists() and backup_path:
                    backup_path_obj = Path(backup_path)
                    if backup_path_obj.exists():
                        if backup_path_obj.is_file():
                            shutil.copy2(backup_path_obj, destination)
                            logger.info(f"Restored destination file: {destination}")
                        else:
                            shutil.rmtree(destination, ignore_errors=True)
                            shutil.copytree(backup_path_obj, destination)
                            logger.info(f"Restored destination directory: {destination}")
                
                # For move operations, also delete the destination
                if file_operation == "move_file":
                    source = Path(op.params.get("source", ""))
                    if destination.exists():
                        # Restore the source copy if available
                        if source.exists():
                            logger.warning(f"Source already exists, not restoring: {source}")
                        else:
                            # Create parent directory if needed
                            source.parent.mkdir(parents=True, exist_ok=True)
                            
                            if destination.is_file():
                                # Copy destination back to source
                                shutil.copy2(destination, source)
                                logger.info(f"Restored source file: {source}")
                            else:
                                # Copy directory
                                shutil.copytree(destination, source)
                                logger.info(f"Restored source directory: {source}")
                        
                        # Remove destination
                        if destination.is_file():
                            destination.unlink()
                        else:
                            shutil.rmtree(destination)
                        logger.info(f"Removed destination: {destination}")
                
                return True
            
            else:
                logger.error(f"Unsupported file operation for rollback: {file_operation}")
                return False
                
        except Exception as e:
            logger.exception(f"Error rolling back file operation: {str(e)}")
            return False
    
    async def _rollback_content_manipulation(self, op: OperationRecord) -> bool:
        """
        Roll back a content manipulation operation.
        
        Args:
            op: The operation record.
            
        Returns:
            True if successful, False otherwise.
        """
        try:
            file_path = op.params.get("file_path")
            if not file_path:
                logger.error("No file path in content manipulation operation")
                return False
            
            path_obj = Path(file_path)
            if not path_obj.exists() or not path_obj.is_file():
                logger.error(f"File not found: {file_path}")
                return False
            
            # Check if there was a diff
            diff = op.undo_info.get("diff")
            if not diff:
                logger.error("No diff found in content manipulation undo info")
                return False
            
            # Read the current content
            try:
                with open(path_obj, 'r', encoding='utf-8', errors='replace') as f:
                    current_content = f.read()
            except Exception as e:
                logger.error(f"Error reading current file content: {str(e)}")
                return False
            
            # Apply the reversed diff
            # The diff is from original to modified, so we reverse it
            # In this simple approach, we swap "+" and "-" in the diff
            reversed_diff = ""
            for line in diff.splitlines():
                if line.startswith('+'):
                    reversed_diff += '-' + line[1:] + '\n'
                elif line.startswith('-'):
                    reversed_diff += '+' + line[1:] + '\n'
                else:
                    reversed_diff += line + '\n'
            
            # Get diff_manager through API
            diff_manager = get_diff_manager()
            
            # Try to apply the reversed diff
            result, success = diff_manager.apply_diff(current_content, reversed_diff)
            
            if not success:
                logger.error("Failed to apply reversed diff")
                return False
            
            # Write the reverted content back to the file
            try:
                with open(path_obj, 'w', encoding='utf-8') as f:
                    f.write(result)
            except Exception as e:
                logger.error(f"Error writing reverted content: {str(e)}")
                return False
            
            logger.info(f"Successfully rolled back content changes for {file_path}")
            return True
            
        except Exception as e:
            logger.exception(f"Error rolling back content manipulation: {str(e)}")
            return False
    
    async def _rollback_command_execution(self, op: OperationRecord) -> bool:
        """
        Roll back a command execution operation.
        
        Args:
            op: The operation record.
            
        Returns:
            True if successful, False otherwise.
        """
        try:
            # Get the compensating action
            compensating_action = op.undo_info.get("compensating_action")
            if not compensating_action:
                logger.warning(f"No compensating action available for command: {op.params.get('command')}")
                return False
            
            # Get the working directory
            cwd = op.params.get("cwd")
            
            # Get execution_engine through API
            execution_engine = get_execution_engine()
            
            # Execute the compensating action
            logger.info(f"Executing compensating action: {compensating_action}")
            stdout, stderr, return_code = await execution_engine.execute_command(
                compensating_action,
                check_safety=False,  # Skip safety checks for compensating actions
                working_dir=cwd
            )
            
            # Check if the compensating action was successful
            if return_code != 0:
                logger.error(f"Compensating action failed: {stderr}")
                return False
            
            logger.info(f"Successfully executed compensating action: {compensating_action}")
            return True
            
        except Exception as e:
            logger.exception(f"Error rolling back command execution: {str(e)}")
            return False
    
    async def _rollback_plan_execution(self, op: OperationRecord) -> bool:
        """
        Roll back a plan execution operation.
        
        Args:
            op: The operation record.
            
        Returns:
            True if successful, False otherwise.
        """
        # For plan execution, there's not much to do at this level
        # as the individual operations within the plan should be rolled back
        logger.info(f"Rolled back plan execution: {op.params.get('goal')}")
        return True
    
    async def rollback_transaction(self, transaction_id: str) -> Dict[str, Any]:
        """
        Roll back all operations in a transaction.
        
        Args:
            transaction_id: ID of the transaction to roll back.
            
        Returns:
            Dictionary with rollback results.
        """
        try:
            # Find the transaction
            if transaction_id in self._transactions:
                transaction = self._transactions[transaction_id]
            elif transaction_id in self._active_transactions:
                transaction = self._active_transactions[transaction_id]
            else:
                return {
                    "success": False,
                    "error": f"Transaction not found: {transaction_id}",
                    "transaction_id": transaction_id
                }
            
            # Collect all operations in this transaction
            operation_ids = transaction.operation_ids
            if not operation_ids:
                logger.warning(f"No operations found in transaction: {transaction_id}")
                return {
                    "success": True,
                    "message": "No operations to roll back",
                    "transaction_id": transaction_id,
                    "rolled_back": 0,
                    "failed": 0
                }
            
            # Sort operations in reverse order (most recent first)
            operation_ids.sort(reverse=True)
            
            # Roll back each operation
            rolled_back = 0
            failed = 0
            results = []
            
            for op_id in operation_ids:
                if op_id >= len(self._operations):
                    logger.error(f"Invalid operation ID in transaction: {op_id}")
                    failed += 1
                    results.append({
                        "operation_id": op_id,
                        "success": False,
                        "error": "Invalid operation ID"
                    })
                    continue
                
                # Roll back this operation
                op = self._operations[op_id]
                description = self._get_operation_description(op)
                
                success = await self.rollback_operation(op_id)
                if success:
                    rolled_back += 1
                    results.append({
                        "operation_id": op_id,
                        "operation_type": op.operation_type,
                        "description": description,
                        "success": True
                    })
                else:
                    failed += 1
                    results.append({
                        "operation_id": op_id,
                        "operation_type": op.operation_type,
                        "description": description,
                        "success": False,
                        "error": "Rollback failed"
                    })
            
            # Update transaction status
            transaction.status = "rolled_back"
            self._save_transaction(transaction)
            
            # Return the results
            return {
                "success": failed == 0,
                "transaction_id": transaction_id,
                "rolled_back": rolled_back,
                "failed": failed,
                "total": len(operation_ids),
                "results": results
            }
            
        except Exception as e:
            logger.exception(f"Error rolling back transaction {transaction_id}: {str(e)}")
            return {
                "success": False,
                "error": str(e),
                "transaction_id": transaction_id
            }
    
    async def create_backup_file(self, path: Path) -> Optional[Path]:
        """
        Create a backup of a file for potential rollback.
        
        Args:
            path: The path of the file to back up.
            
        Returns:
            The path of the backup file or None if backup failed.
        """
        try:
            # Ensure backup directory exists
            BACKUP_DIR.mkdir(parents=True, exist_ok=True)
            
            # Create a unique backup filename
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            backup_name = f"{path.name}.{timestamp}.bak"
            backup_path = BACKUP_DIR / backup_name
            
            # Copy the file to the backup location
            shutil.copy2(path, backup_path)
            logger.debug(f"Created backup of {path} at {backup_path}")
            
            return backup_path
        
        except Exception as e:
            logger.warning(f"Failed to create backup of {path}: {str(e)}")
            return None
    
    async def create_backup_directory(self, path: Path) -> Optional[Path]:
        """
        Create a backup of a directory for potential rollback.
        
        Args:
            path: The path of the directory to back up.
            
        Returns:
            The path of the backup directory or None if backup failed.
        """
        try:
            # Ensure backup directory exists
            BACKUP_DIR.mkdir(parents=True, exist_ok=True)
            
            # Create a unique backup directory name
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            backup_name = f"{path.name}.{timestamp}.bak"
            backup_path = BACKUP_DIR / backup_name
            
            # Copy the directory to the backup location
            shutil.copytree(path, backup_path)
            logger.debug(f"Created backup of directory {path} at {backup_path}")
            
            return backup_path
        
        except Exception as e:
            logger.warning(f"Failed to create backup of directory {path}: {str(e)}")
            return None

# Global rollback manager instance
rollback_manager = RollbackManager()
</file>

<file path="components/generation/__init__.py">
# angela/generation/__init__.py
"""
Generation components for Angela CLI.

This package provides code generation, architecture analysis, and documentation
generation capabilities for creating and managing software projects through:
- Architectural pattern analysis and recommendations
- Documentation generation for projects and APIs
- Code generation for multiple languages and frameworks
- Framework-specific project scaffolding
- Code validation and refinement capabilities
"""

# First import the models to avoid circular imports
from .models import CodeFile, CodeProject

# Then import other components
from .architecture import architectural_analyzer, analyze_project_architecture
from .documentation import documentation_generator
from .engine import code_generation_engine
from .frameworks import framework_generator
from .validators import validate_code
from .refiner import interactive_refiner
from .planner import project_planner, ProjectArchitecture
from .context_manager import generation_context_manager

# Define the public API
__all__ = [
    # Models
    'CodeFile',
    'CodeProject',
    
    # Architecture analysis
    'architectural_analyzer',
    'analyze_project_architecture',
    
    # Documentation generation
    'documentation_generator',
    
    # Code generation
    'code_generation_engine',
    
    # Framework generators
    'framework_generator',
    
    # Code validation
    'validate_code',
    
    # Code refinement
    'interactive_refiner',
    
    # Project planning
    'project_planner',
    'ProjectArchitecture',
    'generation_context_manager'
]
</file>

<file path="components/generation/architecture.py">
# angela/generation/architecture.py
"""
Architectural analysis and improvements for Angela CLI.

This module provides capabilities for analyzing project architecture,
detecting anti-patterns, and suggesting improvements.
"""
import os
import asyncio
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple, Union
import json
import re

from angela.api.ai import get_gemini_client, get_gemini_request_class
from angela.utils.logging import get_logger

logger = get_logger(__name__)
GeminiRequest = get_gemini_request_class()

class ArchitecturalPattern:
    """Base class for architectural patterns."""
    
    def __init__(self, name: str, description: str):
        """
        Initialize the architectural pattern.
        
        Args:
            name: Pattern name
            description: Pattern description
        """
        self.name = name
        self.description = description
    
    async def detect(self, project_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """
        Detect if the pattern is present in the project.
        
        Args:
            project_analysis: Analysis of the project
            
        Returns:
            Dictionary with detection results
        """
        raise NotImplementedError("Subclasses must implement detect")
    
    def get_recommendations(self, detection_result: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        Get recommendations based on detection results.
        
        Args:
            detection_result: Results from the detect method
            
        Returns:
            List of recommendation dictionaries
        """
        raise NotImplementedError("Subclasses must implement get_recommendations")

class AntiPattern:
    """Base class for architectural anti-patterns."""
    
    def __init__(self, name: str, description: str, severity: str):
        """
        Initialize the anti-pattern.
        
        Args:
            name: Anti-pattern name
            description: Anti-pattern description
            severity: Severity level (low, medium, high)
        """
        self.name = name
        self.description = description
        self.severity = severity
    
    async def detect(self, project_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """
        Detect if the anti-pattern is present in the project.
        
        Args:
            project_analysis: Analysis of the project
            
        Returns:
            Dictionary with detection results
        """
        raise NotImplementedError("Subclasses must implement detect")
    
    def get_recommendations(self, detection_result: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        Get recommendations to fix the anti-pattern.
        
        Args:
            detection_result: Results from the detect method
            
        Returns:
            List of recommendation dictionaries
        """
        raise NotImplementedError("Subclasses must implement get_recommendations")

class MvcPattern(ArchitecturalPattern):
    """Model-View-Controller pattern detector."""
    
    def __init__(self):
        """Initialize the MVC pattern detector."""
        super().__init__(
            name="Model-View-Controller",
            description="Separates application logic into three components: Model (data), View (presentation), and Controller (logic)"
        )
    
    async def detect(self, project_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """
        Detect if MVC pattern is used in the project.
        
        Args:
            project_analysis: Analysis of the project
            
        Returns:
            Dictionary with detection results
        """
        logger.debug("Detecting MVC pattern")
        
        # Default result
        result = {
            "pattern": self.name,
            "present": False,
            "confidence": 0.0,
            "components": {
                "models": [],
                "views": [],
                "controllers": []
            }
        }
        
        # Check file structure for MVC pattern
        models = []
        views = []
        controllers = []
        
        for file_info in project_analysis.get("files", []):
            file_path = file_info.get("path", "").lower()
            file_content = file_info.get("content", "")
            
            # Check for models
            if "model" in file_path or "/models/" in file_path:
                models.append(file_path)
            elif file_content and re.search(r'class\s+\w*Model\b', file_content):
                models.append(file_path)
            
            # Check for views
            if "view" in file_path or "/views/" in file_path:
                views.append(file_path)
            elif file_path.endswith((".html", ".jsx", ".tsx", ".vue")) or "template" in file_path:
                views.append(file_path)
            elif file_content and re.search(r'class\s+\w*View\b', file_content):
                views.append(file_path)
            
            # Check for controllers
            if "controller" in file_path or "/controllers/" in file_path:
                controllers.append(file_path)
            elif file_content and re.search(r'class\s+\w*Controller\b', file_content):
                controllers.append(file_path)
        
        # Update result with components found
        result["components"]["models"] = models
        result["components"]["views"] = views
        result["components"]["controllers"] = controllers
        
        # Calculate confidence
        if models and views and controllers:
            result["present"] = True
            result["confidence"] = 0.9  # High confidence if all three components found
        elif (models and views) or (models and controllers) or (views and controllers):
            result["present"] = True
            result["confidence"] = 0.6  # Medium confidence if two components found
        elif models or views or controllers:
            result["present"] = False
            result["confidence"] = 0.3  # Low confidence if only one component found
        
        return result
    
    def get_recommendations(self, detection_result: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        Get recommendations for improving MVC pattern usage.
        
        Args:
            detection_result: Results from the detect method
            
        Returns:
            List of recommendation dictionaries
        """
        recommendations = []
        
        if detection_result["present"]:
            # MVC pattern is present, check if all components are balanced
            models = detection_result["components"]["models"]
            views = detection_result["components"]["views"]
            controllers = detection_result["components"]["controllers"]
            
            if len(models) < len(controllers) / 2:
                recommendations.append({
                    "title": "Insufficient Model Separation",
                    "description": "There are significantly fewer Model files than Controllers, which may indicate business logic leaking into Controllers.",
                    "action": "Consider extracting data models from Controllers into separate Model classes.",
                    "priority": "medium"
                })
            
            if not controllers and models and views:
                recommendations.append({
                    "title": "Missing Controller Layer",
                    "description": "Models and Views are present, but no clear Controller layer was detected.",
                    "action": "Implement Controllers to handle the interaction between Models and Views.",
                    "priority": "high"
                })
        else:
            # MVC pattern is not present
            if detection_result["confidence"] > 0.0:
                # Some components found, but not all
                recommendations.append({
                    "title": "Incomplete MVC Implementation",
                    "description": "Some MVC components are present, but the pattern is not fully implemented.",
                    "action": "Consider fully adopting the MVC pattern by adding the missing components.",
                    "priority": "medium"
                })
            else:
                # No components found
                recommendations.append({
                    "title": "Consider MVC Pattern",
                    "description": "The project doesn't appear to use the MVC pattern, which can help with code organization and maintainability.",
                    "action": "Consider refactoring to separate concerns into Model, View, and Controller components.",
                    "priority": "low"
                })
        
        return recommendations

class SingleResponsibilityAntiPattern(AntiPattern):
    """Detects violations of the Single Responsibility Principle."""
    
    def __init__(self):
        """Initialize the single responsibility anti-pattern detector."""
        super().__init__(
            name="Single Responsibility Violation",
            description="Classes or modules that have more than one reason to change",
            severity="medium"
        )
    
    async def detect(self, project_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """
        Detect violations of the Single Responsibility Principle.
        
        Args:
            project_analysis: Analysis of the project
            
        Returns:
            Dictionary with detection results
        """
        logger.debug("Detecting Single Responsibility violations")
        
        # Default result
        result = {
            "anti_pattern": self.name,
            "detected": False,
            "instances": [],
            "severity": self.severity
        }
        
        # Check for large classes with many methods
        for file_info in project_analysis.get("files", []):
            if not file_info.get("content"):
                continue
            
            file_path = file_info.get("path", "")
            content = file_info.get("content", "")
            
            # Skip non-source files
            if file_info.get("type") != "source_code":
                continue
            
            language = file_info.get("language", "").lower()
            
            if language in ["python", "java", "javascript", "typescript"]:
                # Check for classes with too many methods
                classes = self._extract_classes(content, language)
                
                for class_info in classes:
                    # Check number of methods
                    if len(class_info["methods"]) > 10:  # Arbitrary threshold
                        # Check for different categories of methods
                        categories = self._categorize_methods(class_info["methods"], language)
                        
                        if len(categories) >= 3:  # If methods fall into 3+ categories, likely has multiple responsibilities
                            result["instances"].append({
                                "file": file_path,
                                "class": class_info["name"],
                                "method_count": len(class_info["methods"]),
                                "categories": categories,
                                "confidence": min(0.5 + (len(categories) - 3) * 0.1, 0.9)  # Higher confidence with more categories
                            })
        
        if result["instances"]:
            result["detected"] = True
        
        return result
    
    def _extract_classes(self, content: str, language: str) -> List[Dict[str, Any]]:
        """
        Extract classes and their methods from code.
        
        Args:
            content: Source code content
            language: Programming language
            
        Returns:
            List of dictionaries with class info
        """
        classes = []
        
        if language == "python":
            # Extract Python classes
            class_pattern = r'class\s+(\w+)(?:\(.*?\))?:'
            method_pattern = r'\s+def\s+(\w+)\s*\('
            
            class_matches = re.finditer(class_pattern, content)
            
            for class_match in class_matches:
                class_name = class_match.group(1)
                class_start = class_match.end()
                
                # Find the end of the class (indentation level)
                class_content = ""
                for line in content[class_start:].splitlines():
                    if line.strip() and not line.startswith(" ") and not line.startswith("\t"):
                        break
                    class_content += line + "\n"
                
                # Extract methods
                methods = []
                for method_match in re.finditer(method_pattern, class_content):
                    method_name = method_match.group(1)
                    if method_name != "__init__":  # Skip constructor
                        methods.append(method_name)
                
                classes.append({
                    "name": class_name,
                    "methods": methods
                })
        
        elif language in ["java", "javascript", "typescript"]:
            # Extract classes (simplified)
            class_pattern = r'class\s+(\w+)(?:\s+extends\s+\w+)?(?:\s+implements\s+[\w,\s]+)?\s*{'
            method_pattern = r'(?:public|private|protected)?\s+(?:static\s+)?(?:\w+\s+)?(\w+)\s*\([^)]*\)\s*{(?:[^{}]|{[^{}]*})*}'
            
            class_matches = re.finditer(class_pattern, content)
            
            for class_match in class_matches:
                class_name = class_match.group(1)
                class_start = class_match.start()
                
                # Find the class block by counting braces
                brace_count = 0
                class_end = class_start
                in_class = False
                
                for i, c in enumerate(content[class_start:]):
                    if c == '{':
                        if not in_class:
                            in_class = True
                        brace_count += 1
                    elif c == '}':
                        brace_count -= 1
                        if in_class and brace_count == 0:
                            class_end = class_start + i + 1
                            break
                
                class_content = content[class_start:class_end]
                
                # Extract methods
                methods = []
                for method_match in re.finditer(method_pattern, class_content):
                    method_name = method_match.group(1)
                    if method_name != "constructor" and not method_name.startswith("get") and not method_name.startswith("set"):
                        methods.append(method_name)
                
                classes.append({
                    "name": class_name,
                    "methods": methods
                })
        
        return classes
    
    def _categorize_methods(self, methods: List[str], language: str) -> Dict[str, List[str]]:
        """
        Categorize methods into different responsibilities.
        
        Args:
            methods: List of method names
            language: Programming language
            
        Returns:
            Dictionary mapping categories to method names
        """
        categories = {}
        
        # Common categories and their keywords
        categories_keywords = {
            "data_access": ["save", "load", "read", "write", "fetch", "store", "retrieve", "query", "find", "get", "set", "select", "insert", "update", "delete", "persist", "repository", "dao"],
            "business_logic": ["calculate", "compute", "process", "validate", "check", "verify", "evaluate", "analyze", "generate", "create", "build", "make", "service"],
            "presentation": ["display", "show", "render", "view", "draw", "paint", "print", "format", "transform", "convert", "ui", "gui", "interface"],
            "networking": ["connect", "disconnect", "send", "receive", "post", "get", "put", "delete", "request", "response", "url", "uri", "http", "api", "rest", "soap", "websocket"],
            "file_io": ["file", "stream", "open", "close", "read", "write", "input", "output", "io", "path", "directory", "folder"],
            "concurrency": ["thread", "async", "await", "parallel", "concurrent", "lock", "mutex", "semaphore", "synchronize", "task", "job", "worker", "pool"],
            "utility": ["util", "helper", "common", "shared", "factory", "builder", "converter", "mapper", "utils"]
        }
        
        # Categorize methods based on name
        for method in methods:
            method_lower = method.lower()
            categorized = False
            
            for category, keywords in categories_keywords.items():
                for keyword in keywords:
                    if keyword in method_lower:
                        if category not in categories:
                            categories[category] = []
                        categories[category].append(method)
                        categorized = True
                        break
                
                if categorized:
                    break
            
            # If method doesn't match any category, put it in "other"
            if not categorized:
                if "other" not in categories:
                    categories["other"] = []
                categories["other"].append(method)
        
        return categories
    
    def get_recommendations(self, detection_result: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        Get recommendations for fixing Single Responsibility Principle violations.
        
        Args:
            detection_result: Results from the detect method
            
        Returns:
            List of recommendation dictionaries
        """
        recommendations = []
        
        if detection_result["detected"]:
            for instance in detection_result["instances"]:
                # Create a recommendation for each instance
                categories_str = ", ".join(instance["categories"].keys())
                
                recommendations.append({
                    "title": f"Refactor Class: {instance['class']}",
                    "description": f"This class has multiple responsibilities: {categories_str}",
                    "action": f"Consider splitting '{instance['class']}' into multiple classes, each with a single responsibility.",
                    "priority": "medium" if instance["confidence"] > 0.7 else "low"
                })
        
        return recommendations

class GodObjectAntiPattern(AntiPattern):
    """Detects God Objects - classes that know or do too much."""
    
    def __init__(self):
        """Initialize the God Object anti-pattern detector."""
        super().__init__(
            name="God Object",
            description="Classes that know or do too much, often with excessive size and responsibilities",
            severity="high"
        )
    
    async def detect(self, project_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """
        Detect God Objects in the project.
        
        Args:
            project_analysis: Analysis of the project
            
        Returns:
            Dictionary with detection results
        """
        logger.debug("Detecting God Objects")
        
        # Default result
        result = {
            "anti_pattern": self.name,
            "detected": False,
            "instances": [],
            "severity": self.severity
        }
        
        # Define thresholds for various metrics
        thresholds = {
            "lines": 500,  # Lines of code
            "methods": 20,  # Number of methods
            "fields": 15,   # Number of fields/properties
            "imports": 15,  # Number of imports
            "dependencies": 10  # Number of dependencies on other classes
        }
        
        # Check each file for potential God Objects
        for file_info in project_analysis.get("files", []):
            if not file_info.get("content"):
                continue
            
            file_path = file_info.get("path", "")
            content = file_info.get("content", "")
            
            # Skip non-source files
            if file_info.get("type") != "source_code":
                continue
            
            language = file_info.get("language", "").lower()
            
            if language in ["python", "java", "javascript", "typescript"]:
                # Extract classes
                classes = self._extract_classes_with_metrics(content, language)
                
                for class_info in classes:
                    # Check if any metric exceeds thresholds
                    violations = {}
                    for metric, value in class_info["metrics"].items():
                        if metric in thresholds and value > thresholds[metric]:
                            violations[metric] = value
                    
                    if violations:
                        # Calculate violation severity
                        violation_count = len(violations)
                        violation_ratio = sum(violations[m] / thresholds[m] for m in violations) / len(violations)
                        confidence = min(0.5 + (violation_count * 0.1) + (violation_ratio * 0.2), 0.95)
                        
                        result["instances"].append({
                            "file": file_path,
                            "class": class_info["name"],
                            "violations": violations,
                            "metrics": class_info["metrics"],
                            "confidence": confidence
                        })
        
        if result["instances"]:
            result["detected"] = True
        
        return result
    
    def _extract_classes_with_metrics(self, content: str, language: str) -> List[Dict[str, Any]]:
        """
        Extract classes and calculate metrics.
        
        Args:
            content: Source code content
            language: Programming language
            
        Returns:
            List of dictionaries with class info and metrics
        """
        classes = []
        
        if language == "python":
            # Extract Python classes
            class_pattern = r'class\s+(\w+)(?:\(.*?\))?:'
            method_pattern = r'\s+def\s+(\w+)\s*\('
            field_pattern = r'\s+self\.(\w+)\s*='
            import_pattern = r'(?:import|from)\s+[\w.]+'
            
            # Count imports
            imports = len(re.findall(import_pattern, content))
            
            class_matches = re.finditer(class_pattern, content)
            
            for class_match in class_matches:
                class_name = class_match.group(1)
                class_start = class_match.end()
                
                # Find the end of the class (indentation level)
                class_content = ""
                class_lines = 0
                for line in content[class_start:].splitlines():
                    if line.strip() and not line.startswith(" ") and not line.startswith("\t"):
                        break
                    class_content += line + "\n"
                    class_lines += 1
                
                # Extract methods
                methods = re.findall(method_pattern, class_content)
                
                # Extract fields
                fields = re.findall(field_pattern, class_content)
                
                # Calculate other dependencies (simplified)
                dependencies = set()
                for line in class_content.splitlines():
                    # Look for other class instantiations
                    instance_matches = re.findall(r'=\s*(\w+)\(', line)
                    for instance in instance_matches:
                        if instance != class_name and instance[0].isupper():  # Potential class
                            dependencies.add(instance)
                
                classes.append({
                    "name": class_name,
                    "content": class_content,
                    "metrics": {
                        "lines": class_lines,
                        "methods": len(methods),
                        "fields": len(fields),
                        "imports": imports,
                        "dependencies": len(dependencies)
                    }
                })
        
        elif language in ["java", "javascript", "typescript"]:
            # Extract classes (simplified)
            class_pattern = r'class\s+(\w+)(?:\s+extends\s+\w+)?(?:\s+implements\s+[\w,\s]+)?\s*{'
            method_pattern = r'(?:public|private|protected)?\s+(?:static\s+)?(?:\w+\s+)?(\w+)\s*\(['
            field_pattern = r'(?:public|private|protected)?\s+(?:static\s+)?(?:final\s+)?[\w<>[\],\s]+\s+(\w+)\s*[;=]'
            import_pattern = r'import\s+[\w.]+'
            
            # Count imports
            imports = len(re.findall(import_pattern, content))
            
            class_matches = re.finditer(class_pattern, content)
            
            for class_match in class_matches:
                class_name = class_match.group(1)
                class_start = class_match.start()
                
                # Find the class block by counting braces
                brace_count = 0
                class_end = class_start
                in_class = False
                
                for i, c in enumerate(content[class_start:]):
                    if c == '{':
                        if not in_class:
                            in_class = True
                        brace_count += 1
                    elif c == '}':
                        brace_count -= 1
                        if in_class and brace_count == 0:
                            class_end = class_start + i + 1
                            break
                
                class_content = content[class_start:class_end]
                class_lines = class_content.count('\n')
                
                # Extract methods
                methods = re.findall(method_pattern, class_content)
                
                # Extract fields
                fields = re.findall(field_pattern, class_content)
                
                # Calculate other dependencies (simplified)
                dependencies = set()
                for line in class_content.splitlines():
                    # Look for other class instantiations
                    instance_matches = re.findall(r'new\s+(\w+)\(', line)
                    for instance in instance_matches:
                        if instance != class_name:
                            dependencies.add(instance)
                
                classes.append({
                    "name": class_name,
                    "content": class_content,
                    "metrics": {
                        "lines": class_lines,
                        "methods": len(methods),
                        "fields": len(fields),
                        "imports": imports,
                        "dependencies": len(dependencies)
                    }
                })
        
        return classes
    
    def get_recommendations(self, detection_result: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        Get recommendations for fixing God Objects.
        
        Args:
            detection_result: Results from the detect method
            
        Returns:
            List of recommendation dictionaries
        """
        recommendations = []
        
        if detection_result["detected"]:
            for instance in detection_result["instances"]:
                # Create specific recommendations based on violations
                violations_msg = []
                
                if "lines" in instance["violations"]:
                    violations_msg.append(f"excessive size ({instance['violations']['lines']} lines)")
                if "methods" in instance["violations"]:
                    violations_msg.append(f"too many methods ({instance['violations']['methods']})")
                if "fields" in instance["violations"]:
                    violations_msg.append(f"too many fields ({instance['violations']['fields']})")
                if "dependencies" in instance["violations"]:
                    violations_msg.append(f"too many dependencies ({instance['violations']['dependencies']})")
                
                violations_str = ", ".join(violations_msg)
                
                # Main recommendation
                recommendations.append({
                    "title": f"Refactor God Object: {instance['class']}",
                    "description": f"This class exhibits God Object symptoms: {violations_str}",
                    "action": f"Break '{instance['class']}' into smaller, more focused classes following the Single Responsibility Principle.",
                    "priority": "high" if instance["confidence"] > 0.8 else "medium"
                })
                
                # Add specific tactical recommendations
                if "methods" in instance["violations"] and instance["violations"]["methods"] > 25:
                    recommendations.append({
                        "title": f"Extract Classes from {instance['class']}",
                        "description": f"This class has an excessive number of methods ({instance['violations']['methods']}).",
                        "action": "Group related methods and extract them into new classes with clear responsibilities.",
                        "priority": "high"
                    })
                
                if "dependencies" in instance["violations"] and instance["violations"]["dependencies"] > 12:
                    recommendations.append({
                        "title": f"Reduce Dependencies in {instance['class']}",
                        "description": f"This class depends on too many other classes ({instance['violations']['dependencies']}).",
                        "action": "Use dependency injection or introduce service locators to reduce direct dependencies.",
                        "priority": "medium"
                    })
        
        return recommendations

class ArchitecturalAnalyzer:
    """
    Analyzer for project architecture, detecting patterns and anti-patterns.
    """
    
    def __init__(self):
        """Initialize the architectural analyzer."""
        self._logger = logger
        
        # Register patterns and anti-patterns
        self._patterns = [
            MvcPattern(),
            # Add more patterns here
        ]
        
        self._anti_patterns = [
            SingleResponsibilityAntiPattern(),
            GodObjectAntiPattern(),
            # Add more anti-patterns here
        ]
    
    async def analyze_architecture(
        self, 
        project_path: Union[str, Path],
        context: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Analyze project architecture.
        
        Args:
            project_path: Path to the project
            context: Additional context information
            
        Returns:
            Dictionary with analysis results
        """
        self._logger.info(f"Analyzing architecture of project at {project_path}")
        
        # Analyze project structure
        project_analysis = await self._analyze_project_structure(project_path, context)
        
        # Detect patterns
        patterns_results = await self._detect_patterns(project_analysis)
        
        # Detect anti-patterns
        anti_patterns_results = await self._detect_anti_patterns(project_analysis)
        
        # Generate recommendations
        recommendations = await self._generate_recommendations(patterns_results, anti_patterns_results)
        
        return {
            "project_path": str(project_path),
            "patterns": patterns_results,
            "anti_patterns": anti_patterns_results,
            "recommendations": recommendations,
            "project_analysis": project_analysis
        }
    
    async def _analyze_project_structure(
        self, 
        project_path: Union[str, Path],
        context: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Analyze project structure.
        
        Args:
            project_path: Path to the project
            context: Additional context information
            
        Returns:
            Dictionary with project analysis
        """
        self._logger.info(f"Analyzing project structure at {project_path}")
        
        # Convert to Path object
        project_path = Path(project_path)
        
        # Use context if available
        if context and "enhanced_project" in context:
            return {
                "project_type": context["enhanced_project"].get("type", "unknown"),
                "frameworks": context["enhanced_project"].get("frameworks", {}),
                "dependencies": context["enhanced_project"].get("dependencies", {}),
                "files": context.get("files", []),
                "path": str(project_path)
            }
        
        # Perform a simplified project analysis
        project_analysis = {
            "project_type": "unknown",
            "frameworks": {},
            "dependencies": {},
            "files": [],
            "path": str(project_path)
        }
        
        # Determine project type
        if (project_path / "requirements.txt").exists() or (project_path / "setup.py").exists() or (project_path / "pyproject.toml").exists():
            project_analysis["project_type"] = "python"
        elif (project_path / "package.json").exists():
            project_analysis["project_type"] = "node"
        elif (project_path / "pom.xml").exists() or (project_path / "build.gradle").exists():
            project_analysis["project_type"] = "java"
        
        # Collect file information
        for root, _, filenames in os.walk(project_path):
            for filename in filenames:
                # Skip common directories to ignore
                if any(ignored in root for ignored in [".git", "__pycache__", "node_modules", "venv", ".idea", ".vscode"]):
                    continue
                
                file_path = Path(root) / filename
                rel_path = file_path.relative_to(project_path)
                
                # Get basic file info
                file_info = {
                    "path": str(rel_path),
                    "full_path": str(file_path),
                    "type": None,
                    "language": None,
                    "content": None
                }
                
                # Try to determine file type and language
                try:
                    from angela.context.file_detector import detect_file_type
                    type_info = detect_file_type(file_path)
                    file_info["type"] = type_info.get("type")
                    file_info["language"] = type_info.get("language")
                    
                    # Read content for source code files (limit to prevent memory issues)
                    if type_info.get("type") == "source_code" and file_path.stat().st_size < 100000:
                        with open(file_path, 'r', encoding='utf-8', errors='replace') as f:
                            file_info["content"] = f.read()
                except Exception as e:
                    self._logger.debug(f"Error analyzing file {file_path}: {str(e)}")
                
                project_analysis["files"].append(file_info)
        
        return project_analysis
    
    async def _detect_patterns(self, project_analysis: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        Detect architectural patterns in the project.
        
        Args:
            project_analysis: Analysis of the project
            
        Returns:
            List of pattern detection results
        """
        self._logger.info("Detecting architectural patterns")
        
        results = []
        
        # Run all pattern detectors
        for pattern in self._patterns:
            try:
                pattern_result = await pattern.detect(project_analysis)
                results.append(pattern_result)
                self._logger.debug(f"Pattern '{pattern.name}' detected: {pattern_result['present']}")
            except Exception as e:
                self._logger.error(f"Error detecting pattern '{pattern.name}': {str(e)}")
        
        return results
    
    async def _detect_anti_patterns(self, project_analysis: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        Detect architectural anti-patterns in the project.
        
        Args:
            project_analysis: Analysis of the project
            
        Returns:
            List of anti-pattern detection results
        """
        self._logger.info("Detecting architectural anti-patterns")
        
        results = []
        
        # Run all anti-pattern detectors
        for anti_pattern in self._anti_patterns:
            try:
                anti_pattern_result = await anti_pattern.detect(project_analysis)
                results.append(anti_pattern_result)
                self._logger.debug(f"Anti-pattern '{anti_pattern.name}' detected: {anti_pattern_result['detected']}")
            except Exception as e:
                self._logger.error(f"Error detecting anti-pattern '{anti_pattern.name}': {str(e)}")
        
        return results
    
    async def _generate_recommendations(
        self, 
        patterns_results: List[Dict[str, Any]],
        anti_patterns_results: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """
        Generate recommendations based on detected patterns and anti-patterns.
        
        Args:
            patterns_results: Pattern detection results
            anti_patterns_results: Anti-pattern detection results
            
        Returns:
            List of recommendations
        """
        self._logger.info("Generating architecture recommendations")
        
        recommendations = []
        
        # Generate recommendations from patterns
        for pattern_result in patterns_results:
            pattern_name = pattern_result["pattern"]
            pattern = next((p for p in self._patterns if p.name == pattern_name), None)
            
            if pattern:
                pattern_recommendations = pattern.get_recommendations(pattern_result)
                recommendations.extend(pattern_recommendations)
        
        # Generate recommendations from anti-patterns
        for anti_pattern_result in anti_patterns_results:
            anti_pattern_name = anti_pattern_result["anti_pattern"]
            anti_pattern = next((ap for ap in self._anti_patterns if ap.name == anti_pattern_name), None)
            
            if anti_pattern and anti_pattern_result["detected"]:
                anti_pattern_recommendations = anti_pattern.get_recommendations(anti_pattern_result)
                recommendations.extend(anti_pattern_recommendations)
        
        # Generate general recommendations using AI for more complex analysis
        if patterns_results or anti_patterns_results:
            ai_recommendations = await self._generate_ai_recommendations(patterns_results, anti_patterns_results)
            recommendations.extend(ai_recommendations)
        
        return recommendations
    
    async def _generate_ai_recommendations(
        self, 
        patterns_results: List[Dict[str, Any]],
        anti_patterns_results: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """
        Generate additional recommendations using AI.
        
        Args:
            patterns_results: Pattern detection results
            anti_patterns_results: Anti-pattern detection results
            
        Returns:
            List of AI-generated recommendations
        """
        self._logger.debug("Generating AI recommendations")
        
        # Build prompt for AI
        prompt = """
    You are an expert software architect tasked with providing architectural recommendations based on detected patterns and anti-patterns in a project.
    
    Here are the detected architectural patterns:
    """
        
        # Add pattern information
        for pattern_result in patterns_results:
            prompt += f"\n- Pattern: {pattern_result['pattern']}"
            prompt += f"\n  Present: {pattern_result['present']}"
            prompt += f"\n  Confidence: {pattern_result['confidence']:.2f}"
            
            if "components" in pattern_result:
                prompt += "\n  Components:"
                for component_type, components in pattern_result["components"].items():
                    prompt += f"\n    - {component_type}: {len(components)} files"
        
        prompt += "\n\nHere are the detected architectural anti-patterns:"
        
        # Add anti-pattern information
        for anti_pattern_result in anti_patterns_results:
            prompt += f"\n- Anti-pattern: {anti_pattern_result['anti_pattern']}"
            prompt += f"\n  Detected: {anti_pattern_result['detected']}"
            prompt += f"\n  Severity: {anti_pattern_result['severity']}"
            
            if anti_pattern_result["detected"] and "instances" in anti_pattern_result:
                prompt += f"\n  Instances: {len(anti_pattern_result['instances'])}"
                
                # Add details for the first few instances
                for i, instance in enumerate(anti_pattern_result["instances"][:3]):
                    prompt += f"\n    {i+1}. Class: {instance.get('class', 'Unknown')}"
                    if "violations" in instance:
                        violations = ", ".join(f"{v}: {val}" for v, val in instance["violations"].items())
                        prompt += f"\n       Violations: {violations}"
        
        prompt += """
    
    Based on the above information, provide 3-5 high-level architectural recommendations that would improve the project's design.
    For each recommendation, include:
    1. A title (concise description)
    2. A detailed explanation
    3. Concrete action steps
    4. Priority (high, medium, low)
    
    Format your response as a JSON array of recommendation objects, like this:
    [
      {
        "title": "Clear recommendation title",
        "description": "Detailed explanation of the issue",
        "action": "Specific action steps to implement the recommendation",
        "priority": "high|medium|low"
      },
      ...
    ]
    """
        
        # Call AI service
        gemini_client = get_gemini_client()
        api_request = GeminiRequest(
            prompt=prompt,
            max_tokens=2000,
            temperature=0.3
        )
        
        try:
            response = await gemini_client.generate_text(api_request)
            
            # Parse the response
            recommendations = []
            
            # Extract JSON
            json_match = re.search(r'```(?:json)?\s*(.*?)\s*```', response.text, re.DOTALL)
            if json_match:
                json_str = json_match.group(1)
            else:
                # Try to find JSON without code blocks
                json_match = re.search(r'(\[.*\])', response.text, re.DOTALL)
                if json_match:
                    json_str = json_match.group(1)
                else:
                    # Assume the entire response is JSON
                    json_str = response.text
            
            try:
                recommendations = json.loads(json_str)
            except json.JSONDecodeError:
                self._logger.error("Failed to parse AI recommendations as JSON")
                recommendations = []
            
            # Add source information
            for rec in recommendations:
                rec["source"] = "ai"
            
            return recommendations
            
        except Exception as e:
            self._logger.error(f"Error generating AI recommendations: {str(e)}")
            return []
            

async def analyze_project_architecture(
    project_path: Union[str, Path],
    context: Optional[Dict[str, Any]] = None
) -> Dict[str, Any]:
    """
    Analyze project architecture.
    
    Args:
        project_path: Path to the project
        context: Additional context information
        
    Returns:
        Dictionary with analysis results
    """
    analyzer = ArchitecturalAnalyzer()
    return await analyzer.analyze_architecture(project_path, context)

# Global architectural analyzer instance
architectural_analyzer = ArchitecturalAnalyzer()
</file>

<file path="components/generation/context_manager.py">
# angela/generation/context_manager.py
"""
Multi-file context management for code generation.

This module provides tools for managing context across multiple files during code generation,
ensuring coherence and proper dependency management.
"""
from __future__ import annotations
import asyncio
from typing import Dict, Any, List, Optional, Set, Union, Tuple
from pathlib import Path
import json
import re
from collections import defaultdict


from angela.utils.logging import get_logger
from angela.api.ai import get_gemini_client, get_gemini_request_class
from angela.api.context import get_file_detector


logger = get_logger(__name__)
GeminiRequest = get_gemini_request_class()


class GenerationContextManager:
    """
    Manages context across multiple files during code generation.
    
    This class tracks relationships, shared entities, and dependencies across
    files to ensure generated code remains coherent and consistent.
    """
    
    def __init__(self):
        """Initialize the generation context manager."""
        self._logger = logger
        self._shared_entities = {}  # Maps entity names to their types and definitions
        self._file_dependencies = defaultdict(set)  # Maps file paths to their dependencies
        self._import_statements = defaultdict(set)  # Maps file paths to their imports
        self._global_context = {}  # Shared context for the entire project
        self._modules = defaultdict(dict)  # Maps module names to their exported entities
        self._api_endpoints = []  # List of API endpoints for backend projects
        self._database_models = []  # List of database models
        self._ui_components = []  # List of UI components for frontend projects
        self._entity_references = defaultdict(list)  # Maps entity names to where they're referenced
        
    def reset(self):
        """Reset all context data."""
        self._shared_entities.clear()
        self._file_dependencies.clear()
        self._import_statements.clear()
        self._global_context.clear()
        self._modules.clear()
        self._api_endpoints.clear()
        self._database_models.clear()
        self._ui_components.clear()
        self._entity_references.clear()
        
    def register_entity(self, name: str, entity_type: str, definition: Any, file_path: str):
        """
        Register a shared entity that might be used across files.
        
        Args:
            name: Name of the entity
            entity_type: Type of entity (e.g., 'class', 'function', 'interface')
            definition: Definition or details of the entity
            file_path: Path to the file where the entity is defined
        """
        self._logger.debug(f"Registering entity: {name} ({entity_type}) in {file_path}")
        
        self._shared_entities[name] = {
            'type': entity_type,
            'definition': definition,
            'file_path': file_path
        }
        
        # Add to specific collections based on type
        if entity_type == 'api_endpoint':
            self._api_endpoints.append({
                'name': name,
                'path': definition.get('path', ''),
                'method': definition.get('method', ''),
                'handler': definition.get('handler', '')
            })
        elif entity_type == 'database_model':
            self._database_models.append({
                'name': name,
                'fields': definition.get('fields', {}),
                'relationships': definition.get('relationships', [])
            })
        elif entity_type == 'ui_component':
            self._ui_components.append({
                'name': name,
                'props': definition.get('props', {}),
                'description': definition.get('description', '')
            })
            
        # Add to module exports if applicable
        module_path = Path(file_path).parent.name
        if module_path:
            self._modules[module_path][name] = {
                'type': entity_type,
                'file_path': file_path
            }
            
    def register_dependency(self, file_path: str, dependency_path: str):
        """
        Register a dependency between files.
        
        Args:
            file_path: Path to the dependent file
            dependency_path: Path to the file being depended on
        """
        self._file_dependencies[file_path].add(dependency_path)
        
    def register_import(self, file_path: str, import_statement: str):
        """
        Register an import statement in a file.
        
        Args:
            file_path: Path to the file
            import_statement: The import statement
        """
        self._import_statements[file_path].add(import_statement)
        
    def register_entity_reference(self, entity_name: str, file_path: str, position: Optional[str] = None):
        """
        Register a reference to an entity in a file.
        
        Args:
            entity_name: Name of the entity
            file_path: Path to the file where the entity is referenced
            position: Optional position information
        """
        self._entity_references[entity_name].append({
            'file_path': file_path,
            'position': position
        })
        
    def get_entity(self, name: str) -> Optional[Dict[str, Any]]:
        """
        Get information about a registered entity.
        
        Args:
            name: Name of the entity
            
        Returns:
            Dictionary with entity information or None if not found
        """
        return self._shared_entities.get(name)
        
    def get_dependencies(self, file_path: str) -> Set[str]:
        """
        Get dependencies for a file.
        
        Args:
            file_path: Path to the file
            
        Returns:
            Set of dependency file paths
        """
        return self._file_dependencies.get(file_path, set())
        
    def get_imports(self, file_path: str) -> Set[str]:
        """
        Get import statements for a file.
        
        Args:
            file_path: Path to the file
            
        Returns:
            Set of import statements
        """
        return self._import_statements.get(file_path, set())
        
    def set_global_context(self, key: str, value: Any):
        """
        Set a global context value.
        
        Args:
            key: Context key
            value: Context value
        """
        self._global_context[key] = value
        
    def get_global_context(self, key: str, default: Any = None) -> Any:
        """
        Get a global context value.
        
        Args:
            key: Context key
            default: Default value if key not found
            
        Returns:
            Context value or default
        """
        return self._global_context.get(key, default)
        
    def get_all_global_context(self) -> Dict[str, Any]:
        """
        Get all global context.
        
        Returns:
            Dictionary with all global context
        """
        return self._global_context.copy()
        
    def get_modules(self) -> Dict[str, Dict[str, Any]]:
        """
        Get all modules and their exports.
        
        Returns:
            Dictionary mapping module names to exports
        """
        return self._modules.copy()
        
    def get_api_endpoints(self) -> List[Dict[str, Any]]:
        """
        Get all API endpoints.
        
        Returns:
            List of API endpoint dictionaries
        """
        return self._api_endpoints.copy()
        
    def get_database_models(self) -> List[Dict[str, Any]]:
        """
        Get all database models.
        
        Returns:
            List of database model dictionaries
        """
        return self._database_models.copy()
        
    def get_ui_components(self) -> List[Dict[str, Any]]:
        """
        Get all UI components.
        
        Returns:
            List of UI component dictionaries
        """
        return self._ui_components.copy()
        
    def get_references(self, entity_name: str) -> List[Dict[str, Any]]:
        """
        Get references to an entity.
        
        Args:
            entity_name: Name of the entity
            
        Returns:
            List of references
        """
        return self._entity_references.get(entity_name, [])
        
    async def extract_entities_from_file(self, file: "CodeFile") -> List[Dict[str, Any]]:
        """
        Extract entities from a file.
        
        Args:
            file: CodeFile to extract entities from
            
        Returns:
            List of extracted entities
        """
        self._logger.debug(f"Extracting entities from {file.path}")
        
        # Determine file type
        file_detector = get_file_detector()
        file_type = file_detector.detect_file_type(Path(file.path))
        language = file_type.get("language", "unknown")
        
        # Extract entities based on language
        entities = []
        
        if language == "python":
            entities = self._extract_python_entities(file.content, file.path)
        elif language in ["javascript", "typescript"]:
            entities = self._extract_js_entities(file.content, file.path, language == "typescript")
        elif language in ["java"]:
            entities = self._extract_java_entities(file.content, file.path)
        
        # Register extracted entities
        for entity in entities:
            self.register_entity(
                entity["name"],
                entity["type"],
                entity["definition"],
                file.path
            )
            
        return entities
        
    def _extract_python_entities(self, content: str, file_path: str) -> List[Dict[str, Any]]:
        """
        Extract entities from Python code.
        
        Args:
            content: File content
            file_path: Path to the file
            
        Returns:
            List of extracted entities
        """
        entities = []
        
        # Extract classes
        class_pattern = r'class\s+(\w+)(?:\(.*?\))?:'
        for match in re.finditer(class_pattern, content):
            class_name = match.group(1)
            
            # Check if it might be a database model
            if "model" in file_path.lower() or "models" in file_path.lower() or "db" in file_path.lower():
                if "Base" in content or "Model" in content or "Column" in content:
                    # Extract fields
                    fields = {}
                    field_pattern = r'(\w+)\s*=\s*(?:Column|db\.Column)\(([^)]*)\)'
                    for field_match in re.finditer(field_pattern, content):
                        field_name = field_match.group(1)
                        field_type = field_match.group(2)
                        fields[field_name] = field_type
                    
                    entities.append({
                        "name": class_name,
                        "type": "database_model",
                        "definition": {
                            "fields": fields,
                            "relationships": []  # Would need more complex parsing for relationships
                        }
                    })
                else:
                    entities.append({
                        "name": class_name,
                        "type": "class",
                        "definition": {
                            "methods": []  # Could extract methods if needed
                        }
                    })
            else:
                entities.append({
                    "name": class_name,
                    "type": "class",
                    "definition": {
                        "methods": []  # Could extract methods if needed
                    }
                })
        
        # Extract functions
        function_pattern = r'def\s+(\w+)\s*\('
        for match in re.finditer(function_pattern, content):
            func_name = match.group(1)
            # Skip if it's likely a method (indented)
            # This is a simple check and might miss some cases
            line_start = content[:match.start()].rfind('\n') + 1
            if match.start() - line_start > 0:
                continue
                
            # Check if it might be an API endpoint
            if (
                "route" in file_path.lower() or 
                "api" in file_path.lower() or 
                "app." in content or 
                "@app." in content or
                "blueprint." in content or
                "@blueprint." in content
            ):
                # Look for route decorators
                route_pattern = r'@(?:app|blueprint|router)\.(?:route|get|post|put|delete|patch)\s*\([\'"]([^\'"]*)[\'"]'
                route_match = re.search(route_pattern, content[:match.start()].split('\n')[-5:])
                if route_match:
                    path = route_match.group(1)
                    method = "GET"  # Default
                    if "post" in content[:match.start()].lower():
                        method = "POST"
                    elif "put" in content[:match.start()].lower():
                        method = "PUT"
                    elif "delete" in content[:match.start()].lower():
                        method = "DELETE"
                    elif "patch" in content[:match.start()].lower():
                        method = "PATCH"
                        
                    entities.append({
                        "name": func_name,
                        "type": "api_endpoint",
                        "definition": {
                            "path": path,
                            "method": method,
                            "handler": func_name
                        }
                    })
                else:
                    entities.append({
                        "name": func_name,
                        "type": "function",
                        "definition": {}
                    })
            else:
                entities.append({
                    "name": func_name,
                    "type": "function",
                    "definition": {}
                })
        
        return entities
        
    def _extract_js_entities(self, content: str, file_path: str, is_typescript: bool) -> List[Dict[str, Any]]:
        """
        Extract entities from JavaScript/TypeScript code.
        
        Args:
            content: File content
            file_path: Path to the file
            is_typescript: Whether the file is TypeScript
            
        Returns:
            List of extracted entities
        """
        entities = []
        
        # Extract classes
        class_pattern = r'class\s+(\w+)(?:\s+extends\s+\w+)?(?:\s+implements\s+[\w,\s]+)?\s*{'
        for match in re.finditer(class_pattern, content):
            class_name = match.group(1)
            entities.append({
                "name": class_name,
                "type": "class",
                "definition": {}
            })
        
        # Extract functions/methods
        function_pattern = r'(?:function|const|let|var)\s+(\w+)\s*=?\s*(?:async\s*)?\(?[^)]*\)?\s*=>\s*{|\s*=?\s*function\s*\('
        for match in re.finditer(function_pattern, content):
            func_name = match.group(1)
            entities.append({
                "name": func_name,
                "type": "function",
                "definition": {}
            })
        
        # Extract React components (functional)
        if "react" in content.lower() or "jsx" in file_path.lower() or "tsx" in file_path.lower():
            react_component_pattern = r'(?:function|const|let|var)\s+(\w+)(?:\s*:\s*React\.FC(?:<[^>]*>)?)?\s*=?\s*(?:\([^)]*\))?\s*=>\s*{'
            for match in re.finditer(react_component_pattern, content):
                component_name = match.group(1)
                
                # Extract props
                props = {}
                props_pattern = r'type Props = {([^}]*)}'
                props_match = re.search(props_pattern, content)
                if props_match:
                    props_content = props_match.group(1)
                    prop_lines = props_content.split('\n')
                    for line in prop_lines:
                        prop_match = re.search(r'(\w+)(?:\?)?:\s*([^;]*)', line)
                        if prop_match:
                            props[prop_match.group(1)] = prop_match.group(2).strip()
                
                entities.append({
                    "name": component_name,
                    "type": "ui_component",
                    "definition": {
                        "props": props,
                        "description": f"React component defined in {file_path}"
                    }
                })
        
        # Extract TypeScript interfaces
        if is_typescript:
            interface_pattern = r'interface\s+(\w+)(?:\s+extends\s+\w+)?\s*{'
            for match in re.finditer(interface_pattern, content):
                interface_name = match.group(1)
                entities.append({
                    "name": interface_name,
                    "type": "interface",
                    "definition": {}
                })
                
            # Extract TypeScript types
            type_pattern = r'type\s+(\w+)\s*=\s*'
            for match in re.finditer(type_pattern, content):
                type_name = match.group(1)
                entities.append({
                    "name": type_name,
                    "type": "type",
                    "definition": {}
                })
        
        # Extract API routes (Express/Next.js/etc.)
        if "route" in file_path.lower() or "api" in file_path.lower():
            api_patterns = [
                r'(?:router|app)\.(?:get|post|put|delete|patch)\s*\([\'"]([^\'"]*)[\'"]',  # Express
                r'export\s+(?:async\s+)?function\s+(?:GET|POST|PUT|DELETE|PATCH)\s*\(',  # Next.js API routes
                r'handler\s*\([^)]*\)\s*{'  # Generic API handler
            ]
            
            for pattern in api_patterns:
                for match in re.finditer(pattern, content):
                    name = Path(file_path).stem
                    path = match.group(1) if 'router' in pattern else f"/api/{name}"
                    method = "GET"
                    
                    if "post" in content.lower():
                        method = "POST"
                    elif "put" in content.lower():
                        method = "PUT"
                    elif "delete" in content.lower():
                        method = "DELETE"
                    elif "patch" in content.lower():
                        method = "PATCH"
                    
                    entities.append({
                        "name": name,
                        "type": "api_endpoint",
                        "definition": {
                            "path": path,
                            "method": method,
                            "handler": name
                        }
                    })
        
        return entities
        
    def _extract_java_entities(self, content: str, file_path: str) -> List[Dict[str, Any]]:
        """
        Extract entities from Java code.
        
        Args:
            content: File content
            file_path: Path to the file
            
        Returns:
            List of extracted entities
        """
        entities = []
        
        # Extract classes
        class_pattern = r'(?:public|private|protected)?\s*class\s+(\w+)(?:\s+extends\s+\w+)?(?:\s+implements\s+[\w,\s]+)?\s*{'
        for match in re.finditer(class_pattern, content):
            class_name = match.group(1)
            
            # Check if it might be a database entity/model
            if (
                "@Entity" in content or 
                "@Table" in content or
                "javax.persistence" in content or
                "jakarta.persistence" in content
            ):
                fields = {}
                field_pattern = r'(?:@Column[^)]*\))?\s*(?:private|public|protected)\s+(\w+(?:<[^>]*>)?)\s+(\w+)\s*;'
                for field_match in re.finditer(field_pattern, content):
                    field_type = field_match.group(1)
                    field_name = field_match.group(2)
                    fields[field_name] = field_type
                
                entities.append({
                    "name": class_name,
                    "type": "database_model",
                    "definition": {
                        "fields": fields,
                        "relationships": []
                    }
                })
            else:
                entities.append({
                    "name": class_name,
                    "type": "class",
                    "definition": {}
                })
        
        # Extract methods
        method_pattern = r'(?:public|private|protected)?\s*(?:static\s+)?(?:\w+(?:<[^>]*>)?)\s+(\w+)\s*\([^)]*\)\s*(?:throws\s+[\w,\s]+)?\s*{'
        for match in re.finditer(method_pattern, content):
            method_name = match.group(1)
            entities.append({
                "name": method_name,
                "type": "method",
                "definition": {}
            })
        
        # Extract REST endpoints
        if (
            "@RestController" in content or 
            "@Controller" in content or
            "@RequestMapping" in content or
            "@GetMapping" in content or
            "@PostMapping" in content
        ):
            # Get class-level path
            class_path = ""
            request_mapping_pattern = r'@RequestMapping\s*\((?:[^")]*)?"([^"]*)"'
            class_mapping_match = re.search(request_mapping_pattern, content)
            if class_mapping_match:
                class_path = class_mapping_match.group(1)
            
            # Find endpoint methods
            endpoint_patterns = [
                r'@GetMapping\s*\((?:[^")]*)?"([^"]*)"',
                r'@PostMapping\s*\((?:[^")]*)?"([^"]*)"',
                r'@PutMapping\s*\((?:[^")]*)?"([^"]*)"',
                r'@DeleteMapping\s*\((?:[^")]*)?"([^"]*)"',
                r'@PatchMapping\s*\((?:[^")]*)?"([^"]*)"'
            ]
            
            endpoint_methods = {
                "GetMapping": "GET",
                "PostMapping": "POST",
                "PutMapping": "PUT",
                "DeleteMapping": "DELETE",
                "PatchMapping": "PATCH"
            }
            
            for pattern in endpoint_patterns:
                for match in re.finditer(pattern, content):
                    endpoint_path = match.group(1)
                    full_path = class_path + endpoint_path
                    
                    # Find the method for this endpoint
                    pattern_type = pattern.split('@')[1].split('Mapping')[0]
                    method_type = endpoint_methods.get(pattern_type + "Mapping", "GET")
                    
                    # Find the method name (assuming it follows the annotation)
                    method_lines = content[match.end():].split('\n', 10)
                    for line in method_lines:
                        method_match = re.search(method_pattern, line)
                        if method_match:
                            method_name = method_match.group(1)
                            entities.append({
                                "name": method_name,
                                "type": "api_endpoint",
                                "definition": {
                                    "path": full_path,
                                    "method": method_type,
                                    "handler": method_name
                                }
                            })
                            break
        
        return entities
        
    async def analyze_code_relationships(self, files: List["CodeFile"]) -> Dict[str, Any]:
        """
        Analyze relationships between code files.
        
        Args:
            files: List of code files
            
        Returns:
            Dictionary with analysis results
        """
        self._logger.info(f"Analyzing code relationships among {len(files)} files")
        
        # Reset the context
        self.reset()
        
        # Extract entities from all files
        for file in files:
            await self.extract_entities_from_file(file)
            
        # Build dependency graph
        for file in files:
            # Extract imports and register dependencies
            self._extract_and_register_imports(file)
            
            # Check for references to other entities
            for entity_name in self._shared_entities:
                if entity_name in file.content:
                    self.register_entity_reference(entity_name, file.path)
        
        # Analyze architecture patterns
        architecture_patterns = await self._analyze_architecture_patterns(files)
        
        # Return the analysis
        return {
            "entity_count": len(self._shared_entities),
            "dependency_count": sum(len(deps) for deps in self._file_dependencies.values()),
            "api_endpoints": len(self._api_endpoints),
            "database_models": len(self._database_models),
            "ui_components": len(self._ui_components),
            "architecture_patterns": architecture_patterns
        }
    
    def _extract_and_register_imports(self, file: CodeFile):
        """
        Extract imports from a file and register dependencies.
        
        Args:
            file: CodeFile to extract imports from
        """
        # Detect file type
        file_type = detect_file_type(Path(file.path))
        language = file_type.get("language", "unknown")
        
        if language == "python":
            # Extract Python imports
            import_patterns = [
                r'from\s+([\w.]+)\s+import\s+(?:[\w,\s]+)',
                r'import\s+([\w.]+)'
            ]
            
            for pattern in import_patterns:
                for match in re.finditer(pattern, file.content):
                    module = match.group(1)
                    
                    # Skip standard library imports
                    if module in ["os", "sys", "json", "datetime", "typing", "re", "pathlib"]:
                        continue
                    
                    # Register import statement
                    self.register_import(file.path, match.group(0))
                    
                    # Check if this is a local module
                    for other_file in self._file_dependencies:
                        module_path = module.replace('.', '/')
                        if module_path in other_file or module.split('.')[-1] in other_file:
                            self.register_dependency(file.path, other_file)
        
        elif language in ["javascript", "typescript"]:
            # Extract JS/TS imports
            import_patterns = [
                r'import\s+(?:{[^}]*}|\*\s+as\s+\w+|\w+)\s+from\s+[\'"]([^\'"]*)[\'"]',
                r'require\([\'"]([^\'"]*)[\'"]'
            ]
            
            for pattern in import_patterns:
                for match in re.finditer(pattern, file.content):
                    module = match.group(1)
                    
                    # Skip node modules
                    if not module.startswith('.') and not module.startswith('/'):
                        continue
                    
                    # Register import statement
                    self.register_import(file.path, match.group(0))
                    
                    # Resolve relative path
                    if module.startswith('.'):
                        # Remove file extension if present
                        if module.endswith('.js') or module.endswith('.ts') or module.endswith('.jsx') or module.endswith('.tsx'):
                            module = module[:-3]
                        
                        # Get parent directory of current file
                        parent = Path(file.path).parent
                        
                        # Resolve relative path
                        if module.startswith('./'):
                            module_path = parent / module[2:]
                        elif module.startswith('../'):
                            module_path = parent.parent / module[3:]
                        else:
                            module_path = parent / module
                        
                        # Check if this path matches any file
                        for other_file in self._file_dependencies:
                            if (
                                str(module_path) in other_file or 
                                module.split('/')[-1] in other_file
                            ):
                                self.register_dependency(file.path, other_file)
        
        elif language == "java":
            # Extract Java imports
            import_pattern = r'import\s+([\w.]+);'
            
            for match in re.finditer(import_pattern, file.content):
                full_class = match.group(1)
                
                # Skip standard library imports
                if full_class.startswith("java.") or full_class.startswith("javax."):
                    continue
                
                # Register import statement
                self.register_import(file.path, match.group(0))
                
                # Check if this is a local class
                class_name = full_class.split('.')[-1]
                package_path = '.'.join(full_class.split('.')[:-1])
                
                # Convert package to directory structure
                package_dir = package_path.replace('.', '/')
                
                # Check if this matches any file
                for other_file in self._file_dependencies:
                    if (
                        package_dir in other_file and class_name in other_file or
                        class_name in other_file
                    ):
                        self.register_dependency(file.path, other_file)
    
    async def _analyze_architecture_patterns(self, files: List[CodeFile]) -> List[str]:
        """
        Analyze architecture patterns in the files.
        
        Args:
            files: List of code files
            
        Returns:
            List of detected architecture pattern names
        """
        patterns = []
        
        # Check for MVC pattern
        if self._check_mvc_pattern(files):
            patterns.append("MVC (Model-View-Controller)")
        
        # Check for Clean Architecture
        if self._check_clean_architecture(files):
            patterns.append("Clean Architecture")
        
        # Check for Microservices
        if self._check_microservices(files):
            patterns.append("Microservices")
        
        # Check for Repository Pattern
        if self._check_repository_pattern(files):
            patterns.append("Repository Pattern")
        
        # Check for Service Layer
        if self._check_service_layer(files):
            patterns.append("Service Layer")
        
        # Check for CQRS
        if self._check_cqrs(files):
            patterns.append("CQRS (Command Query Responsibility Segregation)")
        
        return patterns
    
    def _check_mvc_pattern(self, files: List[CodeFile]) -> bool:
        """Check if the MVC pattern is present."""
        # Count files in different categories
        models_count = sum(1 for f in files if "model" in f.path.lower() or "models" in f.path.lower())
        views_count = sum(1 for f in files if "view" in f.path.lower() or "views" in f.path.lower())
        controllers_count = sum(1 for f in files if "controller" in f.path.lower() or "controllers" in f.path.lower())
        
        # Check if all three components exist
        return models_count > 0 and views_count > 0 and controllers_count > 0
    
    def _check_clean_architecture(self, files: List[CodeFile]) -> bool:
        """Check if Clean Architecture is present."""
        # Look for key components of Clean Architecture
        has_entities = any("entity" in f.path.lower() or "entities" in f.path.lower() for f in files)
        has_use_cases = any("usecase" in f.path.lower() or "usecases" in f.path.lower() for f in files)
        has_interfaces = any("interface" in f.path.lower() or "interfaces" in f.path.lower() for f in files)
        has_infrastructure = any("infra" in f.path.lower() or "infrastructure" in f.path.lower() for f in files)
        
        return has_entities and has_use_cases and (has_interfaces or has_infrastructure)
    
    def _check_microservices(self, files: List[CodeFile]) -> bool:
        """Check if Microservices architecture is present."""
        # Look for multiple service directories
        service_dirs = set()
        for file in files:
            path_parts = Path(file.path).parts
            if "service" in path_parts or "services" in path_parts:
                service_index = -1
                for i, part in enumerate(path_parts):
                    if "service" in part.lower():
                        service_index = i
                        break
                
                if service_index >= 0 and service_index + 1 < len(path_parts):
                    service_dirs.add(path_parts[service_index + 1])
        
        # If we have multiple service directories, it's likely microservices
        return len(service_dirs) >= 2
    
    def _check_repository_pattern(self, files: List[CodeFile]) -> bool:
        """Check if Repository Pattern is present."""
        # Look for repository classes
        has_repositories = any("repository" in f.path.lower() or "repositories" in f.path.lower() for f in files)
        has_entities = len(self._database_models) > 0
        
        return has_repositories and has_entities
    
    def _check_service_layer(self, files: List[CodeFile]) -> bool:
        """Check if Service Layer pattern is present."""
        # Look for service classes
        has_services = any("service" in f.path.lower() or "services" in f.path.lower() for f in files)
        
        # Check if service classes actually exist in content
        if has_services:
            service_files = [f for f in files if "service" in f.path.lower() or "services" in f.path.lower()]
            for file in service_files:
                if "class" in file.content.lower() and "service" in file.content.lower():
                    return True
        
        return False
    
    def _check_cqrs(self, files: List[CodeFile]) -> bool:
        """Check if CQRS pattern is present."""
        # Look for command and query separation
        has_commands = any("command" in f.path.lower() or "commands" in f.path.lower() for f in files)
        has_queries = any("query" in f.path.lower() or "queries" in f.path.lower() for f in files)
        
        return has_commands and has_queries
    
    async def enhance_prompt_with_context(
        self, 
        prompt: str, 
        file_path: str, 
        related_files: Optional[List[str]] = None,
        max_tokens: int = 4000
    ) -> str:
        """
        Enhance a prompt with relevant context for better code generation.
        
        Args:
            prompt: Original prompt
            file_path: Path of the file being generated
            related_files: Optional list of related file paths
            max_tokens: Maximum context tokens to include
            
        Returns:
            Enhanced prompt with context
        """
        self._logger.debug(f"Enhancing prompt for {file_path}")
        
        # Start with the original prompt
        enhanced_prompt = prompt
        
        # Add global context
        enhanced_prompt += "\n\nGlobal context for this project:\n"
        for key, value in self._global_context.items():
            if isinstance(value, (str, int, float, bool)):
                enhanced_prompt += f"- {key}: {value}\n"
        
        # Add information about APIs if relevant
        if "api" in file_path.lower() or "controller" in file_path.lower() or "routes" in file_path.lower():
            if self._api_endpoints:
                enhanced_prompt += "\nAPI Endpoints already defined in the project:\n"
                for endpoint in self._api_endpoints:
                    enhanced_prompt += f"- {endpoint['method']} {endpoint['path']} (handler: {endpoint['name']})\n"
        
        # Add information about database models if relevant
        if "model" in file_path.lower() or "entity" in file_path.lower() or "repository" in file_path.lower():
            if self._database_models:
                enhanced_prompt += "\nDatabase Models already defined in the project:\n"
                for model in self._database_models:
                    enhanced_prompt += f"- {model['name']} with fields: {', '.join(model['fields'].keys())}\n"
        
        # Add information about UI components if relevant
        if "component" in file_path.lower() or "view" in file_path.lower() or any(ext in file_path.lower() for ext in [".jsx", ".tsx", ".vue"]):
            if self._ui_components:
                enhanced_prompt += "\nUI Components already defined in the project:\n"
                for component in self._ui_components:
                    enhanced_prompt += f"- {component['name']} with props: {', '.join(component['props'].keys())}\n"
        
        # Add dependencies if this file has any
        dependencies = self.get_dependencies(file_path)
        if dependencies:
            enhanced_prompt += "\nThis file depends on:\n"
            for dep in dependencies:
                enhanced_prompt += f"- {dep}\n"
        
        # Add entities that are referenced by this file
        referenced_entities = [name for name, refs in self._entity_references.items() if any(ref["file_path"] == file_path for ref in refs)]
        if referenced_entities:
            enhanced_prompt += "\nThis file references these entities:\n"
            for entity_name in referenced_entities:
                entity = self.get_entity(entity_name)
                if entity:
                    enhanced_prompt += f"- {entity_name} ({entity['type']})\n"
        
        # Add content from related files if provided
        if related_files:
            # Sort related files by relationship importance
            sorted_related = self._sort_related_files_by_importance(file_path, related_files)
            
            # Add content with a token limit
            added_tokens = len(enhanced_prompt.split())
            max_tokens_for_related = max_tokens - added_tokens
            
            added_files = []
            for related_file in sorted_related:
                # Find the file content
                file_content = None
                for f in [file for file in self._file_dependencies if file == related_file]:
                    file_content = f
                    break
                
                if not file_content:
                    continue
                
                # Estimate tokens in this content
                tokens_in_file = len(file_content.split())
                
                # If we would exceed the limit, skip this file
                if tokens_in_file + added_tokens > max_tokens_for_related:
                    continue
                
                # Add the file content
                enhanced_prompt += f"\n\nContent of related file {related_file}:\n```\n{file_content}\n```\n"
                added_tokens += tokens_in_file
                added_files.append(related_file)
                
                # If we're approaching the limit, stop adding files
                if added_tokens > max_tokens_for_related * 0.9:
                    break
        
        return enhanced_prompt
    
    def _sort_related_files_by_importance(self, file_path: str, related_files: List[str]) -> List[str]:
        """
        Sort related files by their importance to the current file.
        
        Args:
            file_path: Current file path
            related_files: List of related file paths
            
        Returns:
            Sorted list of related file paths
        """
        # Calculate importance scores
        file_scores = {}
        for related in related_files:
            score = 0
            
            # Direct dependency gets high score
            if related in self.get_dependencies(file_path):
                score += 10
                
            # File that depends on this file gets high score
            if file_path in self.get_dependencies(related):
                score += 8
                
            # Files in the same directory are important
            if Path(related).parent == Path(file_path).parent:
                score += 5
                
            # Files with shared entity references
            file_entities = set()
            for entity, refs in self._entity_references.items():
                if any(ref["file_path"] == file_path for ref in refs):
                    file_entities.add(entity)
            
            related_entities = set()
            for entity, refs in self._entity_references.items():
                if any(ref["file_path"] == related for ref in refs):
                    related_entities.add(entity)
            
            shared_entities = file_entities.intersection(related_entities)
            score += len(shared_entities) * 2
            
            file_scores[related] = score
        
        # Sort by score, descending
        return sorted(related_files, key=lambda f: file_scores.get(f, 0), reverse=True)

# Global instance
generation_context_manager = GenerationContextManager()
</file>

<file path="components/generation/documentation.py">
# angela/generation/documentation.py
"""
Documentation generation for Angela CLI.

This module provides capabilities for generating documentation for projects,
including READMEs, API docs, and user guides.
"""
import os
import asyncio
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple, Union
import json
import re

from angela.api.ai import get_gemini_client, get_gemini_request_class
from angela.utils.logging import get_logger
from angela.api.context import get_context_manager, get_file_detector

logger = get_logger(__name__)
GeminiRequest = get_gemini_request_class()

class DocumentationGenerator:
    """
    Generator for project documentation.
    """
    
    def __init__(self):
        """Initialize the documentation generator."""
        self._logger = logger
    
    async def generate_readme(
        self, 
        project_path: Union[str, Path],
        project_info: Optional[Dict[str, Any]] = None,
        context: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Generate a README file for a project.
        
        Args:
            project_path: Path to the project
            project_info: Optional project information
            context: Additional context information
            
        Returns:
            Dictionary with the generated README
        """
        self._logger.info(f"Generating README for project at {project_path}")
        
        # Analyze project if project_info not provided
        if not project_info:
            project_info = await self._analyze_project(project_path, context)
        
        # Build prompt for the AI
        prompt = self._build_readme_prompt(project_info)
        
        # Call AI service
        api_request = GeminiRequest(
            prompt=prompt,
            max_tokens=4000,
            temperature=0.3
        )
        
        self._logger.debug("Sending README generation request to AI service")
        response = await gemini_client.generate_text(api_request)
        
        # Extract README content from the response
        readme_content = self._extract_markdown_content(response.text)
        
        return {
            "content": readme_content,
            "file_name": "README.md",
            "project_path": str(project_path)
        }
    
    async def generate_api_docs(
        self, 
        project_path: Union[str, Path],
        files: Optional[List[Dict[str, Any]]] = None,
        context: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Generate API documentation for a project.
        
        Args:
            project_path: Path to the project
            files: Optional list of files to document
            context: Additional context information
            
        Returns:
            Dictionary with the generated API docs
        """
        self._logger.info(f"Generating API docs for project at {project_path}")
        
        # Convert to Path object
        project_path = Path(project_path)
        
        # Get files if not provided
        if not files:
            project_info = await self._analyze_project(project_path, context)
            files = project_info.get("files", [])
        
        # Filter for source code files
        source_files = [f for f in files if f.get("type") == "source_code"]
        
        # Determine project type
        project_type = "unknown"
        if context and "enhanced_project" in context:
            project_type = context["enhanced_project"].get("type", "unknown")
        elif any(f.get("path", "").endswith(".py") for f in source_files):
            project_type = "python"
        elif any(f.get("path", "").endswith((".js", ".jsx", ".ts", ".tsx")) for f in source_files):
            project_type = "node"
        elif any(f.get("path", "").endswith(".java") for f in source_files):
            project_type = "java"
        
        # Generate docs based on project type
        if project_type == "python":
            return await self._generate_python_api_docs(project_path, source_files)
        elif project_type == "node":
            return await self._generate_js_api_docs(project_path, source_files)
        elif project_type == "java":
            return await self._generate_java_api_docs(project_path, source_files)
        else:
            return await self._generate_generic_api_docs(project_path, source_files)
    
    async def generate_user_guide(
        self, 
        project_path: Union[str, Path],
        project_info: Optional[Dict[str, Any]] = None,
        context: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Generate a user guide for a project.
        
        Args:
            project_path: Path to the project
            project_info: Optional project information
            context: Additional context information
            
        Returns:
            Dictionary with the generated user guide
        """
        self._logger.info(f"Generating user guide for project at {project_path}")
        
        # Analyze project if project_info not provided
        if not project_info:
            project_info = await self._analyze_project(project_path, context)
        
        # Build prompt for the AI
        prompt = self._build_user_guide_prompt(project_info)
        
        # Call AI service
        api_request = GeminiRequest(
            prompt=prompt,
            max_tokens=6000,
            temperature=0.3
        )
        
        self._logger.debug("Sending user guide generation request to AI service")
        response = await gemini_client.generate_text(api_request)
        
        # Extract user guide content from the response
        guide_content = self._extract_markdown_content(response.text)
        
        return {
            "content": guide_content,
            "file_name": "USER_GUIDE.md",
            "project_path": str(project_path)
        }
    
    async def generate_contributing_guide(
        self, 
        project_path: Union[str, Path],
        project_info: Optional[Dict[str, Any]] = None,
        context: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Generate a CONTRIBUTING guide for a project.
        
        Args:
            project_path: Path to the project
            project_info: Optional project information
            context: Additional context information
            
        Returns:
            Dictionary with the generated contributing guide
        """
        self._logger.info(f"Generating contributing guide for project at {project_path}")
        
        # Analyze project if project_info not provided
        if not project_info:
            project_info = await self._analyze_project(project_path, context)
        
        # Build prompt for the AI
        prompt = self._build_contributing_prompt(project_info)
        
        # Call AI service
        api_request = GeminiRequest(
            prompt=prompt,
            max_tokens=4000,
            temperature=0.3
        )
        
        self._logger.debug("Sending contributing guide generation request to AI service")
        response = await gemini_client.generate_text(api_request)
        
        # Extract contributing guide content from the response
        guide_content = self._extract_markdown_content(response.text)
        
        return {
            "content": guide_content,
            "file_name": "CONTRIBUTING.md",
            "project_path": str(project_path)
        }
    
    async def _analyze_project(
        self, 
        project_path: Union[str, Path],
        context: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Analyze a project to gather information for documentation.
        
        Args:
            project_path: Path to the project
            context: Additional context information
            
        Returns:
            Dictionary with project information
        """
        self._logger.info(f"Analyzing project at {project_path}")
        
        # Convert to Path object
        project_path = Path(project_path)
        
        # Use context if available
        if context and "enhanced_project" in context:
            return {
                "project_type": context["enhanced_project"].get("type", "unknown"),
                "frameworks": context["enhanced_project"].get("frameworks", {}),
                "dependencies": context["enhanced_project"].get("dependencies", {}),
                "files": context.get("files", []),
                "path": str(project_path),
                "name": project_path.name
            }
        
        # Perform a simplified project analysis
        project_info = {
            "project_type": "unknown",
            "frameworks": {},
            "dependencies": {},
            "files": [],
            "path": str(project_path),
            "name": project_path.name
        }
        
        # Determine project type
        if (project_path / "requirements.txt").exists() or (project_path / "setup.py").exists() or (project_path / "pyproject.toml").exists():
            project_info["project_type"] = "python"
        elif (project_path / "package.json").exists():
            project_info["project_type"] = "node"
        elif (project_path / "pom.xml").exists() or (project_path / "build.gradle").exists():
            project_info["project_type"] = "java"
        
        # Get dependencies
        if project_info["project_type"] == "python":
            if (project_path / "requirements.txt").exists():
                try:
                    with open(project_path / "requirements.txt", 'r') as f:
                        deps = [line.strip() for line in f if line.strip() and not line.strip().startswith('#')]
                        project_info["dependencies"] = {"runtime": deps}
                except Exception as e:
                    self._logger.error(f"Error reading requirements.txt: {str(e)}")
        elif project_info["project_type"] == "node":
            if (project_path / "package.json").exists():
                try:
                    with open(project_path / "package.json", 'r') as f:
                        package_data = json.load(f)
                        project_info["dependencies"] = {
                            "runtime": list(package_data.get("dependencies", {}).keys()),
                            "development": list(package_data.get("devDependencies", {}).keys())
                        }
                        # Get project name
                        if "name" in package_data:
                            project_info["name"] = package_data["name"]
                except Exception as e:
                    self._logger.error(f"Error reading package.json: {str(e)}")
        
        # Collect file information
        for root, _, filenames in os.walk(project_path):
            for filename in filenames:
                # Skip common directories to ignore
                if any(ignored in root for ignored in [".git", "__pycache__", "node_modules", "venv", ".idea", ".vscode"]):
                    continue
                
                file_path = Path(root) / filename
                rel_path = file_path.relative_to(project_path)
                
                # Skip files over 1MB
                if file_path.stat().st_size > 1000000:
                    continue
                
                # Get basic file info
                file_info = {
                    "path": str(rel_path),
                    "full_path": str(file_path),
                    "type": None,
                    "language": None,
                    "content": None
                }
                
                # Try to determine file type and language
                try:
                    file_detector = get_file_detector()
                    type_info = file_detector.detect_file_type(file_path)
                    file_info["type"] = type_info.get("type")
                    file_info["language"] = type_info.get("language")
                    
                    # Read content for source code files and documentation files
                    if file_info["type"] in ["source_code", "document"] and file_path.stat().st_size < 100000:
                        with open(file_path, 'r', encoding='utf-8', errors='replace') as f:
                            file_info["content"] = f.read()
                except Exception as e:
                    self._logger.debug(f"Error analyzing file {file_path}: {str(e)}")
                
                project_info["files"].append(file_info)
        
        # Try to find entry points
        project_info["entry_points"] = self._find_entry_points(project_info)
        
        return project_info
    
    def _find_entry_points(self, project_info: Dict[str, Any]) -> List[str]:
        """
        Find potential entry points for the project.
        
        Args:
            project_info: Project information
            
        Returns:
            List of potential entry point files
        """
        entry_points = []
        
        if project_info["project_type"] == "python":
            # Look for Python entry points
            main_files = [f for f in project_info["files"] if f.get("path") in [
                "main.py", "__main__.py", "app.py", "server.py", "run.py"
            ]]
            
            # If no common entry point, look for files with main function
            if not main_files:
                for file_info in project_info["files"]:
                    if file_info.get("content") and "if __name__ == '__main__'" in file_info.get("content"):
                        main_files.append(file_info)
            
            entry_points.extend([f.get("path") for f in main_files])
            
        elif project_info["project_type"] == "node":
            # Look for Node.js entry points
            main_files = [f for f in project_info["files"] if f.get("path") in [
                "index.js", "server.js", "app.js", "main.js"
            ]]
            
            # Check package.json main field
            for file_info in project_info["files"]:
                if file_info.get("path") == "package.json" and file_info.get("content"):
                    try:
                        package_data = json.loads(file_info.get("content"))
                        if "main" in package_data:
                            main_file = package_data["main"]
                            # Add to entry points if not already there
                            if main_file not in [f.get("path") for f in main_files]:
                                main_files.append({"path": main_file})
                    except Exception:
                        pass
            
            entry_points.extend([f.get("path") for f in main_files])
        
        return entry_points
    
    def _build_readme_prompt(self, project_info: Dict[str, Any]) -> str:
        """
        Build a prompt for README generation.
        
        Args:
            project_info: Project information
            
        Returns:
            Prompt string for the AI service
        """
        prompt = f"""
You are an expert technical writer tasked with creating a comprehensive README.md file for a {project_info.get('project_type', 'software')} project named "{project_info.get('name', 'Project')}".

Project details:
- Type: {project_info.get('project_type', 'Unknown')}
"""
        
        # Add dependencies information
        if "dependencies" in project_info:
            prompt += "- Dependencies:\n"
            
            if "runtime" in project_info["dependencies"]:
                runtime_deps = project_info["dependencies"]["runtime"]
                if runtime_deps:
                    prompt += f"  - Runtime: {', '.join(runtime_deps[:10])}"
                    if len(runtime_deps) > 10:
                        prompt += f" and {len(runtime_deps) - 10} more"
                    prompt += "\n"
            
            if "development" in project_info["dependencies"]:
                dev_deps = project_info["dependencies"]["development"]
                if dev_deps:
                    prompt += f"  - Development: {', '.join(dev_deps[:10])}"
                    if len(dev_deps) > 10:
                        prompt += f" and {len(dev_deps) - 10} more"
                    prompt += "\n"
        
        # Add entry points information
        if "entry_points" in project_info and project_info["entry_points"]:
            prompt += f"- Entry points: {', '.join(project_info['entry_points'])}\n"
        
        # Add file structure information
        file_types = {}
        for file_info in project_info.get("files", []):
            file_type = file_info.get("type")
            if file_type:
                if file_type not in file_types:
                    file_types[file_type] = []
                file_types[file_type].append(file_info.get("path"))
        
        prompt += "- File structure summary:\n"
        for file_type, files in file_types.items():
            prompt += f"  - {file_type} files: {len(files)}\n"
        
        # Add main source files
        source_files = file_types.get("source_code", [])
        if source_files:
            prompt += "- Main source files (up to 10):\n"
            for file in source_files[:10]:
                prompt += f"  - {file}\n"
        
        prompt += """
Create a comprehensive README.md file that follows these best practices:
1. Clear project title and description
2. Installation instructions
3. Usage examples
4. Features list
5. API documentation overview (if applicable)
6. Project structure explanation
7. Contributing guidelines reference
8. License information
9. Badges for build status, version, etc. (if applicable)

The README should be well-formatted with Markdown, including:
- Proper headings (# for main title, ## for sections, etc.)
- Code blocks with appropriate syntax highlighting
- Lists (ordered and unordered)
- Links to important resources
- Tables where appropriate

Make the README user-friendly, comprehensive, and professional.
"""
        
        return prompt
    
    async def _generate_python_api_docs(
        self, 
        project_path: Path,
        source_files: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """
        Generate API documentation for a Python project.
        
        Args:
            project_path: Path to the project
            source_files: List of source files
            
        Returns:
            Dictionary with the generated API docs
        """
        self._logger.info("Generating Python API docs")
        
        # Filter for Python files
        python_files = [f for f in source_files if f.get("path", "").endswith(".py")]
        
        # Organize files by module/package
        modules = {}
        
        for file_info in python_files:
            file_path = file_info.get("path", "")
            
            # Skip __init__.py files with no content
            if file_path.endswith("__init__.py") and not file_info.get("content", "").strip():
                continue
            
            # Determine module name
            if "/" in file_path:
                # File in a package
                package_parts = file_path.split("/")
                module_name = ".".join(package_parts[:-1])
                if module_name not in modules:
                    modules[module_name] = []
                modules[module_name].append(file_info)
            else:
                # File in root
                module_name = "root"
                if module_name not in modules:
                    modules[module_name] = []
                modules[module_name].append(file_info)
        
        # Generate docs for each module
        module_docs = {}
        
        for module_name, files in modules.items():
            module_docs[module_name] = await self._generate_python_module_docs(module_name, files)
        
        # Create docs structure
        docs_structure = {
            "index.md": self._generate_python_docs_index(modules, project_path.name),
            "modules": module_docs
        }
        
        return {
            "structure": docs_structure,
            "format": "markdown",
            "project_path": str(project_path)
        }
    
    async def _generate_python_module_docs(
        self, 
        module_name: str,
        files: List[Dict[str, Any]]
    ) -> Dict[str, str]:
        """
        Generate documentation for a Python module.
        
        Args:
            module_name: Name of the module
            files: List of files in the module
            
        Returns:
            Dictionary mapping file names to documentation content
        """
        module_docs = {}
        
        for file_info in files:
            file_path = file_info.get("path", "")
            content = file_info.get("content", "")
            
            if not content:
                continue
            
            # Extract file name without extension
            file_name = os.path.basename(file_path)
            doc_name = os.path.splitext(file_name)[0]
            
            # Parse Python content
            doc_content = await self._parse_python_file(file_path, content)
            
            # Generate markdown
            markdown = f"""# {doc_name}

{doc_content.get('module_docstring', 'No description available.')}

## Classes

"""
            # Add classes
            for class_name, class_info in doc_content.get("classes", {}).items():
                markdown += f"### {class_name}\n\n{class_info.get('docstring', 'No description available.')}\n\n"
                
                # Add methods
                if class_info.get("methods"):
                    markdown += "#### Methods\n\n"
                    for method_name, method_info in class_info.get("methods", {}).items():
                        markdown += f"##### `{method_name}{method_info.get('signature', '()') }`\n\n{method_info.get('docstring', 'No description available.')}\n\n"
            
            # Add functions
            if doc_content.get("functions"):
                markdown += "## Functions\n\n"
                for func_name, func_info in doc_content.get("functions", {}).items():
                    markdown += f"### `{func_name}{func_info.get('signature', '()') }`\n\n{func_info.get('docstring', 'No description available.')}\n\n"
            
            module_docs[doc_name + ".md"] = markdown
        
        return module_docs
    
    async def _parse_python_file(self, file_path: str, content: str) -> Dict[str, Any]:
        """
        Parse Python file for documentation.
        
        Args:
            file_path: Path to the file
            content: File content
            
        Returns:
            Dictionary with parsed documentation
        """
        # Basic structure
        doc_info = {
            "module_docstring": "",
            "classes": {},
            "functions": {}
        }
        
        # Extract module docstring
        module_docstring_match = re.search(r'"""(.*?)"""', content, re.DOTALL)
        if module_docstring_match:
            doc_info["module_docstring"] = module_docstring_match.group(1).strip()
        
        # Extract classes
        class_pattern = r'class\s+(\w+)(?:\([^)]*\))?:\s*(?:"""(.*?)""")?'
        for match in re.finditer(class_pattern, content, re.DOTALL):
            class_name = match.group(1)
            class_docstring = match.group(2) if match.group(2) else ""
            
            # Get class content
            class_start = match.end()
            class_content = ""
            
            # Find the end of the class (by indentation)
            for line in content[class_start:].splitlines():
                if line.strip() and not line.startswith(" ") and not line.startswith("\t"):
                    break
                class_content += line + "\n"
            
            # Extract methods
            methods = {}
            method_pattern = r'^\s+def\s+(\w+)\s*\((self(?:,\s*[^)]*)?)\):\s*(?:"""(.*?)""")?'
            for method_match in re.finditer(method_pattern, class_content, re.MULTILINE | re.DOTALL):
                method_name = method_match.group(1)
                method_signature = method_match.group(2).strip()
                method_docstring = method_match.group(3) if method_match.group(3) else ""
                
                methods[method_name] = {
                    "signature": f"({method_signature})",
                    "docstring": method_docstring.strip()
                }
            
            doc_info["classes"][class_name] = {
                "docstring": class_docstring.strip(),
                "methods": methods
            }
        
        # Extract functions
        function_pattern = r'^def\s+(\w+)\s*\(([^)]*)\):\s*(?:"""(.*?)""")?'
        for match in re.finditer(function_pattern, content, re.MULTILINE | re.DOTALL):
            func_name = match.group(1)
            func_signature = match.group(2).strip()
            func_docstring = match.group(3) if match.group(3) else ""
            
            doc_info["functions"][func_name] = {
                "signature": f"({func_signature})",
                "docstring": func_docstring.strip()
            }
        
        return doc_info
    
    def _generate_python_docs_index(self, modules: Dict[str, List[Dict[str, Any]]], project_name: str) -> str:
        """
        Generate index page for Python API docs.
        
        Args:
            modules: Dictionary mapping module names to files
            project_name: Name of the project
            
        Returns:
            Markdown content for index page
        """
        markdown = f"""# {project_name} API Documentation

## Modules

"""
        # Add module links
        for module_name, files in modules.items():
            if module_name == "root":
                markdown += "### Root Module\n\n"
            else:
                markdown += f"### {module_name}\n\n"
            
            for file_info in files:
                file_path = file_info.get("path", "")
                file_name = os.path.basename(file_path)
                doc_name = os.path.splitext(file_name)[0]
                
                markdown += f"- [{doc_name}](modules/{doc_name}.md)\n"
            
            markdown += "\n"
        
        return markdown
    
    async def _generate_js_api_docs(
        self, 
        project_path: Path,
        source_files: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """
        Generate API documentation for a JavaScript/TypeScript project.
        
        Args:
            project_path: Path to the project
            source_files: List of source files
            
        Returns:
            Dictionary with the generated API docs
        """
        self._logger.info("Generating JavaScript/TypeScript API docs")
        
        # Filter for JS/TS files
        js_files = [f for f in source_files if f.get("path", "").endswith((".js", ".jsx", ".ts", ".tsx"))]
        
        # Organize files by directory
        directories = {}
        
        for file_info in js_files:
            file_path = file_info.get("path", "")
            
            if "/" in file_path:
                # File in a directory
                dir_parts = file_path.split("/")
                dir_name = dir_parts[0]
                if dir_name not in directories:
                    directories[dir_name] = []
                directories[dir_name].append(file_info)
            else:
                # File in root
                dir_name = "root"
                if dir_name not in directories:
                    directories[dir_name] = []
                directories[dir_name].append(file_info)
        
        # Generate docs for each directory
        dir_docs = {}
        
        for dir_name, files in directories.items():
            dir_docs[dir_name] = await self._generate_js_directory_docs(dir_name, files)
        
        # Create docs structure
        docs_structure = {
            "index.md": self._generate_js_docs_index(directories, project_path.name),
            "modules": dir_docs
        }
        
        return {
            "structure": docs_structure,
            "format": "markdown",
            "project_path": str(project_path)
        }
    
    async def _generate_js_directory_docs(
        self, 
        dir_name: str,
        files: List[Dict[str, Any]]
    ) -> Dict[str, str]:
        """
        Generate documentation for a JavaScript/TypeScript directory.
        
        Args:
            dir_name: Name of the directory
            files: List of files in the directory
            
        Returns:
            Dictionary mapping file names to documentation content
        """
        dir_docs = {}
        
        for file_info in files:
            file_path = file_info.get("path", "")
            content = file_info.get("content", "")
            
            if not content:
                continue
            
            # Extract file name without extension
            file_name = os.path.basename(file_path)
            doc_name = os.path.splitext(file_name)[0]
            
            # Parse JS/TS content
            is_typescript = file_path.endswith((".ts", ".tsx"))
            doc_content = await self._parse_js_file(file_path, content, is_typescript)
            
            # Generate markdown
            markdown = f"""# {doc_name}

{doc_content.get('file_description', 'No description available.')}

## Exports

"""
            # Add classes
            for class_name, class_info in doc_content.get("classes", {}).items():
                markdown += f"### Class: {class_name}\n\n{class_info.get('description', 'No description available.')}\n\n"
                
                # Add methods
                if class_info.get("methods"):
                    markdown += "#### Methods\n\n"
                    for method_name, method_info in class_info.get("methods", {}).items():
                        markdown += f"##### `{method_name}{method_info.get('signature', '()') }`\n\n{method_info.get('description', 'No description available.')}\n\n"
            
            # Add functions
            if doc_content.get("functions"):
                markdown += "## Functions\n\n"
                for func_name, func_info in doc_content.get("functions", {}).items():
                    markdown += f"### `{func_name}{func_info.get('signature', '()') }`\n\n{func_info.get('description', 'No description available.')}\n\n"
            
            # Add interfaces (TypeScript only)
            if is_typescript and doc_content.get("interfaces"):
                markdown += "## Interfaces\n\n"
                for interface_name, interface_info in doc_content.get("interfaces", {}).items():
                    markdown += f"### Interface: {interface_name}\n\n{interface_info.get('description', 'No description available.')}\n\n"
                    
                    if interface_info.get("properties"):
                        markdown += "#### Properties\n\n"
                        for prop_name, prop_info in interface_info.get("properties", {}).items():
                            markdown += f"- `{prop_name}: {prop_info.get('type', 'any')}` - {prop_info.get('description', 'No description available.')}\n"
                    
                    markdown += "\n"
            
            dir_docs[doc_name + ".md"] = markdown
        
        return dir_docs
    
    async def _parse_js_file(
            self, 
            file_path: str, 
            content: str, 
            is_typescript: bool = False
        ) -> Dict[str, Any]:
            """
            Parse JavaScript/TypeScript file for documentation.
            
            Args:
                file_path: Path to the file
                content: File content
                is_typescript: Whether the file is TypeScript
                
            Returns:
                Dictionary with parsed documentation
            """
            doc_info = {
                "file_description": "",
                "classes": {},
                "functions": {}
            }
            if is_typescript:
                doc_info["interfaces"] = {}
    
            # Helper to find matching brace
            def find_matching_brace(text_content: str, start_index: int) -> int:
                open_braces = 0
                for i in range(start_index, len(text_content)):
                    if text_content[i] == '{':
                        open_braces += 1
                    elif text_content[i] == '}':
                        open_braces -= 1
                        if open_braces == 0:
                            return i
                return -1 # Not found
    
            # Extract file description from initial comment block
            # Regex for JSDoc: captures comment text, handling * at line starts
            jsdoc_block_pattern = r'\s*/\*\*((?:[^*]|\*(?!/))*?)\*/'
    
            file_comment_match = re.match(jsdoc_block_pattern, content, re.DOTALL)
            if file_comment_match:
                doc_info["file_description"] = self._parse_js_comment(file_comment_match.group(1))
            
            # Regex patterns with preceding JSDoc capture group
            # Group 1: JSDoc, Group 2: Modifiers (export, async etc), Group 3: Name, Group 4: Params/Signature, Group 5: Body start
            base_element_prefix = r'(?:export\s+)?(?:default\s+)?'
            
            # Class: JSDoc (1), 'export class' etc (2), Name (3), Extends/Implements (4), { (5)
            class_pattern_str = jsdoc_block_pattern + r'?\s*' + \
                                r'(' + base_element_prefix + r'class)\s+([A-Za-z_]\w*)' + \
                                r'((?:\s+extends\s+[\w.]+)?(?:\s+implements\s+[\w.,\s<>]+)?)?\s*({)'
            
            # Method: JSDoc (1), Modifiers (static, async, get, set) (2), Name (may include *) (3), Params (4), Body start or ; (5)
            method_pattern_str = jsdoc_block_pattern + r'?\s*' + \
                                 r'(public\s+|private\s+|protected\s+|static\s+|get\s+|set\s+|async\s+)*' + \
                                 r'(\*?\s*[A-Za-z_]\w*)\s*\(([^)]*)\)' + \
                                 r'(?:\s*:\s*[\w<>\[\],\s|&]+)?\s*(?:{|;)' # Match { or ; for abstract methods
    
            # Function: JSDoc (1), 'export function' or 'const name =' (2), Name (3), Params (4), Body start (5)
            # This covers: function foo() {}, const foo = function() {}, const foo = () => {}
            function_pattern_str = jsdoc_block_pattern + r'?\s*' + \
                                   r'(?:' + base_element_prefix + r'(?:function(?:\s*\*)?|async\s+function(?:\s*\*)?)\s+([A-Za-z_]\w*)' + \
                                   r'|' + base_element_prefix + r'(?:const|let|var)\s+([A-Za-z_]\w*)\s*=\s*(?:async\s*)?(?:function(?:\s*\*)?)?' + \
                                   r')\s*\(([^)]*)\)' + \
                                   r'(?:\s*:\s*[\w<>\[\],\s|&]+)?\s*(?:=>\s*(?:{|[\w(])|{)'
    
    
            # Interface (TS): JSDoc (1), 'export interface' (2), Name (3), Extends (4), { (5)
            interface_pattern_str = jsdoc_block_pattern + r'?\s*' + \
                                    r'(' + base_element_prefix + r'interface)\s+([A-Za-z_]\w*)' + \
                                    r'((?:\s+extends\s+[\w.,\s<>]+)?)?\s*({)'
            
            # Interface Property (TS): JSDoc (1), Name (2), Optional (?) (3), Type (4)
            ts_property_pattern_str = jsdoc_block_pattern + r'?\s*' + \
                                      r'([A-Za-z_]\w*)\s*(\?)?\s*:\s*([\w<>\[\],\s|&]+)(?:;|\n|,)'
    
    
            # Extract Classes and their Methods
            for class_match in re.finditer(class_pattern_str, content, re.DOTALL | re.MULTILINE):
                class_jsdoc = class_match.group(1)
                class_name = class_match.group(3)
                
                class_body_start_offset = class_match.end(5) # Start of class body content (after '{')
                class_body_end_offset = find_matching_brace(content, class_body_start_offset)
    
                if class_body_end_offset == -1:
                    self._logger.debug(f"Could not find matching brace for class {class_name} in {file_path}")
                    continue
                
                class_body_content = content[class_body_start_offset:class_body_end_offset]
                
                methods = {}
                for method_match in re.finditer(method_pattern_str, class_body_content, re.DOTALL | re.MULTILINE):
                    method_jsdoc = method_match.group(1)
                    # Group 2 are modifiers like 'async', 'static'
                    method_name = method_match.group(3).replace('*', '').strip() # remove generator *
                    method_params_str = method_match.group(4)
                    
                    methods[method_name] = {
                        "signature": f"({method_params_str.strip()})",
                        "description": self._parse_js_comment(method_jsdoc) if method_jsdoc else ""
                    }
                
                doc_info["classes"][class_name] = {
                    "description": self._parse_js_comment(class_jsdoc) if class_jsdoc else "",
                    "methods": methods
                }
    
            # Extract Functions (top-level)
            for func_match in re.finditer(function_pattern_str, content, re.DOTALL | re.MULTILINE):
                func_jsdoc = func_match.group(1)
                # Name can be in group 3 (for 'function name()') or group 4 (for 'const name =')
                func_name = func_match.group(3) if func_match.group(3) else func_match.group(4)
                if not func_name: continue # Should not happen with current regex logic
    
                func_params_str = func_match.group(5)
                
                # Avoid matching methods inside classes again if class parsing is imperfect
                # This is a heuristic: if a function looks like it's inside a known class, skip.
                is_likely_method = False
                for class_name_iter in doc_info["classes"]:
                    if f"class {class_name_iter}" in content[:func_match.start()]: # Simplified check
                        # A more robust check would involve checking if func_match.start() is within parsed class bounds
                        pass # This check needs to be more robust or might be removed if class parsing is good enough.
    
                if not is_likely_method:
                     doc_info["functions"][func_name] = {
                        "signature": f"({func_params_str.strip()})",
                        "description": self._parse_js_comment(func_jsdoc) if func_jsdoc else ""
                    }
    
            # Extract TypeScript Interfaces and their Properties
            if is_typescript:
                for interface_match in re.finditer(interface_pattern_str, content, re.DOTALL | re.MULTILINE):
                    interface_jsdoc = interface_match.group(1)
                    interface_name = interface_match.group(3)
                    
                    interface_body_start_offset = interface_match.end(5) # Start of interface body (after '{')
                    interface_body_end_offset = find_matching_brace(content, interface_body_start_offset)
    
                    if interface_body_end_offset == -1:
                        self._logger.debug(f"Could not find matching brace for interface {interface_name} in {file_path}")
                        continue
                    
                    interface_body_content = content[interface_body_start_offset:interface_body_end_offset]
                    
                    properties = {}
                    for prop_match in re.finditer(ts_property_pattern_str, interface_body_content, re.DOTALL | re.MULTILINE):
                        prop_jsdoc = prop_match.group(1)
                        prop_name = prop_match.group(2)
                        # prop_optional = prop_match.group(3) # '?'
                        prop_type = prop_match.group(4).strip()
                        
                        properties[prop_name] = {
                            "type": prop_type,
                            "description": self._parse_js_comment(prop_jsdoc) if prop_jsdoc else ""
                        }
                    
                    doc_info["interfaces"][interface_name] = {
                        "description": self._parse_js_comment(interface_jsdoc) if interface_jsdoc else "",
                        "properties": properties
                    }
            
            return doc_info
    
    def _parse_js_comment(self, comment: str) -> str:
        """
        Parse JSDoc comment.
        
        Args:
            comment: JSDoc comment
            
        Returns:
            Parsed description
        """
        # Remove * at the beginning of lines
        lines = [line.strip().lstrip('*') for line in comment.splitlines()]
        
        # Join and clean up
        description = " ".join(line for line in lines if line and not line.startswith('@'))
        
        return description.strip()
    
    def _generate_js_docs_index(self, directories: Dict[str, List[Dict[str, Any]]], project_name: str) -> str:
        """
        Generate index page for JavaScript/TypeScript API docs.
        
        Args:
            directories: Dictionary mapping directory names to files
            project_name: Name of the project
            
        Returns:
            Markdown content for index page
        """
        markdown = f"""# {project_name} API Documentation

## Modules

"""
        # Add directory links
        for dir_name, files in directories.items():
            if dir_name == "root":
                markdown += "### Root Module\n\n"
            else:
                markdown += f"### {dir_name}\n\n"
            
            for file_info in files:
                file_path = file_info.get("path", "")
                file_name = os.path.basename(file_path)
                doc_name = os.path.splitext(file_name)[0]
                
                markdown += f"- [{doc_name}](modules/{doc_name}.md)\n"
            
            markdown += "\n"
        
        return markdown
    
    async def _generate_java_api_docs(
        self, 
        project_path: Path,
        source_files: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """
        Generate API documentation for a Java project.
        
        Args:
            project_path: Path to the project
            source_files: List of source files
            
        Returns:
            Dictionary with the generated API docs
        """
        self._logger.info("Generating Java API docs")
        
        # Filter for Java files
        java_files = [f for f in source_files if f.get("path", "").endswith(".java")]
        
        # Organize files by package
        packages = {}
        
        for file_info in java_files:
            file_path = file_info.get("path", "")
            content = file_info.get("content", "")
            
            if not content:
                continue
            
            # Extract package name
            package_match = re.search(r'package\s+([\w.]+);', content)
            if package_match:
                package_name = package_match.group(1)
            else:
                package_name = "default"
            
            if package_name not in packages:
                packages[package_name] = []
            packages[package_name].append(file_info)
        
        # Generate docs for each package
        package_docs = {}
        
        for package_name, files in packages.items():
            package_docs[package_name] = await self._generate_java_package_docs(package_name, files)
        
        # Create docs structure
        docs_structure = {
            "index.md": self._generate_java_docs_index(packages, project_path.name),
            "packages": package_docs
        }
        
        return {
            "structure": docs_structure,
            "format": "markdown",
            "project_path": str(project_path)
        }
    
    async def _generate_java_package_docs(
        self, 
        package_name: str,
        files: List[Dict[str, Any]]
    ) -> Dict[str, str]:
        """
        Generate documentation for a Java package.
        
        Args:
            package_name: Name of the package
            files: List of files in the package
            
        Returns:
            Dictionary mapping file names to documentation content
        """
        package_docs = {}
        
        for file_info in files:
            file_path = file_info.get("path", "")
            content = file_info.get("content", "")
            
            if not content:
                continue
            
            # Extract file name without extension
            file_name = os.path.basename(file_path)
            class_name = os.path.splitext(file_name)[0]
            
            # Parse Java content
            doc_content = await self._parse_java_file(file_path, content)
            
            # Generate markdown
            markdown = f"""# {class_name}

Package: `{package_name}`

{doc_content.get('class_javadoc', 'No description available.')}

"""
            # Add class info
            if "is_interface" in doc_content and doc_content["is_interface"]:
                markdown += "## Interface Methods\n\n"
            else:
                markdown += "## Methods\n\n"
            
            for method_name, method_info in doc_content.get("methods", {}).items():
                markdown += f"### `{method_name}{method_info.get('signature', '()') }`\n\n{method_info.get('javadoc', 'No description available.')}\n\n"
                
                # Add parameters documentation
                if method_info.get("params"):
                    markdown += "#### Parameters\n\n"
                    for param_name, param_desc in method_info.get("params", {}).items():
                        markdown += f"- `{param_name}` - {param_desc}\n"
                    markdown += "\n"
                
                # Add return documentation
                if method_info.get("returns"):
                    markdown += f"#### Returns\n\n{method_info.get('returns')}\n\n"
                
                # Add throws documentation
                if method_info.get("throws"):
                    markdown += "#### Throws\n\n"
                    for exception, desc in method_info.get("throws", {}).items():
                        markdown += f"- `{exception}` - {desc}\n"
                    markdown += "\n"
            
            # Add fields
            if doc_content.get("fields"):
                markdown += "## Fields\n\n"
                for field_name, field_info in doc_content.get("fields", {}).items():
                    markdown += f"### `{field_info.get('type', 'Object')} {field_name}`\n\n{field_info.get('javadoc', 'No description available.')}\n\n"
            
            package_docs[class_name + ".md"] = markdown
        
        return package_docs
    
    async def _parse_java_file(self, file_path: str, content: str) -> Dict[str, Any]:
        """
        Parse Java file for documentation.
        
        Args:
            file_path: Path to the file
            content: File content
            
        Returns:
            Dictionary with parsed documentation
        """
        # Basic structure
        doc_info = {
            "class_javadoc": "",
            "methods": {},
            "fields": {}
        }
        
        # Check if it's an interface
        if re.search(r'(?:public\s+)?interface\s+\w+', content):
            doc_info["is_interface"] = True
        
        # Extract class javadoc
        class_javadoc_match = re.search(r'/\*\*(.*?)\*/', content, re.DOTALL)
        if class_javadoc_match:
            doc_info["class_javadoc"] = self._parse_javadoc(class_javadoc_match.group(1))
        
        # Extract methods
        method_pattern = r'(?:/\*\*(.*?)\*/\s*)?(?:public|private|protected)?\s+(?:static\s+)?(?:final\s+)?(?:[\w<>[\],\s]+)\s+(\w+)\s*\(([^)]*)\)(?:\s+throws\s+[\w,\s]+)?(?:\s*{)?'
        for match in re.finditer(method_pattern, content, re.DOTALL):
            javadoc = match.group(1)
            method_name = match.group(2)
            params_str = match.group(3)
            
            # Skip constructor if it has the same name as the class
            class_name = os.path.splitext(os.path.basename(file_path))[0]
            if method_name == class_name:
                continue
            
            # Parse javadoc
            javadoc_info = self._parse_javadoc_with_tags(javadoc) if javadoc else {}
            
            # Format parameters
            formatted_params = []
            for param in params_str.split(","):
                param = param.strip()
                if param:
                    parts = param.split()
                    if len(parts) >= 2:
                        param_type = " ".join(parts[:-1])
                        param_name = parts[-1]
                        formatted_params.append(f"{param_type} {param_name}")
            
            # Extract throws info
            throws_match = re.search(r'throws\s+([\w,\s]+)', match.group(0))
            throws = throws_match.group(1).split(",") if throws_match else []
            
            method_info = {
                "signature": f"({', '.join(formatted_params)})",
                "javadoc": javadoc_info.get("description", ""),
                "params": javadoc_info.get("params", {}),
                "returns": javadoc_info.get("returns", ""),
                "throws": javadoc_info.get("throws", {})
            }
            
            doc_info["methods"][method_name] = method_info
        
        # Extract fields
        field_pattern = r'(?:/\*\*(.*?)\*/\s*)?(?:public|private|protected)?\s+(?:static\s+)?(?:final\s+)?(?:[\w<>[\],\s]+)\s+(\w+)\s*(?:=\s*[^;]+)?;'
        for match in re.finditer(field_pattern, content, re.DOTALL):
            javadoc = match.group(1)
            field_declaration = match.group(0)
            
            # Extract field name and type
            field_name = match.group(2)
            
            # Extract field type
            type_match = re.search(r'(?:public|private|protected)?\s+(?:static\s+)?(?:final\s+)?([\w<>[\],\s]+)\s+\w+\s*(?:=|;)', field_declaration)
            field_type = type_match.group(1).strip() if type_match else "Object"
            
            # Parse javadoc
            field_javadoc = self._parse_javadoc(javadoc) if javadoc else ""
            
            doc_info["fields"][field_name] = {
                "type": field_type,
                "javadoc": field_javadoc
            }
        
        return doc_info
    
    def _parse_javadoc(self, javadoc: str) -> str:
        """
        Parse basic Javadoc comment.
        
        Args:
            javadoc: Javadoc comment
            
        Returns:
            Parsed description
        """
        if not javadoc:
            return ""
        
        # Remove * at the beginning of lines
        lines = [line.strip().lstrip('*') for line in javadoc.splitlines()]
        
        # Join and clean up
        description = " ".join(line for line in lines if line and not line.startswith('@'))
        
        return description.strip()
    
    def _parse_javadoc_with_tags(self, javadoc: str) -> Dict[str, Any]:
        """
        Parse Javadoc comment with tags.
        
        Args:
            javadoc: Javadoc comment
            
        Returns:
            Dictionary with parsed javadoc
        """
        if not javadoc:
            return {"description": ""}
        
        result = {
            "description": "",
            "params": {},
            "returns": "",
            "throws": {}
        }
        
        # Remove * at the beginning of lines
        lines = [line.strip().lstrip('*') for line in javadoc.splitlines()]
        
        # Extract description (text before tags)
        description_lines = []
        tag_lines = []
        in_description = True
        
        for line in lines:
            if line.startswith('@'):
                in_description = False
                tag_lines.append(line)
            elif in_description:
                description_lines.append(line)
            else:
                tag_lines.append(line)
        
        result["description"] = " ".join(line for line in description_lines if line).strip()
        
        # Process tags
        current_tag = None
        current_text = []
        
        for line in tag_lines:
            if line.startswith('@'):
                # Save previous tag
                if current_tag and current_text:
                    self._add_tag_to_result(result, current_tag, " ".join(current_text).strip())
                
                # Start new tag
                parts = line.split(' ', 1)
                current_tag = parts[0][1:]  # Remove @ and get tag name
                current_text = [parts[1].strip()] if len(parts) > 1 else []
            elif current_tag:
                current_text.append(line)
        
        # Save last tag
        if current_tag and current_text:
            self._add_tag_to_result(result, current_tag, " ".join(current_text).strip())
        
        return result
    
    def _add_tag_to_result(self, result: Dict[str, Any], tag: str, text: str) -> None:
        """
        Add a parsed javadoc tag to the result.
        
        Args:
            result: Result dictionary
            tag: Tag name
            text: Tag text
        """
        if tag == "param":
            # Extract parameter name
            parts = text.split(' ', 1)
            if len(parts) > 1:
                param_name = parts[0]
                param_description = parts[1]
                result["params"][param_name] = param_description
        elif tag == "return":
            result["returns"] = text
        elif tag in ["throws", "exception"]:
            # Extract exception class
            parts = text.split(' ', 1)
            if len(parts) > 1:
                exception_class = parts[0]
                exception_description = parts[1]
                result["throws"][exception_class] = exception_description
    
    def _generate_java_docs_index(self, packages: Dict[str, List[Dict[str, Any]]], project_name: str) -> str:
        """
        Generate index page for Java API docs.
        
        Args:
            packages: Dictionary mapping package names to files
            project_name: Name of the project
            
        Returns:
            Markdown content for index page
        """
        markdown = f"""# {project_name} API Documentation

## Packages

"""
        # Add package links
        for package_name, files in packages.items():
            markdown += f"### {package_name}\n\n"
            
            for file_info in files:
                file_path = file_info.get("path", "")
                file_name = os.path.basename(file_path)
                class_name = os.path.splitext(file_name)[0]
                
                markdown += f"- [{class_name}](packages/{class_name}.md)\n"
            
            markdown += "\n"
        
        return markdown
    
    async def _generate_generic_api_docs(
        self, 
        project_path: Path,
        source_files: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """
        Generate generic API documentation for an unknown project type.
        
        Args:
            project_path: Path to the project
            source_files: List of source files
            
        Returns:
            Dictionary with the generated API docs
        """
        self._logger.info("Generating generic API docs")
        
        # Organize files by directory
        directories = {}
        
        for file_info in source_files:
            file_path = file_info.get("path", "")
            
            if "/" in file_path:
                # File in a directory
                dir_parts = file_path.split("/")
                dir_name = dir_parts[0]
                if dir_name not in directories:
                    directories[dir_name] = []
                directories[dir_name].append(file_info)
            else:
                # File in root
                dir_name = "root"
                if dir_name not in directories:
                    directories[dir_name] = []
                directories[dir_name].append(file_info)
        
        # Generate docs for each directory
        dir_docs = {}
        
        for dir_name, files in directories.items():
            dir_docs[dir_name] = await self._generate_generic_directory_docs(dir_name, files)
        
        # Create docs structure
        docs_structure = {
            "index.md": self._generate_generic_docs_index(directories, project_path.name),
            "modules": dir_docs
        }
        
        return {
            "structure": docs_structure,
            "format": "markdown",
            "project_path": str(project_path)
        }
    
    async def _generate_generic_directory_docs(
        self, 
        dir_name: str,
        files: List[Dict[str, Any]]
    ) -> Dict[str, str]:
        """
        Generate documentation for a generic directory.
        
        Args:
            dir_name: Name of the directory
            files: List of files in the directory
            
        Returns:
            Dictionary mapping file names to documentation content
        """
        dir_docs = {}
        
        for file_info in files:
            file_path = file_info.get("path", "")
            content = file_info.get("content", "")
            
            if not content:
                continue
            
            # Extract file name without extension
            file_name = os.path.basename(file_path)
            doc_name = os.path.splitext(file_name)[0]
            
            # Generate markdown using AI
            markdown = await self._generate_file_docs_with_ai(file_info)
            
            dir_docs[doc_name + ".md"] = markdown
        
        return dir_docs
    
    def _generate_generic_docs_index(self, directories: Dict[str, List[Dict[str, Any]]], project_name: str) -> str:
        """
        Generate index page for generic API docs.
        
        Args:
            directories: Dictionary mapping directory names to files
            project_name: Name of the project
            
        Returns:
            Markdown content for index page
        """
        markdown = f"""# {project_name} API Documentation

## Modules

"""
        # Add directory links
        for dir_name, files in directories.items():
            if dir_name == "root":
                markdown += "### Root Module\n\n"
            else:
                markdown += f"### {dir_name}\n\n"
            
            for file_info in files:
                file_path = file_info.get("path", "")
                file_name = os.path.basename(file_path)
                doc_name = os.path.splitext(file_name)[0]
                
                markdown += f"- [{doc_name}](modules/{doc_name}.md)\n"
            
            markdown += "\n"
        
        return markdown
    
    async def _generate_file_docs_with_ai(self, file_info: Dict[str, Any]) -> str:
        """
        Generate documentation for a file using AI.
        
        Args:
            file_info: File information
            
        Returns:
            Markdown documentation
        """
        file_path = file_info.get("path", "")
        content = file_info.get("content", "")
        language = file_info.get("language", "Unknown")
        
        # Build prompt for AI
        prompt = f"""
You are an expert technical writer tasked with documenting a {language} file.

File path: {file_path}

File content:
```
{content[:5000] if len(content) > 5000 else content}
```
{f"..." if len(content) > 5000 else ""}

Create comprehensive documentation in Markdown format for this file, including:
1. File overview/purpose
2. Main functions/classes/components
3. Usage examples (if applicable)
4. Any dependencies or relationships with other files (if detectable)

Follow these formatting guidelines:
- Use Markdown headings appropriately (# for title, ## for sections, etc.)
- Use code blocks with appropriate syntax highlighting
- Document parameters, return values, and exceptions where applicable
- Be concise but thorough

DO NOT reproduce the entire file content - focus on documenting functionality and usage.
"""
        
        # Call AI service
        api_request = GeminiRequest(
            prompt=prompt,
            max_tokens=3000,
            temperature=0.2
        )
        
        self._logger.debug(f"Sending file documentation request to AI for {file_path}")
        response = await gemini_client.generate_text(api_request)
        
        # Extract documentation
        return response.text
    
    def _build_user_guide_prompt(self, project_info: Dict[str, Any]) -> str:
        """
        Build a prompt for user guide generation.
        
        Args:
            project_info: Project information
            
        Returns:
            Prompt string for the AI service
        """
        prompt = f"""
You are an expert technical writer tasked with creating a comprehensive user guide for a {project_info.get('project_type', 'software')} project named "{project_info.get('name', 'Project')}".

Project details:
- Type: {project_info.get('project_type', 'Unknown')}
"""
        
        # Add dependencies information
        if "dependencies" in project_info:
            prompt += "- Dependencies:\n"
            
            if "runtime" in project_info["dependencies"]:
                runtime_deps = project_info["dependencies"]["runtime"]
                if runtime_deps:
                    prompt += f"  - Runtime: {', '.join(runtime_deps[:10])}"
                    if len(runtime_deps) > 10:
                        prompt += f" and {len(runtime_deps) - 10} more"
                    prompt += "\n"
        
        # Add entry points information
        if "entry_points" in project_info and project_info["entry_points"]:
            prompt += f"- Entry points: {', '.join(project_info['entry_points'])}\n"
        
        # Add important source files
        source_files = [f for f in project_info.get("files", []) if f.get("type") == "source_code"]
        if source_files:
            prompt += "- Important source files:\n"
            for file in source_files[:5]:  # Limit to 5 files
                prompt += f"  - {file.get('path')}\n"
        
        prompt += """
Create a comprehensive user guide that follows these best practices:
1. Introduction and overview
2. Getting started (installation, setup)
3. Basic usage
4. Advanced features
5. Troubleshooting
6. API/command reference
7. Examples and use cases

The user guide should be well-formatted with Markdown, including:
- Proper headings (# for main title, ## for sections, etc.)
- Code blocks with appropriate syntax highlighting
- Lists (ordered and unordered)
- Tables where appropriate
- Screenshots (described with placeholders)

Make the user guide user-friendly, comprehensive, and suitable for users with varying levels of technical expertise.
"""
        
        return prompt
    
    def _build_contributing_prompt(self, project_info: Dict[str, Any]) -> str:
        """
        Build a prompt for contributing guide generation.
        
        Args:
            project_info: Project information
            
        Returns:
            Prompt string for the AI service
        """
        prompt = f"""
You are an expert technical writer tasked with creating a comprehensive CONTRIBUTING.md file for a {project_info.get('project_type', 'software')} project named "{project_info.get('name', 'Project')}".

Project details:
- Type: {project_info.get('project_type', 'Unknown')}
"""
        
        # Add file structure information
        file_types = {}
        for file_info in project_info.get("files", []):
            file_type = file_info.get("type")
            if file_type:
                if file_type not in file_types:
                    file_types[file_type] = []
                file_types[file_type].append(file_info.get("path"))
        
        prompt += "- File structure summary:\n"
        for file_type, files in file_types.items():
            prompt += f"  - {file_type} files: {len(files)}\n"
        
        prompt += """
Create a comprehensive CONTRIBUTING.md file that follows these best practices:
1. Introduction and welcome message
2. Code of conduct reference
3. Getting started for contributors
4. Development environment setup
5. Coding standards and conventions
6. Pull request process
7. Issue reporting guidelines
8. Testing instructions
9. Documentation guidelines

The contributing guide should be well-formatted with Markdown, including:
- Proper headings (# for main title, ## for sections, etc.)
- Code blocks with appropriate syntax highlighting
- Lists (ordered and unordered)
- Links to important resources

Make the contributing guide friendly, comprehensive, and helpful for new contributors.
"""
        
        return prompt
    
    def _extract_markdown_content(self, content: str) -> str:
        """
        Extract markdown content from AI response.
        
        Args:
            content: AI response text
            
        Returns:
            Markdown content
        """
        # Check if content is already markdown
        if content.startswith('#') or content.startswith('# '):
            return content
        
        # Try to extract markdown from code blocks
        markdown_match = re.search(r'```(?:markdown)?\s*(.*?)\s*```', content, re.DOTALL)
        if markdown_match:
            return markdown_match.group(1)
        
        # Otherwise, just return the response
        return content

# Global documentation generator instance
documentation_generator = DocumentationGenerator()
</file>

<file path="components/generation/engine.py">
"""
Advanced code generation engine for Angela CLI.

This module provides capabilities for generating entire directory structures
and multiple code files based on high-level natural language descriptions.
"""
import os
import asyncio
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple, Union, Set
import json
import re
import time
import sys

# Import models from the new models module instead of defining them here
from angela.api.generation import get_generation_context_manager, validate_code, get_code_file_class, get_code_project_class
from angela.api.ai import get_gemini_client, get_gemini_request_class
from angela.api.context import get_context_manager, get_context_enhancer
from angela.utils.logging import get_logger
from angela.api.execution import get_filesystem_functions

logger = get_logger(__name__)
GeminiRequest = get_gemini_request_class()
CodeFile = get_code_file_class()
CodeProject = get_code_project_class()

class CodeGenerationEngine:
    """
    Advanced code generation engine that can create entire projects
    based on natural language descriptions.
    """
    
    def __init__(self):
        """Initialize the code generation engine."""
        self._logger = logger
    
    async def generate_project(
        self, 
        description: str, 
        output_dir: Optional[str] = None,
        project_type: Optional[str] = None,
        context: Optional[Dict[str, Any]] = None
    ) -> CodeProject:
        """
        Generate a complete project from a description.
        
        Args:
            description: Natural language description of the project
            output_dir: Directory where the project should be generated (defaults to cwd)
            project_type: Optional type of project to generate (auto-detected if None)
            context: Additional context information
            
        Returns:
            CodeProject object representing the generated project
        """
        self._logger.info(f"Generating project from description: {description}")
        
        # Get current context if not provided
        if context is None:
            context_manager = get_context_manager()
            context_enhancer = get_context_enhancer()
            context = context_manager.get_context_dict()
            context = await context_enhancer.enrich_context(context)
        
        # Determine output directory
        if output_dir is None:
            output_dir = context.get("cwd", os.getcwd())
        
        # Create project plan
        project_plan = await self._create_project_plan(description, output_dir, project_type, context)
        self._logger.info(f"Created project plan with {len(project_plan.files)} files")
        
        # Validate the project plan
        is_valid, validation_errors = await self._validate_project_plan(project_plan)
        
        if not is_valid:
            self._logger.error(f"Project plan validation failed: {validation_errors}")
            # Try to fix validation errors
            project_plan = await self._fix_validation_errors(project_plan, validation_errors)
        
        return project_plan
    
    async def create_project_files(
        self, 
        project: CodeProject,
        dry_run: bool = False
    ) -> Dict[str, Any]:
        """
        Create the actual files for a project.
        
        Args:
            project: CodeProject to generate
            dry_run: Whether to simulate file creation without making changes
            
        Returns:
            Dictionary with creation results
        """
        self._logger.info(f"Creating project files for: {project.name}")
        
        # Create the root directory if it doesn't exist
        root_path = Path(project.root_dir)
        filesystem = get_filesystem_functions()
        
        if not root_path.exists() and not dry_run:
            await filesystem.create_directory(root_path, parents=True)
        
        # Create files in dependency order
        created_files = []
        file_errors = []
        dependency_graph = self._build_dependency_graph(project.files)
        
        # Process files in dependency order
        for file in self._get_ordered_files(project.files, dependency_graph):
            file_path = root_path / file.path
            
            # Create parent directories if needed
            if not dry_run:
                await filesystem.create_directory(file_path.parent, parents=True)
            
            # Write file content
            try:
                if not dry_run:
                    await filesystem.write_file(file_path, file.content)
                created_files.append(str(file_path))
                self._logger.debug(f"Created file: {file_path}")
            except Exception as e:
                self._logger.error(f"Error creating file {file_path}: {str(e)}")
                file_errors.append({"path": str(file_path), "error": str(e)})
        
        return {
            "project_name": project.name,
            "root_dir": str(root_path),
            "created_files": created_files,
            "file_errors": file_errors,
            "file_count": len(created_files),
            "success": len(file_errors) == 0,
            "dry_run": dry_run
        }
    
    async def _create_project_plan(
        self, 
        description: str, 
        output_dir: str,
        project_type: Optional[str],
        context: Dict[str, Any]
    ) -> CodeProject:
        """
        Create a plan for a project based on the description.
        
        Args:
            description: Natural language description of the project
            output_dir: Directory where the project should be generated
            project_type: Optional type of project to generate
            context: Additional context information
            
        Returns:
            CodeProject object with the plan
        """
        # Determine project type if not specified
        if project_type is None:
            project_type = await self._infer_project_type(description, context)
            self._logger.debug(f"Inferred project type: {project_type}")
        
        # Build prompt for project planning
        prompt = self._build_project_planning_prompt(description, project_type, context)
        
        # Call AI service to generate project plan
        gemini_client = get_gemini_client()
        api_request = GeminiRequest(
            prompt=prompt,
            max_tokens=8000,  # Large token limit for complex project plans
            temperature=0.2   # Low temperature for more deterministic output
        )
        
        self._logger.debug("Sending project planning request to AI service")
        response = await gemini_client.generate_text(api_request)
        
        # Parse the response to extract the project plan
        project_plan = await self._parse_project_plan(response.text, output_dir, project_type)
        
        # Generate detailed content for each file
        project_plan = await self._generate_file_contents(project_plan, context)
        
        return project_plan
    
    async def _infer_project_type(
        self, 
        description: str, 
        context: Dict[str, Any]
    ) -> str:
        """
        Infer the project type from the description.
        
        Args:
            description: Natural language description of the project
            context: Additional context information
            
        Returns:
            String indicating the project type
        """
        # Check for explicit mentions of languages/frameworks
        tech_indicators = {
            "python": ["python", "flask", "django", "fastapi", "sqlalchemy", "pytest"],
            "node": ["node", "javascript", "express", "react", "vue", "angular", "npm"],
            "java": ["java", "spring", "maven", "gradle", "junit"],
            "go": ["go", "golang", "gin", "echo"],
            "ruby": ["ruby", "rails", "sinatra", "rspec"],
            "rust": ["rust", "cargo", "actix", "rocket"],
        }
        
        # Lowercase description for easier matching
        description_lower = description.lower()
        
        # Count mentions of each technology
        tech_counts = {}
        for tech, indicators in tech_indicators.items():
            count = sum(indicator in description_lower for indicator in indicators)
            if count > 0:
                tech_counts[tech] = count
        
        # If we found clear indicators, return the most mentioned
        if tech_counts:
            return max(tech_counts.items(), key=lambda x: x[1])[0]
        
        # No clear indicators, use AI to infer
        prompt = f"""
    Determine the most suitable programming language/framework for this project:
    
    "{description}"
    
    Return only the project type as a single word, using one of these options:
    python, node, java, go, ruby, rust, or other.
    """
        
        gemini_client = get_gemini_client()
        api_request = GeminiRequest(prompt=prompt, max_tokens=10)
        response = await gemini_client.generate_text(api_request)
        
        # Extract the project type from the response
        project_type = response.text.strip().lower()
        
        # Default to python if we couldn't determine
        if project_type not in {"python", "node", "java", "go", "ruby", "rust"}:
            return "python"
        
        return project_type
    
    def _build_project_planning_prompt(
        self, 
        description: str,
        project_type: str,
        context: Dict[str, Any]
    ) -> str:
        """
        Build a prompt for project planning.
        
        Args:
            description: Natural language description of the project
            project_type: Type of project to generate
            context: Additional context information
            
        Returns:
            Prompt string for the AI service
        """
        prompt = f"""
    You are an expert software architect tasked with planning a {project_type} project based on this description:
    
    "{description}"
    
    First, analyze the requirements and identify:
    1. Core components needed
    2. Data models and their relationships
    3. Key functionality to be implemented
    4. External dependencies required
    
    Then, create a detailed project structure plan in JSON format, including:
    
    ```json
    {{
      "name": "project_name",
      "description": "brief project description",
      "project_type": "{project_type}",
      "dependencies": {{
        "runtime": ["list", "of", "dependencies"],
        "development": ["test", "frameworks", "etc"]
      }},
      "files": [
        {{
          "path": "relative/path/to/file.ext",
          "purpose": "description of the file's purpose",
          "dependencies": ["other/files/this/depends/on"],
          "language": "programming language"
        }}
      ],
      "structure_explanation": "explanation of why this structure was chosen"
    }}
    Focus on creating a well-structured, maintainable project following best practices for {project_type} projects. Include appropriate configuration files, tests, documentation, and proper project organization.
    The project should be modular, follow SOLID principles, and be easy to extend.
    """
        return prompt

    async def _parse_project_plan(
        self, 
        response: str, 
        output_dir: str,
        project_type: str
    ) -> CodeProject:
        """
        Parse the AI response to extract the project plan.
        
        Args:
            response: AI response text
            output_dir: Directory where the project should be generated
            project_type: Type of project to generate
            
        Returns:
            CodeProject object with the plan
        """
        try:
            # Look for JSON block in the response
            json_match = re.search(r'```(?:json)?\s*(.*?)\s*```', response, re.DOTALL)
            if json_match:
                json_str = json_match.group(1)
            else:
                # Try to find JSON without code blocks
                json_match = re.search(r'({.*})', response, re.DOTALL)
                if json_match:
                    json_str = json_match.group(1)
                else:
                    # Assume the entire response is JSON
                    json_str = response
            
            # Parse the JSON
            plan_data = json.loads(json_str)
            
            # Create CodeFile objects
            files = []
            for file_data in plan_data.get("files", []):
                files.append(CodeFile(
                    path=file_data["path"],
                    content="",  # Content will be generated later
                    purpose=file_data["purpose"],
                    dependencies=file_data.get("dependencies", []),
                    language=file_data.get("language")
                ))
            
            # Create CodeProject object
            project = CodeProject(
                name=plan_data.get("name", f"new_{project_type}_project"),
                description=plan_data.get("description", "Generated project"),
                root_dir=output_dir,
                files=files,
                dependencies=plan_data.get("dependencies", {}),
                project_type=project_type,
                structure_explanation=plan_data.get("structure_explanation", "")
            )
            
            return project
            
        except Exception as e:
            self._logger.exception(f"Error parsing project plan: {str(e)}")
            
            # Create a minimal fallback project
            fallback_file = CodeFile(
                path="main.py" if project_type == "python" else "index.js",
                content="",
                purpose="Main entry point",
                dependencies=[],
                language=project_type
            )
            
            return CodeProject(
                name=f"new_{project_type}_project",
                description="Generated project (fallback)",
                root_dir=output_dir,
                files=[fallback_file],
                dependencies={},
                project_type=project_type,
                structure_explanation="Fallback project structure due to parsing error."
            )

    async def _generate_file_contents(
        self, 
        project: CodeProject,
        context: Dict[str, Any]
    ) -> CodeProject:
        """
        Generate content for each file in the project.
        
        Args:
            project: CodeProject with file information
            context: Additional context information
            
        Returns:
            Updated CodeProject with file contents
        """
        self._logger.info(f"Generating content for {len(project.files)} files")
        
        # Process files in dependency order
        dependency_graph = self._build_dependency_graph(project.files)
        
        # Prepare batches of files to generate (to avoid too many concurrent requests)
        batches = self._create_file_batches(project.files, dependency_graph)
        
        # Generate content for each batch
        for batch_idx, batch in enumerate(batches):
            self._logger.debug(f"Processing batch {batch_idx+1}/{len(batches)} with {len(batch)} files")
            
            # Generate content for each file in the batch concurrently
            tasks = []
            for file in batch:
                # Load previous file contents for dependencies
                dependencies_content = {}
                for dep_path in file.dependencies:
                    # Find the dependent file
                    for dep_file in project.files:
                        if dep_file.path == dep_path and dep_file.content:
                            dependencies_content[dep_path] = dep_file.content
                
                task = self._generate_file_content(
                    file, 
                    project, 
                    dependencies_content,
                    context
                )
                tasks.append(task)
            
            # Wait for all tasks in this batch to complete
            results = await asyncio.gather(*tasks)
            
            # Update file contents
            for file, content in zip(batch, results):
                file.content = content
        
        return project

    async def _generate_file_content(
        self, 
        file: CodeFile, 
        project: CodeProject,
        dependencies_content: Dict[str, str],
        context: Dict[str, Any]
    ) -> str:
        """
        Generate content for a single file.
        
        Args:
            file: CodeFile to generate content for
            project: Parent CodeProject
            dependencies_content: Content of files this depends on
            context: Additional context information
            
        Returns:
            Generated file content
        """
        self._logger.debug(f"Generating content for file: {file.path}")
        
        # Build prompt for file content generation
        prompt = self._build_file_content_prompt(file, project, dependencies_content)
        
        # Call AI service to generate file content
        gemini_client = get_gemini_client()
        api_request = GeminiRequest(
            prompt=prompt,
            max_tokens=4000,
            temperature=0.2
        )
        
        response = await gemini_client.generate_text(api_request)
        
        # Extract code from the response
        content = self._extract_code_from_response(response.text, file.path)
        
        # Validate the generated code
        is_valid, validation_message = validate_code(content, file.path)
        
        # If validation failed, try once more with the error message
        if not is_valid:
            self._logger.warning(f"Validation failed for {file.path}: {validation_message}")
            
            # Build a new prompt with the validation error
            fix_prompt = f"""
    The code you generated has an issue that needs to be fixed:
    {validation_message}
    Here is the original code:
    {content}
    Please provide the corrected code for file '{file.path}'.
    Only respond with the corrected code, nothing else.
    """
            # Call AI service to fix the code
            fix_request = GeminiRequest(
                prompt=fix_prompt,
                max_tokens=4000,
                temperature=0.1
            )
            
            fix_response = await gemini_client.generate_text(fix_request)
            
            # Extract fixed code
            fixed_content = self._extract_code_from_response(fix_response.text, file.path)
            
            # Validate again
            is_valid, _ = validate_code(fixed_content, file.path)
            if is_valid:
                content = fixed_content
        
        return content

    def _build_file_content_prompt(
        self, 
        file: CodeFile, 
        project: CodeProject,
        dependencies_content: Dict[str, str]
    ) -> str:
        """
        Build a prompt for generating file content.
        
        Args:
            file: CodeFile to generate content for
            project: Parent CodeProject
            dependencies_content: Content of files this depends on
            
        Returns:
            Prompt string for the AI service
        """
        # Add language context based on file extension
        language_hints = ""
        if file.language:
            language_hints = f"The file should be written in {file.language}."
        
        # Add dependencies context
        dependencies_context = ""
        if dependencies_content:
            dependencies_context = "This file depends on the following files:\n\n"
            
            for dep_path, content in dependencies_content.items():
                # Limit content size to avoid token limits
                if len(content) > 1000:
                    content = content[:1000] + "\n... (truncated)"
                
                dependencies_context += f"File: {dep_path}\n```\n{content}\n```\n\n"
        
        prompt = f"""
You are an expert software developer working on a {project.project_type} project named "{project.name}".
{project.description}
You need to create the file "{file.path}" with the following purpose:
{file.purpose}
{language_hints}
The project has the following overall structure:
{project.structure_explanation}
{dependencies_context}
Generate only the code for this file. The code should be well-structured, properly formatted, and follow best practices for its language. Include appropriate comments and documentation.
Only return the file content, nothing else.
"""
        return prompt

    def _extract_code_from_response(self, response: str, file_path: str) -> str:
        """
        Extract code from the AI response.
        
        Args:
            response: AI response text
            file_path: Path of the file being generated
            
        Returns:
            Extracted code content
        """
        # Try to extract code from markdown code blocks
        code_match = re.search(r'```(?:\w+)?\s*(.*?)\s*```', response, re.DOTALL)
        if code_match:
            return code_match.group(1)
        
        # No code block found, use the entire response
        return response.strip()

    def _build_dependency_graph(self, files: List[CodeFile]) -> Dict[str, Set[str]]:
        """
        Build a dependency graph for the files.
        
        Args:
            files: List of files to process
            
        Returns:
            Dictionary mapping file paths to sets of dependent file paths
        """
        # Map file paths to indices
        path_to_index = {file.path: i for i, file in enumerate(files)}
        
        # Initialize the graph
        graph = {}
        for file in files:
            graph[file.path] = set()
            for dep_path in file.dependencies:
                if dep_path in path_to_index:
                    graph[file.path].add(dep_path)
        
        return graph

    def _get_ordered_files(
        self, 
        files: List[CodeFile], 
        graph: Dict[str, Set[str]]
    ) -> List[CodeFile]:
        """
        Get files in dependency order (topological sort).
        
        Args:
            files: List of files to order
            graph: Dependency graph
            
        Returns:
            Ordered list of files
        """
        # Map file paths to objects
        path_to_file = {file.path: file for file in files}
        
        # Keep track of visited and ordered nodes
        visited = set()
        ordered = []
        
        def visit(path):
            """DFS visit function for topological sort."""
            if path in visited:
                return
            
            visited.add(path)
            
            # Visit dependencies first
            for dep_path in graph.get(path, set()):
                visit(dep_path)
            
            # Add to ordered list
            if path in path_to_file:
                ordered.append(path_to_file[path])
        
        # Visit all nodes
        for file in files:
            visit(file.path)
        
        return ordered

    def _create_file_batches(
        self, 
        files: List[CodeFile], 
        graph: Dict[str, Set[str]]
    ) -> List[List[CodeFile]]:
        """
        Create batches of files that can be generated concurrently.
        
        Args:
            files: List of files to batch
            graph: Dependency graph
            
        Returns:
            List of file batches
        """
        # Get files in dependency order
        ordered_files = self._get_ordered_files(files, graph)
        
        # Group files by their dependency level
        levels = {}
        path_to_level = {}
        
        # Compute the dependency level for each file
        for file in ordered_files:
            # The level is 1 + the maximum level of dependencies
            max_dep_level = 0
            for dep_path in file.dependencies:
                if dep_path in path_to_level:
                    max_dep_level = max(max_dep_level, path_to_level[dep_path])
            
            level = max_dep_level + 1
            path_to_level[file.path] = level
            
            if level not in levels:
                levels[level] = []
            
            levels[level].append(file)
        
        # Create batches from levels
        batches = []
        for level in sorted(levels.keys()):
            batches.append(levels[level])
        
        return batches

    async def _validate_project_plan(
        self, 
        project: CodeProject
    ) -> Tuple[bool, List[str]]:
        """
        Validate a project plan for consistency.
        
        Args:
            project: CodeProject to validate
            
        Returns:
            Tuple of (is_valid, list_of_errors)
        """
        errors = []
        
        # Check for duplicate file paths
        paths = [file.path for file in project.files]
        if len(paths) != len(set(paths)):
            duplicate_paths = [path for path in paths if paths.count(path) > 1]
            errors.append(f"Duplicate file paths: {set(duplicate_paths)}")
        
        # Check for circular dependencies
        try:
            graph = self._build_dependency_graph(project.files)
            self._get_ordered_files(project.files, graph)
        except Exception as e:
            errors.append(f"Circular dependencies detected: {str(e)}")
        
        # Check for missing dependencies
        path_set = set(paths)
        for file in project.files:
            for dep_path in file.dependencies:
                if dep_path not in path_set:
                    errors.append(f"File {file.path} depends on non-existent file {dep_path}")
        
        return len(errors) == 0, errors

    async def _fix_validation_errors(
        self, 
        project: CodeProject, 
        errors: List[str]
    ) -> CodeProject:
        """
        Try to fix validation errors in a project plan.
        
        Args:
            project: The project plan to fix
            errors: List of validation errors
            
        Returns:
            Fixed CodeProject
        """
        self._logger.info(f"Attempting to fix {len(errors)} validation errors")
        
        # Handle duplicate paths
        if any("Duplicate file paths" in error for error in errors):
            # Create a map of paths to files
            path_to_files = {}
            for file in project.files:
                if file.path not in path_to_files:
                    path_to_files[file.path] = []
                path_to_files[file.path].append(file)
            
            # Keep only the first instance of each duplicate
            new_files = []
            for path, files in path_to_files.items():
                new_files.append(files[0])
            
            project.files = new_files
        
        # Handle circular dependencies
        if any("Circular dependencies" in error for error in errors):
            # Remove dependencies that create cycles
            graph = {}
            for file in project.files:
                graph[file.path] = set(file.dependencies)
            
            # Find and break cycles
            visited = set()
            path = []
            
            def find_cycles(node):
                if node in path:
                    # Cycle detected, break it
                    cycle_start = path.index(node)
                    cycle = path[cycle_start:] + [node]
                    
                    # Remove the last dependency in the cycle
                    source = cycle[-2]
                    target = cycle[-1]
                    for file in project.files:
                        if file.path == source:
                            if target in file.dependencies:
                                file.dependencies.remove(target)
                                self._logger.debug(f"Removed dependency from {source} to {target} to break cycle")
                    
                    return True
                
                if node in visited:
                    return False
                
                visited.add(node)
                path.append(node)
                
                for neighbor in graph.get(node, set()):
                    if find_cycles(neighbor):
                        return True
                
                path.pop()
                return False
            
            # Find and fix all cycles
            for node in list(graph.keys()):
                while find_cycles(node):
                    visited = set()
                    path = []
            
        # Handle missing dependencies
        if any("depends on non-existent file" in error for error in errors):
            # Remove dependencies that don't exist
            valid_paths = {file.path for file in project.files}
            for file in project.files:
                file.dependencies = [dep for dep in file.dependencies if dep in valid_paths]
        
        return project

    async def add_feature_to_project(
        self, 
        description: str, 
        project_dir: Union[str, Path],
        context: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Add a new feature to an existing project.
        
        Args:
            description: Natural language description of the feature to add
            project_dir: Path to the project directory
            context: Additional context information
            
        Returns:
            Dictionary with information about the added feature
        """
        self._logger.info(f"Adding feature to project: {description}")
        
        # Get context if not provided
        if context is None:
            context_manager = get_context_manager()
            context_enhancer = get_context_enhancer()
            context = context_manager.get_context_dict()
            context = await context_enhancer.enrich_context(context)
        
        # Convert to Path object
        project_path = Path(project_dir)
        
        # Step 1: Analyze existing project structure
        project_analysis = await self._analyze_existing_project(project_path, context)
        project_type = project_analysis.get("project_type")
        
        if not project_type:
            self._logger.error("Could not determine project type")
            return {
                "success": False,
                "error": "Could not determine project type",
                "project_dir": str(project_path)
            }
        
        # Step 2: Generate feature plan based on description and existing project
        feature_plan = await self._generate_feature_plan(description, project_analysis, context)
        
        # Step 3: Generate file contents for new/modified files
        feature_files = await self._generate_feature_files(feature_plan, project_analysis, context)
        
        # Step 4: Apply changes to the project
        result = await self._apply_feature_changes(feature_files, project_path)
        
        return {
            "success": result.get("success", False),
            "description": description,
            "project_type": project_type,
            "new_files": result.get("created_files", []),
            "modified_files": result.get("modified_files", []),
            "errors": result.get("errors", []),
            "project_dir": str(project_path)
        }

    async def _analyze_existing_project(
        self, 
        project_path: Path, 
        context: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Analyze an existing project structure.
        
        Args:
            project_path: Path to the project directory
            context: Context information
            
        Returns:
            Dictionary with project analysis information
        """
        self._logger.info(f"Analyzing existing project structure at {project_path}")
        
        # Get project information from context if available
        project_info = {}
        
        if "enhanced_project" in context:
            project_info = {
                "project_type": context["enhanced_project"].get("type", "unknown"),
                "frameworks": context["enhanced_project"].get("frameworks", {}),
                "dependencies": context["enhanced_project"].get("dependencies", {})
            }
        
        # If project type is unknown or not in context, detect it
        if project_info.get("project_type") == "unknown" or not project_info:
            # Import through API layer
            from angela.api.toolchain import get_ci_cd_integration
            ci_cd_integration = get_ci_cd_integration()
            detection_result = await ci_cd_integration.detect_project_type(project_path)
            project_info["project_type"] = detection_result.get("project_type")
        
        # Get file structure
        files = []
        for root, _, filenames in os.walk(project_path):
            for filename in filenames:
                # Skip common directories to ignore
                if any(ignored in root for ignored in [".git", "__pycache__", "node_modules", "venv"]):
                    continue
                    
                file_path = Path(root) / filename
                rel_path = file_path.relative_to(project_path)
                
                # Get basic file info
                file_info = {
                    "path": str(rel_path),
                    "full_path": str(file_path),
                    "type": None,
                    "language": None,
                    "content": None
                }
                
                # Try to determine file type and language
                try:
                    file_detector = get_file_detector()
                    type_info = file_detector.detect_file_type(file_path)
                    file_info["type"] = type_info.get("type")
                    file_info["language"] = type_info.get("language")
                    
                    # Read content for source code files (limit to prevent memory issues)
                    if type_info.get("type") == "source_code" and file_path.stat().st_size < 100000:
                        with open(file_path, 'r', encoding='utf-8', errors='replace') as f:
                            file_info["content"] = f.read()
                except Exception as e:
                    self._logger.debug(f"Error analyzing file {file_path}: {str(e)}")
                
                files.append(file_info)
        
        # Add files to project info
        project_info["files"] = files
        project_info["main_files"] = []
        
        # Try to identify important files based on project type
        if project_info.get("project_type") == "python":
            # Look for main Python files
            for file_info in files:
                if file_info["path"].endswith(".py"):
                    if any(name in file_info["path"].lower() for name in ["main", "app", "index", "server"]):
                        project_info["main_files"].append(file_info["path"])
        elif project_info.get("project_type") == "node":
            # Look for main JavaScript/TypeScript files
            for file_info in files:
                if file_info["path"].endswith((".js", ".ts", ".jsx", ".tsx")):
                    if any(name in file_info["path"].lower() for name in ["main", "app", "index", "server"]):
                        project_info["main_files"].append(file_info["path"])
        
        return project_info

    async def _generate_feature_plan(
        self, 
        description: str,
        project_analysis: Dict[str, Any],
        context: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Generate a plan for adding a feature to the project.
        
        Args:
            description: Feature description
            project_analysis: Analysis of the existing project
            context: Context information
            
        Returns:
            Dictionary with the feature plan
        """
        self._logger.info(f"Generating feature plan for: {description}")
        
        project_type = project_analysis.get("project_type", "unknown")
        
        # Build a prompt for the AI to generate a feature plan
        prompt = f"""
You are an expert software developer tasked with planning how to add a new feature to an existing {project_type} project.

Feature description: "{description}"

Based on the existing project structure, determine:
1. What new files need to be created
2. What existing files need to be modified
3. How the new feature integrates with the existing codebase

Project Information:
- Project Type: {project_type}
- Main Files: {project_analysis.get('main_files', [])}
- Frameworks: {project_analysis.get('frameworks', {})}

Project Structure:
"""
        
        # Add information about existing files
        files_by_type = {}
        for file_info in project_analysis.get("files", []):
            file_type = file_info.get("type", "unknown")
            if file_type not in files_by_type:
                files_by_type[file_type] = []
            files_by_type[file_type].append(file_info["path"])
        
        for file_type, files in files_by_type.items():
            prompt += f"\n{file_type.upper()} FILES:\n"
            for file_path in files[:10]:  # Limit to 10 files per type to avoid token limits
                prompt += f"- {file_path}\n"
            if len(files) > 10:
                prompt += f"- ... and {len(files) - 10} more {file_type} files\n"
        
        # Add content of main files to give context
        prompt += "\nMain File Contents:\n"
        for file_path in project_analysis.get("main_files", [])[:3]:  # Limit to 3 main files
            for file_info in project_analysis.get("files", []):
                if file_info["path"] == file_path and file_info.get("content"):
                    content = file_info["content"]
                    if len(content) > 1000:  # Limit content size
                        content = content[:1000] + "\n... (truncated)"
                    prompt += f"\nFile: {file_path}\n```\n{content}\n```\n"
        
        prompt += """
Provide your response as a JSON object with this structure:
```json
{
  "new_files": [
    {
      "path": "relative/path/to/file.ext",
      "purpose": "description of the file's purpose",
      "content_template": "template for file content with {{placeholders}}",
      "language": "programming language"
    }
  ],
  "modified_files": [
    {
      "path": "relative/path/to/existing/file.ext",
      "purpose": "description of the modifications",
      "modifications": [
        {
          "type": "add_import",
          "content": "import statement to add",
          "line": 0
        },
        {
          "type": "add_function",
          "content": "function to add",
          "after": "existing function or pattern"
        },
        {
          "type": "replace",
          "search": "code to search for",
          "replace": "replacement code"
        }
      ]
    }
  ],
  "integration_points": [
    "Description of how the feature integrates with existing code"
  ]
}
```
Focus on creating a clean, maintainable implementation that follows the project's existing patterns and best practices.
"""
        
        # Call AI service
        api_request = GeminiRequest(
            prompt=prompt,
            max_tokens=8000,
            temperature=0.2
        )
        
        self._logger.debug("Sending feature plan request to AI service")
        response = await gemini_client.generate_text(api_request)
        
        # Parse the response
        plan = await self._parse_feature_plan(response.text)
        
        return plan

    async def _parse_feature_plan(self, response: str) -> Dict[str, Any]:
        """
        Parse the AI response to extract the feature plan.
        
        Args:
            response: AI response text
            
        Returns:
            Dictionary with the feature plan
        """
        # Look for JSON block in the response
        try:
            json_match = re.search(r'```(?:json)?\s*(.*?)\s*```', response, re.DOTALL)
            if json_match:
                json_str = json_match.group(1)
            else:
                # Try to find JSON without code blocks
                json_match = re.search(r'({.*})', response, re.DOTALL)
                if json_match:
                    json_str = json_match.group(1)
                else:
                    # Assume the entire response is JSON
                    json_str = response
            
            # Parse the JSON
            plan_data = json.loads(json_str)
            
            return plan_data
        
        except Exception as e:
            self._logger.exception(f"Error parsing feature plan: {str(e)}")
            
            # Return a minimal fallback plan
            return {
                "new_files": [],
                "modified_files": [],
                "integration_points": [
                    "Unable to parse AI response. Consider providing more specific feature description."
                ]
            }

    async def _generate_feature_files(
        self, 
        feature_plan: Dict[str, Any],
        project_analysis: Dict[str, Any],
        context: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Generate content for new files and modifications for existing files.
        
        Args:
            feature_plan: Feature plan from _generate_feature_plan
            project_analysis: Analysis of the existing project
            context: Context information
            
        Returns:
            Dictionary with file contents and modifications
        """
        self._logger.info("Generating feature file contents")
        
        feature_files = {
            "new_files": [],
            "modified_files": []
        }
        
        # Process new files
        for file_info in feature_plan.get("new_files", []):
            # Generate content for the new file
            content = await self._generate_new_file_content(
                file_info,
                project_analysis,
                feature_plan,
                context
            )
            
            feature_files["new_files"].append({
                "path": file_info["path"],
                "content": content,
                "purpose": file_info.get("purpose", "")
            })
        
        # Process modified files
        for file_info in feature_plan.get("modified_files", []):
            # Get original content
            original_content = self._get_file_content(file_info["path"], project_analysis)
            
            if original_content is None:
                self._logger.warning(f"Could not find content for file: {file_info['path']}")
                continue
            
            # Apply modifications
            modified_content = await self._apply_file_modifications(
                original_content,
                file_info.get("modifications", []),
                file_info,
                project_analysis,
                feature_plan,
                context
            )
            
            feature_files["modified_files"].append({
                "path": file_info["path"],
                "original_content": original_content,
                "modified_content": modified_content,
                "purpose": file_info.get("purpose", "")
            })
        
        return feature_files

    def _get_file_content(self, file_path: str, project_analysis: Dict[str, Any]) -> Optional[str]:
        """
        Get the content of a file from the project analysis.
        
        Args:
            file_path: Path to the file
            project_analysis: Analysis of the existing project
            
        Returns:
            File content or None if not found
        """
        for file_info in project_analysis.get("files", []):
            if file_info["path"] == file_path:
                return file_info.get("content")
        return None

    async def _generate_new_file_content(
        self, 
        file_info: Dict[str, Any],
        project_analysis: Dict[str, Any],
        feature_plan: Dict[str, Any],
        context: Dict[str, Any]
    ) -> str:
        """
        Generate content for a new file.
        
        Args:
            file_info: Information about the new file
            project_analysis: Analysis of the existing project
            feature_plan: Overall feature plan
            context: Context information
            
        Returns:
            Generated file content
        """
        self._logger.debug(f"Generating content for new file: {file_info['path']}")
        
        # Get template if provided
        template = file_info.get("content_template", "")
        
        # If template has placeholders, we should fill them in
        # This is simplified; in a real implementation you would have more context
        if template and "{{" in template:
            # Process template with placeholders
            # This is just a simple example
            template = template.replace("{{project_type}}", project_analysis.get("project_type", ""))
        
        # If template is not provided or is minimal, generate content with AI
        if len(template.strip()) < 50:  # Arbitrary threshold
            # Build prompt for file generation
            prompt = f"""
Generate the content for a new file in a {project_analysis.get('project_type', 'unknown')} project.

File path: {file_info['path']}
File purpose: {file_info.get('purpose', 'Unknown')}

This file is part of a new feature described as:
{feature_plan.get('integration_points', ['Unknown'])[0] if feature_plan.get('integration_points') else 'Unknown'}

The project already has files like:
"""
            # Add a few relevant existing files for context
            file_extension = Path(file_info['path']).suffix
            for existing_file in project_analysis.get("files", [])[:5]:
                if existing_file.get("path", "").endswith(file_extension):
                    prompt += f"- {existing_file['path']}\n"
            
            # Add content of a similar file for style reference
            similar_files = [f for f in project_analysis.get("files", []) 
                            if f.get("path", "").endswith(file_extension) and f.get("content")]
            
            if similar_files:
                similar_file = similar_files[0]
                content = similar_file.get("content", "")
                if len(content) > 1000:  # Limit content size
                    content = content[:1000] + "\n... (truncated)"
                prompt += f"\nReference file ({similar_file['path']}) for style consistency:\n"
                prompt += f"```\n{content}\n```\n"
            
            prompt += "\nGenerate the complete content for the new file, following the project's style and conventions."
            
            # Call AI service
            api_request = GeminiRequest(
                prompt=prompt,
                max_tokens=4000,
                temperature=0.2
            )
            
            response = await gemini_client.generate_text(api_request)
            
            # Extract code from the response
            content = self._extract_code_from_response(response.text, file_info['path'])
            return content
        
        return template

    async def _apply_file_modifications(
        self, 
        original_content: str,
        modifications: List[Dict[str, Any]],
        file_info: Dict[str, Any],
        project_analysis: Dict[str, Any],
        feature_plan: Dict[str, Any],
        context: Dict[str, Any]
    ) -> str:
        """
        Apply modifications to an existing file.
        
        Args:
            original_content: Original file content
            modifications: List of modifications to apply
            file_info: Information about the file
            project_analysis: Analysis of the existing project
            feature_plan: Overall feature plan
            context: Context information
            
        Returns:
            Modified file content
        """
        self._logger.debug(f"Applying modifications to file: {file_info['path']}")
        
        modified_content = original_content
        
        # Apply each modification in sequence
        for mod in modifications:
            mod_type = mod.get("type", "")
            
            if mod_type == "add_import":
                # Add import statement at the top
                import_stmt = mod.get("content", "")
                if import_stmt:
                    # Find where imports end
                    lines = modified_content.splitlines()
                    import_end_line = 0
                    
                    # Look for existing imports
                    for i, line in enumerate(lines):
                        if line.strip().startswith(("import ", "from ")):
                            import_end_line = i + 1
                    
                    # Insert import at the right position
                    lines.insert(import_end_line, import_stmt)
                    modified_content = "\n".join(lines)
            
            elif mod_type == "add_function":
                # Add function/method to the file
                function_content = mod.get("content", "")
                after_pattern = mod.get("after", "")
                
                if function_content:
                    if after_pattern and after_pattern in modified_content:
                        # Insert after specific pattern
                        parts = modified_content.split(after_pattern, 1)
                        modified_content = parts[0] + after_pattern + "\n\n" + function_content + "\n" + parts[1]
                    else:
                        # Append to the end of the file
                        if not modified_content.endswith("\n"):
                            modified_content += "\n"
                        modified_content += "\n" + function_content + "\n"
            
            elif mod_type == "replace":
                # Replace text in the file
                search_text = mod.get("search", "")
                replace_text = mod.get("replace", "")
                
                if search_text and replace_text:
                    modified_content = modified_content.replace(search_text, replace_text)
        
        # If no modifications were applied successfully or instructions were unclear,
        # use AI to apply the modifications
        if modified_content == original_content:
            modified_content = await self._generate_file_modifications_with_ai(
                original_content,
                file_info,
                project_analysis,
                feature_plan,
                context
            )
        
        return modified_content

    async def _generate_file_modifications_with_ai(
        self, 
        original_content: str,
        file_info: Dict[str, Any],
        project_analysis: Dict[str, Any],
        feature_plan: Dict[str, Any],
        context: Dict[str, Any]
    ) -> str:
        """
        Use AI to generate modifications for a file when structured modifications fail.
        
        Args:
            original_content: Original file content
            file_info: Information about the file
            project_analysis: Analysis of the existing project
            feature_plan: Overall feature plan
            context: Context information
            
        Returns:
            Modified file content
        """
        self._logger.debug(f"Using AI to generate modifications for: {file_info['path']}")
        
        # Build prompt for generating modifications
        prompt = f"""
Modify the content of an existing file in a {project_analysis.get('project_type', 'unknown')} project to implement a new feature.

File path: {file_info['path']}
Modification purpose: {file_info.get('purpose', 'Unknown')}

This modification is part of a new feature described as:
{feature_plan.get('integration_points', ['Unknown'])[0] if feature_plan.get('integration_points') else 'Unknown'}

Original file content:
```
{original_content}
```

Your task is to modify this file to implement the specified feature. Return the complete modified content.
Follow the project's existing coding style and patterns.
"""
        
        # Call AI service
        api_request = GeminiRequest(
            prompt=prompt,
            max_tokens=8000,
            temperature=0.2
        )
        
        response = await gemini_client.generate_text(api_request)
        
        # Extract code from the response
        modified_content = self._extract_code_from_response(response.text, file_info['path'])
        return modified_content

    async def _apply_feature_changes(
        self, 
        feature_files: Dict[str, Any],
        project_path: Path
    ) -> Dict[str, Any]:
        """
        Apply the generated feature changes to the project.
        
        Args:
            feature_files: Generated file contents and modifications
            project_path: Path to the project directory
            
        Returns:
            Dictionary with application results
        """
        self._logger.info("Applying feature changes to project")
        
        result = {
            "success": True,
            "created_files": [],
            "modified_files": [],
            "errors": []
        }
        
        # Get filesystem functions
        filesystem = get_filesystem_functions()
        
        # Create new files
        for file_info in feature_files.get("new_files", []):
            file_path = project_path / file_info["path"]
            
            try:
                # Create parent directories if needed
                await filesystem.create_directory(file_path.parent, parents=True)
                
                # Write file content
                await filesystem.write_file(file_path, file_info["content"])
                
                result["created_files"].append(str(file_path))
                self._logger.debug(f"Created new file: {file_path}")
            except Exception as e:
                self._logger.error(f"Error creating file {file_path}: {str(e)}")
                result["errors"].append({"path": str(file_path), "error": str(e)})
                result["success"] = False
        
        # Modify existing files
        for file_info in feature_files.get("modified_files", []):
            file_path = project_path / file_info["path"]
            
            try:
                # Check if file exists
                if not file_path.exists():
                    raise FileNotFoundError(f"File not found: {file_path}")
                
                # Write modified content
                await filesystem.write_file(file_path, file_info["modified_content"])
                
                result["modified_files"].append(str(file_path))
                self._logger.debug(f"Modified file: {file_path}")
            except Exception as e:
                self._logger.error(f"Error modifying file {file_path}: {str(e)}")
                result["errors"].append({"path": str(file_path), "error": str(e)})
                result["success"] = False
        
        return result

    async def _extract_dependencies_from_feature(
        self, 
        feature_files: Dict[str, Any],
        project_type: str
    ) -> Dict[str, List[str]]:
        """
        Extract dependencies from newly added or modified feature files.
        
        Args:
            feature_files: Dictionary with new and modified files
            project_type: Type of the project (python, node, etc.)
            
        Returns:
            Dictionary with runtime and development dependencies
        """
        self._logger.info("Extracting dependencies from feature files")
        
        dependencies = {
            "runtime": [],
            "development": []
        }
        
        # Process based on project type
        if project_type == "python":
            # Look for Python import statements in new files
            for file_info in feature_files.get("new_files", []):
                if file_info["path"].endswith(".py"):
                    content = file_info["content"]
                    
                    # Extract import statements using regex
                    import_lines = re.findall(r'^(?:import|from)\s+([^\s.]+)(?:\.|.*?import)', content, re.MULTILINE)
                    
                    # Filter out standard library modules
                    for module in import_lines:
                        if module not in sys.modules or (hasattr(sys.modules[module], '__file__') and 
                                                        sys.modules[module].__file__ and 
                                                        'site-packages' in sys.modules[module].__file__):
                            if module not in dependencies["runtime"] and module not in ["__future__", "typing", "os", "sys", "re", "json", "time", "datetime"]:
                                dependencies["runtime"].append(module)
            
            # Also check modified files for new imports
            for file_info in feature_files.get("modified_files", []):
                if file_info["path"].endswith(".py"):
                    original_content = file_info["original_content"]
                    modified_content = file_info["modified_content"]
                    
                    # Extract imports from original and modified content
                    original_imports = set(re.findall(r'^(?:import|from)\s+([^\s.]+)(?:\.|.*?import)', original_content, re.MULTILINE))
                    modified_imports = set(re.findall(r'^(?:import|from)\s+([^\s.]+)(?:\.|.*?import)', modified_content, re.MULTILINE))
                    
                    # Find new imports
                    new_imports = modified_imports - original_imports
                    
                    # Add to dependencies list
                    for module in new_imports:
                        if module not in dependencies["runtime"] and module not in ["__future__", "typing", "os", "sys", "re", "json", "time", "datetime"]:
                            dependencies["runtime"].append(module)
                            
        elif project_type == "node":
            # Look for import/require statements in JavaScript/TypeScript files
            js_extensions = [".js", ".jsx", ".ts", ".tsx"]
            
            for file_info in feature_files.get("new_files", []):
                if any(file_info["path"].endswith(ext) for ext in js_extensions):
                    content = file_info["content"]
                    
                    # Look for ES6 imports
                    es6_imports = re.findall(r'import\s+.*?from\s+[\'"]([^\'".][^\'"]*)[\'"]\s*;?', content)
                    
                    # Look for require statements
                    require_imports = re.findall(r'(?:const|let|var)\s+.*?=\s+require\([\'"]([^\'".][^\'"]*)[\'"]\)\s*;?', content)
                    
                    # Combine all found imports
                    all_imports = es6_imports + require_imports
                    
                    # Filter out relative imports
                    for module in all_imports:
                        if not module.startswith(".") and module not in dependencies["runtime"]:
                            dependencies["runtime"].append(module)
            
            # Also check modified files
            for file_info in feature_files.get("modified_files", []):
                if any(file_info["path"].endswith(ext) for ext in js_extensions):
                    original_content = file_info["original_content"]
                    modified_content = file_info["modified_content"]
                    
                    # Extract imports from original content
                    original_es6 = set(re.findall(r'import\s+.*?from\s+[\'"]([^\'".][^\'"]*)[\'"]\s*;?', original_content))
                    original_require = set(re.findall(r'(?:const|let|var)\s+.*?=\s+require\([\'"]([^\'".][^\'"]*)[\'"]\)\s*;?', original_content))
                    original_imports = original_es6.union(original_require)
                    
                    # Extract imports from modified content
                    modified_es6 = set(re.findall(r'import\s+.*?from\s+[\'"]([^\'".][^\'"]*)[\'"]\s*;?', modified_content))
                    modified_require = set(re.findall(r'(?:const|let|var)\s+.*?=\s+require\([\'"]([^\'".][^\'"]*)[\'"]\)\s*;?', modified_content))
                    modified_imports = modified_es6.union(modified_require)
                    
                    # Find new imports
                    new_imports = modified_imports - original_imports
                    
                    # Add to dependencies list
                    for module in new_imports:
                        if not module.startswith(".") and module not in dependencies["runtime"]:
                            dependencies["runtime"].append(module)
        
        # Add common testing libraries based on project type
        if project_type == "python":
            dependencies["development"] = ["pytest", "pytest-cov"]
        elif project_type == "node":
            dependencies["development"] = ["jest", "eslint"]
        
        return dependencies

    async def generate_complex_project(
        self, 
        description: str, 
        output_dir: Optional[str] = None,
        project_type: Optional[str] = None,
        framework: Optional[str] = None,
        use_detailed_planning: bool = True,
        context: Optional[Dict[str, Any]] = None
    ) -> CodeProject:
        """
        Generate a complex multi-file project with enhanced architecture planning.
        
        Args:
            description: Natural language description of the project
            output_dir: Directory where the project should be generated (defaults to cwd)
            project_type: Optional type of project to generate (auto-detected if None)
            framework: Optional framework to use (e.g., 'react', 'django')
            use_detailed_planning: Whether to use detailed architecture planning
            context: Additional context information
            
        Returns:
            CodeProject object representing the generated project
        """
        self._logger.info(f"Generating complex project from description: {description}")
        
        start_time = time.time()
        
        # Get current context if not provided
        if context is None:
            context_manager = get_context_manager()
            context_enhancer = get_context_enhancer()
            context = context_manager.get_context_dict()
            context = await context_enhancer.enrich_context(context)
        
        # Determine output directory
        if output_dir is None:
            output_dir = context.get("cwd", os.getcwd())
        
        # Determine project type if not specified
        if project_type is None:
            project_type = await self._infer_project_type(description, context)
            self._logger.debug(f"Inferred project type: {project_type}")
        
        # Get project name from description
        project_name = await self._extract_project_name(description, project_type)
        self._logger.debug(f"Project name: {project_name}")
        
        # Determine framework if not specified
        if framework is None and project_type in ["python", "node", "java"]:
            framework = await self._infer_framework(description, project_type)
            self._logger.debug(f"Inferred framework: {framework}")
        
        # Get project planner through the API layer
        from angela.api.generation import get_project_planner, get_project_architecture_class
        project_planner = get_project_planner()
        ProjectArchitecture = get_project_architecture_class()
        
        # Create a detailed architecture if requested
        if use_detailed_planning:
            # Generate a detailed architecture
            architecture = await project_planner.create_detailed_project_architecture(
                description=description,
                project_type=project_type,
                framework=framework,
                context=context
            )
            
            self._logger.info(f"Created detailed architecture with {len(architecture.components)} components")
            
            # Generate dependency graph for visualization
            dependency_graph = await project_planner.generate_dependency_graph(architecture)
            
            # Create project plan from architecture
            project_plan = await project_planner.create_project_plan_from_architecture(
                architecture=architecture,
                project_name=project_name,
                project_type=project_type,
                description=description,
                framework=framework,
                context=context
            )
        else:
            # Use the standard project planning approach
            project_plan = await self._create_project_plan(
                description=description,
                output_dir=output_dir,
                project_type=project_type,
                context=context
            )
        
        self._logger.info(f"Created project plan with {len(project_plan.files)} files")
        
        # Analyze project structure to understand relationships between files
        generation_context_manager = get_generation_context_manager()
        analysis_result = await generation_context_manager.analyze_code_relationships(project_plan.files)
        self._logger.debug(f"Code relationship analysis complete: {analysis_result}")
        
        # Generate file contents with enhanced context
        project_plan = await self._generate_complex_file_contents(project_plan, context)
        
        # Log generation time
        elapsed_time = time.time() - start_time
        self._logger.info(f"Project generation completed in {elapsed_time:.2f} seconds")
        
        return project_plan
    
    async def _extract_project_name(self, description: str, project_type: str) -> str:
        """
        Extract a project name from the description.
        
        Args:
            description: Project description
            project_type: Type of project
            
        Returns:
            Project name
        """
        # Build prompt for project name extraction
        prompt = f"""
Extract a concise, appropriate project name from this description for a {project_type} project:

"{description}"

Return ONLY the project name as a single phrase, nothing else.
The name should be concise, memorable, and related to the project's purpose.
"""
        
        # Call AI service
        api_request = GeminiRequest(
            prompt=prompt,
            max_tokens=20,  # Very small limit since we only need the name
            temperature=0.2
        )
        
        response = await gemini_client.generate_text(api_request)
        
        # Clean up the response
        name = response.text.strip().strip('"\'').strip()
        
        # If the name is too long, truncate it
        if len(name) > 50:
            name = name[:50]
        
        # Replace spaces with underscores and remove special characters
        name = re.sub(r'[^a-zA-Z0-9 _-]', '', name)
        
        # If extraction failed, use a default name
        if not name:
            name = f"new_{project_type}_project"
        
        return name
    
    async def _infer_framework(self, description: str, project_type: str) -> Optional[str]:
        """
        Infer the framework from the description.
        
        Args:
            description: Project description
            project_type: Type of project
            
        Returns:
            Framework name or None
        """
        # Build prompt for framework inference
        prompt = f"""
Determine the most appropriate framework for this {project_type} project:

"{description}"

Consider only the most popular, mainstream frameworks for {project_type}.
For Python, consider: Django, Flask, FastAPI
For Node.js, consider: Express, React, Vue, Angular, Next.js
For Java, consider: Spring Boot, Jakarta EE, Quarkus

Return ONLY the framework name, nothing else.
If no specific framework is clearly indicated, return "None".
"""
        
        # Call AI service
        api_request = GeminiRequest(
            prompt=prompt,
            max_tokens=20,  # Very small limit since we only need the framework name
            temperature=0.2
        )
        
        response = await gemini_client.generate_text(api_request)
        
        # Clean up the response
        framework = response.text.strip().strip('"\'').strip()
        
        # Check if a framework was identified
        if framework.lower() in ["none", "no framework", "unknown"]:
            return None
        
        return framework
    
    async def _generate_complex_file_contents(
        self, 
        project: CodeProject,
        context: Dict[str, Any]
    ) -> CodeProject:
        """
        Generate content for each file in the project with enhanced context awareness.
        
        Args:
            project: CodeProject with file information
            context: Additional context information
            
        Returns:
            Updated CodeProject with file contents
        """
        self._logger.info(f"Generating content for {len(project.files)} files with enhanced context")
        
        # Process files in dependency order
        dependency_graph = self._build_dependency_graph(project.files)
        
        # Prepare batches of files to generate
        batches = self._create_file_batches(project.files, dependency_graph)
        
        # Generate content for each batch
        for batch_idx, batch in enumerate(batches):
            self._logger.debug(f"Processing batch {batch_idx+1}/{len(batches)} with {len(batch)} files")
            
            # Generate content for each file in the batch concurrently
            tasks = []
            for file in batch:
                # Get files that this file depends on
                dependencies = self._get_dependency_files(file, project.files, dependency_graph)
                
                task = self._generate_complex_file_content(
                    file, 
                    project, 
                    dependencies,
                    context
                )
                tasks.append(task)
            
            # Wait for all tasks in this batch to complete
            results = await asyncio.gather(*tasks)
            
            # Update file contents
            for file, content in zip(batch, results):
                file.content = content
                
                # Register file in generation context manager
                await generation_context_manager.extract_entities_from_file(file)
        
        return project
    
    def _get_dependency_files(
        self, 
        file: CodeFile, 
        all_files: List[CodeFile],
        dependency_graph: Dict[str, Set[str]]
    ) -> List[CodeFile]:
        """
        Get the files that this file depends on.
        
        Args:
            file: The file to get dependencies for
            all_files: All files in the project
            dependency_graph: Dependency graph
            
        Returns:
            List of dependency files
        """
        # Get dependency paths
        dep_paths = dependency_graph.get(file.path, set())
        
        # Find the corresponding files
        return [f for f in all_files if f.path in dep_paths]
    
    async def _generate_complex_file_content(
        self, 
        file: CodeFile, 
        project: CodeProject,
        dependencies: List[CodeFile],
        context: Dict[str, Any]
    ) -> str:
        """
        Generate content for a single file with enhanced context.
        
        Args:
            file: CodeFile to generate content for
            project: Parent CodeProject
            dependencies: Files this file depends on
            context: Additional context information
            
        Returns:
            Generated file content
        """
        self._logger.debug(f"Generating content for file: {file.path}")
        
        # Build prompt for file content generation
        prompt = self._build_complex_file_content_prompt(file, project, dependencies)
        
        # Enhance prompt with generation context
        generation_context_manager = get_generation_context_manager()
        related_files = [dep.path for dep in dependencies]
        enhanced_prompt = await generation_context_manager.enhance_prompt_with_context(
            prompt=prompt,
            file_path=file.path,
            related_files=related_files
        )
        
        # Set reasonable token limits based on file complexity
        max_tokens = self._determine_max_tokens_for_file(file)
        
        # Call AI service to generate file content
        gemini_client = get_gemini_client()
        api_request = GeminiRequest(
            prompt=enhanced_prompt,
            max_tokens=max_tokens,
            temperature=0.2
        )
        
        response = await gemini_client.generate_text(api_request)
        
        # Extract code from the response
        content = self._extract_code_from_response(response.text, file.path)
        
        # Validate the generated code
        is_valid, validation_message = validate_code(content, file.path)
        
        # If validation failed, try once more with the error message
        if not is_valid:
            self._logger.warning(f"Validation failed for {file.path}: {validation_message}")
            
            # Build a new prompt with the validation error
            fix_prompt = f"""
    The code you generated has an issue that needs to be fixed:
    {validation_message}
    
    Here is the original code:
    {content}
    
    Please provide the corrected code for file '{file.path}'.
    Only respond with the corrected code, nothing else.
    """
            # Call AI service to fix the code
            fix_request = GeminiRequest(
                prompt=fix_prompt,
                max_tokens=max_tokens,
                temperature=0.1
            )
            
            fix_response = await gemini_client.generate_text(fix_request)
            
            # Extract fixed code
            fixed_content = self._extract_code_from_response(fix_response.text, file.path)
            
            # Validate again
            is_valid, _ = validate_code(fixed_content, file.path)
            if is_valid:
                content = fixed_content
        
        return content
    
    def _build_complex_file_content_prompt(
        self, 
        file: CodeFile, 
        project: CodeProject,
        dependencies: List[CodeFile]
    ) -> str:
        """
        Build a prompt for generating file content with enhanced context.
        
        Args:
            file: CodeFile to generate content for
            project: Parent CodeProject
            dependencies: Files this file depends on
            
        Returns:
            Prompt string for the AI service
        """
        # Get file extension and determine language
        ext = Path(file.path).suffix.lower()
        language = file.language or self._get_language_from_extension(ext)
        
        # Determine the file's role in the project
        file_role = self._determine_file_role(file, project)
        
        # Project structure details
        structure_details = f"Project follows {project.structure_explanation}" if project.structure_explanation else ""
        
        # List dependencies with their purposes
        dependencies_context = ""
        if dependencies:
            dependencies_context = "This file depends on:\n"
            for dep in dependencies:
                dependencies_context += f"- {dep.path}: {dep.purpose}\n"
                
                # Include top sections of dependency content if available
                if dep.content:
                    # Show first 20 non-empty lines or 500 chars, whichever is less
                    preview_lines = [line for line in dep.content.split('\n') if line.strip()][:20]
                    preview = '\n'.join(preview_lines)
                    if len(preview) > 500:
                        preview = preview[:500] + "...(truncated)"
                    
                    dependencies_context += f"  Content preview:\n```\n{preview}\n```\n"
        
        prompt = f"""
You are an expert software developer working on a {project.project_type} project named "{project.name}".

PROJECT DESCRIPTION:
{project.description}

FILE DETAILS:
Path: {file.path}
Purpose: {file.purpose}
Language: {language}
Role: {file_role}

PROJECT STRUCTURE:
{structure_details}

{dependencies_context}

Your task is to generate high-quality, production-ready code for this file that:
1. Fulfills the stated purpose
2. Integrates properly with dependencies
3. Follows best practices and conventions for {language}
4. Is well-structured and commented appropriately
5. Includes proper error handling
6. Is efficient and maintainable

Generate ONLY the content for this file, nothing else.
"""
        
        # Add language-specific guidance
        if language == "python":
            prompt += """
Python-specific guidance:
- Follow PEP 8 style guidelines
- Use proper type hints
- Include docstrings for classes and functions
- Use proper exception handling
- Avoid circular imports
"""
        elif language in ["javascript", "typescript"]:
            prompt += """
JavaScript/TypeScript guidance:
- Use modern ES6+ syntax
- Handle async operations properly with async/await
- Use proper error handling
- Include JSDoc comments for functions
- Use consistent naming conventions
"""
        elif language in ["java"]:
            prompt += """
Java guidance:
- Follow standard Java conventions
- Use proper exception handling
- Include JavaDoc comments
- Use appropriate access modifiers
- Follow SOLID principles
"""
        
        # Add role-specific guidance
        if "model" in file_role.lower() or "entity" in file_role.lower():
            prompt += """
For data models/entities:
- Define clear field types and validation
- Include necessary relationships
- Add appropriate methods for data access
- Consider serialization needs
- Include proper constraints and indexes if applicable
"""
        elif "controller" in file_role.lower() or "view" in file_role.lower():
            prompt += """
For controllers/views:
- Focus on handling user input and presentation
- Keep business logic separate
- Include proper validation
- Handle errors gracefully
- Consider user experience
"""
        elif "service" in file_role.lower() or "util" in file_role.lower():
            prompt += """
For services/utilities:
- Create reusable, focused components
- Include comprehensive error handling
- Ensure thread safety if applicable
- Make functions pure when possible
- Use dependency injection where appropriate
"""
        
        return prompt
    
    def _determine_file_role(self, file: CodeFile, project: CodeProject) -> str:
        """
        Determine the role of a file in the project.
        
        Args:
            file: The file to determine role for
            project: The parent project
            
        Returns:
            String describing the file's role
        """
        path = file.path.lower()
        
        # Check based on directory/path pattern
        if "model" in path or "entity" in path or "schema" in path:
            return "Data Model/Entity"
        elif "controller" in path:
            return "Controller"
        elif "view" in path or "template" in path or "component" in path:
            return "View/UI Component"
        elif "service" in path:
            return "Service"
        elif "repository" in path or "dao" in path:
            return "Data Access"
        elif "util" in path or "helper" in path:
            return "Utility"
        elif "config" in path or "setting" in path:
            return "Configuration"
        elif "middleware" in path:
            return "Middleware"
        elif "test" in path:
            return "Test"
        elif "route" in path or "url" in path:
            return "Routing"
        elif "api" in path:
            return "API Endpoint"
        elif "__init__.py" in path:
            return "Package Initialization"
        elif any(path.endswith(ext) for ext in [".js", ".ts"]) and "index" in path:
            return "Module Entry Point"
        
        # Check based on file purpose
        purpose = file.purpose.lower()
        if "model" in purpose or "entity" in purpose or "schema" in purpose:
            return "Data Model/Entity"
        elif "controller" in purpose:
            return "Controller"
        elif "view" in purpose or "template" in purpose or "component" in purpose or "ui" in purpose:
            return "View/UI Component"
        elif "service" in purpose:
            return "Service"
        elif "repository" in purpose or "dao" in purpose:
            return "Data Access"
        
        # Default
        return "General Component"
    
    def _determine_max_tokens_for_file(self, file: CodeFile) -> int:
        """
        Determine the maximum tokens to allocate for generating a file.
        
        Args:
            file: The file to determine token limit for
            
        Returns:
            Maximum token count
        """
        # Base token limit
        base_limit = 8000
        
        # Adjust based on expected file complexity
        path = file.path.lower()
        
        # Configuration files are typically smaller
        if "config" in path or path.endswith((".json", ".yaml", ".yml", ".toml", ".ini")):
            return base_limit // 2
        
        # Main application files often need more tokens
        if "main" in path or "app" in path or "index" in path:
            return base_limit * 2
        
        # Complex components like controllers, services might need more
        if "controller" in path or "service" in path:
            return int(base_limit * 1.5)
        
        # Test files may need more tokens to properly test functionality
        if "test" in path:
            return int(base_limit * 1.25)
        
        return base_limit
    
    def _get_language_from_extension(self, ext: str) -> str:
        """
        Get language from file extension.
        
        Args:
            ext: File extension with dot
            
        Returns:
            Language name
        """
        ext_map = {
            ".py": "Python",
            ".js": "JavaScript",
            ".jsx": "JavaScript (React)",
            ".ts": "TypeScript",
            ".tsx": "TypeScript (React)",
            ".java": "Java",
            ".html": "HTML",
            ".css": "CSS",
            ".scss": "SCSS",
            ".json": "JSON",
            ".xml": "XML",
            ".yaml": "YAML",
            ".yml": "YAML",
            ".md": "Markdown",
            ".sql": "SQL",
            ".go": "Go",
            ".rs": "Rust",
            ".rb": "Ruby",
            ".php": "PHP",
            ".c": "C",
            ".cpp": "C++",
            ".h": "C/C++ Header",
            ".cs": "C#",
            ".swift": "Swift",
            ".kt": "Kotlin",
            ".vue": "Vue"
        }
        
        return ext_map.get(ext, "Unknown")
    
    async def generate_file_summaries(self, project: CodeProject) -> Dict[str, str]:
        """
        Generate concise summaries for each file in the project.
        
        Args:
            project: The CodeProject to summarize
            
        Returns:
            Dictionary mapping file paths to summaries
        """
        self._logger.info(f"Generating summaries for {len(project.files)} files")
        
        summaries = {}
        
        # Process files in batches to avoid overwhelming the API
        batch_size = 10
        for i in range(0, len(project.files), batch_size):
            batch = project.files[i:i+batch_size]
            
            # Generate summaries concurrently
            tasks = [self._generate_file_summary(file) for file in batch]
            batch_summaries = await asyncio.gather(*tasks)
            
            # Add to results
            for file, summary in zip(batch, batch_summaries):
                summaries[file.path] = summary
        
        return summaries
    
    async def _generate_file_summary(self, file: CodeFile) -> str:
        """
        Generate a concise summary for a file.
        
        Args:
            file: The file to summarize
            
        Returns:
            Summary string
        """
        # Skip if no content
        if not file.content:
            return "Empty file"
        
        # Build prompt
        prompt = f"""
Provide a concise 1-2 sentence summary of this {file.language if file.language else ''} file:
{file.content[:4000] if len(file.content) > 4000 else file.content}

{f"File content truncated due to length ({len(file.content)} chars)" if len(file.content) > 4000 else ""}

Focus on what the file does rather than how it does it. Highlight its purpose, key functionality, and role in the system.
Keep your response to 1-2 sentences maximum.
"""
        
        # Call AI service
        api_request = GeminiRequest(
            prompt=prompt,
            max_tokens=100,  # Very short summary
            temperature=0.2
        )
        
        response = await gemini_client.generate_text(api_request)
        
        # Return cleaned summary
        return response.text.strip()

code_generation_engine = CodeGenerationEngine()
</file>

<file path="components/generation/frameworks.py">
# angela/generation/frameworks.py
"""
Specialized framework generators for Angela CLI.

This module provides framework-specific code generation capabilities
for popular frameworks like React, Django, Spring, etc.
"""
import os
import asyncio
from pathlib import Path
from typing import Dict, Any, List, Optional, Union, Tuple
import json
import re
import sys

from angela.api.ai import get_gemini_client, get_gemini_request_class
from angela.utils.logging import get_logger
from angela.api.generation import get_code_file_class

logger = get_logger(__name__)
GeminiRequest = get_gemini_request_class()
CodeFile = get_code_file_class()

class FrameworkGenerator:
    """
    Generator for framework-specific code structures.
    
    This class provides specialized generation capabilities for various
    web and application frameworks, creating standardized project structures
    with appropriate files, configurations, and boilerplate code.
    """
    
    def __init__(self):
        """Initialize the framework generator with registered framework handlers."""
        self._logger = logger
        self._logger.info("Initializing FrameworkGenerator")
        
        # Register specialized framework generators
        self._framework_generators = {
            "react": self._generate_react,
            "django": self._generate_django,
            "flask": self._generate_flask,
            "spring": self._generate_spring,
            "express": self._generate_express,
            "fastapi": self._generate_fastapi,
            "vue": self._generate_vue,
            "angular": self._generate_angular
        }
        
        # Framework to project type mapping for better type inference
        self._framework_project_types = {
            "react": "node",
            "vue": "node",
            "angular": "node",
            "express": "node",
            "django": "python",
            "flask": "python",
            "fastapi": "python",
            "spring": "java",
            "rails": "ruby",
            "laravel": "php"
        }
        
        self._logger.debug(f"Registered frameworks: {', '.join(self._framework_generators.keys())}")
    
    async def generate_framework_structure(
        self, 
        framework: str,
        description: str,
        output_dir: Union[str, Path],
        options: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Generate a framework-specific project structure.
        
        Args:
            framework: Framework to generate for (e.g., "react", "django")
            description: Description of the project
            output_dir: Directory where the project should be generated
            options: Additional options for the framework (e.g., typescript, variant)
            
        Returns:
            Dictionary with generation results containing:
            - success: Whether generation was successful
            - framework: The framework that was generated
            - files: List of CodeFile objects
            - project_type: Type of project (node, python, etc.)
            - Additional framework-specific information
            
        Raises:
            ValueError: If the framework is not recognized and generic generation fails
        """
        options = options or {}
        framework = framework.lower()
        self._logger.info(f"Generating {framework} structure for: {description}")
        
        try:
            # Get the specialized generator function if available
            generator_func = self._framework_generators.get(framework)
            
            if generator_func:
                # Use specialized generator
                self._logger.debug(f"Using specialized generator for {framework}")
                return await generator_func(description, output_dir, options)
            else:
                # Fallback to generic generator
                self._logger.debug(f"No specialized generator for {framework}, using generic generator")
                return await self._generate_generic(framework, description, output_dir, options)
        except Exception as e:
            self._logger.error(f"Error generating {framework} project: {str(e)}", exc_info=True)
            return {
                "success": False,
                "framework": framework,
                "error": f"Failed to generate {framework} project: {str(e)}",
                "files": []
            }
    
    async def list_supported_frameworks(self) -> List[Dict[str, Any]]:
        """
        Get a list of supported frameworks with details.
        
        Returns:
            List of framework information dictionaries
        """
        frameworks = []
        
        # Add specialized frameworks
        for framework in self._framework_generators.keys():
            frameworks.append({
                "name": framework,
                "type": "specialized",
                "project_type": self._framework_project_types.get(framework, "unknown")
            })
        
        # We could add more supported frameworks here that would use the generic generator
        additional_frameworks = [
            {"name": "svelte", "project_type": "node"},
            {"name": "rails", "project_type": "ruby"},
            {"name": "laravel", "project_type": "php"},
            {"name": "dotnet", "project_type": "csharp"}
        ]
        
        for framework in additional_frameworks:
            if framework["name"] not in self._framework_generators:
                framework["type"] = "generic"
                frameworks.append(framework)
        
        return frameworks
    
    async def _generate_react(
        self,
        description: str,
        output_dir: Union[str, Path],
        options: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Generate a React project structure.
        
        Args:
            description: Description of the project
            output_dir: Output directory
            options: Additional options
            
        Returns:
            Dictionary with generation results
        """
        self._logger.info(f"Generating React project: {description}")
        
        # Determine React variant (Next.js, Create React App, etc.)
        variant = options.get("variant", "cra").lower()
        
        if variant == "nextjs":
            # Call Next.js generator
            return await self._generate_nextjs(description, output_dir, options)
        
        # Default: Create React App structure
        files = []
        
        # Define project structure
        structure = [
            {
                "path": "public/index.html",
                "content": await self._generate_content("react/index.html", description, options),
                "purpose": "Main HTML file",
                "language": "html"
            },
            {
                "path": "src/index.js",
                "content": await self._generate_content("react/index.js", description, options),
                "purpose": "Application entry point",
                "language": "javascript"
            },
            {
                "path": "src/App.js",
                "content": await self._generate_content("react/App.js", description, options),
                "purpose": "Main application component",
                "language": "javascript"
            },
            {
                "path": "src/App.css",
                "content": await self._generate_content("react/App.css", description, options),
                "purpose": "Application styles",
                "language": "css"
            },
            {
                "path": "package.json",
                "content": await self._generate_content("react/package.json", description, options),
                "purpose": "NPM package configuration",
                "language": "json"
            },
            {
                "path": "README.md",
                "content": await self._generate_content("react/README.md", description, options),
                "purpose": "Project documentation",
                "language": "markdown"
            }
        ]
        
        # TypeScript support
        if options.get("typescript", False):
            # Replace .js files with .tsx
            structure = [
                {
                    "path": f.get("path").replace(".js", ".tsx") if f.get("path").endswith(".js") else f.get("path"),
                    "content": await self._generate_content(f.get("path").replace(".js", ".tsx") if f.get("path").endswith(".js") else f.get("path"), description, options),
                    "purpose": f.get("purpose"),
                    "language": "typescript" if f.get("language") == "javascript" else f.get("language")
                }
                for f in structure
            ]
            
            # Add TypeScript config
            structure.append({
                "path": "tsconfig.json",
                "content": await self._generate_content("react/tsconfig.json", description, options),
                "purpose": "TypeScript configuration",
                "language": "json"
            })
        
        # Add testing setup if requested
        if options.get("testing", False):
            test_files = await self._generate_react_testing_files(description, options)
            structure.extend(test_files)
        
        # Generate files
        for file_info in structure:
            files.append(CodeFile(
                path=file_info["path"],
                content=file_info["content"],
                purpose=file_info["purpose"],
                dependencies=[],
                language=file_info["language"]
            ))
        
        return {
            "success": True,
            "framework": "react",
            "variant": variant,
            "files": files,
            "project_type": "node"
        }
    
    async def _generate_nextjs(
        self,
        description: str,
        output_dir: Union[str, Path],
        options: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Generate a Next.js project structure.
        
        Args:
            description: Description of the project
            output_dir: Output directory
            options: Additional options
            
        Returns:
            Dictionary with generation results
        """
        self._logger.info(f"Generating Next.js project: {description}")
        
        files = []
        
        # Define project structure
        structure = [
            {
                "path": "pages/index.js",
                "content": await self._generate_content("nextjs/pages/index.js", description, options),
                "purpose": "Home page component",
                "language": "javascript"
            },
            {
                "path": "pages/_app.js",
                "content": await self._generate_content("nextjs/pages/_app.js", description, options),
                "purpose": "Application wrapper component",
                "language": "javascript"
            },
            {
                "path": "styles/globals.css",
                "content": await self._generate_content("nextjs/styles/globals.css", description, options),
                "purpose": "Global styles",
                "language": "css"
            },
            {
                "path": "package.json",
                "content": await self._generate_content("nextjs/package.json", description, options),
                "purpose": "NPM package configuration",
                "language": "json"
            },
            {
                "path": "next.config.js",
                "content": await self._generate_content("nextjs/next.config.js", description, options),
                "purpose": "Next.js configuration",
                "language": "javascript"
            },
            {
                "path": "README.md",
                "content": await self._generate_content("nextjs/README.md", description, options),
                "purpose": "Project documentation",
                "language": "markdown"
            }
        ]
        
        # Add app directory structure if using the new App Router pattern
        if options.get("app_router", False):
            structure.extend([
                {
                    "path": "app/page.js",
                    "content": await self._generate_content("nextjs/app/page.js", description, options),
                    "purpose": "Home page using App Router",
                    "language": "javascript"
                },
                {
                    "path": "app/layout.js",
                    "content": await self._generate_content("nextjs/app/layout.js", description, options),
                    "purpose": "Root layout using App Router",
                    "language": "javascript"
                }
            ])
        
        # TypeScript support
        if options.get("typescript", False):
            # Replace .js files with .tsx
            structure = [
                {
                    "path": f.get("path").replace(".js", ".tsx") if f.get("path").endswith(".js") else f.get("path"),
                    "content": await self._generate_content(f.get("path").replace(".js", ".tsx") if f.get("path").endswith(".js") else f.get("path"), description, options),
                    "purpose": f.get("purpose"),
                    "language": "typescript" if f.get("language") == "javascript" else f.get("language")
                }
                for f in structure
            ]
            
            # Add TypeScript config
            structure.append({
                "path": "tsconfig.json",
                "content": await self._generate_content("nextjs/tsconfig.json", description, options),
                "purpose": "TypeScript configuration",
                "language": "json"
            })
        
        # Generate files
        for file_info in structure:
            files.append(CodeFile(
                path=file_info["path"],
                content=file_info["content"],
                purpose=file_info["purpose"],
                dependencies=[],
                language=file_info["language"]
            ))
        
        return {
            "success": True,
            "framework": "react",
            "variant": "nextjs",
            "files": files,
            "project_type": "node"
        }
    
    async def _generate_react_testing_files(
        self,
        description: str,
        options: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """
        Generate React testing files.
        
        Args:
            description: Project description
            options: Additional options
            
        Returns:
            List of file information dictionaries
        """
        # Determine testing framework
        testing_framework = options.get("testing_framework", "jest").lower()
        
        files = []
        file_extension = ".tsx" if options.get("typescript", False) else ".js"
        
        if testing_framework == "jest":
            files = [
                {
                    "path": f"src/App.test{file_extension}",
                    "content": await self._generate_content(f"react/App.test{file_extension}", description, options),
                    "purpose": "App component tests",
                    "language": "typescript" if options.get("typescript", False) else "javascript"
                },
                {
                    "path": "jest.config.js",
                    "content": await self._generate_content("react/jest.config.js", description, options),
                    "purpose": "Jest configuration",
                    "language": "javascript"
                }
            ]
        elif testing_framework == "cypress":
            files = [
                {
                    "path": "cypress/e2e/home.cy.js",
                    "content": await self._generate_content("react/cypress/e2e/home.cy.js", description, options),
                    "purpose": "Home page E2E tests",
                    "language": "javascript"
                },
                {
                    "path": "cypress.config.js",
                    "content": await self._generate_content("react/cypress.config.js", description, options),
                    "purpose": "Cypress configuration",
                    "language": "javascript"
                }
            ]
        elif testing_framework == "testing-library":
            files = [
                {
                    "path": f"src/App.test{file_extension}",
                    "content": await self._generate_content(f"react/App.test-rtl{file_extension}", description, options),
                    "purpose": "App component tests with React Testing Library",
                    "language": "typescript" if options.get("typescript", False) else "javascript"
                }
            ]
        
        return files
    
    async def _generate_django(
        self,
        description: str,
        output_dir: Union[str, Path],
        options: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Generate a Django project structure.
        
        Args:
            description: Description of the project
            output_dir: Output directory
            options: Additional options
            
        Returns:
            Dictionary with generation results
        """
        self._logger.info(f"Generating Django project: {description}")
        
        # Get project name
        project_name = options.get("project_name", "django_project")
        project_name = re.sub(r'[^a-zA-Z0-9_]', '_', project_name)
        
        # Get app name
        app_name = options.get("app_name", "main")
        app_name = re.sub(r'[^a-zA-Z0-9_]', '_', app_name)
        
        files = []
        
        # Define project structure
        structure = [
            {
                "path": f"{project_name}/settings.py",
                "content": await self._generate_content("django/settings.py", description, {"project_name": project_name, "app_name": app_name, **options}),
                "purpose": "Django settings",
                "language": "python"
            },
            {
                "path": f"{project_name}/urls.py",
                "content": await self._generate_content("django/urls.py", description, {"project_name": project_name, "app_name": app_name, **options}),
                "purpose": "URL configuration",
                "language": "python"
            },
            {
                "path": f"{project_name}/wsgi.py",
                "content": await self._generate_content("django/wsgi.py", description, {"project_name": project_name, **options}),
                "purpose": "WSGI configuration",
                "language": "python"
            },
            {
                "path": f"{project_name}/asgi.py",
                "content": await self._generate_content("django/asgi.py", description, {"project_name": project_name, **options}),
                "purpose": "ASGI configuration",
                "language": "python"
            },
            {
                "path": f"{project_name}/__init__.py",
                "content": "",
                "purpose": "Package initialization",
                "language": "python"
            },
            {
                "path": f"{app_name}/models.py",
                "content": await self._generate_content("django/models.py", description, {"app_name": app_name, **options}),
                "purpose": "Data models",
                "language": "python"
            },
            {
                "path": f"{app_name}/views.py",
                "content": await self._generate_content("django/views.py", description, {"app_name": app_name, **options}),
                "purpose": "View functions",
                "language": "python"
            },
            {
                "path": f"{app_name}/urls.py",
                "content": await self._generate_content("django/app_urls.py", description, {"app_name": app_name, **options}),
                "purpose": "App URL configuration",
                "language": "python"
            },
            {
                "path": f"{app_name}/__init__.py",
                "content": "",
                "purpose": "App package initialization",
                "language": "python"
            },
            {
                "path": f"{app_name}/templates/{app_name}/index.html",
                "content": await self._generate_content("django/index.html", description, {"app_name": app_name, **options}),
                "purpose": "Main template",
                "language": "html"
            },
            {
                "path": "manage.py",
                "content": await self._generate_content("django/manage.py", description, {"project_name": project_name, **options}),
                "purpose": "Django management script",
                "language": "python"
            },
            {
                "path": "requirements.txt",
                "content": await self._generate_content("django/requirements.txt", description, options),
                "purpose": "Python dependencies",
                "language": "text"
            },
            {
                "path": "README.md",
                "content": await self._generate_content("django/README.md", description, {"project_name": project_name, "app_name": app_name, **options}),
                "purpose": "Project documentation",
                "language": "markdown"
            }
        ]
        
        # Add tests if requested
        if options.get("tests", True):
            structure.append({
                "path": f"{app_name}/tests.py",
                "content": await self._generate_content("django/tests.py", description, {"app_name": app_name, **options}),
                "purpose": "Test cases",
                "language": "python"
            })
        
        # Add forms if requested
        if options.get("forms", False):
            structure.append({
                "path": f"{app_name}/forms.py",
                "content": await self._generate_content("django/forms.py", description, {"app_name": app_name, **options}),
                "purpose": "Form definitions",
                "language": "python"
            })
        
        # Generate files
        for file_info in structure:
            files.append(CodeFile(
                path=file_info["path"],
                content=file_info["content"],
                purpose=file_info["purpose"],
                dependencies=[],
                language=file_info["language"]
            ))
        
        return {
            "success": True,
            "framework": "django",
            "files": files,
            "project_type": "python",
            "project_name": project_name,
            "app_name": app_name
        }
    
    async def _generate_flask(
        self,
        description: str,
        output_dir: Union[str, Path],
        options: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Generate a Flask project structure.
        
        Args:
            description: Description of the project
            output_dir: Output directory
            options: Additional options
            
        Returns:
            Dictionary with generation results
        """
        self._logger.info(f"Generating Flask project: {description}")
        
        # Get app name
        app_name = options.get("app_name", "app")
        app_name = re.sub(r'[^a-zA-Z0-9_]', '_', app_name)
        
        files = []
        
        # Define project structure
        structure = [
            {
                "path": "app.py",
                "content": await self._generate_content("flask/app.py", description, {"app_name": app_name, **options}),
                "purpose": "Main application",
                "language": "python"
            },
            {
                "path": "config.py",
                "content": await self._generate_content("flask/config.py", description, options),
                "purpose": "Configuration",
                "language": "python"
            },
            {
                "path": f"{app_name}/__init__.py",
                "content": await self._generate_content("flask/init.py", description, {"app_name": app_name, **options}),
                "purpose": "Application initialization",
                "language": "python"
            },
            {
                "path": f"{app_name}/routes.py",
                "content": await self._generate_content("flask/routes.py", description, options),
                "purpose": "Route definitions",
                "language": "python"
            },
            {
                "path": f"{app_name}/models.py",
                "content": await self._generate_content("flask/models.py", description, options),
                "purpose": "Data models",
                "language": "python"
            },
            {
                "path": "templates/index.html",
                "content": await self._generate_content("flask/index.html", description, options),
                "purpose": "Main template",
                "language": "html"
            },
            {
                "path": "templates/layout.html",
                "content": await self._generate_content("flask/layout.html", description, options),
                "purpose": "Base template",
                "language": "html"
            },
            {
                "path": "static/css/style.css",
                "content": await self._generate_content("flask/style.css", description, options),
                "purpose": "Main stylesheet",
                "language": "css"
            },
            {
                "path": "requirements.txt",
                "content": await self._generate_content("flask/requirements.txt", description, options),
                "purpose": "Python dependencies",
                "language": "text"
            },
            {
                "path": "README.md",
                "content": await self._generate_content("flask/README.md", description, {"app_name": app_name, **options}),
                "purpose": "Project documentation",
                "language": "markdown"
            }
        ]
        
        # Add Docker support if requested
        if options.get("docker", False):
            structure.extend([
                {
                    "path": "Dockerfile",
                    "content": await self._generate_content("flask/Dockerfile", description, {"app_name": app_name, **options}),
                    "purpose": "Docker configuration",
                    "language": "dockerfile"
                },
                {
                    "path": "docker-compose.yml",
                    "content": await self._generate_content("flask/docker-compose.yml", description, {"app_name": app_name, **options}),
                    "purpose": "Docker Compose configuration",
                    "language": "yaml"
                }
            ])
        
        # Generate files
        for file_info in structure:
            files.append(CodeFile(
                path=file_info["path"],
                content=file_info["content"],
                purpose=file_info["purpose"],
                dependencies=[],
                language=file_info["language"]
            ))
        
        return {
            "success": True,
            "framework": "flask",
            "files": files,
            "project_type": "python",
            "app_name": app_name
        }
    
    async def _generate_express(
        self,
        description: str,
        output_dir: Union[str, Path],
        options: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Generate an Express.js project structure.
        
        Args:
            description: Description of the project
            output_dir: Output directory
            options: Additional options
            
        Returns:
            Dictionary with generation results
        """
        self._logger.info(f"Generating Express project: {description}")
        
        files = []
        
        # Define project structure
        structure = [
            {
                "path": "app.js",
                "content": await self._generate_content("express/app.js", description, options),
                "purpose": "Main application",
                "language": "javascript"
            },
            {
                "path": "routes/index.js",
                "content": await self._generate_content("express/routes/index.js", description, options),
                "purpose": "Main routes",
                "language": "javascript"
            },
            {
                "path": "routes/users.js",
                "content": await self._generate_content("express/routes/users.js", description, options),
                "purpose": "User routes",
                "language": "javascript"
            },
            {
                "path": "views/index.ejs",
                "content": await self._generate_content("express/views/index.ejs", description, options),
                "purpose": "Main view template",
                "language": "html"
            },
            {
                "path": "views/error.ejs",
                "content": await self._generate_content("express/views/error.ejs", description, options),
                "purpose": "Error view template",
                "language": "html"
            },
            {
                "path": "public/stylesheets/style.css",
                "content": await self._generate_content("express/public/stylesheets/style.css", description, options),
                "purpose": "Main stylesheet",
                "language": "css"
            },
            {
                "path": "package.json",
                "content": await self._generate_content("express/package.json", description, options),
                "purpose": "NPM package configuration",
                "language": "json"
            },
            {
                "path": "README.md",
                "content": await self._generate_content("express/README.md", description, options),
                "purpose": "Project documentation",
                "language": "markdown"
            }
        ]
        
        # Add configuration file
        structure.append({
            "path": "config/config.js",
            "content": await self._generate_content("express/config/config.js", description, options),
            "purpose": "Configuration settings",
            "language": "javascript"
        })
        
        # Add middleware directory
        structure.append({
            "path": "middleware/auth.js",
            "content": await self._generate_content("express/middleware/auth.js", description, options),
            "purpose": "Authentication middleware",
            "language": "javascript"
        })
        
        # TypeScript support
        if options.get("typescript", False):
            # Replace .js files with .ts
            structure = [
                {
                    "path": f.get("path").replace(".js", ".ts") if f.get("path").endswith(".js") else f.get("path"),
                    "content": await self._generate_content(f.get("path").replace(".js", ".ts") if f.get("path").endswith(".js") else f.get("path"), description, options),
                    "purpose": f.get("purpose"),
                    "language": "typescript" if f.get("language") == "javascript" else f.get("language")
                }
                for f in structure
            ]
            
            # Add TypeScript config
            structure.append({
                "path": "tsconfig.json",
                "content": await self._generate_content("express/tsconfig.json", description, options),
                "purpose": "TypeScript configuration",
                "language": "json"
            })
        
        # Generate files
        for file_info in structure:
            files.append(CodeFile(
                path=file_info["path"],
                content=file_info["content"],
                purpose=file_info["purpose"],
                dependencies=[],
                language=file_info["language"]
            ))
        
        return {
            "success": True,
            "framework": "express",
            "files": files,
            "project_type": "node"
        }
    
    async def _generate_fastapi(
        self,
        description: str,
        output_dir: Union[str, Path],
        options: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Generate a FastAPI project structure.
        
        Args:
            description: Description of the project
            output_dir: Output directory
            options: Additional options
            
        Returns:
            Dictionary with generation results
        """
        self._logger.info(f"Generating FastAPI project: {description}")
        
        # Get app name
        app_name = options.get("app_name", "app")
        app_name = re.sub(r'[^a-zA-Z0-9_]', '_', app_name)
        
        files = []
        
        # Define project structure
        structure = [
            {
                "path": "main.py",
                "content": await self._generate_content("fastapi/main.py", description, {"app_name": app_name, **options}),
                "purpose": "Main application",
                "language": "python"
            },
            {
                "path": f"{app_name}/__init__.py",
                "content": "",
                "purpose": "Package initialization",
                "language": "python"
            },
            {
                "path": f"{app_name}/routes.py",
                "content": await self._generate_content("fastapi/routes.py", description, options),
                "purpose": "API routes",
                "language": "python"
            },
            {
                "path": f"{app_name}/models.py",
                "content": await self._generate_content("fastapi/models.py", description, options),
                "purpose": "Data models",
                "language": "python"
            },
            {
                "path": f"{app_name}/schemas.py",
                "content": await self._generate_content("fastapi/schemas.py", description, options),
                "purpose": "Pydantic schemas",
                "language": "python"
            },
            {
                "path": f"{app_name}/database.py",
                "content": await self._generate_content("fastapi/database.py", description, options),
                "purpose": "Database connection",
                "language": "python"
            },
            {
                "path": "requirements.txt",
                "content": await self._generate_content("fastapi/requirements.txt", description, options),
                "purpose": "Python dependencies",
                "language": "text"
            },
            {
                "path": "README.md",
                "content": await self._generate_content("fastapi/README.md", description, {"app_name": app_name, **options}),
                "purpose": "Project documentation",
                "language": "markdown"
            }
        ]
        
        # Add dependencies directory for better organization
        structure.append({
            "path": f"{app_name}/dependencies.py",
            "content": await self._generate_content("fastapi/dependencies.py", description, options),
            "purpose": "Dependency injection functions",
            "language": "python"
        })
        
        # Add config module
        structure.append({
            "path": f"{app_name}/config.py",
            "content": await self._generate_content("fastapi/config.py", description, options),
            "purpose": "Configuration settings",
            "language": "python"
        })
        
        # Add Docker support if requested
        if options.get("docker", True):
            structure.extend([
                {
                    "path": "Dockerfile",
                    "content": await self._generate_content("fastapi/Dockerfile", description, {"app_name": app_name, **options}),
                    "purpose": "Docker configuration",
                    "language": "dockerfile"
                },
                {
                    "path": ".dockerignore",
                    "content": await self._generate_content("fastapi/.dockerignore", description, options),
                    "purpose": "Docker ignore file",
                    "language": "text"
                }
            ])
        
        # Generate files
        for file_info in structure:
            files.append(CodeFile(
                path=file_info["path"],
                content=file_info["content"],
                purpose=file_info["purpose"],
                dependencies=[],
                language=file_info["language"]
            ))
        
        return {
            "success": True,
            "framework": "fastapi",
            "files": files,
            "project_type": "python",
            "app_name": app_name
        }
    
    async def _generate_spring(
        self,
        description: str,
        output_dir: Union[str, Path],
        options: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Generate a Spring Boot project structure.
        
        Args:
            description: Description of the project
            output_dir: Output directory
            options: Additional options
            
        Returns:
            Dictionary with generation results
        """
        self._logger.info(f"Generating Spring Boot project: {description}")
        
        # Get package name
        package_name = options.get("package_name", "com.example.demo")
        package_path = package_name.replace(".", "/")
        
        files = []
        
        # Define project structure
        structure = [
            {
                "path": f"src/main/java/{package_path}/Application.java",
                "content": await self._generate_content("spring/Application.java", description, {"package_name": package_name, **options}),
                "purpose": "Main application class",
                "language": "java"
            },
            {
                "path": f"src/main/java/{package_path}/controller/MainController.java",
                "content": await self._generate_content("spring/MainController.java", description, {"package_name": package_name, **options}),
                "purpose": "Main controller",
                "language": "java"
            },
            {
                "path": f"src/main/java/{package_path}/model/User.java",
                "content": await self._generate_content("spring/User.java", description, {"package_name": package_name, **options}),
                "purpose": "User model",
                "language": "java"
            },
            {
                "path": f"src/main/resources/application.properties",
                "content": await self._generate_content("spring/application.properties", description, options),
                "purpose": "Application properties",
                "language": "properties"
            },
            {
                "path": f"src/main/resources/templates/index.html",
                "content": await self._generate_content("spring/index.html", description, options),
                "purpose": "Main template",
                "language": "html"
            },
            {
                "path": "build.gradle",
                "content": await self._generate_content("spring/build.gradle", description, {"package_name": package_name, **options}),
                "purpose": "Gradle build configuration",
                "language": "gradle"
            },
            {
                "path": "README.md",
                "content": await self._generate_content("spring/README.md", description, {"package_name": package_name, **options}),
                "purpose": "Project documentation",
                "language": "markdown"
            }
        ]
        
        # Add repository and service layers for better organization
        structure.extend([
            {
                "path": f"src/main/java/{package_path}/repository/UserRepository.java",
                "content": await self._generate_content("spring/UserRepository.java", description, {"package_name": package_name, **options}),
                "purpose": "User repository interface",
                "language": "java"
            },
            {
                "path": f"src/main/java/{package_path}/service/UserService.java",
                "content": await self._generate_content("spring/UserService.java", description, {"package_name": package_name, **options}),
                "purpose": "User service interface",
                "language": "java"
            },
            {
                "path": f"src/main/java/{package_path}/service/impl/UserServiceImpl.java",
                "content": await self._generate_content("spring/UserServiceImpl.java", description, {"package_name": package_name, **options}),
                "purpose": "User service implementation",
                "language": "java"
            }
        ])
        
        # Add Maven support
        if options.get("maven", False) or options.get("build_tool", "gradle") == "maven":
            structure.append({
                "path": "pom.xml",
                "content": await self._generate_content("spring/pom.xml", description, {"package_name": package_name, **options}),
                "purpose": "Maven build configuration",
                "language": "xml"
            })
        
        # Generate files
        for file_info in structure:
            files.append(CodeFile(
                path=file_info["path"],
                content=file_info["content"],
                purpose=file_info["purpose"],
                dependencies=[],
                language=file_info["language"]
            ))
        
        return {
            "success": True,
            "framework": "spring",
            "files": files,
            "project_type": "java",
            "package_name": package_name
        }
    
    async def _generate_vue(
        self,
        description: str,
        output_dir: Union[str, Path],
        options: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Generate a Vue.js project structure.
        
        Args:
            description: Description of the project
            output_dir: Output directory
            options: Additional options
            
        Returns:
            Dictionary with generation results
        """
        self._logger.info(f"Generating Vue.js project: {description}")
        
        files = []
        
        # Define project structure
        structure = [
            {
                "path": "src/main.js",
                "content": await self._generate_content("vue/main.js", description, options),
                "purpose": "Application entry point",
                "language": "javascript"
            },
            {
                "path": "src/App.vue",
                "content": await self._generate_content("vue/App.vue", description, options),
                "purpose": "Main application component",
                "language": "vue"
            },
            {
                "path": "src/components/HelloWorld.vue",
                "content": await self._generate_content("vue/HelloWorld.vue", description, options),
                "purpose": "Example component",
                "language": "vue"
            },
            {
                "path": "src/router/index.js",
                "content": await self._generate_content("vue/router.js", description, options),
                "purpose": "Vue Router configuration",
                "language": "javascript"
            },
            {
                "path": "src/views/Home.vue",
                "content": await self._generate_content("vue/Home.vue", description, options),
                "purpose": "Home page component",
                "language": "vue"
            },
            {
                "path": "src/views/About.vue",
                "content": await self._generate_content("vue/About.vue", description, options),
                "purpose": "About page component",
                "language": "vue"
            },
            {
                "path": "public/index.html",
                "content": await self._generate_content("vue/index.html", description, options),
                "purpose": "Main HTML file",
                "language": "html"
            },
            {
                "path": "package.json",
                "content": await self._generate_content("vue/package.json", description, options),
                "purpose": "NPM package configuration",
                "language": "json"
            },
            {
                "path": "vue.config.js",
                "content": await self._generate_content("vue/vue.config.js", description, options),
                "purpose": "Vue CLI configuration",
                "language": "javascript"
            },
            {
                "path": "README.md",
                "content": await self._generate_content("vue/README.md", description, options),
                "purpose": "Project documentation",
                "language": "markdown"
            }
        ]
        
        # Add Vuex store if requested
        if options.get("store", True):
            structure.extend([
                {
                    "path": "src/store/index.js",
                    "content": await self._generate_content("vue/store/index.js", description, options),
                    "purpose": "Vuex store configuration",
                    "language": "javascript"
                },
                {
                    "path": "src/store/modules/auth.js",
                    "content": await self._generate_content("vue/store/modules/auth.js", description, options),
                    "purpose": "Auth store module",
                    "language": "javascript"
                }
            ])
        
        # TypeScript support
        if options.get("typescript", False):
            # Replace .js files with .ts
            structure = [
                {
                    "path": f.get("path").replace(".js", ".ts") if f.get("path").endswith(".js") else f.get("path"),
                    "content": await self._generate_content(f.get("path").replace(".js", ".ts") if f.get("path").endswith(".js") else f.get("path"), description, options),
                    "purpose": f.get("purpose"),
                    "language": "typescript" if f.get("language") == "javascript" else f.get("language")
                }
                for f in structure
            ]
            
            # Add TypeScript config
            structure.append({
                "path": "tsconfig.json",
                "content": await self._generate_content("vue/tsconfig.json", description, options),
                "purpose": "TypeScript configuration",
                "language": "json"
            })
        
        # Generate files
        for file_info in structure:
            files.append(CodeFile(
                path=file_info["path"],
                content=file_info["content"],
                purpose=file_info["purpose"],
                dependencies=[],
                language=file_info["language"]
            ))
        
        return {
            "success": True,
            "framework": "vue",
            "files": files,
            "project_type": "node"
        }
    
    async def _generate_angular(
        self,
        description: str,
        output_dir: Union[str, Path],
        options: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Generate an Angular project structure.
        
        Args:
            description: Description of the project
            output_dir: Output directory
            options: Additional options
            
        Returns:
            Dictionary with generation results
        """
        self._logger.info(f"Generating Angular project: {description}")
        
        files = []
        
        # Define project structure
        structure = [
            {
                "path": "src/main.ts",
                "content": await self._generate_content("angular/main.ts", description, options),
                "purpose": "Application entry point",
                "language": "typescript"
            },
            {
                "path": "src/app/app.module.ts",
                "content": await self._generate_content("angular/app.module.ts", description, options),
                "purpose": "Main application module",
                "language": "typescript"
            },
            {
                "path": "src/app/app.component.ts",
                "content": await self._generate_content("angular/app.component.ts", description, options),
                "purpose": "Main application component",
                "language": "typescript"
            },
            {
                "path": "src/app/app.component.html",
                "content": await self._generate_content("angular/app.component.html", description, options),
                "purpose": "Main component template",
                "language": "html"
            },
            {
                "path": "src/app/app.component.css",
                "content": await self._generate_content("angular/app.component.css", description, options),
                "purpose": "Main component styles",
                "language": "css"
            },
            {
                "path": "src/app/app-routing.module.ts",
                "content": await self._generate_content("angular/app-routing.module.ts", description, options),
                "purpose": "Routing configuration",
                "language": "typescript"
            },
            {
                "path": "src/index.html",
                "content": await self._generate_content("angular/index.html", description, options),
                "purpose": "Main HTML file",
                "language": "html"
            },
            {
                "path": "src/styles.css",
                "content": await self._generate_content("angular/styles.css", description, options),
                "purpose": "Global styles",
                "language": "css"
            },
            {
                "path": "angular.json",
                "content": await self._generate_content("angular/angular.json", description, options),
                "purpose": "Angular CLI configuration",
                "language": "json"
            },
            {
                "path": "package.json",
                "content": await self._generate_content("angular/package.json", description, options),
                "purpose": "NPM package configuration",
                "language": "json"
            },
            {
                "path": "tsconfig.json",
                "content": await self._generate_content("angular/tsconfig.json", description, options),
                "purpose": "TypeScript configuration",
                "language": "json"
            },
            {
                "path": "README.md",
                "content": await self._generate_content("angular/README.md", description, options),
                "purpose": "Project documentation",
                "language": "markdown"
            }
        ]
        
        # Add feature module for better organization
        structure.extend([
            {
                "path": "src/app/features/home/home.component.ts",
                "content": await self._generate_content("angular/features/home/home.component.ts", description, options),
                "purpose": "Home feature component",
                "language": "typescript"
            },
            {
                "path": "src/app/features/home/home.component.html",
                "content": await self._generate_content("angular/features/home/home.component.html", description, options),
                "purpose": "Home feature template",
                "language": "html"
            },
            {
                "path": "src/app/features/home/home.module.ts",
                "content": await self._generate_content("angular/features/home/home.module.ts", description, options),
                "purpose": "Home feature module",
                "language": "typescript"
            }
        ])
        
        # Add shared module
        structure.extend([
            {
                "path": "src/app/shared/shared.module.ts",
                "content": await self._generate_content("angular/shared/shared.module.ts", description, options),
                "purpose": "Shared module",
                "language": "typescript"
            },
            {
                "path": "src/app/shared/components/header/header.component.ts",
                "content": await self._generate_content("angular/shared/components/header/header.component.ts", description, options),
                "purpose": "Header component",
                "language": "typescript"
            },
            {
                "path": "src/app/shared/components/header/header.component.html",
                "content": await self._generate_content("angular/shared/components/header/header.component.html", description, options),
                "purpose": "Header template",
                "language": "html"
            }
        ])
        
        # Generate files
        for file_info in structure:
            files.append(CodeFile(
                path=file_info["path"],
                content=file_info["content"],
                purpose=file_info["purpose"],
                dependencies=[],
                language=file_info["language"]
            ))
        
        return {
            "success": True,
            "framework": "angular",
            "files": files,
            "project_type": "node"
        }
    
    async def _generate_generic(
        self,
        framework: str,
        description: str,
        output_dir: Union[str, Path],
        options: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Generate a generic framework structure using AI.
        
        Args:
            framework: Framework name
            description: Description of the project
            output_dir: Output directory
            options: Additional options
            
        Returns:
            Dictionary with generation results
        """
        self._logger.info(f"Generating generic {framework} project: {description}")
        
        # Use AI to generate a project structure
        prompt = f"""
Generate a typical file structure for a {framework} project that matches this description:
"{description}"

Your response should be a JSON object with this structure:
```json
{{
  "files": [
    {{
      "path": "relative/path/to/file.ext",
      "purpose": "brief description of the file's purpose",
      "language": "programming language/file type"
    }}
  ],
  "project_type": "main programming language (e.g., python, node, java)",
  "description": "brief description of the project structure"
}}
Include all essential files for a working {framework} project, including:

Main entry point file(s)
Configuration files
View/template files
Model definitions
Routing or controller files
Package management files (e.g., package.json, requirements.txt)
Documentation

Keep the structure focused on the core framework files, don't include optional or very specific files.
Ensure the structure follows best practices for {framework} projects.
"""
        try:
            # Call AI service
            api_request = GeminiRequest(
                prompt=prompt,
                max_tokens=4000,
                temperature=0.2
            )
            
            response = await gemini_client.generate_text(api_request)
        
        # Extract JSON from the response
            structure_match = re.search(r'```(?:json)?\s*(.*?)\s*```', response.text, re.DOTALL)
            if structure_match:
                structure_json = structure_match.group(1)
            else:
                structure_json = response.text
            
            structure_data = json.loads(structure_json)
            
            # Generate content for each file
            files = []
            
            for file_info in structure_data.get("files", []):
                # Generate content for the file
                content = await self._generate_file_content(
                    framework,
                    file_info["path"],
                    file_info["purpose"],
                    description,
                    options
                )
                
                files.append(CodeFile(
                    path=file_info["path"],
                    content=content,
                    purpose=file_info["purpose"],
                    dependencies=[],
                    language=file_info.get("language")
                ))
            
            return {
                "success": True,
                "framework": framework,
                "files": files,
                "project_type": structure_data.get("project_type", self._infer_project_type(framework))
            }
        
        except json.JSONDecodeError as e:
            self._logger.error(f"Error parsing AI response for {framework} project structure: {e}")
            return {
                "success": False,
                "error": f"Could not generate {framework} project structure: Invalid JSON response",
                "framework": framework
            }
        except Exception as e:
            self._logger.error(f"Error generating {framework} project: {str(e)}", exc_info=True)
            return {
                "success": False,
                "error": f"Could not generate {framework} project structure: {str(e)}",
                "framework": framework
            }

    def _infer_project_type(self, framework: str) -> str:
        """
        Infer project type from framework name.
        
        Args:
            framework: Framework name
            
        Returns:
            Inferred project type
        """
        return self._framework_project_types.get(framework.lower(), "unknown")

    async def _generate_content(
        self, 
        template_path: str,
        description: str,
        options: Dict[str, Any]
    ) -> str:
        """
        Generate content for a file based on a template path.
        
        Args:
            template_path: Path to template relative to framework
            description: Project description
            options: Additional options
            
        Returns:
            Generated file content
        """
        self._logger.debug(f"Generating content for template: {template_path}")
        
        # Extract framework and file path
        parts = template_path.split("/", 1)
        if len(parts) < 2:
            # Invalid template path
            framework = "generic"
            file_path = template_path
        else:
            framework = parts[0]
            file_path = parts[1]
        
        return await self._generate_file_content(
            framework,
            file_path,
            "Framework-specific file",
            description,
            options
        )

    async def generate_standard_project_structure(
        self, 
        framework: str,
        description: str,
        output_dir: Union[str, Path],
        options: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Generate a standardized project structure for a framework.
        
        This method creates a more complete, production-ready project structure
        compared to the basic structure from generate_framework_structure.
        
        Args:
            framework: Framework to generate for (e.g., "react", "django")
            description: Description of the project
            output_dir: Directory where the project should be generated
            options: Additional options for the framework
            
        Returns:
            Dictionary with generation results
        """
        options = options or {}
        framework = framework.lower()
        self._logger.info(f"Generating standard project for {framework}: {description}")
        
        try:
            # Determine project type based on framework
            project_type = self._framework_project_types.get(framework, "unknown")
            
            # Get enhanced project structure if available
            result = await self._generate_enhanced_framework_structure(
                framework=framework,
                description=description,
                output_dir=output_dir,
                options=options
            )
            
            # If no specialized enhanced structure, fall back to basic
            if not result:
                result = await self.generate_framework_structure(
                    framework=framework,
                    description=description,
                    output_dir=output_dir,
                    options=options
                )
                
            return result
        except Exception as e:
            self._logger.error(f"Error generating standard project for {framework}: {str(e)}")
            return {
                "success": False,
                "framework": framework,
                "error": f"Failed to generate standard project for {framework}: {str(e)}",
                "files": []
            }
    
    async def _generate_enhanced_framework_structure(
        self,
        framework: str,
        description: str,
        output_dir: Union[str, Path],
        options: Dict[str, Any]
    ) -> Optional[Dict[str, Any]]:
        """
        Generate an enhanced framework-specific project structure with best practices.
        
        Args:
            framework: Framework to generate for
            description: Description of the project
            output_dir: Output directory
            options: Additional options
            
        Returns:
            Dictionary with generation results or None if not supported
        """
        # Check for enhanced framework handlers
        handler_method = f"_generate_enhanced_{framework.replace('-', '_')}"
        if hasattr(self, handler_method):
            return await getattr(self, handler_method)(description, output_dir, options)
        
        # No enhanced handler available
        return None
    
    async def _generate_enhanced_react(
        self, 
        description: str,
        output_dir: Union[str, Path],
        options: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Generate an enhanced React project structure following best practices.
        
        Args:
            description: Description of the project
            output_dir: Output directory
            options: Additional options
            
        Returns:
            Dictionary with generation results
        """
        self._logger.info(f"Generating enhanced React project: {description}")
        
        # Determine React variant
        variant = options.get("variant", "cra").lower()
        
        # Use TypeScript by default for enhanced projects
        use_typescript = options.get("typescript", True)
        
        # Define common file extensions based on TypeScript usage
        ext = ".tsx" if use_typescript else ".jsx"
        style_ext = options.get("style_ext", ".css")
        
        # Common options
        routing = options.get("routing", True)
        state_management = options.get("state_management", "context")  # context, redux, mobx
        styling = options.get("styling", "css")  # css, sass, styled-components, tailwind
        testing = options.get("testing", True)
        
        # Initialize files list
        files = []
        
        # Define project structure based on variant
        if variant == "nextjs":
            # NextJS project structure
            
            # Core configuration files
            files.extend([
                {
                    "path": "next.config.js",
                    "content": await self._generate_content("nextjs/enhanced/next.config.js", description, options),
                    "purpose": "Next.js configuration",
                    "language": "javascript"
                },
                {
                    "path": "package.json",
                    "content": await self._generate_content("nextjs/enhanced/package.json", description, options),
                    "purpose": "Package configuration",
                    "language": "json"
                },
                {
                    "path": ".env.local.example",
                    "content": await self._generate_content("nextjs/enhanced/.env.local.example", description, options),
                    "purpose": "Environment variables example",
                    "language": "env"
                },
                {
                    "path": ".gitignore",
                    "content": await self._generate_content("nextjs/enhanced/.gitignore", description, options),
                    "purpose": "Git ignore configuration",
                    "language": "gitignore"
                },
                {
                    "path": "README.md",
                    "content": await self._generate_content("nextjs/enhanced/README.md", description, options),
                    "purpose": "Project documentation",
                    "language": "markdown"
                }
            ])
            
            # TypeScript configuration
            if use_typescript:
                files.extend([
                    {
                        "path": "tsconfig.json",
                        "content": await self._generate_content("nextjs/enhanced/tsconfig.json", description, options),
                        "purpose": "TypeScript configuration",
                        "language": "json"
                    }
                ])
            
            # Pages or App directory structure
            if options.get("app_router", True):
                # Modern App Router structure
                files.extend([
                    {
                        "path": "app/layout" + ext,
                        "content": await self._generate_content("nextjs/enhanced/app/layout" + ext, description, options),
                        "purpose": "Root layout component",
                        "language": "typescript" if use_typescript else "javascript"
                    },
                    {
                        "path": "app/page" + ext,
                        "content": await self._generate_content("nextjs/enhanced/app/page" + ext, description, options),
                        "purpose": "Home page component",
                        "language": "typescript" if use_typescript else "javascript"
                    },
                    {
                        "path": "app/globals.css",
                        "content": await self._generate_content("nextjs/enhanced/app/globals.css", description, options),
                        "purpose": "Global styles",
                        "language": "css"
                    }
                ])
                
                # Add API route
                files.extend([
                    {
                        "path": f"app/api/example/route{'.ts' if use_typescript else '.js'}",
                        "content": await self._generate_content("nextjs/enhanced/app/api/example/route.ts", description, options),
                        "purpose": "Example API route",
                        "language": "typescript" if use_typescript else "javascript"
                    }
                ])
                
                # Add components directory
                files.extend([
                    {
                        "path": "components/ui/Button" + ext,
                        "content": await self._generate_content("nextjs/enhanced/components/ui/Button" + ext, description, options),
                        "purpose": "Reusable button component",
                        "language": "typescript" if use_typescript else "javascript"
                    },
                    {
                        "path": "components/layout/Header" + ext,
                        "content": await self._generate_content("nextjs/enhanced/components/layout/Header" + ext, description, options),
                        "purpose": "Header component",
                        "language": "typescript" if use_typescript else "javascript"
                    }
                ])
            else:
                # Legacy Pages Router structure
                files.extend([
                    {
                        "path": "pages/index" + ext,
                        "content": await self._generate_content("nextjs/enhanced/pages/index" + ext, description, options),
                        "purpose": "Home page",
                        "language": "typescript" if use_typescript else "javascript"
                    },
                    {
                        "path": "pages/_app" + ext,
                        "content": await self._generate_content("nextjs/enhanced/pages/_app" + ext, description, options),
                        "purpose": "App component",
                        "language": "typescript" if use_typescript else "javascript"
                    },
                    {
                        "path": "pages/_document" + ext,
                        "content": await self._generate_content("nextjs/enhanced/pages/_document" + ext, description, options),
                        "purpose": "Document component",
                        "language": "typescript" if use_typescript else "javascript"
                    }
                ])
                
                # Add API route
                files.extend([
                    {
                        "path": f"pages/api/example{'.ts' if use_typescript else '.js'}", # Fixed JavaScript ternary
                        "content": await self._generate_content("nextjs/enhanced/pages/api/example.ts", description, options),
                        "purpose": "Example API endpoint",
                        "language": "typescript" if use_typescript else "javascript"
                    }
                ])
                
                # Add components directory
                files.extend([
                    {
                        "path": "components/ui/Button" + ext,
                        "content": await self._generate_content("nextjs/enhanced/components/ui/Button" + ext, description, options),
                        "purpose": "Reusable button component",
                        "language": "typescript" if use_typescript else "javascript"
                    },
                    {
                        "path": "components/layout/Header" + ext,
                        "content": await self._generate_content("nextjs/enhanced/components/layout/Header" + ext, description, options),
                        "purpose": "Header component",
                        "language": "typescript" if use_typescript else "javascript"
                    }
                ])
            
            # Add utilities
            files.extend([
                {
                    "path": f"lib/utils{'.ts' if use_typescript else '.js'}", # Fixed JavaScript ternary
                    "content": await self._generate_content("nextjs/enhanced/lib/utils.ts", description, options),
                    "purpose": "Utility functions",
                    "language": "typescript" if use_typescript else "javascript"
                }
            ])
            
            # Add public directory
            files.extend([
                {
                    "path": "public/favicon.ico",
                    "content": "",  # Binary content would be handled differently
                    "purpose": "Favicon",
                    "language": "binary"
                }
            ])
            
            # Add testing if requested
            if testing:
                files.extend([
                    {
                        "path": "__tests__/Home.test" + ext,
                        "content": await self._generate_content("nextjs/enhanced/__tests__/Home.test" + ext, description, options),
                        "purpose": "Home page tests",
                        "language": "typescript" if use_typescript else "javascript"
                    },
                    {
                        "path": f"jest.config{'.ts' if use_typescript else '.js'}", # Fixed JavaScript ternary
                        "content": await self._generate_content("nextjs/enhanced/jest.config.ts", description, options),
                        "purpose": "Jest configuration",
                        "language": "typescript" if use_typescript else "javascript"
                    }
                ])
        else:
            # Create React App or similar structure
            
            # Core configuration files
            files.extend([
                {
                    "path": "package.json",
                    "content": await self._generate_content("react/enhanced/package.json", description, options),
                    "purpose": "Package configuration",
                    "language": "json"
                },
                {
                    "path": ".env.example",
                    "content": await self._generate_content("react/enhanced/.env.example", description, options),
                    "purpose": "Environment variables example",
                    "language": "env"
                },
                {
                    "path": ".gitignore",
                    "content": await self._generate_content("react/enhanced/.gitignore", description, options),
                    "purpose": "Git ignore configuration",
                    "language": "gitignore"
                },
                {
                    "path": "README.md",
                    "content": await self._generate_content("react/enhanced/README.md", description, options),
                    "purpose": "Project documentation",
                    "language": "markdown"
                },
                {
                    "path": "public/index.html",
                    "content": await self._generate_content("react/enhanced/public/index.html", description, options),
                    "purpose": "HTML entry point",
                    "language": "html"
                }
            ])
            
            # TypeScript configuration
            if use_typescript:
                files.extend([
                    {
                        "path": "tsconfig.json",
                        "content": await self._generate_content("react/enhanced/tsconfig.json", description, options),
                        "purpose": "TypeScript configuration",
                        "language": "json"
                    }
                ])
            
            # Core application files
            files.extend([
                {
                    "path": f"src/index{'.tsx' if use_typescript else '.jsx'}", # Fixed JavaScript ternary
                    "content": await self._generate_content("react/enhanced/src/index" + ext, description, options),
                    "purpose": "Application entry point",
                    "language": "typescript" if use_typescript else "javascript"
                },
                {
                    "path": "src/App" + ext,
                    "content": await self._generate_content("react/enhanced/src/App" + ext, description, options),
                    "purpose": "Main App component",
                    "language": "typescript" if use_typescript else "javascript"
                },
                {
                    "path": "src/index.css",
                    "content": await self._generate_content("react/enhanced/src/index.css", description, options),
                    "purpose": "Global styles",
                    "language": "css"
                }
            ])
            
            # Add TypeScript types if needed
            if use_typescript:
                files.extend([
                    {
                        "path": "src/types/index.ts",
                        "content": await self._generate_content("react/enhanced/src/types/index.ts", description, options),
                        "purpose": "TypeScript type definitions",
                        "language": "typescript"
                    }
                ])
            
            # Add routing if requested
            if routing:
                files.extend([
                    {
                        "path": "src/pages/Home" + ext,
                        "content": await self._generate_content("react/enhanced/src/pages/Home" + ext, description, options),
                        "purpose": "Home page component",
                        "language": "typescript" if use_typescript else "javascript"
                    },
                    {
                        "path": "src/pages/About" + ext,
                        "content": await self._generate_content("react/enhanced/src/pages/About" + ext, description, options),
                        "purpose": "About page component",
                        "language": "typescript" if use_typescript else "javascript"
                    },
                    {
                        "path": "src/routes" + ext,
                        "content": await self._generate_content("react/enhanced/src/routes" + ext, description, options),
                        "purpose": "Route definitions",
                        "language": "typescript" if use_typescript else "javascript"
                    }
                ])
            
            # Add state management
            if state_management == "redux":
                files.extend([
                    {
                        "path": f"src/store/index{'.ts' if use_typescript else '.js'}", # Fixed JavaScript ternary
                        "content": await self._generate_content("react/enhanced/src/store/index.ts", description, options),
                        "purpose": "Redux store configuration",
                        "language": "typescript" if use_typescript else "javascript"
                    },
                    {
                        "path": f"src/store/slices/counterSlice{'.ts' if use_typescript else '.js'}", # Fixed JavaScript ternary
                        "content": await self._generate_content("react/enhanced/src/store/slices/counterSlice.ts", description, options),
                        "purpose": "Example Redux slice",
                        "language": "typescript" if use_typescript else "javascript"
                    }
                ])
            elif state_management == "mobx":
                files.extend([
                    {
                        "path": f"src/stores/RootStore{'.ts' if use_typescript else '.js'}", # Fixed JavaScript ternary
                        "content": await self._generate_content("react/enhanced/src/stores/RootStore.ts", description, options),
                        "purpose": "MobX root store",
                        "language": "typescript" if use_typescript else "javascript"
                    },
                    {
                        "path": f"src/stores/CounterStore{'.ts' if use_typescript else '.js'}", # Fixed JavaScript ternary
                        "content": await self._generate_content("react/enhanced/src/stores/CounterStore.ts", description, options),
                        "purpose": "Example MobX store",
                        "language": "typescript" if use_typescript else "javascript"
                    }
                ])
            else:  # context
                files.extend([
                    {
                        "path": "src/context/AppContext" + ext,
                        "content": await self._generate_content("react/enhanced/src/context/AppContext" + ext, description, options),
                        "purpose": "Application context",
                        "language": "typescript" if use_typescript else "javascript"
                    }
                ])
            
            # Add components directory
            files.extend([
                {
                    "path": "src/components/common/Button" + ext,
                    "content": await self._generate_content("react/enhanced/src/components/common/Button" + ext, description, options),
                    "purpose": "Reusable button component",
                    "language": "typescript" if use_typescript else "javascript"
                },
                {
                    "path": "src/components/layout/Header" + ext,
                    "content": await self._generate_content("react/enhanced/src/components/layout/Header" + ext, description, options),
                    "purpose": "Header component",
                    "language": "typescript" if use_typescript else "javascript"
                },
                {
                    "path": "src/components/layout/Footer" + ext,
                    "content": await self._generate_content("react/enhanced/src/components/layout/Footer" + ext, description, options),
                    "purpose": "Footer component",
                    "language": "typescript" if use_typescript else "javascript"
                }
            ])
            
            # Add utilities
            files.extend([
                {
                    "path": f"src/utils/helpers{'.ts' if use_typescript else '.js'}", # Fixed JavaScript ternary
                    "content": await self._generate_content("react/enhanced/src/utils/helpers.ts", description, options),
                    "purpose": "Helper functions",
                    "language": "typescript" if use_typescript else "javascript"
                },
                {
                    "path": f"src/utils/api{'.ts' if use_typescript else '.js'}", # Fixed JavaScript ternary
                    "content": await self._generate_content("react/enhanced/src/utils/api.ts", description, options),
                    "purpose": "API utilities",
                    "language": "typescript" if use_typescript else "javascript"
                }
            ])
            
            # Add testing if requested
            if testing:
                files.extend([
                    {
                        "path": "src/App.test" + ext,
                        "content": await self._generate_content("react/enhanced/src/App.test" + ext, description, options),
                        "purpose": "App component tests",
                        "language": "typescript" if use_typescript else "javascript"
                    },
                    {
                        "path": "src/components/common/Button.test" + ext,
                        "content": await self._generate_content("react/enhanced/src/components/common/Button.test" + ext, description, options),
                        "purpose": "Button component tests",
                        "language": "typescript" if use_typescript else "javascript"
                    }
                ])
        
        # Generate CodeFile objects from the file information
        result_files = []
        for file_info in files:
            result_files.append(CodeFile(
                path=file_info["path"],
                content=file_info["content"],
                purpose=file_info["purpose"],
                dependencies=[],
                language=file_info["language"]
            ))
        
        return {
            "success": True,
            "framework": "react",
            "variant": variant,
            "files": result_files,
            "project_type": "node",
            "typescript": use_typescript,
            "routing": routing,
            "state_management": state_management,
            "styling": styling,
            "testing": testing
        }
    
    async def _generate_enhanced_django(
        self, 
        description: str,
        output_dir: Union[str, Path],
        options: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Generate an enhanced Django project structure following best practices.
        
        Args:
            description: Description of the project
            output_dir: Output directory
            options: Additional options
            
        Returns:
            Dictionary with generation results
        """
        self._logger.info(f"Generating enhanced Django project: {description}")
        
        # Get project name and app name
        project_name = options.get("project_name", "django_project")
        project_name = re.sub(r'[^a-zA-Z0-9_]', '_', project_name)
        
        app_name = options.get("app_name", "main")
        app_name = re.sub(r'[^a-zA-Z0-9_]', '_', app_name)
        
        # Common options
        include_rest_framework = options.get("rest_framework", True)
        include_authentication = options.get("authentication", True)
        include_admin = options.get("admin", True)
        include_static = options.get("static", True)
        include_templates = options.get("templates", True)
        
        # Initialize files list
        files = []
        
        # Core Django files
        files.extend([
            {
                "path": "manage.py",
                "content": await self._generate_content("django/enhanced/manage.py", description, {"project_name": project_name, **options}),
                "purpose": "Django management script",
                "language": "python"
            },
            {
                "path": "requirements.txt",
                "content": await self._generate_content("django/enhanced/requirements.txt", description, {"rest_framework": include_rest_framework, **options}),
                "purpose": "Python dependencies",
                "language": "text"
            },
            {
                "path": "README.md",
                "content": await self._generate_content("django/enhanced/README.md", description, {"project_name": project_name, "app_name": app_name, **options}),
                "purpose": "Project documentation",
                "language": "markdown"
            },
            {
                "path": ".gitignore",
                "content": await self._generate_content("django/enhanced/.gitignore", description, options),
                "purpose": "Git ignore configuration",
                "language": "gitignore"
            }
        ])
        
        # Add Docker support
        if options.get("docker", True):
            files.extend([
                {
                    "path": "Dockerfile",
                    "content": await self._generate_content("django/enhanced/Dockerfile", description, {"app_name": app_name, **options}),
                    "purpose": "Docker configuration",
                    "language": "dockerfile"
                },
                {
                    "path": "docker-compose.yml",
                    "content": await self._generate_content("django/enhanced/docker-compose.yml", description, {"project_name": project_name, **options}),
                    "purpose": "Docker Compose configuration",
                    "language": "yaml"
                },
                {
                    "path": ".dockerignore",
                    "content": await self._generate_content("django/enhanced/.dockerignore", description, options),
                    "purpose": "Docker ignore configuration",
                    "language": "text"
                }
            ])
        # Removed the extra closing brace that was here
        
        # Project configuration
        files.extend([
            {
                "path": f"{project_name}/__init__.py",
                "content": "",
                "purpose": "Package initialization",
                "language": "python"
            },
            {
                "path": f"{project_name}/settings.py",
                "content": await self._generate_content("django/enhanced/settings.py", description, {
                    "project_name": project_name, 
                    "app_name": app_name,
                    "rest_framework": include_rest_framework,
                    **options
                }),
                "purpose": "Django settings",
                "language": "python"
            },
            {
                "path": f"{project_name}/urls.py",
                "content": await self._generate_content("django/enhanced/urls.py", description, {
                    "project_name": project_name, 
                    "app_name": app_name,
                    "admin": include_admin,
                    "rest_framework": include_rest_framework,
                    **options
                }),
                "purpose": "URL configuration",
                "language": "python"
            },
            {
                "path": f"{project_name}/wsgi.py",
                "content": await self._generate_content("django/enhanced/wsgi.py", description, {"project_name": project_name, **options}),
                "purpose": "WSGI configuration",
                "language": "python"
            },
            {
                "path": f"{project_name}/asgi.py",
                "content": await self._generate_content("django/enhanced/asgi.py", description, {"project_name": project_name, **options}),
                "purpose": "ASGI configuration",
                "language": "python"
            }
        ])
        
        # Main app structure
        files.extend([
            {
                "path": f"{app_name}/__init__.py",
                "content": "",
                "purpose": "App initialization",
                "language": "python"
            },
            {
                "path": f"{app_name}/admin.py",
                "content": await self._generate_content("django/enhanced/admin.py", description, {"app_name": app_name, **options}),
                "purpose": "Admin configuration",
                "language": "python"
            },
            {
                "path": f"{app_name}/apps.py",
                "content": await self._generate_content("django/enhanced/apps.py", description, {"app_name": app_name, **options}),
                "purpose": "App configuration",
                "language": "python"
            },
            {
                "path": f"{app_name}/models.py",
                "content": await self._generate_content("django/enhanced/models.py", description, {"app_name": app_name, **options}),
                "purpose": "Data models",
                "language": "python"
            },
            {
                "path": f"{app_name}/views.py",
                "content": await self._generate_content("django/enhanced/views.py", description, {
                    "app_name": app_name,
                    "rest_framework": include_rest_framework,
                    **options
                }),
                "purpose": "View functions",
                "language": "python"
            },
            {
                "path": f"{app_name}/urls.py",
                "content": await self._generate_content("django/enhanced/app_urls.py", description, {
                    "app_name": app_name,
                    "rest_framework": include_rest_framework,
                    **options
                }),
                "purpose": "App URL configuration",
                "language": "python"
            },
            {
                "path": f"{app_name}/tests.py",
                "content": await self._generate_content("django/enhanced/tests.py", description, {"app_name": app_name, **options}),
                "purpose": "Test cases",
                "language": "python"
            }
        ])
        
        # Add REST framework files if requested
        if include_rest_framework:
            files.extend([
                {
                    "path": f"{app_name}/serializers.py",
                    "content": await self._generate_content("django/enhanced/serializers.py", description, {"app_name": app_name, **options}),
                    "purpose": "API serializers",
                    "language": "python"
                },
                {
                    "path": f"{app_name}/api.py",
                    "content": await self._generate_content("django/enhanced/api.py", description, {"app_name": app_name, **options}),
                    "purpose": "API views",
                    "language": "python"
                }
            ])
        
        # Add authentication files if requested
        if include_authentication:
            files.extend([
                {
                    "path": f"{app_name}/auth.py",
                    "content": await self._generate_content("django/enhanced/auth.py", description, {"app_name": app_name, **options}),
                    "purpose": "Authentication utilities",
                    "language": "python"
                }
            ])
        
        # Add templates if requested
        if include_templates:
            files.extend([
                {
                    "path": f"{app_name}/templates/{app_name}/base.html",
                    "content": await self._generate_content("django/enhanced/templates/base.html", description, {"app_name": app_name, **options}),
                    "purpose": "Base template",
                    "language": "html"
                },
                {
                    "path": f"{app_name}/templates/{app_name}/index.html",
                    "content": await self._generate_content("django/enhanced/templates/index.html", description, {"app_name": app_name, **options}),
                    "purpose": "Index template",
                    "language": "html"
                }
            ])
        
        # Add static files if requested
        if include_static:
            files.extend([
                {
                    "path": f"{app_name}/static/{app_name}/css/style.css",
                    "content": await self._generate_content("django/enhanced/static/style.css", description, options),
                    "purpose": "Main stylesheet",
                    "language": "css"
                },
                {
                    "path": f"{app_name}/static/{app_name}/js/main.js",
                    "content": await self._generate_content("django/enhanced/static/main.js", description, options),
                    "purpose": "Main JavaScript file",
                    "language": "javascript"
                }
            ])
        
        # Generate files
        generated_files = []
        for file_info in files:
            generated_files.append(CodeFile(
                path=file_info["path"],
                content=file_info["content"],
                purpose=file_info["purpose"],
                dependencies=[],
                language=file_info["language"]
            ))
        
        return {
            "success": True,
            "framework": "django",
            "files": generated_files,
            "project_type": "python",
            "project_name": project_name,
            "app_name": app_name,
            "rest_framework": include_rest_framework,
            "authentication": include_authentication,
            "admin": include_admin,
            "templates": include_templates,
            "static": include_static
        }


    async def _generate_file_content(
        self, 
        framework: str,
        file_path: str,
        purpose: str,
        description: str,
        options: Dict[str, Any]
    ) -> str:
        """
        Generate content for a file using AI.
        
        Args:
            framework: Framework name
            file_path: Path to the file
            purpose: Purpose of the file
            description: Project description
            options: Additional options
            
        Returns:
            Generated file content
        """
        # Determine the programming language from the file extension
        ext = Path(file_path).suffix.lower()
        language_map = {
            ".py": "Python",
            ".js": "JavaScript",
            ".jsx": "JavaScript (React)",
            ".ts": "TypeScript",
            ".tsx": "TypeScript (React)",
            ".java": "Java",
            ".html": "HTML",
            ".css": "CSS",
            ".scss": "SCSS",
            ".json": "JSON",
            ".xml": "XML",
            ".yaml": "YAML",
            ".yml": "YAML",
            ".md": "Markdown",
            ".sql": "SQL",
            ".go": "Go",
            ".rs": "Rust",
            ".rb": "Ruby",
            ".php": "PHP",
            ".c": "C",
            ".cpp": "C++",
            ".h": "C/C++ Header",
            ".cs": "C#",
            ".swift": "Swift",
            ".kt": "Kotlin",
            ".vue": "Vue"
        }
        language = language_map.get(ext, "Unknown")
        
        # Build the prompt
        prompt = f"""
    Generate content for a {language} file in a {framework} project.
    File path: {file_path}
    File purpose: {purpose}
    Project description: "{description}"
    Requirements:
    
    Generate complete, valid code for a {language} file
    Ensure the code follows best practices for {framework} projects
    Make the code clean, well-structured, and well-commented
    Only include code relevant to the file's purpose and path
    Match the style and idioms typically used in {framework} projects
    
    Only respond with the file content, nothing else.
    """
        # Add language-specific instructions
        if language == "Python":
            prompt += "\nInclude appropriate imports and docstrings. Follow PEP 8 guidelines."
        elif language in ["JavaScript", "TypeScript"]:
            prompt += "\nUse modern ES6+ syntax. Include appropriate imports/exports."
        elif language in ["JavaScript (React)", "TypeScript (React)"]:
            prompt += "\nUse functional components with hooks. Include appropriate imports."
        elif language == "Java":
            prompt += "\nInclude appropriate package declaration, imports, and JavaDoc comments."
        # Add framework-specific information
        if framework == "react":
            if "variant" in options and options["variant"] == "nextjs":
                prompt += "\nThis is a Next.js project. Use Next.js-specific patterns and API."
            else:
                prompt += "\nThis is a Create React App project. Use appropriate React patterns."
        elif framework == "django":
            prompt += f"\nProject name: {options.get('project_name', 'django_project')}"
            prompt += f"\nApp name: {options.get('app_name', 'main')}"
        
        try:
            # Call AI service
            api_request = GeminiRequest(
                prompt=prompt,
                max_tokens=4000,
                temperature=0.2
            )
            
            response = await gemini_client.generate_text(api_request)
            
            # Extract code from the response
            code_match = re.search(r'```(?:\w+)?\s*(.*?)\s*```', response.text, re.DOTALL)
            if code_match:
                return code_match.group(1)
            
            # No code block found, use the entire response
            return response.text.strip()
        except Exception as e:
            self._logger.error(f"Error generating content for {file_path}: {str(e)}", exc_info=True)
            return f"# Error generating content: {str(e)}\n# Please regenerate this file"

framework_generator = FrameworkGenerator()
</file>

<file path="components/generation/models.py">
# angela/generation/models.py
"""
Data models for code generation.

This module defines the data models used throughout the code generation system,
extracted to avoid circular dependencies between modules.
"""
from typing import Dict, Any, List, Optional, Set, Union
from pathlib import Path
from pydantic import BaseModel, Field

class CodeFile(BaseModel):
    """Model for a code file to be generated."""
    path: str = Field(..., description="Relative path to the file")
    content: str = Field(..., description="Content of the file")
    purpose: str = Field(..., description="Purpose/description of the file")
    dependencies: List[str] = Field(default_factory=list, description="Paths of files this depends on")
    language: Optional[str] = Field(None, description="Programming language of the file")

class CodeProject(BaseModel):
    """Model for a complete code project to be generated."""
    name: str = Field(..., description="Name of the project")
    description: str = Field(..., description="Description of the project")
    root_dir: str = Field(..., description="Root directory for the project")
    files: List[CodeFile] = Field(..., description="List of files to generate")
    dependencies: Dict[str, List[str]] = Field(default_factory=dict, description="External dependencies")
    project_type: str = Field(..., description="Type of project (e.g., python, node)")
    structure_explanation: str = Field(..., description="Explanation of the project structure")
</file>

<file path="components/generation/planner.py">
# angela/generation/planner.py
"""
Project structure planning for Angela CLI.

This module is responsible for planning the structure of a new project,
identifying necessary files, their roles, and interdependencies.
"""
import os
import asyncio
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple, Union
import json
import re
from pydantic import BaseModel, Field


from angela.api.generation import get_generation_context_manager, get_code_file_class, get_code_project_class
from angela.api.ai import get_gemini_client, get_gemini_request_class
from angela.api.context import get_context_manager
from angela.utils.logging import get_logger



logger = get_logger(__name__)
GeminiRequest = get_gemini_request_class()
CodeFile = get_code_file_class()
CodeProject = get_code_project_class()

class ComponentRelationship(BaseModel):
    """Model for relationships between architecture components."""
    source: str = Field(..., description="Source component")
    target: str = Field(..., description="Target component")
    type: str = Field(..., description="Type of relationship (e.g., 'uses', 'inherits', 'implements')")
    description: Optional[str] = Field(None, description="Optional description of the relationship")
    
    
class ArchitectureComponent(BaseModel):
    """Model for a component in the project architecture."""
    name: str = Field(..., description="Name of the component")
    description: str = Field(..., description="Description of the component")
    responsibilities: List[str] = Field(..., description="Responsibilities of the component")
    files: List[str] = Field(default_factory=list, description="Files implementing this component")
    dependencies: List[str] = Field(default_factory=list, description="Components this depends on")

class ProjectArchitecture(BaseModel):
    """Model for a project's architecture."""
    components: List[ArchitectureComponent] = Field(..., description="Components in the architecture")
    layers: List[str] = Field(default_factory=list, description="Architectural layers")
    patterns: List[str] = Field(default_factory=list, description="Design patterns used")
    data_flow: List[str] = Field(default_factory=list, description="Data flow descriptions")
    relationships: List[ComponentRelationship] = Field(default_factory=list, description="Relationships between components")
    structure_type: str = Field("layered", description="Type of architecture structure (e.g., 'layered', 'modular', 'microservices')")


class ProjectPlanner:
    """
    Project planner for designing and organizing code projects.
    """
    
    def __init__(self):
        """Initialize the project planner."""
        self._logger = logger
    
    async def create_project_architecture(
        self, 
        description: str,
        project_type: str,
        context: Optional[Dict[str, Any]] = None
    ) -> ProjectArchitecture:
        """
        Create a high-level architecture for a project.
        
        Args:
            description: Natural language description of the project
            project_type: Type of project to generate
            context: Additional context information
            
        Returns:
            ProjectArchitecture object
        """
        self._logger.info(f"Creating project architecture for: {description}")
        
        # Get context if not provided
        if context is None:
            context = context_manager.get_context_dict()
        
        # Build prompt for architecture planning
        prompt = self._build_architecture_prompt(description, project_type, context)
        
        # Call AI service to generate architecture
        api_request = GeminiRequest(
            prompt=prompt,
            max_tokens=4000,
            temperature=0.2
        )
        
        self._logger.debug("Sending architecture planning request to AI service")
        response = await gemini_client.generate_text(api_request)
        
        # Parse the response to extract the architecture
        architecture = await self._parse_architecture(response.text)
        
        return architecture
    
    async def refine_project_plan(
        self, 
        project: CodeProject,
        architecture: ProjectArchitecture,
        context: Optional[Dict[str, Any]] = None
    ) -> CodeProject:
        """
        Refine a project plan based on architecture.
        
        Args:
            project: Initial CodeProject
            architecture: ProjectArchitecture to use for refinement
            context: Additional context information
            
        Returns:
            Refined CodeProject
        """
        self._logger.info(f"Refining project plan for: {project.name}")
        
        # Get context if not provided
        if context is None:
            context = context_manager.get_context_dict()
        
        # Build prompt for plan refinement
        prompt = self._build_plan_refinement_prompt(project, architecture, context)
        
        # Call AI service to refine plan
        api_request = GeminiRequest(
            prompt=prompt,
            max_tokens=4000,
            temperature=0.2
        )
        
        self._logger.debug("Sending plan refinement request to AI service")
        response = await gemini_client.generate_text(api_request)
        
        # Parse the response to extract the refined plan
        refined_plan = await self._parse_refined_plan(response.text, project)
        
        return refined_plan
    
    def _build_architecture_prompt(
        self, 
        description: str,
        project_type: str,
        context: Dict[str, Any]
    ) -> str:
        """
        Build a prompt for architecture planning.
        
        Args:
            description: Natural language description of the project
            project_type: Type of project to generate
            context: Additional context information
            
        Returns:
            Prompt string for the AI service
        """
        prompt = f"""
    As an experienced software architect, design a high-level architecture for a {project_type} project based on this description:
    
    "{description}"
    
    Analyze the requirements and create a comprehensive architecture that is:
    - Modular and maintainable
    - Follows SOLID principles
    - Anticipates future changes/extensions
    - Accounts for scalability and performance
    
    Your response should be a JSON object with this structure:
    
    ```json
    {{
      "components": [
        {{
          "name": "component_name",
          "description": "what this component does",
          "responsibilities": ["resp1", "resp2", ...],
          "files": ["expected/path/to/file.ext", ...],
          "dependencies": ["other_component_names", ...]
        }},
        ...
      ],
      "layers": ["Layer1", "Layer2", ...],
      "patterns": ["Design patterns used in the architecture"],
      "data_flow": ["Descriptions of data flow between components"]
    }}
    Focus on a clean separation of concerns, appropriate design patterns for {project_type}, and efficient data flow.
    """
        return prompt

    async def _parse_architecture(self, response: str) -> ProjectArchitecture:
        """
        Parse the AI response to extract the architecture.
        
        Args:
            response: AI response text
            
        Returns:
            ProjectArchitecture object
        """
        try:
            # Look for JSON block in the response
            json_match = re.search(r'```(?:json)?\s*(.*?)\s*```', response, re.DOTALL)
            if json_match:
                json_str = json_match.group(1)
            else:
                # Try to find JSON without code blocks
                json_match = re.search(r'({.*})', response, re.DOTALL)
                if json_match:
                    json_str = json_match.group(1)
                else:
                    # Assume the entire response is JSON
                    json_str = response
            
            # Parse the JSON
            arch_data = json.loads(json_str)
            
            # Create ArchitectureComponent objects
            components = []
            for comp_data in arch_data.get("components", []):
                components.append(ArchitectureComponent(
                    name=comp_data["name"],
                    description=comp_data["description"],
                    responsibilities=comp_data.get("responsibilities", []),
                    files=comp_data.get("files", []),
                    dependencies=comp_data.get("dependencies", [])
                ))
            
            # Create ProjectArchitecture object
            architecture = ProjectArchitecture(
                components=components,
                layers=arch_data.get("layers", []),
                patterns=arch_data.get("patterns", []),
                data_flow=arch_data.get("data_flow", [])
            )
            
            return architecture
            
        except Exception as e:
            self._logger.exception(f"Error parsing architecture: {str(e)}")
            
            # Create a minimal fallback architecture
            fallback_component = ArchitectureComponent(
                name="Core",
                description="Core application functionality",
                responsibilities=["Main application logic"],
                files=[],
                dependencies=[]
            )
            
            return ProjectArchitecture(
                components=[fallback_component],
                layers=["Presentation", "Business Logic", "Data Access"],
                patterns=["MVC"],
                data_flow=["User input -> Core processing -> Storage"]
            )

    async def _build_plan_refinement_prompt(
        self, 
        project: CodeProject,
        architecture: ProjectArchitecture,
        context: Dict[str, Any]
    ) -> str:
        """
        Build a prompt for plan refinement.
        
        Args:
            project: Initial CodeProject
            architecture: ProjectArchitecture to use for refinement
            context: Additional context information
            
        Returns:
            Prompt string for the AI service
        """
        # Extract architecture info
        arch_json = {}
        arch_json["components"] = [comp.dict() for comp in architecture.components]
        arch_json["layers"] = architecture.layers
        arch_json["patterns"] = architecture.patterns
        arch_json["data_flow"] = architecture.data_flow
        
        # Extract project info
        project_json = {}
        project_json["name"] = project.name
        project_json["description"] = project.description
        project_json["project_type"] = project.project_type
        project_json["dependencies"] = project.dependencies
        project_json["files"] = [
            {
                "path": file.path,
                "purpose": file.purpose,
                "dependencies": file.dependencies
            }
            for file in project.files
        ]
        
        prompt = f"""
    You are refining a project plan based on a high-level architecture.
    Here is the current project plan:
    json{json.dumps(project_json, indent=2)}
    And here is the architecture design:
    json{json.dumps(arch_json, indent=2)}
    Your task is to refine the project plan to better align with the architecture.
    This may involve:
    
    1. Adding missing files that would be needed for components in the architecture
    2. Updating existing file purposes to match component responsibilities
    3. Adjusting file dependencies to match component dependencies
    4. Ensuring the project structure follows the architectural layers
    
    Return a refined project plan in this JSON format:
    json{{
      "name": "project_name",
      "description": "project description",
      "project_type": "{project.project_type}",
      "dependencies": {{
        "runtime": ["dep1", "dep2"],
        "development": ["dev_dep1", "dev_dep2"]
      }},
      "files": [
        {{
          "path": "path/to/file.ext",
          "purpose": "file purpose",
          "dependencies": ["other/file/paths"],
          "component": "associated_component_name"
        }}
      ],
      "structure_explanation": "explanation of the refined structure"
    }}
    Make sure the refined plan implements all components and follows all architectural patterns in the design.
    """
        return prompt

    async def create_detailed_project_architecture(
        self, 
        description: str,
        project_type: str,
        framework: Optional[str] = None,
        context: Optional[Dict[str, Any]] = None
    ) -> ProjectArchitecture:
        """
        Create a detailed architecture for a project with component relationships.
        
        Args:
            description: Natural language description of the project
            project_type: Type of project to generate
            framework: Optional framework to use (e.g., 'react', 'django')
            context: Additional context information
            
        Returns:
            ProjectArchitecture object with detailed component relationships
        """
        self._logger.info(f"Creating detailed project architecture for: {description}")
        
        # Get context if not provided
        if context is None:
            context_manager = get_context_manager()
            context = context_manager.get_context_dict()
        
        # Determine appropriate architecture style based on project type and framework
        architecture_style = self._determine_architecture_style(project_type, framework)
        
        # Build enhanced prompt for architecture planning
        prompt = self._build_detailed_architecture_prompt(description, project_type, framework, architecture_style, context)
        
        # Call AI service to generate architecture
        gemini_client = get_gemini_client()
        api_request = GeminiRequest(
            prompt=prompt,
            max_tokens=8000,  # Increased token limit for more detailed planning
            temperature=0.2
        )
        
        self._logger.debug("Sending detailed architecture planning request to AI service")
        response = await gemini_client.generate_text(api_request)
        
        # Parse the response to extract the architecture
        architecture = await self._parse_detailed_architecture(response.text, architecture_style)
        
        # Store in global context
        generation_context_manager = get_generation_context_manager()
        generation_context_manager.set_global_context("architecture", architecture.dict())
        generation_context_manager.set_global_context("project_type", project_type)
        if framework:
            generation_context_manager.set_global_context("framework", framework)
        
        return architecture
    
    def _determine_architecture_style(self, project_type: str, framework: Optional[str]) -> str:
        """
        Determine appropriate architecture style based on project type and framework.
        
        Args:
            project_type: Type of project
            framework: Optional framework name
            
        Returns:
            Architecture style string
        """
        # Default to layered architecture
        style = "layered"
        
        # Web application frameworks often use MVC or similar
        if framework in ["django", "flask", "spring", "rails", "laravel"]:
            style = "mvc"
        # Modern JavaScript frameworks often use component-based architecture
        elif framework in ["react", "vue", "angular"]:
            style = "component-based"
        # API-focused projects might use clean architecture
        elif "api" in project_type or project_type == "node" and not framework:
            style = "clean-architecture"
        # Microservices for larger distributed systems
        elif "microservice" in project_type:
            style = "microservices"
        
        return style
    
    def _build_detailed_architecture_prompt(
        self, 
        description: str,
        project_type: str,
        framework: Optional[str],
        architecture_style: str,
        context: Dict[str, Any]
    ) -> str:
        """
        Build a detailed prompt for architecture planning.
        
        Args:
            description: Project description
            project_type: Type of project
            framework: Optional framework
            architecture_style: Architecture style
            context: Additional context
            
        Returns:
            Prompt string for the AI service
        """
        framework_str = f" using {framework}" if framework else ""
        
        prompt = f"""
As an expert software architect, design a detailed architecture for a {project_type}{framework_str} project based on this description:

"{description}"

I'm looking for a {architecture_style} style architecture that is:
- Modular and maintainable
- Follows SOLID principles
- Anticipates future changes/extensions
- Accounts for scalability and performance

In your design:
1. Follow best practices for {project_type}{framework_str} projects
2. Use patterns and conventions typical of {architecture_style} architectures
3. Include clear separation of concerns
4. Show relationships between components

Your response should be a JSON object with this structure:

```json
{{
  "components": [
    {{
      "name": "component_name",
      "description": "detailed description of this component",
      "responsibilities": ["resp1", "resp2", ...],
      "files": ["expected/path/to/file.ext", ...],
      "dependencies": ["other_component_names", ...]
    }},
    ...
  ],
  "layers": ["Layer1", "Layer2", ...],
  "patterns": ["Design patterns used in the architecture"],
  "data_flow": ["Descriptions of data flow between components"],
  "relationships": [
    {{
      "source": "component_name",
      "target": "other_component",
      "type": "relationship_type", 
      "description": "details about how they relate"
    }},
    ...
  ],
  "structure_type": "{architecture_style}"
}}
"""

        # Add specific guidance based on architecture style
        if architecture_style == "mvc":
            prompt += """
For MVC architecture, include these components:
- Models (data and business logic)
- Views (user interface elements)
- Controllers (handle requests and coordinate models and views)
- Routes/URL configuration
- Services (optional business logic layer)
- Data access/repositories (for database interactions)
"""
        elif architecture_style == "component-based":
            prompt += """
For component-based architecture, consider:
- UI Components (reusable interface elements)
- Container Components (manage state and data flow)
- Services (handle data fetching and processing)
- Stores/State Management (centralized state)
- Utilities/Helpers (reusable functions)
- API Integration (services for external communication)
"""
        elif architecture_style == "clean-architecture":
            prompt += """
For clean architecture, include these layers:
- Entities (core business objects)
- Use Cases/Interactors (application-specific business rules)
- Interface Adapters (controllers, presenters, gateways)
- Frameworks & Drivers (external interfaces, web, DB, UI)

Ensure the dependency rule is followed: outer layers can depend on inner layers, but inner layers cannot depend on outer layers.
"""
        elif architecture_style == "microservices":
            prompt += """
For microservices architecture, consider:
- Individual service components (each with its own responsibility)
- API Gateways
- Service Discovery
- Communication patterns between services
- Data management strategies (database per service or shared)
- Cross-cutting concerns (logging, monitoring, security)
"""
        
        return prompt
    
    async def _parse_detailed_architecture(
        self, 
        response: str,
        architecture_style: str
    ) -> ProjectArchitecture:
        """
        Parse the AI response to extract the detailed architecture.
        
        Args:
            response: AI response text
            architecture_style: The architecture style
            
        Returns:
            ProjectArchitecture object
        """
        try:
            # Look for JSON block in the response
            json_match = re.search(r'```(?:json)?\s*(.*?)\s*```', response, re.DOTALL)
            if json_match:
                json_str = json_match.group(1)
            else:
                # Try to find JSON without code blocks
                json_match = re.search(r'({.*})', response, re.DOTALL)
                if json_match:
                    json_str = json_match.group(1)
                else:
                    # Assume the entire response is JSON
                    json_str = response
            
            # Parse the JSON
            arch_data = json.loads(json_str)
            
            # Create ArchitectureComponent objects
            components = []
            for comp_data in arch_data.get("components", []):
                components.append(ArchitectureComponent(
                    name=comp_data["name"],
                    description=comp_data["description"],
                    responsibilities=comp_data.get("responsibilities", []),
                    files=comp_data.get("files", []),
                    dependencies=comp_data.get("dependencies", [])
                ))
            
            # Create ComponentRelationship objects
            relationships = []
            for rel_data in arch_data.get("relationships", []):
                relationships.append(ComponentRelationship(
                    source=rel_data["source"],
                    target=rel_data["target"],
                    type=rel_data["type"],
                    description=rel_data.get("description")
                ))
            
            # Create ProjectArchitecture object
            architecture = ProjectArchitecture(
                components=components,
                layers=arch_data.get("layers", []),
                patterns=arch_data.get("patterns", []),
                data_flow=arch_data.get("data_flow", []),
                relationships=relationships,
                structure_type=arch_data.get("structure_type", architecture_style)
            )
            
            return architecture
            
        except Exception as e:
            self._logger.exception(f"Error parsing detailed architecture: {str(e)}")
            
            # Create a minimal fallback architecture
            fallback_component = ArchitectureComponent(
                name="Core",
                description="Core application functionality",
                responsibilities=["Main application logic"],
                files=[],
                dependencies=[]
            )
            
            return ProjectArchitecture(
                components=[fallback_component],
                layers=["Presentation", "Business Logic", "Data Access"],
                patterns=["MVC"],
                data_flow=["User input -> Core processing -> Storage"],
                relationships=[],
                structure_type=architecture_style
            )
            
    async def generate_dependency_graph(self, architecture: ProjectArchitecture) -> Dict[str, Any]:
        """
        Generate a dependency graph from the architecture.
        
        Args:
            architecture: The project architecture
            
        Returns:
            Dictionary with nodes and edges for visualization
        """
        nodes = []
        edges = []
        
        # Create nodes for each component
        for component in architecture.components:
            nodes.append({
                "id": component.name,
                "label": component.name,
                "type": "component"
            })
            
            # Add edges for component dependencies
            for dependency in component.dependencies:
                edges.append({
                    "source": component.name,
                    "target": dependency,
                    "type": "depends_on"
                })
        
        # Add edges for explicit relationships
        for relationship in architecture.relationships:
            # Check if the edge already exists
            edge_exists = False
            for edge in edges:
                if edge["source"] == relationship.source and edge["target"] == relationship.target:
                    edge_exists = True
                    break
            
            if not edge_exists:
                edges.append({
                    "source": relationship.source,
                    "target": relationship.target,
                    "type": relationship.type
                })
        
        return {
            "nodes": nodes,
            "edges": edges
        }
        
    async def create_project_plan_from_architecture(
        self, 
        architecture: ProjectArchitecture,
        project_name: str,
        project_type: str,
        description: str,
        framework: Optional[str] = None,
        context: Optional[Dict[str, Any]] = None
    ) -> CodeProject:
        """
        Create a more detailed project plan based on architecture.
        
        Args:
            architecture: The project architecture
            project_name: Name of the project
            project_type: Type of project
            description: Project description
            framework: Optional framework
            context: Additional context
            
        Returns:
            CodeProject with detailed file structure
        """
        self._logger.info(f"Creating project plan from architecture for: {project_name}")
        
        # Get context if not provided
        if context is None:
            context = context_manager.get_context_dict()
            
        # Get root directory
        root_dir = context.get("cwd", ".")
        
        # Determine dependencies based on project type and framework
        dependencies = await self._determine_project_dependencies(project_type, framework, description)
        
        # Create files from architecture components
        files = []
        for component in architecture.components:
            component_files = await self._create_files_for_component(
                component, 
                project_type, 
                framework
            )
            files.extend(component_files)
        
        # Add standard project files
        standard_files = await self._add_standard_project_files(
            project_type, 
            framework, 
            project_name
        )
        files.extend(standard_files)
        
        # Create the project
        project = CodeProject(
            name=project_name,
            description=description,
            root_dir=root_dir,
            files=files,
            dependencies=dependencies,
            project_type=project_type,
            structure_explanation=f"Project follows {architecture.structure_type} architecture with {len(architecture.components)} components across {len(architecture.layers)} layers."
        )
        
        return project
    
    async def _determine_project_dependencies(
        self, 
        project_type: str, 
        framework: Optional[str],
        description: str
    ) -> Dict[str, List[str]]:
        """
        Determine project dependencies based on type and framework.
        
        Args:
            project_type: Type of project
            framework: Optional framework
            description: Project description
            
        Returns:
            Dictionary with runtime and development dependencies
        """
        # Build prompt for dependency determination
        prompt = f"""
    As an expert software developer, determine the necessary dependencies for a {project_type} project{f' using {framework}' if framework else ''} based on this description:
    
    "{description}"
    
    Return a JSON object with runtime and development dependencies:
    ```json
    {{
      "runtime": ["dep1", "dep2", ...],
      "development": ["dev_dep1", "dev_dep2", ...]
    }}
    Include only the necessary dependencies for the core functionality described.
    Use the latest stable versions and follow best practices for {project_type} projects.
    """
        # Call AI service
        gemini_client = get_gemini_client()
        api_request = GeminiRequest(
            prompt=prompt,
            max_tokens=2000,
            temperature=0.2
        )
        
        self._logger.debug("Sending dependency determination request to AI service")
        response = await gemini_client.generate_text(api_request)
        
        try:
            # Extract JSON from response
            json_match = re.search(r'```(?:json)?\s*(.*?)\s*```', response.text, re.DOTALL)
            if json_match:
                json_str = json_match.group(1)
            else:
                # Try to find JSON without code blocks
                json_match = re.search(r'({.*})', response.text, re.DOTALL)
                if json_match:
                    json_str = json_match.group(1)
                else:
                    # Assume the entire response is JSON
                    json_str = response.text
            
            # Parse JSON
            dependencies = json.loads(json_str)
            
            # Ensure correct structure
            if "runtime" not in dependencies:
                dependencies["runtime"] = []
            if "development" not in dependencies:
                dependencies["development"] = []
            
            return dependencies
            
        except Exception as e:
            self._logger.error(f"Error parsing dependencies: {str(e)}")
            
            # Return default dependencies based on project type and framework
            if project_type == "python":
                if framework == "django":
                    return {
                        "runtime": ["django", "django-rest-framework", "psycopg2-binary"],
                        "development": ["pytest", "pytest-django", "flake8", "black"]
                    }
                elif framework == "flask":
                    return {
                        "runtime": ["flask", "flask-sqlalchemy", "flask-migrate", "flask-cors"],
                        "development": ["pytest", "pytest-flask", "flake8", "black"]
                    }
                elif framework == "fastapi":
                    return {
                        "runtime": ["fastapi", "uvicorn", "sqlalchemy", "pydantic"],
                        "development": ["pytest", "black", "isort", "mypy"]
                    }
                else:
                    return {
                        "runtime": ["requests", "pydantic"],
                        "development": ["pytest", "black", "isort"]
                    }
            elif project_type == "node":
                if framework == "react":
                    return {
                        "runtime": ["react", "react-dom", "react-router-dom"],
                        "development": ["@testing-library/react", "jest", "eslint", "prettier"]
                    }
                elif framework == "express":
                    return {
                        "runtime": ["express", "cors", "mongoose", "dotenv"],
                        "development": ["nodemon", "jest", "supertest", "eslint"]
                    }
                else:
                    return {
                        "runtime": ["axios", "dotenv"],
                        "development": ["jest", "eslint", "prettier"]
                    }
            else:
                return {
                    "runtime": [],
                    "development": []
                }

    async def _create_files_for_component(
        self, 
        component: ArchitectureComponent,
        project_type: str,
        framework: Optional[str]
    ) -> List[CodeFile]:
        """
        Create CodeFile objects for a component based on its responsibilities.
        
        Args:
            component: The architecture component
            project_type: Type of project
            framework: Optional framework
            
        Returns:
            List of CodeFile objects
        """
        files = []
        
        # If component already has files defined, use those
        if component.files:
            for file_path in component.files:
                files.append(CodeFile(
                    path=file_path,
                    content="",  # Content will be generated later
                    purpose=f"Part of the {component.name} component: {component.description}",
                    dependencies=[],
                    language=self._get_language_from_file_path(file_path, project_type)
                ))
            
            return files
        
        # Otherwise, generate files based on the component name and responsibilities
        component_path = component.name.lower().replace(" ", "_")
        
        # Handle different project types
        if project_type == "python":
            # Create a Python package
            files.append(CodeFile(
                path=f"{component_path}/__init__.py",
                content="",
                purpose=f"Package initialization for {component.name}",
                dependencies=[],
                language="python"
            ))
            
            # Add main module
            files.append(CodeFile(
                path=f"{component_path}/main.py",
                content="",
                purpose=f"Main module for {component.name}: {component.description}",
                dependencies=[],
                language="python"
            ))
            
            # Add files based on responsibilities
            for resp in component.responsibilities:
                # Convert responsibility to a file name
                resp_name = resp.lower().replace(" ", "_").replace("-", "_")
                resp_name = re.sub(r'[^a-z0-9_]', '', resp_name)
                
                # Skip if too generic
                if resp_name in ["main", "init", "core", "base"]:
                    continue
                
                # Create file
                files.append(CodeFile(
                    path=f"{component_path}/{resp_name}.py",
                    content="",
                    purpose=f"Handles {resp} in the {component.name} component",
                    dependencies=[f"{component_path}/__init__.py"],
                    language="python"
                ))
                
        elif project_type == "node":
            if framework in ["react", "vue", "angular"]:
                # Frontend component structure
                component_path = f"src/components/{component_path}"
                
                if framework == "react":
                    # React component
                    files.append(CodeFile(
                        path=f"{component_path}/index.js",
                        content="",
                        purpose=f"Main file for {component.name} React component",
                        dependencies=[],
                        language="javascript"
                    ))
                    
                    files.append(CodeFile(
                        path=f"{component_path}/{component.name.replace(' ', '')}.js",
                        content="",
                        purpose=f"{component.name} React component: {component.description}",
                        dependencies=[],
                        language="javascript"
                    ))
                    
                    files.append(CodeFile(
                        path=f"{component_path}/{component.name.replace(' ', '')}.css",
                        content="",
                        purpose=f"Styles for {component.name} React component",
                        dependencies=[],
                        language="css"
                    ))
                    
                elif framework == "vue":
                    # Vue component
                    files.append(CodeFile(
                        path=f"{component_path}/{component.name.replace(' ', '')}.vue",
                        content="",
                        purpose=f"{component.name} Vue component: {component.description}",
                        dependencies=[],
                        language="vue"
                    ))
                    
                elif framework == "angular":
                    # Angular component
                    component_selector = component.name.toLowerCase().replace(" ", "-")
                    files.append(CodeFile(
                        path=f"{component_path}/{component_selector}.component.ts",
                        content="",
                        purpose=f"{component.name} Angular component: {component.description}",
                        dependencies=[],
                        language="typescript"
                    ))
                    
                    files.append(CodeFile(
                        path=f"{component_path}/{component_selector}.component.html",
                        content="",
                        purpose=f"Template for {component.name} Angular component",
                        dependencies=[],
                        language="html"
                    ))
                    
                    files.append(CodeFile(
                        path=f"{component_path}/{component_selector}.component.css",
                        content="",
                        purpose=f"Styles for {component.name} Angular component",
                        dependencies=[],
                        language="css"
                    ))
            else:
                # Backend Node.js structure
                if "controller" in component.name.lower() or "route" in component.name.lower():
                    # API controllers/routes
                    files.append(CodeFile(
                        path=f"src/routes/{component_path}.js",
                        content="",
                        purpose=f"{component.name}: {component.description}",
                        dependencies=[],
                        language="javascript"
                    ))
                elif "model" in component.name.lower():
                    # Database models
                    files.append(CodeFile(
                        path=f"src/models/{component_path}.js",
                        content="",
                        purpose=f"{component.name}: {component.description}",
                        dependencies=[],
                        language="javascript"
                    ))
                elif "service" in component.name.lower():
                    # Services
                    files.append(CodeFile(
                        path=f"src/services/{component_path}.js",
                        content="",
                        purpose=f"{component.name}: {component.description}",
                        dependencies=[],
                        language="javascript"
                    ))
                else:
                    # Generic module
                    files.append(CodeFile(
                        path=f"src/{component_path}/index.js",
                        content="",
                        purpose=f"Main file for {component.name}",
                        dependencies=[],
                        language="javascript"
                    ))
        
        elif project_type == "java":
            # Java package structure
            base_package = "com.example.app"
            component_package = component.name.toLowerCase().replace(" ", "")
            
            files.append(CodeFile(
                path=f"src/main/java/{base_package.replace('.', '/')}/{component_package}/{component.name.replace(' ', '')}.java",
                content="",
                purpose=f"Main class for {component.name}: {component.description}",
                dependencies=[],
                language="java"
            ))
            
            # Add files based on responsibilities
            for resp in component.responsibilities:
                # Convert responsibility to a class name
                class_name = "".join(word.capitalize() for word in resp.split())
                class_name = re.sub(r'[^a-zA-Z0-9]', '', class_name)
                
                # Skip if too generic
                if class_name.lower() in ["main", "core", "base", "app"]:
                    continue
                
                # Create file
                files.append(CodeFile(
                    path=f"src/main/java/{base_package.replace('.', '/')}/{component_package}/{class_name}.java",
                    content="",
                    purpose=f"Handles {resp} in the {component.name} component",
                    dependencies=[],
                    language="java"
                ))
        
        return files
    
    def _get_language_from_file_path(self, file_path: str, project_type: str) -> Optional[str]:
        """
        Determine language from file path.
        
        Args:
            file_path: Path to the file
            project_type: Type of project
            
        Returns:
            Language string or None
        """
        # Extract file extension
        ext = Path(file_path).suffix.lower()
        
        # Map extensions to languages
        if ext == ".py":
            return "python"
        elif ext in [".js", ".jsx"]:
            return "javascript"
        elif ext in [".ts", ".tsx"]:
            return "typescript"
        elif ext == ".java":
            return "java"
        elif ext == ".go":
            return "go"
        elif ext == ".rs":
            return "rust"
        elif ext == ".rb":
            return "ruby"
        elif ext == ".php":
            return "php"
        elif ext == ".html":
            return "html"
        elif ext == ".css":
            return "css"
        elif ext == ".vue":
            return "vue"
        elif ext == ".json":
            return "json"
        elif ext == ".md":
            return "markdown"
        elif ext == ".xml":
            return "xml"
        elif ext == ".yaml" or ext == ".yml":
            return "yaml"
        
        # Fallback to project type
        return project_type
    
    async def _add_standard_project_files(
        self, 
        project_type: str,
        framework: Optional[str],
        project_name: str
    ) -> List[CodeFile]:
        """
        Add standard files for the project type.
        
        Args:
            project_type: Type of project
            framework: Optional framework
            project_name: Name of the project
            
        Returns:
            List of CodeFile objects
        """
        files = []
        
        # Common files for all projects
        files.append(CodeFile(
            path="README.md",
            content="",
            purpose="Project documentation",
            dependencies=[],
            language="markdown"
        ))
        
        files.append(CodeFile(
            path=".gitignore",
            content="",
            purpose="Git ignore file",
            dependencies=[],
            language="gitignore"
        ))
        
        # Project-specific files
        if project_type == "python":
            files.append(CodeFile(
                path="requirements.txt",
                content="",
                purpose="Python dependencies",
                dependencies=[],
                language="text"
            ))
            
            files.append(CodeFile(
                path="setup.py",
                content="",
                purpose="Python package setup",
                dependencies=[],
                language="python"
            ))
            
            if framework == "django":
                files.append(CodeFile(
                    path="manage.py",
                    content="",
                    purpose="Django management script",
                    dependencies=[],
                    language="python"
                ))
                
                files.append(CodeFile(
                    path=f"{project_name.lower().replace(' ', '_')}/settings.py",
                    content="",
                    purpose="Django settings",
                    dependencies=[],
                    language="python"
                ))
                
                files.append(CodeFile(
                    path=f"{project_name.lower().replace(' ', '_')}/urls.py",
                    content="",
                    purpose="Django URL configuration",
                    dependencies=[],
                    language="python"
                ))
                
                files.append(CodeFile(
                    path=f"{project_name.lower().replace(' ', '_')}/__init__.py",
                    content="",
                    purpose="Django project initialization",
                    dependencies=[],
                    language="python"
                ))
            
            elif framework == "flask":
                files.append(CodeFile(
                    path="app.py",
                    content="",
                    purpose="Flask application entry point",
                    dependencies=[],
                    language="python"
                ))
                
                files.append(CodeFile(
                    path="config.py",
                    content="",
                    purpose="Flask configuration",
                    dependencies=[],
                    language="python"
                ))
            
            elif framework == "fastapi":
                files.append(CodeFile(
                    path="main.py",
                    content="",
                    purpose="FastAPI application entry point",
                    dependencies=[],
                    language="python"
                ))
        
        elif project_type == "node":
            files.append(CodeFile(
                path="package.json",
                content="",
                purpose="Node.js package configuration",
                dependencies=[],
                language="json"
            ))
            
            files.append(CodeFile(
                path=".env.example",
                content="",
                purpose="Example environment variables",
                dependencies=[],
                language="env"
            ))
            
            if framework == "react":
                files.append(CodeFile(
                    path="src/index.js",
                    content="",
                    purpose="React application entry point",
                    dependencies=[],
                    language="javascript"
                ))
                
                files.append(CodeFile(
                    path="src/App.js",
                    content="",
                    purpose="Main React component",
                    dependencies=[],
                    language="javascript"
                ))
                
                files.append(CodeFile(
                    path="public/index.html",
                    content="",
                    purpose="HTML entry point",
                    dependencies=[],
                    language="html"
                ))
            
            elif framework == "express":
                files.append(CodeFile(
                    path="src/index.js",
                    content="",
                    purpose="Express application entry point",
                    dependencies=[],
                    language="javascript"
                ))
                
                files.append(CodeFile(
                    path="src/app.js",
                    content="",
                    purpose="Express application setup",
                    dependencies=[],
                    language="javascript"
                ))
        
        elif project_type == "java":
            files.append(CodeFile(
                path="pom.xml",
                content="",
                purpose="Maven project configuration",
                dependencies=[],
                language="xml"
            ))
            
            base_package = "com.example.app"
            
            if framework == "spring":
                files.append(CodeFile(
                    path=f"src/main/java/{base_package.replace('.', '/')}/Application.java",
                    content="",
                    purpose="Spring Boot application entry point",
                    dependencies=[],
                    language="java"
                ))
                
                files.append(CodeFile(
                    path="src/main/resources/application.properties",
                    content="",
                    purpose="Spring Boot configuration",
                    dependencies=[],
                    language="properties"
                ))
        
        return files
    
    
    
    
    async def _parse_refined_plan(
        self, 
        response: str, 
        original_project: CodeProject
    ) -> CodeProject:
        """
        Parse the AI response to extract the refined plan.
        
        Args:
            response: AI response text
            original_project: The original project to refine
            
        Returns:
            Refined CodeProject
        """
        try:
            # Look for JSON block in the response
            json_match = re.search(r'```(?:json)?\s*(.*?)\s*```', response, re.DOTALL)
            if json_match:
                json_str = json_match.group(1)
            else:
                # Try to find JSON without code blocks
                json_match = re.search(r'({.*})', response, re.DOTALL)
                if json_match:
                    json_str = json_match.group(1)
                else:
                    # Assume the entire response is JSON
                    json_str = response
            
            # Parse the JSON
            plan_data = json.loads(json_str)
            
            # Create CodeFile objects
            files = []
            for file_data in plan_data.get("files", []):
                # Check if this file existed in the original project
                original_file = next((f for f in original_project.files if f.path == file_data["path"]), None)
                
                files.append(CodeFile(
                    path=file_data["path"],
                    content=original_file.content if original_file else "",
                    purpose=file_data["purpose"],
                    dependencies=file_data.get("dependencies", []),
                    language=original_file.language if original_file else None
                ))
            
            # Create CodeProject object
            project = CodeProject(
                name=plan_data.get("name", original_project.name),
                description=plan_data.get("description", original_project.description),
                root_dir=original_project.root_dir,
                files=files,
                dependencies=plan_data.get("dependencies", original_project.dependencies),
                project_type=original_project.project_type,
                structure_explanation=plan_data.get("structure_explanation", original_project.structure_explanation)
            )
            
            return project
            
        except Exception as e:
            self._logger.exception(f"Error parsing refined plan: {str(e)}")
            
            # Return the original project if parsing failed
            return original_project
        
        
project_planner = ProjectPlanner()
</file>

<file path="components/generation/refiner.py">
# angela/generation/refiner.py
"""
Interactive code refinement for Angela CLI.

This module provides capabilities for interactively refining generated code
based on natural language feedback.
"""
import os
import asyncio
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple, Union, Set
import json
import re
import difflib

from angela.utils.logging import get_logger
from angela.api.ai import get_gemini_client, get_gemini_request_class
from angela.api.generation import get_code_file_class, get_code_project_class
from angela.api.review import get_feedback_manager, get_diff_manager
from angela.api.generation import get_generation_context_manager

logger = get_logger(__name__)
GeminiRequest = get_gemini_request_class()
CodeFile = get_code_file_class()
CodeProject = get_code_project_class()

class InteractiveRefiner:
    """
    Interactive refiner for generated code projects.
    
    This class handles the refinement of generated code based on natural language
    feedback, allowing for iterative improvement of generated projects.
    """
    
    def __init__(self):
        """Initialize the interactive refiner."""
        self._logger = logger
    
    async def process_refinement_feedback(
        self, 
        feedback: str,
        project: CodeProject,
        focus_files: Optional[List[str]] = None
    ) -> Tuple[CodeProject, Dict[str, Any]]:
        """
        Process feedback to refine a generated project.
        
        Args:
            feedback: Natural language feedback
            project: The project to refine
            focus_files: Optional list of files to focus on
            
        Returns:
            Tuple of (refined project, refinement results)
        """
        self._logger.info(f"Processing refinement feedback: {feedback}")
        
        # Analyze feedback to determine affected files
        affected_files = await self._analyze_feedback_for_files(feedback, project, focus_files)
        self._logger.debug(f"Affected files: {[f.path for f in affected_files]}")
        
        # Process each affected file
        refinement_results = []
        
        for file in affected_files:
            # Process feedback for this file
            self._logger.debug(f"Processing feedback for file: {file.path}")
            
            file_result = await self._process_file_feedback(
                feedback=feedback,
                file=file,
                project=project
            )
            
            # Add to results
            refinement_results.append({
                "file_path": file.path,
                "has_changes": file_result["original_code"] != file_result["improved_code"],
                "diff": file_result["diff"],
                "explanation": file_result["explanation"]
            })
            
            # Update file content if changed
            if file_result["original_code"] != file_result["improved_code"]:
                for project_file in project.files:
                    if project_file.path == file.path:
                        project_file.content = file_result["improved_code"]
                        break
        
        # Return updated project and results
        return project, {
            "success": True,
            "feedback": feedback,
            "results": refinement_results,
            "files_processed": len(refinement_results)
        }
    
    async def _analyze_feedback_for_files(
        self, 
        feedback: str, 
        project: CodeProject,
        focus_files: Optional[List[str]] = None
    ) -> List[CodeFile]:
        """
        Analyze feedback to determine which files are affected.
        
        Args:
            feedback: Natural language feedback
            project: The project to refine
            focus_files: Optional list of files to focus on
            
        Returns:
            List of affected files
        """
        # If focus files are provided, use those
        if focus_files:
            return [file for file in project.files if any(
                self._match_file_pattern(file.path, pattern) for pattern in focus_files
            )]
        
        # Extract file mentions from feedback
        file_mentions = self._extract_file_mentions(feedback)
        
        if file_mentions:
            # Find mentioned files
            mentioned_files = []
            for mention in file_mentions:
                for file in project.files:
                    if self._match_file_mention(file.path, mention):
                        mentioned_files.append(file)
                        break
            
            if mentioned_files:
                return mentioned_files
        
        # If no files were found from feedback or focus_files, determine files based on feedback intent
        return await self._determine_files_by_intent(feedback, project)
    
    def _extract_file_mentions(self, feedback: str) -> List[str]:
        """
        Extract mentions of files from feedback.
        
        Args:
            feedback: Natural language feedback
            
        Returns:
            List of file mentions
        """
        # Common file mention patterns
        patterns = [
            r'(?:file|in|modify|change|update)\s+["\']?([.\w/-]+\.\w+)["\']?',  # file foo.py
            r'["\']([.\w/-]+\.\w+)["\']',  # "foo.py"
        ]
        
        mentions = []
        for pattern in patterns:
            matches = re.finditer(pattern, feedback, re.IGNORECASE)
            for match in matches:
                mentions.append(match.group(1))
        
        return mentions
    
    def _match_file_mention(self, file_path: str, mention: str) -> bool:
        """
        Check if a file path matches a mention.
        
        Args:
            file_path: Path to the file
            mention: File mention from feedback
            
        Returns:
            True if the file matches the mention
        """
        # Exact match
        if mention == file_path:
            return True
        
        # Basename match
        if Path(mention).name == Path(file_path).name:
            return True
        
        # Partial path match
        if mention in file_path:
            return True
        
        return False
    
    def _match_file_pattern(self, file_path: str, pattern: str) -> bool:
        """
        Check if a file path matches a pattern.
        
        Args:
            file_path: Path to the file
            pattern: Pattern to match against
            
        Returns:
            True if the file matches the pattern
        """
        # Exact match
        if pattern == file_path:
            return True
        
        # Wildcard match
        if '*' in pattern:
            # Convert glob pattern to regex
            regex_pattern = pattern.replace('.', '\\.').replace('*', '.*')
            return re.match(regex_pattern, file_path) is not None
        
        # Partial path match
        if pattern in file_path:
            return True
        
        return False
    
    async def _determine_files_by_intent(
        self, 
        feedback: str, 
        project: CodeProject
    ) -> List[CodeFile]:
        """
        Determine which files to modify based on feedback intent.
        
        Args:
            feedback: Natural language feedback
            project: The project to refine
            
        Returns:
            List of files to modify
        """
        # Build prompt for AI to determine affected files
        prompt = f"""
Given this feedback for a software project:
"{feedback}"

Determine which types of files would need to be modified to implement this feedback.
Consider the feedback's intent and what components of a software system it affects.

Here are the available files in the project:
{', '.join(file.path for file in project.files)}

Return a JSON array of file paths that should be modified, like this:
["path/to/file1.ext", "path/to/file2.ext"]

Only include files that are directly relevant to the feedback.
"""
        
        # Call AI service
        api_request = GeminiRequest(
            prompt=prompt,
            max_tokens=1000,
            temperature=0.2
        )
        
        response = await gemini_client.generate_text(api_request)
        
        try:
            # Extract JSON from response
            json_match = re.search(r'```(?:json)?\s*(.*?)\s*```', response.text, re.DOTALL)
            if json_match:
                json_str = json_match.group(1)
            else:
                # Try to find JSON without code blocks
                json_match = re.search(r'(\[.*\])', response.text, re.DOTALL)
                if json_match:
                    json_str = json_match.group(1)
                else:
                    # Assume the entire response is JSON
                    json_str = response.text
                    
            # Parse JSON
            file_paths = json.loads(json_str)
            
            # Match these paths to actual files
            affected_files = []
            for path in file_paths:
                for file in project.files:
                    if self._match_file_pattern(file.path, path):
                        affected_files.append(file)
                        break
            
            return affected_files
            
        except Exception as e:
            self._logger.error(f"Error determining files by intent: {str(e)}")
            
            # Fallback: return a few important files based on naming conventions
            important_files = []
            
            # Look for main files
            for file in project.files:
                if 'main' in file.path.lower() or 'app' in file.path.lower() or 'index' in file.path.lower():
                    important_files.append(file)
            
            # If we found some files, return those
            if important_files:
                return important_files[:3]  # Limit to 3 files
            
            # Otherwise, just return a few files
            return project.files[:3]  # Limit to 3 files
    
    async def _process_file_feedback(
        self, 
        feedback: str,
        file: CodeFile,
        project: CodeProject
    ) -> Dict[str, Any]:
        """
        Process feedback for a specific file.
        
        Args:
            feedback: Natural language feedback
            file: The file to refine
            project: The parent project
            
        Returns:
            Dictionary with refinement results
        """
        # Get context for this file
        file_context = self._build_file_context(file, project)
        
        # Use the feedback manager to process the feedback
        feedback_manager = get_feedback_manager()
        result = await feedback_manager.process_feedback(
            feedback=feedback,
            original_code=file.content,
            file_path=file.path,
            context=file_context
        )
        
        return result
    
    async def _build_file_context(self, file: CodeFile, project: CodeProject) -> Dict[str, Any]:
        """
        Build context for a file for feedback processing.
        
        Args:
            file: The file to build context for
            project: The parent project
            
        Returns:
            Dictionary with file context
        """
        # Start with basic context
        context = {
            "file_path": file.path,
            "file_purpose": file.purpose,
            "language": file.language,
            "project_name": project.name,
            "project_type": project.project_type,
            "project_description": project.description
        }
        
        # Add dependency information if available
        if file.dependencies:
            context["dependencies"] = file.dependencies
        
        # Add global context from generation context manager
        generation_context_manager = get_generation_context_manager()
        context.update(generation_context_manager.get_all_global_context())
        
        # Add project architecture if available
        architecture = generation_context_manager.get_global_context("architecture")
        if architecture:
            context["architecture"] = architecture
        
        return context
    
    async def summarize_refinements(
        self, 
        refinement_results: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Generate a human-readable summary of refinements.
        
        Args:
            refinement_results: Results from process_refinement_feedback
            
        Returns:
            Dictionary with summarized results
        """
        results = refinement_results.get("results", [])
        modified_count = sum(1 for r in results if r.get("has_changes", False))
        
        # Create summary
        summary = {
            "feedback": refinement_results.get("feedback", ""),
            "files_processed": len(results),
            "files_modified": modified_count,
            "file_summaries": []
        }
        
        # Summarize each file
        for result in results:
            if result.get("has_changes", False):
                # Count lines changed
                diff_lines = result.get("diff", "").splitlines()
                additions = sum(1 for line in diff_lines if line.startswith('+') and not line.startswith('+++'))
                deletions = sum(1 for line in diff_lines if line.startswith('-') and not line.startswith('---'))
                
                # Add to summary
                summary["file_summaries"].append({
                    "file_path": result.get("file_path", ""),
                    "lines_added": additions,
                    "lines_deleted": deletions,
                    "explanation": result.get("explanation", "")
                })
        
        return summary

# Global instance
interactive_refiner = InteractiveRefiner()
</file>

<file path="components/generation/validators.py">
# angela/generation/validators.py
"""
Code validation for Angela CLI.

This module provides validators for different programming languages
to ensure generated code is syntactically correct and follows best practices.
"""
import os
import subprocess
import tempfile
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple, Union
import re

from angela.utils.logging import get_logger

logger = get_logger(__name__)

# Map file extensions to language validators
LANGUAGE_VALIDATORS = {
    ".py": "validate_python",
    ".js": "validate_javascript",
    ".jsx": "validate_javascript",
    ".ts": "validate_typescript",
    ".tsx": "validate_typescript",
    ".java": "validate_java",
    ".go": "validate_go",
    ".rb": "validate_ruby",
    ".rs": "validate_rust",
    ".html": "validate_html",
    ".css": "validate_css",
    ".php": "validate_php"
}

def validate_code(content: str, file_path: str) -> Tuple[bool, str]:
    """
    Validate code based on file extension.
    
    Args:
        content: Code content to validate
        file_path: Path of the file (used to determine language)
        
    Returns:
        Tuple of (is_valid, error_message)
    """
    logger.info(f"Validating code for: {file_path}")
    
    _, extension = os.path.splitext(file_path.lower())
    
    # Get the validator function for this extension
    validator_name = LANGUAGE_VALIDATORS.get(extension)
    
    if validator_name and validator_name in globals():
        validator_func = globals()[validator_name]
        return validator_func(content, file_path)
    
    # If no specific validator, do basic checks
    return validate_generic(content, file_path)

def validate_generic(content: str, file_path: str) -> Tuple[bool, str]:
    """
    Generic validator for any code file.
    
    Args:
        content: Code content to validate
        file_path: Path of the file
        
    Returns:
        Tuple of (is_valid, error_message)
    """
    # Check for basic issues like unmatched brackets
    bracket_pairs = [('(', ')'), ('[', ']'), ('{', '}')]
    
    for opening, closing in bracket_pairs:
        if content.count(opening) != content.count(closing):
            return False, f"Unmatched brackets: {opening}{closing}"
    
    return True, ""

def validate_python(content: str, file_path: str) -> Tuple[bool, str]:
    """
    Validate Python code.
    
    Args:
        content: Python code to validate
        file_path: Path of the file
        
    Returns:
        Tuple of (is_valid, error_message)
    """
    # Create a temporary file
    with tempfile.NamedTemporaryFile(suffix='.py', delete=False) as tmp:
        tmp_path = tmp.name
        tmp.write(content.encode('utf-8'))
    
    try:
        # Use Python's compile function to check syntax
        result = subprocess.run(
            ['python', '-m', 'py_compile', tmp_path],
            capture_output=True,
            text=True
        )
        
        if result.returncode != 0:
            # Parse the error message
            error_msg = result.stderr
            
            # Simplify the error message
            error_lines = error_msg.splitlines()
            for line in error_lines:
                if "File " in line and ", line " in line:
                    continue
                if line.strip():
                    error_msg = line.strip()
                    break
            
            return False, f"Python syntax error: {error_msg}"
        
        # Additional checks for common Python issues
        issues = []
        
        # Check for undefined or unused imports
        import_lines = []
        imported_modules = []
        
        for line in content.splitlines():
            if line.strip().startswith(('import ', 'from ')):
                import_lines.append(line)
                
                if line.strip().startswith('import '):
                    modules = line.strip()[7:].split(',')
                    for module in modules:
                        if ' as ' in module:
                            module = module.split(' as ')[1]
                        imported_modules.append(module.strip())
                elif line.strip().startswith('from '):
                    parts = line.strip().split(' import ')
                    if len(parts) == 2:
                        modules = parts[1].split(',')
                        for module in modules:
                            if ' as ' in module:
                                module = module.split(' as ')[1]
                            imported_modules.append(module.strip())
        
        # Check if imports are used
        for module in imported_modules:
            if module not in content.replace(f"import {module}", "").replace(f"from {module}", ""):
                issues.append(f"Potentially unused import: {module}")
        
        # If we found issues but not syntax errors, still consider it valid
        # but report the issues
        if issues:
            return True, f"Code is valid but has issues: {'; '.join(issues)}"
        
        return True, ""
    
    except Exception as e:
        logger.error(f"Error validating Python code: {str(e)}")
        return False, f"Error validating Python code: {str(e)}"
    
    finally:
        # Clean up the temporary file
        try:
            os.unlink(tmp_path)
        except Exception:
            pass

def validate_javascript(content: str, file_path: str) -> Tuple[bool, str]:
    """
    Validate JavaScript code.
    
    Args:
        content: JavaScript code to validate
        file_path: Path of the file
        
    Returns:
        Tuple of (is_valid, error_message)
    """
    # Check if node is available
    try:
        subprocess.run(['node', '--version'], capture_output=True, check=True)
    except (subprocess.SubprocessError, FileNotFoundError):
        logger.warning("Node.js not found, falling back to basic JS validation")
        return validate_javascript_basic(content, file_path)
    
    # Create a temporary file
    with tempfile.NamedTemporaryFile(suffix='.js', delete=False) as tmp:
        tmp_path = tmp.name
        tmp.write(content.encode('utf-8'))
    
    try:
        # Use Node.js to check syntax
        result = subprocess.run(
            ['node', '--check', tmp_path],
            capture_output=True,
            text=True
        )
        
        if result.returncode != 0:
            # Parse the error message
            error_msg = result.stderr
            return False, f"JavaScript syntax error: {error_msg.strip()}"
        
        return True, ""
    
    except Exception as e:
        logger.error(f"Error validating JavaScript code: {str(e)}")
        return False, f"Error validating JavaScript code: {str(e)}"
    
    finally:
        # Clean up the temporary file
        try:
            os.unlink(tmp_path)
        except Exception:
            pass

def validate_javascript_basic(content: str, file_path: str) -> Tuple[bool, str]:
    """
    Basic validation for JavaScript code without using Node.js.
    
    Args:
        content: JavaScript code to validate
        file_path: Path of the file
        
    Returns:
        Tuple of (is_valid, error_message)
    """
    # Check for basic syntax issues
    
    # Check for unmatched brackets
    bracket_pairs = [('(', ')'), ('[', ']'), ('{', '}')]
    
    for opening, closing in bracket_pairs:
        if content.count(opening) != content.count(closing):
            return False, f"Unmatched brackets: {opening}{closing}"
    
    # Check for missing semicolons (simplified)
    lines = content.splitlines()
    for i, line in enumerate(lines):
        line = line.strip()
        if line and not line.endswith(';') and not line.endswith('{') and not line.endswith('}') and \
           not line.endswith(':') and not line.startswith('//') and not line.startswith('/*') and \
           not line.endswith('*/') and not line.startswith('import ') and not line.startswith('export '):
            # This is a simplistic check and might have false positives
            logger.debug(f"Line {i+1} might be missing a semicolon: {line}")
    
    # Check for common React/JSX issues if it seems to be a React file
    if ".jsx" in file_path or "React" in content or "react" in content:
        # Check if JSX elements are closed
        jsx_tags = re.findall(r'<([a-zA-Z0-9]+)(?:\s+[^>]*)?>', content)
        for tag in jsx_tags:
            if tag[0].isupper() and f"</{tag}>" not in content and "/>" not in content:
                return False, f"Unclosed JSX element: <{tag}>"
    
    return True, ""

def validate_typescript(content: str, file_path: str) -> Tuple[bool, str]:
    """
    Validate TypeScript code.
    
    Args:
        content: TypeScript code to validate
        file_path: Path of the file
        
    Returns:
        Tuple of (is_valid, error_message)
    """
    # Check if tsc is available
    try:
        subprocess.run(['tsc', '--version'], capture_output=True, check=True)
    except (subprocess.SubprocessError, FileNotFoundError):
        logger.warning("TypeScript compiler not found, falling back to JavaScript validation")
        return validate_javascript(content, file_path)
    
    # Create a temporary file
    with tempfile.NamedTemporaryFile(suffix='.ts', delete=False) as tmp:
        tmp_path = tmp.name
        tmp.write(content.encode('utf-8'))
    
    try:
        # Use tsc to check syntax (without emitting JS files)
        result = subprocess.run(
            ['tsc', '--noEmit', tmp_path],
            capture_output=True,
            text=True
        )
        
        if result.returncode != 0:
            # Parse the error message
            error_msg = result.stderr or result.stdout
            return False, f"TypeScript error: {error_msg.strip()}"
        
        return True, ""
    
    except Exception as e:
        logger.error(f"Error validating TypeScript code: {str(e)}")
        return False, f"Error validating TypeScript code: {str(e)}"
    
    finally:
        # Clean up the temporary file
        try:
            os.unlink(tmp_path)
        except Exception:
            pass

def validate_java(content: str, file_path: str) -> Tuple[bool, str]:
    """
    Validate Java code.
    
    Args:
        content: Java code to validate
        file_path: Path of the file
        
    Returns:
        Tuple of (is_valid, error_message)
    """
    # Check if javac is available
    try:
        subprocess.run(['javac', '-version'], capture_output=True, check=True)
    except (subprocess.SubprocessError, FileNotFoundError):
        logger.warning("Java compiler not found, falling back to basic Java validation")
        return validate_java_basic(content, file_path)
    
    # Extract class name from the code
    class_name = None
    class_match = re.search(r'public\s+class\s+(\w+)', content)
    if class_match:
        class_name = class_match.group(1)
    else:
        # Try to get class name from file path
        base_name = os.path.basename(file_path)
        if base_name.endswith('.java'):
            class_name = base_name[:-5]
    
    if not class_name:
        return False, "Could not determine Java class name"
    
    # Update content to match class name from file if needed
    if class_match and class_match.group(1) != class_name:
        content = content.replace(f"class {class_match.group(1)}", f"class {class_name}")
    
    # Create a temporary file
    with tempfile.NamedTemporaryFile(suffix='.java', delete=False) as tmp:
        tmp_path = tmp.name
        tmp.write(content.encode('utf-8'))
    
    try:
        # Use javac to check syntax
        result = subprocess.run(
            ['javac', tmp_path],
            capture_output=True,
            text=True
        )
        
        if result.returncode != 0:
            # Parse the error message
            error_msg = result.stderr
            return False, f"Java syntax error: {error_msg.strip()}"
        
        return True, ""
    
    except Exception as e:
        logger.error(f"Error validating Java code: {str(e)}")
        return False, f"Error validating Java code: {str(e)}"
    
    finally:
        # Clean up the temporary file
        try:
            os.unlink(tmp_path)
        except Exception:
            pass

def validate_java_basic(content: str, file_path: str) -> Tuple[bool, str]:
    """
    Basic validation for Java code without using javac.
    
    Args:
        content: Java code to validate
        file_path: Path of the file
        
    Returns:
        Tuple of (is_valid, error_message)
    """
    # Check for basic syntax issues
    
    # Check for unmatched brackets
    bracket_pairs = [('(', ')'), ('[', ']'), ('{', '}')]
    
    for opening, closing in bracket_pairs:
        if content.count(opening) != content.count(closing):
            return False, f"Unmatched brackets: {opening}{closing}"
    
    # Check for missing semicolons (simplified)
    lines = content.splitlines()
    for i, line in enumerate(lines):
        line = line.strip()
        if line and not line.endswith(';') and not line.endswith('{') and not line.endswith('}') and \
           not line.endswith(':') and not line.startswith('//') and not line.startswith('/*') and \
           not line.endswith('*/'):
            if not any(keyword in line for keyword in ['class ', 'interface ', 'enum ', 'import ', 'package ']):
                # This is a simplistic check and might have false positives
                logger.debug(f"Line {i+1} might be missing a semicolon: {line}")
    
    # Check for class name matching file name
    class_match = re.search(r'public\s+class\s+(\w+)', content)
    if class_match:
        class_name = class_match.group(1)
        base_name = os.path.basename(file_path)
        if base_name.endswith('.java') and base_name[:-5] != class_name:
            return False, f"Class name '{class_name}' does not match file name '{base_name[:-5]}'"
    
    return True, ""

def validate_go(content: str, file_path: str) -> Tuple[bool, str]:
    """
    Validate Go code.
    
    Args:
        content: Go code to validate
        file_path: Path of the file
        
    Returns:
        Tuple of (is_valid, error_message)
    """
    # Check if go is available
    try:
        subprocess.run(['go', 'version'], capture_output=True, check=True)
    except (subprocess.SubprocessError, FileNotFoundError):
        logger.warning("Go compiler not found, falling back to basic Go validation")
        return validate_go_basic(content, file_path)
    
    # Create a temporary file
    with tempfile.NamedTemporaryFile(suffix='.go', delete=False) as tmp:
        tmp_path = tmp.name
        tmp.write(content.encode('utf-8'))
    
    try:
        # Use go vet to check syntax and common issues
        result = subprocess.run(
            ['go', 'vet', tmp_path],
            capture_output=True,
            text=True
        )
        
        if result.returncode != 0:
            # Parse the error message
            error_msg = result.stderr
            return False, f"Go error: {error_msg.strip()}"
        
        return True, ""
    
    except Exception as e:
        logger.error(f"Error validating Go code: {str(e)}")
        return False, f"Error validating Go code: {str(e)}"
    
    finally:
        # Clean up the temporary file
        try:
            os.unlink(tmp_path)
        except Exception:
            pass

def validate_go_basic(content: str, file_path: str) -> Tuple[bool, str]:
    """
    Basic validation for Go code without using go compiler.
    
    Args:
        content: Go code to validate
        file_path: Path of the file
        
    Returns:
        Tuple of (is_valid, error_message)
    """
    # Check for basic syntax issues
    
    # Check for unmatched brackets
    bracket_pairs = [('(', ')'), ('[', ']'), ('{', '}')]
    
    for opening, closing in bracket_pairs:
        if content.count(opening) != content.count(closing):
            return False, f"Unmatched brackets: {opening}{closing}"
    
    # Check for package declaration
    if not re.search(r'package\s+\w+', content):
        return False, "Missing package declaration"
    
    return True, ""

def validate_ruby(content: str, file_path: str) -> Tuple[bool, str]:
    """
    Validate Ruby code.
    
    Args:
        content: Ruby code to validate
        file_path: Path of the file
        
    Returns:
        Tuple of (is_valid, error_message)
    """
    # Check if ruby is available
    try:
        subprocess.run(['ruby', '--version'], capture_output=True, check=True)
    except (subprocess.SubprocessError, FileNotFoundError):
        logger.warning("Ruby interpreter not found, falling back to basic Ruby validation")
        return validate_ruby_basic(content, file_path)
    
    # Create a temporary file
    with tempfile.NamedTemporaryFile(suffix='.rb', delete=False) as tmp:
        tmp_path = tmp.name
        tmp.write(content.encode('utf-8'))
    
    try:
        # Use ruby -c to check syntax
        result = subprocess.run(
            ['ruby', '-c', tmp_path],
            capture_output=True,
            text=True
        )
        
        if result.returncode != 0:
            # Parse the error message
            error_msg = result.stderr
            return False, f"Ruby syntax error: {error_msg.strip()}"
        
        return True, ""
    
    except Exception as e:
        logger.error(f"Error validating Ruby code: {str(e)}")
        return False, f"Error validating Ruby code: {str(e)}"
    
    finally:
        # Clean up the temporary file
        try:
            os.unlink(tmp_path)
        except Exception:
            pass

def validate_ruby_basic(content: str, file_path: str) -> Tuple[bool, str]:
    """
    Basic validation for Ruby code without using ruby interpreter.
    
    Args:
        content: Ruby code to validate
        file_path: Path of the file
        
    Returns:
        Tuple of (is_valid, error_message)
    """
    # Check for basic syntax issues
    
    # Check for unmatched brackets
    bracket_pairs = [('(', ')'), ('[', ']'), ('{', '}')]
    
    for opening, closing in bracket_pairs:
        if content.count(opening) != content.count(closing):
            return False, f"Unmatched brackets: {opening}{closing}"
    
    # Check for unmatched 'do' blocks
    do_count = len(re.findall(r'\bdo\b(?:\s*\|.*?\|)?', content))
    end_count = len(re.findall(r'\bend\b', content))
    
    if do_count != end_count:
        return False, f"Unmatched 'do' and 'end' blocks: {do_count} 'do' vs {end_count} 'end'"
    
    return True, ""

def validate_rust(content: str, file_path: str) -> Tuple[bool, str]:
    """
    Validate Rust code.
    
    Args:
        content: Rust code to validate
        file_path: Path of the file
        
    Returns:
        Tuple of (is_valid, error_message)
    """
    # Check if rustc is available
    try:
        subprocess.run(['rustc', '--version'], capture_output=True, check=True)
    except (subprocess.SubprocessError, FileNotFoundError):
        logger.warning("Rust compiler not found, falling back to basic Rust validation")
        return validate_rust_basic(content, file_path)
    
    # Create a temporary file
    with tempfile.NamedTemporaryFile(suffix='.rs', delete=False) as tmp:
        tmp_path = tmp.name
        tmp.write(content.encode('utf-8'))
    
    try:
        # Use rustc to check syntax (with --emit=metadata to avoid producing binaries)
        result = subprocess.run(
            ['rustc', '--emit=metadata', tmp_path],
            capture_output=True,
            text=True
        )
        
        if result.returncode != 0:
            # Parse the error message
            error_msg = result.stderr
            return False, f"Rust syntax error: {error_msg.strip()}"
        
        return True, ""
    
    except Exception as e:
        logger.error(f"Error validating Rust code: {str(e)}")
        return False, f"Error validating Rust code: {str(e)}"
    
    finally:
        # Clean up the temporary files
        try:
            os.unlink(tmp_path)
            # Also try to remove any generated metadata files
            metadata_path = tmp_path.replace('.rs', '')
            if os.path.exists(metadata_path):
                os.unlink(metadata_path)
        except Exception:
            pass

def validate_rust_basic(content: str, file_path: str) -> Tuple[bool, str]:
    """
    Basic validation for Rust code without using rustc.
    
    Args:
        content: Rust code to validate
        file_path: Path of the file
        
    Returns:
        Tuple of (is_valid, error_message)
    """
    # Check for basic syntax issues
    
    # Check for unmatched brackets
    bracket_pairs = [('(', ')'), ('[', ']'), ('{', '}')]
    
    for opening, closing in bracket_pairs:
        if content.count(opening) != content.count(closing):
            return False, f"Unmatched brackets: {opening}{closing}"
    
    return True, ""

def validate_html(content: str, file_path: str) -> Tuple[bool, str]:
    """
    Validate HTML code.
    
    Args:
        content: HTML code to validate
        file_path: Path to the file
        
    Returns:
        Tuple of (is_valid, error_message)
    """
    # Basic HTML validation without external tools
    
    # Check for DOCTYPE declaration
    if not re.search(r'<!DOCTYPE\s+html>', content, re.IGNORECASE):
        logger.warning("HTML file missing DOCTYPE declaration")
    
    # Check for basic required elements
    if not re.search(r'<html', content, re.IGNORECASE):
        return False, "Missing <html> element"
    
    if not re.search(r'<head', content, re.IGNORECASE):
        return False, "Missing <head> element"
    
    if not re.search(r'<body', content, re.IGNORECASE):
        return False, "Missing <body> element"
    
    # Check for unmatched tags (simplified)
    html_tags = re.findall(r'<([a-zA-Z0-9]+)[^>]*>', content)
    void_elements = {'area', 'base', 'br', 'col', 'embed', 'hr', 'img', 'input', 
                     'link', 'meta', 'param', 'source', 'track', 'wbr'}
    
    tag_stack = []
    
    for tag in html_tags:
        tag_lower = tag.lower()
        if tag_lower not in void_elements:
            tag_stack.append(tag_lower)
    
    closing_tags = re.findall(r'</([a-zA-Z0-9]+)>', content)
    
    for tag in closing_tags:
        tag_lower = tag.lower()
        if tag_stack and tag_stack[-1] == tag_lower:
            tag_stack.pop()
        else:
            return False, f"Unmatched closing tag: </{ tag_lower }>"
    
    # Check if all tags were properly closed
    if tag_stack:
        return False, f"Unclosed tags: {', '.join(tag_stack)}"
    
    return True, ""
            
            
def validate_css(content: str, file_path: str) -> Tuple[bool, str]:
    """
    Validate CSS code.
    
    Args:
        content: CSS code to validate
        file_path: Path of the file
        
    Returns:
        Tuple of (is_valid, error_message)
    """
    # Basic CSS validation without external tools
    
    # Check for unmatched brackets
    if content.count('{') != content.count('}'):
        return False, "Unmatched brackets in CSS"
    
    # Check for missing semicolons in property declarations
    lines = content.splitlines()
    in_rule_block = False
    
    for i, line in enumerate(lines):
        line = line.strip()
        
        if not line or line.startswith('/*') or line.endswith('*/'):
            continue
        
        if '{' in line:
            in_rule_block = True
            continue
            
        if '}' in line:
            in_rule_block = False
            continue
        
        if in_rule_block and ':' in line and not line.endswith(';') and not line.endswith('{'):
            # This might be a property without a semicolon
            # Check if it's not the last property in a block
            next_line_idx = i + 1
            while next_line_idx < len(lines) and not lines[next_line_idx].strip():
                next_line_idx += 1
                
            if next_line_idx < len(lines) and not lines[next_line_idx].strip().startswith('}'):
                return False, f"Missing semicolon at line {i+1}: {line}"
    
    return True, ""

def validate_php(content: str, file_path: str) -> Tuple[bool, str]:
    """
    Validate PHP code.
    
    Args:
        content: PHP code to validate
        file_path: Path of the file
        
    Returns:
        Tuple of (is_valid, error_message)
    """
    # Check if php is available
    try:
        subprocess.run(['php', '--version'], capture_output=True, check=True)
    except (subprocess.SubprocessError, FileNotFoundError):
        logger.warning("PHP interpreter not found, falling back to basic PHP validation")
        return validate_php_basic(content, file_path)
    
    # Create a temporary file
    with tempfile.NamedTemporaryFile(suffix='.php', delete=False) as tmp:
        tmp_path = tmp.name
        tmp.write(content.encode('utf-8'))
    
    try:
        # Use php -l to check syntax
        result = subprocess.run(
            ['php', '-l', tmp_path],
            capture_output=True,
            text=True
        )
        
        if result.returncode != 0:
            # Parse the error message
            error_msg = result.stderr or result.stdout
            return False, f"PHP syntax error: {error_msg.strip()}"
        
        return True, ""
    
    except Exception as e:
        logger.error(f"Error validating PHP code: {str(e)}")
        return False, f"Error validating PHP code: {str(e)}"
    
    finally:
        # Clean up the temporary file
        try:
            os.unlink(tmp_path)
        except Exception:
            pass

def validate_php_basic(content: str, file_path: str) -> Tuple[bool, str]:
    """
    Basic validation for PHP code without using php interpreter.
    
    Args:
        content: PHP code to validate
        file_path: Path of the file
        
    Returns:
        Tuple of (is_valid, error_message)
    """
    # Check for basic syntax issues
    
    # Check for PHP opening tag
    if not re.search(r'<\?php', content):
        return False, "Missing PHP opening tag (<?php)"
    
    # Check for unmatched brackets
    bracket_pairs = [('(', ')'), ('[', ']'), ('{', '}')]
    
    for opening, closing in bracket_pairs:
        if content.count(opening) != content.count(closing):
            return False, f"Unmatched brackets: {opening}{closing}"
    
    return True, ""
</file>

<file path="components/intent/__init__.py">
# angela/components/intent/__init__.py
"""
Intent components for Angela CLI.

This package provides functionality for understanding user intent,
planning and executing tasks, and orchestrating complex workflows
across multiple levels of abstraction.
"""

# Export core intent models
from angela.components.intent.models import IntentType, Intent, ActionPlan

# Export base planner components
from angela.components.intent.planner import (
    PlanStep, TaskPlan, PlanStepType, 
    AdvancedPlanStep, AdvancedTaskPlan,
    task_planner
)

# Export enhanced planner components - now that we have the API layer, 
# direct imports are safe since they'll be accessed through the API
from angela.components.intent.enhanced_task_planner import EnhancedTaskPlanner, enhanced_task_planner
from angela.components.intent.semantic_task_planner import SemanticTaskPlanner, semantic_task_planner, IntentClarification
from angela.components.intent.complex_workflow_planner import (
    ComplexWorkflowPlanner, complex_workflow_planner, 
    WorkflowStepType, ComplexWorkflowPlan
)

# Define the public API
__all__ = [
    # Core intent models
    'IntentType', 'Intent', 'ActionPlan',
    
    # Base planning components
    'PlanStep', 'TaskPlan', 'PlanStepType', 
    'AdvancedPlanStep', 'AdvancedTaskPlan',
    'task_planner',
    
    # Enhanced planning components
    'EnhancedTaskPlanner', 'enhanced_task_planner',
    'SemanticTaskPlanner', 'semantic_task_planner', 'IntentClarification',
    'ComplexWorkflowPlanner', 'complex_workflow_planner', 
    'WorkflowStepType', 'ComplexWorkflowPlan',
]
</file>

<file path="components/intent/complex_workflow_planner.py">
# angela/components/intent/complex_workflow_planner.py
"""
Complex Workflow Orchestration for Angela CLI.

This module extends the existing enhanced task planner with specialized
capabilities for orchestrating workflows across multiple CLI tools and
services, enabling end-to-end automation of complex development and
deployment pipelines.
"""
import asyncio
import json
import re
import shlex
import uuid
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, List, Optional, Set, Tuple, Union, Callable
from enum import Enum

from pydantic import BaseModel, Field, validator

# Updated imports using the new architecture
from angela.api.ai import get_gemini_client, get_gemini_request_class, get_semantic_analyzer
from angela.api.context import (
    get_context_manager, get_project_state_analyzer, get_file_resolver
)
from angela.api.shell import get_inline_feedback
from angela.api.execution import get_adaptive_engine
from angela.api.toolchain import get_universal_cli_translator
from angela.utils.logging import get_logger
from angela.core.registry import registry

# Import directly from local module to avoid circular deps
from angela.components.intent.enhanced_task_planner import (
    EnhancedTaskPlanner, StepExecutionContext, 
    AdvancedTaskPlan, AdvancedPlanStep, PlanStepType,
    ExecutionResult
)

logger = get_logger(__name__)

class WorkflowStepType(str, Enum):
    """Types of steps in a complex workflow."""
    COMMAND = "command"              # Standard shell command
    TOOL = "tool"                    # External CLI tool command
    API = "api"                      # API call
    DECISION = "decision"            # Decision point
    WAIT = "wait"                    # Wait for a condition
    PARALLEL = "parallel"            # Parallel execution
    CUSTOM_CODE = "custom_code"      # Custom code execution
    NOTIFICATION = "notification"    # Send notification
    VALIDATION = "validation"        # Validate a condition
    FILE = "file"                    # File operation

class WorkflowVariable(BaseModel):
    """Model for a variable in a workflow."""
    name: str = Field(..., description="Name of the variable")
    description: Optional[str] = Field(None, description="Description of the variable")
    default_value: Optional[Any] = Field(None, description="Default value")
    required: bool = Field(False, description="Whether the variable is required")
    type: str = Field("string", description="Data type (string, number, boolean)")
    scope: str = Field("global", description="Variable scope (global, step, local)")
    source_step: Optional[str] = Field(None, description="Step that produces this variable")

class WorkflowStepDependency(BaseModel):
    """Model for a dependency between workflow steps."""
    step_id: str = Field(..., description="ID of the dependent step")
    type: str = Field("success", description="Type of dependency (success, completion, failure)")
    condition: Optional[str] = Field(None, description="Optional condition for the dependency")

class WorkflowStep(BaseModel):
    """Model for a step in a complex workflow."""
    id: str = Field(..., description="Unique identifier for this step")
    name: str = Field(..., description="Human-readable name for the step")
    type: WorkflowStepType = Field(..., description="Type of workflow step")
    description: str = Field(..., description="Detailed description of what this step does")
    tool: Optional[str] = Field(None, description="Tool name for TOOL type")
    command: Optional[str] = Field(None, description="Command to execute")
    api_url: Optional[str] = Field(None, description="URL for API call")
    api_method: Optional[str] = Field("GET", description="HTTP method for API call")
    api_headers: Dict[str, str] = Field(default_factory=dict, description="Headers for API call")
    api_data: Optional[Any] = Field(None, description="Data payload for API call")
    code: Optional[str] = Field(None, description="Custom code to execute")
    condition: Optional[str] = Field(None, description="Condition for DECISION or VALIDATION type")
    wait_condition: Optional[str] = Field(None, description="Condition to wait for in WAIT type")
    timeout: Optional[int] = Field(None, description="Timeout in seconds")
    retry: Optional[int] = Field(None, description="Number of retry attempts")
    parallel_steps: List[str] = Field(default_factory=list, description="Steps to execute in parallel")
    dependencies: List[WorkflowStepDependency] = Field(default_factory=list, description="Dependencies on other steps")
    inputs: Dict[str, Any] = Field(default_factory=dict, description="Input values for the step")
    outputs: List[str] = Field(default_factory=list, description="Output variables produced by this step")
    environment: Dict[str, str] = Field(default_factory=dict, description="Environment variables for this step")
    working_dir: Optional[str] = Field(None, description="Working directory for this step")
    on_success: Optional[str] = Field(None, description="Step to execute on success")
    on_failure: Optional[str] = Field(None, description="Step to execute on failure")
    estimated_risk: int = Field(0, description="Risk level (0-4)")
    tags: List[str] = Field(default_factory=list, description="Tags for categorization")

class ComplexWorkflowPlan(BaseModel):
    """Model for a complex workflow plan."""
    id: str = Field(..., description="Unique identifier for this workflow")
    name: str = Field(..., description="Name of the workflow")
    description: str = Field(..., description="Detailed description of the workflow")
    goal: str = Field(..., description="Original goal that prompted this workflow")
    steps: Dict[str, WorkflowStep] = Field(..., description="Steps of the workflow")
    variables: Dict[str, WorkflowVariable] = Field(default_factory=dict, description="Workflow variables")
    entry_points: List[str] = Field(..., description="Step IDs to start execution with")
    exit_points: List[str] = Field(default_factory=list, description="Step IDs that conclude the workflow")
    context: Dict[str, Any] = Field(default_factory=dict, description="Context information")
    created: datetime = Field(default_factory=datetime.now, description="When the workflow was created")
    metadata: Dict[str, Any] = Field(default_factory=dict, description="Additional metadata")

class ComplexWorkflowPlanner(EnhancedTaskPlanner):
    """
    Planner specialized for creating and executing complex workflows that
    orchestrate multiple tools and services in a controlled, reliable manner.
    """
    
    def __init__(self):
        """Initialize the complex workflow planner."""
        super().__init__()
        self._logger = logger
        
        # Track currently executing workflows
        self._active_workflows: Dict[str, Dict[str, Any]] = {}
        
        # Register supported workflow step handlers
        self._step_handlers = {
            WorkflowStepType.COMMAND: self._execute_command_step,
            WorkflowStepType.TOOL: self._execute_tool_step,
            WorkflowStepType.API: self._execute_api_step,
            WorkflowStepType.DECISION: self._execute_decision_step,
            WorkflowStepType.WAIT: self._execute_wait_step,
            WorkflowStepType.PARALLEL: self._execute_parallel_step,
            WorkflowStepType.CUSTOM_CODE: self._execute_custom_code_step,
            WorkflowStepType.NOTIFICATION: self._execute_notification_step,
            WorkflowStepType.VALIDATION: self._execute_validation_step,
        }
    
    async def plan_complex_workflow(
        self,
        request: str,
        context: Dict[str, Any],
        max_steps: int = 30
    ) -> ComplexWorkflowPlan:
        """
        Plan a complex workflow involving multiple tools based on a natural language request.
        
        Args:
            request: Natural language request describing the workflow
            context: Context information for enhanced planning
            max_steps: Maximum number of steps to include in the plan
            
        Returns:
            A ComplexWorkflowPlan object
        """
        self._logger.info(f"Planning complex workflow: {request}")
        
        try:
            # Generate the workflow plan data using AI
            plan_data = await self._generate_workflow_plan(request, context, max_steps)
            
            # Extract the plan components
            workflow_id = plan_data.get("id", str(uuid.uuid4()))
            workflow_name = plan_data.get("name", f"Workflow {workflow_id[:8]}")
            description = plan_data.get("description", request)
            
            # Convert step data to model objects
            steps_data = plan_data.get("steps", {})
            steps = self._convert_step_data_to_models(steps_data)
            
            # Convert variable data to model objects
            variables_data = plan_data.get("variables", {})
            variables = self._convert_variable_data_to_models(variables_data)
            
            # Get entry points
            entry_points = plan_data.get("entry_points", [])
            if not entry_points and steps:
                # If no entry points specified, use the first step
                entry_points = [next(iter(steps.keys()))]
            
            # Create the workflow plan
            workflow_plan = ComplexWorkflowPlan(
                id=workflow_id,
                name=workflow_name,
                description=description,
                steps=steps,
                variables=variables,
                entry_points=entry_points,
                request=request,
                context_snapshot=self._take_context_snapshot(context)
            )
            
            self._logger.info(f"Created complex workflow plan with {len(steps)} steps")
            return workflow_plan
            
        except Exception as e:
            self._logger.error(f"Error generating complex workflow plan: {str(e)}")
            # Create a fallback workflow
            return self._create_fallback_workflow(request, context)
    
    async def _generate_workflow_plan(
        self, 
        request: str, 
        context: Dict[str, Any],
        max_steps: int
    ) -> Dict[str, Any]:
        """
        Generate a workflow plan using AI.
        
        Args:
            request: Natural language request
            context: Context information
            max_steps: Maximum number of steps
            
        Returns:
            Dictionary with workflow plan data
        """
        self._logger.debug(f"Generating workflow plan for: {request}")
        
        # Build a prompt enriched with system context
        cwd = context.get("cwd", "/")
        project_root = context.get("project_root", cwd)
        project_type = context.get("project_type", "unknown")
        
        # Include available tools
        # Get universal CLI translator from API
        universal_cli_translator = get_universal_cli_translator()
        available_tools = await universal_cli_translator.get_tool_suggestions()
        tools_str = ", ".join(available_tools[:15])
        if len(available_tools) > 15:
            tools_str += f" and {len(available_tools) - 15} more"
        
        prompt = f"""
    You are an expert DevOps engineer and workflow automation specialist. Create a detailed workflow plan for this request:
    
    REQUEST: "{request}"
    
    CONTEXT:
    - Current directory: {cwd}
    - Project root: {project_root}
    - Project type: {project_type}
    - Available tools: {tools_str}
    
    Design a comprehensive workflow that addresses all aspects of the request. Follow these guidelines:
    1. Break down the workflow into logical steps with clear dependencies
    2. Identify exact tools and commands needed for each step
    3. Define variables needed throughout the workflow
    4. Include appropriate validation and error handling
    5. Structure the plan with parallel execution where possible
    6. Ensure proper sequencing with dependencies
    
    Return a structured JSON object with:
    - name: A descriptive name for the workflow
    - description: Detailed explanation of what this workflow accomplishes
    - steps: Object mapping step IDs to step details (see step structure below)
    - variables: Object mapping variable names to details (see variable structure below)
    - entry_points: Array of step IDs that start the workflow
    - exit_points: Array of step IDs that conclude the workflow
    
    Step structure:
    {{
      "id": "unique_step_id",
      "name": "Human-readable step name",
      "type": "command|tool|api|decision|wait|parallel|custom_code|notification|validation",
      "description": "Detailed description of this step",
      "tool": "Tool name for TOOL type",
      "command": "Command to execute",
      "condition": "Condition for DECISION or VALIDATION type",
      "dependencies": [
        {{ "step_id": "another_step_id", "type": "success|completion|failure" }}
      ],
      "inputs": {{ "key": "value" }},
      "outputs": ["variable_name1", "variable_name2"],
      "estimated_risk": 0-4
    }}
    
    Variable structure:
    {{
      "name": "variable_name",
      "description": "Purpose of this variable",
      "default_value": "default if any",
      "required": true|false,
      "type": "string|number|boolean",
      "source_step": "step_id that produces this variable"
    }}
    
    Ensure the workflow is complete, practical, and executable. Include approximately {max_steps} steps or fewer, favoring quality over quantity.
    """
        
        # Get gemini client from API
        gemini_client = get_gemini_client()
        GeminiRequest = get_gemini_request_class()
        
        # Call AI service
        api_request = GeminiRequest(prompt=prompt, max_tokens=4000)
        response = await gemini_client.generate_text(api_request)
        
        try:
            # Extract JSON from the response
            import json
            
            # Try to find JSON in the response
            json_match = re.search(r'```(?:json)?\s*(.*?)\s*```', response.text, re.DOTALL)
            if json_match:
                json_str = json_match.group(1)
            else:
                # Assume the entire response is JSON
                json_str = response.text
            
            # Parse JSON
            plan_data = json.loads(json_str)
            return plan_data
            
        except Exception as e:
            self._logger.error(f"Error parsing workflow plan JSON: {str(e)}")
            return {
                "error": str(e),
                "name": f"Workflow for: {request[:30]}...",
                "description": "Error parsing workflow plan",
                "steps": {},
                "variables": {},
                "entry_points": [],
                "exit_points": []
            }
    
    def _convert_step_data_to_models(
        self, 
        steps_data: Dict[str, Dict[str, Any]]
    ) -> Dict[str, WorkflowStep]:
        """
        Convert step data from AI to WorkflowStep models.
        
        Args:
            steps_data: Dictionary of step data
            
        Returns:
            Dictionary of WorkflowStep models
        """
        result = {}
        
        for step_id, step_data in steps_data.items():
            # Ensure step_data has required fields
            if not step_data.get("type"):
                self._logger.warning(f"Step {step_id} missing 'type' field, defaulting to 'command'")
                step_data["type"] = "command"
            
            if not step_data.get("name"):
                step_data["name"] = f"Step {step_id}"
            
            if not step_data.get("description"):
                step_data["description"] = f"Step {step_id} ({step_data['type']})"
            
            # Convert dependencies to models if present
            dependencies = []
            for dep in step_data.get("dependencies", []):
                if isinstance(dep, dict) and "step_id" in dep:
                    # Already in correct format
                    dependencies.append(WorkflowStepDependency(**dep))
                elif isinstance(dep, str):
                    # Just a step ID, assume success dependency
                    dependencies.append(WorkflowStepDependency(step_id=dep, type="success"))
            
            # Set proper dependencies format
            step_data["dependencies"] = dependencies
            
            # Ensure ID is set
            step_data["id"] = step_id
            
            # Create the model
            try:
                result[step_id] = WorkflowStep(**step_data)
            except Exception as e:
                self._logger.error(f"Error creating WorkflowStep model for {step_id}: {str(e)}")
                # Create a simplified step as fallback
                result[step_id] = WorkflowStep(
                    id=step_id,
                    name=step_data.get("name", f"Step {step_id}"),
                    type=WorkflowStepType.COMMAND,
                    description=step_data.get("description", f"Step {step_id} (fallback)"),
                    command=step_data.get("command", "echo 'Step execution error'")
                )
        
        return result
    
    def _convert_variable_data_to_models(
        self, 
        variables_data: Dict[str, Dict[str, Any]]
    ) -> Dict[str, WorkflowVariable]:
        """
        Convert variable data from AI to WorkflowVariable models.
        
        Args:
            variables_data: Dictionary of variable data
            
        Returns:
            Dictionary of WorkflowVariable models
        """
        result = {}
        
        for var_name, var_data in variables_data.items():
            # Ensure var_data has name field
            var_data["name"] = var_name
            
            # Create the model
            try:
                result[var_name] = WorkflowVariable(**var_data)
            except Exception as e:
                self._logger.error(f"Error creating WorkflowVariable model for {var_name}: {str(e)}")
                # Create a simplified variable as fallback
                result[var_name] = WorkflowVariable(
                    name=var_name,
                    description=var_data.get("description", f"Variable {var_name}"),
                    type=var_data.get("type", "string")
                )
        
        return result
    
    def _create_fallback_workflow(
        self, 
        request: str, 
        context: Dict[str, Any]
    ) -> ComplexWorkflowPlan:
        """
        Create a fallback workflow plan when generation fails.
        
        Args:
            request: Natural language request
            context: Context information
            
        Returns:
            A simple ComplexWorkflowPlan
        """
        self._logger.info(f"Creating fallback workflow for: {request}")
        
        # Create a unique ID
        workflow_id = str(uuid.uuid4())
        
        # Create a simple step that echoes the error
        step_id = "fallback_step"
        step = WorkflowStep(
            id=step_id,
            name="Fallback Step",
            type=WorkflowStepType.COMMAND,
            description="Fallback step due to workflow planning error",
            command=f"echo 'Failed to create complex workflow for: {request}'",
            dependencies=[]
        )
        
        # Create the workflow plan
        return ComplexWorkflowPlan(
            id=workflow_id,
            name=f"Fallback Workflow {workflow_id[:8]}",
            description=f"Fallback workflow for: {request}",
            goal=request,
            steps={step_id: step},
            entry_points=[step_id],
            exit_points=[step_id],
            context=context,
            created=datetime.now()
        )
    
    async def execute_complex_workflow(
        self,
        workflow: ComplexWorkflowPlan,
        dry_run: bool = False,
        transaction_id: Optional[str] = None,
        initial_variables: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Execute a complex workflow plan.
        
        Args:
            workflow: The workflow plan to execute
            dry_run: Whether to simulate execution without making changes
            transaction_id: Optional transaction ID for rollback support
            initial_variables: Optional initial variables to set
            
        Returns:
            Dictionary with execution results
        """
        self._logger.info(f"Executing complex workflow: {workflow.name}")
        
        # Initialize execution state
        execution_state = {
            "workflow_id": workflow.id,
            "started_at": datetime.now().isoformat(),
            "dry_run": dry_run,
            "current_step": None,
            "completed_steps": set(),
            "failed_steps": set(),
            "results": {},
            "variables": {},
            "transaction_id": transaction_id,
            "status": "running"
        }
        
        # Add initial variables if provided
        if initial_variables:
            execution_state["variables"].update(initial_variables)
        
        # Initialize variables from workflow definition
        for var_name, var_obj in workflow.variables.items():
            if var_obj.default_value is not None:
                execution_state["variables"][var_name] = var_obj.default_value
        
        # Start with entry points
        steps_to_execute = set(workflow.entry_points)
        all_steps = set(workflow.steps.keys())
        
        # Execute steps until there are no more steps to execute
        while steps_to_execute:
            # Get the next step to execute
            executable_steps = set()
            
            for step_id in steps_to_execute:
                if step_id in execution_state["completed_steps"] or step_id in execution_state["failed_steps"]:
                    continue
                    
                step = workflow.steps.get(step_id)
                if not step:
                    self._logger.warning(f"Step {step_id} not found in workflow")
                    continue
                    
                # Check if all dependencies are satisfied
                dependencies_satisfied = True
                for dep in step.dependencies:
                    if not self._is_dependency_satisfied(dep, execution_state):
                        dependencies_satisfied = False
                        break
                        
                if dependencies_satisfied:
                    executable_steps.add(step_id)
            
            # If no steps can be executed, check if we're stuck or done
            if not executable_steps:
                if len(execution_state["completed_steps"]) + len(execution_state["failed_steps"]) < len(all_steps):
                    # We're stuck - there are steps that can't be executed
                    self._logger.warning("Workflow execution is stuck - some steps cannot be executed")
                    execution_state["status"] = "stuck"
                    
                    # Find the steps that couldn't be executed
                    remaining_steps = all_steps - execution_state["completed_steps"] - execution_state["failed_steps"]
                    self._logger.warning(f"Steps not executed: {remaining_steps}")
                    
                    # Check each remaining step for its blocker
                    for step_id in remaining_steps:
                        step = workflow.steps.get(step_id)
                        if step:
                            for dep in step.dependencies:
                                if not self._is_dependency_satisfied(dep, execution_state):
                                    self._logger.warning(f"Step {step_id} blocked by dependency {dep.step_id} ({dep.type})")
                    
                    break
                else:
                    # All steps have been processed
                    self._logger.info("All workflow steps processed")
                    execution_state["status"] = "completed"
                    break
            
            # Execute the steps in parallel or sequentially
            execute_in_parallel = (
                len(executable_steps) > 1 and
                self._can_execute_in_parallel(executable_steps, workflow.steps)
            )
            
            if execute_in_parallel:
                # Execute steps in parallel
                self._logger.info(f"Executing {len(executable_steps)} steps in parallel")
                
                tasks = []
                for step_id in executable_steps:
                    step = workflow.steps[step_id]
                    execution_state["current_step"] = step_id
                    tasks.append(self._execute_step(step, execution_state))
                
                # Wait for all tasks to complete
                step_results = await asyncio.gather(*tasks, return_exceptions=True)
                
                # Process results
                for i, step_id in enumerate(executable_steps):
                    result = step_results[i]
                    
                    if isinstance(result, Exception):
                        self._logger.error(f"Step {step_id} failed with exception: {str(result)}")
                        execution_state["failed_steps"].add(step_id)
                        execution_state["results"][step_id] = {
                            "success": False,
                            "error": str(result),
                            "exception": True
                        }
                    else:
                        execution_state["results"][step_id] = result
                        
                        if result.get("success", False):
                            execution_state["completed_steps"].add(step_id)
                            
                            # Extract variables from the result
                            if "variables" in result:
                                for var_name, var_value in result["variables"].items():
                                    execution_state["variables"][var_name] = var_value
                        else:
                            execution_state["failed_steps"].add(step_id)
            else:
                # Execute steps sequentially
                for step_id in executable_steps:
                    step = workflow.steps[step_id]
                    execution_state["current_step"] = step_id
                    
                    # Execute the step
                    self._logger.info(f"Executing step {step_id}: {step.name}")
                    try:
                        result = await self._execute_step(step, execution_state)
                        execution_state["results"][step_id] = result
                        
                        if result.get("success", False):
                            execution_state["completed_steps"].add(step_id)
                            
                            # Extract variables from the result
                            if "variables" in result:
                                for var_name, var_value in result["variables"].items():
                                    execution_state["variables"][var_name] = var_value
                        else:
                            execution_state["failed_steps"].add(step_id)
                            
                            # Stop execution on failure unless step is marked as non-critical
                            if not step.continue_on_failure:
                                self._logger.warning(f"Step {step_id} failed and is critical - stopping workflow")
                                execution_state["status"] = "failed"
                                break
                    except Exception as e:
                        self._logger.error(f"Error executing step {step_id}: {str(e)}")
                        execution_state["failed_steps"].add(step_id)
                        execution_state["results"][step_id] = {
                            "success": False,
                            "error": str(e),
                            "exception": True
                        }
                        
                        # Stop execution on exception unless step is marked as non-critical
                        if not step.continue_on_failure:
                            self._logger.warning(f"Step {step_id} failed with exception and is critical - stopping workflow")
                            execution_state["status"] = "failed"
                            break
            
            # Update steps to execute - remove completed and failed steps
            steps_to_execute -= execution_state["completed_steps"]
            steps_to_execute -= execution_state["failed_steps"]
            
            # Add any new steps that might have become executable
            for step_id in all_steps - execution_state["completed_steps"] - execution_state["failed_steps"]:
                if step_id not in steps_to_execute:
                    step = workflow.steps.get(step_id)
                    if step:
                        # Check if all dependencies are satisfied
                        dependencies_satisfied = True
                        for dep in step.dependencies:
                            if not self._is_dependency_satisfied(dep, execution_state):
                                dependencies_satisfied = False
                                break
                                
                        if dependencies_satisfied:
                            steps_to_execute.add(step_id)
            
            # If status is failed, stop execution
            if execution_state["status"] == "failed":
                break
        
        # Calculate success based on critical steps
        critical_steps = [step_id for step_id, step in workflow.steps.items() if not step.continue_on_failure]
        failed_critical_steps = execution_state["failed_steps"].intersection(critical_steps)
        
        execution_state["success"] = (
            execution_state["status"] != "failed" and
            execution_state["status"] != "stuck" and
            len(failed_critical_steps) == 0
        )
        
        # Add end time
        execution_state["ended_at"] = datetime.now().isoformat()
        
        # Log completion
        status_str = "succeeded" if execution_state["success"] else "failed"
        self._logger.info(f"Workflow execution {status_str}: {len(execution_state['completed_steps'])} completed, {len(execution_state['failed_steps'])} failed")
        
        # Clean up the execution state for the return value
        return {
            "workflow_id": execution_state["workflow_id"],
            "success": execution_state["success"],
            "status": execution_state["status"],
            "steps_total": len(all_steps),
            "steps_completed": len(execution_state["completed_steps"]),
            "steps_failed": len(execution_state["failed_steps"]),
            "started_at": execution_state["started_at"],
            "ended_at": execution_state["ended_at"],
            "results": execution_state["results"],
            "variables": execution_state["variables"]
        }
    
    def _is_dependency_satisfied(
        self, 
        dependency: WorkflowStepDependency, 
        execution_state: Dict[str, Any]
    ) -> bool:
        """
        Check if a dependency is satisfied in the current execution state.
        
        Args:
            dependency: The dependency to check
            execution_state: Current execution state
            
        Returns:
            True if dependency is satisfied, False otherwise
        """
        step_id = dependency.step_id
        
        # Check if the step exists in completed steps
        if step_id not in execution_state["completed_steps"]:
            return False
        
        # Get the result of the dependent step
        result = execution_state["results"].get(step_id, {})
        
        # Check dependency type
        if dependency.type == "completion":
            # Only check that the step completed (success or failure)
            return True
        elif dependency.type == "success":
            # Check that the step completed successfully
            return result.get("success", False)
        elif dependency.type == "failure":
            # Check that the step failed
            return not result.get("success", False)
        
        # Unknown dependency type
        self._logger.warning(f"Unknown dependency type: {dependency.type}")
        return False


    async def _execute_step(self, step: WorkflowStep, execution_state: Dict[str, Any]) -> Dict[str, Any]:
        """
        Execute a single workflow step.
        
        Args:
            step: The workflow step to execute
            execution_state: Current execution state
            
        Returns:
            Dictionary with step execution results
        """
        self._logger.debug(f"Executing step {step.id}: {step.name} (Type: {step.type})")
        
        # Check if we need to substitute variables
        try:
            # Find and replace all variable references
            if hasattr(step, "command") and step.command:
                step.command = self._substitute_variables(step.command, execution_state["variables"])
                
            if hasattr(step, "api_url") and step.api_url:
                step.api_url = self._substitute_variables(step.api_url, execution_state["variables"])
                
            if hasattr(step, "condition") and step.condition:
                step.condition = self._substitute_variables(step.condition, execution_state["variables"])
                
            if hasattr(step, "file_path") and step.file_path:
                step.file_path = self._substitute_variables(step.file_path, execution_state["variables"])
                
            if hasattr(step, "file_content") and step.file_content:
                step.file_content = self._substitute_variables(step.file_content, execution_state["variables"])
        except Exception as e:
            self._logger.error(f"Error substituting variables: {str(e)}")
        
        # Log step execution
        step_info = f"Executing step {step.id}"
        if step.description:
            step_info += f": {step.description}"
            
        self._logger.info(step_info)
        
        # Check if this is a dry run
        if execution_state["dry_run"]:
            return {
                "success": True,
                "dry_run": True,
                "message": f"[DRY RUN] Would execute step: {step.name} ({step.type})"
            }
        
        # Get the appropriate handler for this step type
        handler = self._step_handlers.get(step.type)
        if not handler:
            return {
                "success": False,
                "error": f"Unsupported step type: {step.type}"
            }
        
        # Execute the step
        try:
            result = await handler(step, execution_state)
            
            # Check for variables to extract
            if result.get("success", False) and "output" in result:
                output = result["output"]
                if isinstance(output, str):
                    extracted_vars = self._extract_variables_from_output(output)
                    if extracted_vars:
                        if "variables" not in result:
                            result["variables"] = {}
                        result["variables"].update(extracted_vars)
            
            return result
        except Exception as e:
            self._logger.error(f"Error executing step {step.id}: {str(e)}")
            return {
                "success": False,
                "error": str(e),
                "exception": True
            }
    
    async def execute_tool_across_environments(
        self,
        request: str,
        tools: List[str],
        environments: List[str] = None,
        context: Dict[str, Any] = None,
        dry_run: bool = False
    ) -> Dict[str, Any]:
        """
        Execute a natural language request across multiple tools and environments.
        
        This is a higher-level method that creates and executes a workflow spanning
        multiple tools across different environments (like dev, staging, prod).
        
        Args:
            request: Natural language request
            tools: List of tools to include in the workflow
            environments: Optional list of environments to target
            context: Optional context information
            dry_run: Whether to simulate execution without making changes
            
        Returns:
            Dictionary with execution results
        """
        self._logger.info(f"Executing request across environments: {request}")
        
        # Get context if not provided
        if context is None:
            # Get context manager from API
            context_manager = get_context_manager()
            context = context_manager.get_context_dict()
        
        # Set default environments if not provided
        if not environments:
            environments = ["dev"]
        
        # Build a complex workflow request that includes all tools and environments
        enhanced_request = f"{request}\n\nTools to use: {', '.join(tools)}\n"
        enhanced_request += f"Target environments: {', '.join(environments)}\n"
        
        # Add additional context about the tools if available
        tool_context = "Tool information:\n"
        for tool in tools:
            tool_info = await self._get_tool_information(tool)
            if tool_info:
                tool_context += f"- {tool}: {tool_info}\n"
        
        enhanced_request += f"\n{tool_context}"
        
        # Plan and execute the workflow
        workflow = await self.plan_complex_workflow(
            request=enhanced_request,
            context=context
        )
        
        # Execute the workflow
        result = await self.execute_complex_workflow(
            workflow=workflow,
            dry_run=dry_run
        )
        
        return {
            "original_request": request,
            "enhanced_request": enhanced_request,
            "tools": tools,
            "environments": environments,
            "workflow": workflow.dict(exclude={"context_snapshot"}),
            "execution_result": result
        }
    
    async def _get_tool_information(self, tool: str) -> Optional[str]:
        """
        Get information about a CLI tool by running its help command.
        
        Args:
            tool: The tool name
            
        Returns:
            Tool information string or None if not available
        """
        try:
            # Get universal CLI translator from API
            universal_cli_translator = get_universal_cli_translator()
            suggestions = await universal_cli_translator.get_tool_suggestions(tool)
            
            if tool in suggestions:
                # Tool exists, try to get its help info
                help_cmd = f"{tool} --help"
                
                # Get execution engine from API
                execution_engine = get_execution_engine()
                stdout, stderr, return_code = await execution_engine.execute_command(
                    command=help_cmd,
                    check_safety=True
                )
                
                if return_code == 0:
                    # Parse the help output to extract a brief description
                    lines = stdout.split('\n')
                    # Filter out blank lines and syntax lines
                    lines = [line for line in lines if line.strip() and not line.strip().startswith("usage:")]
                    
                    if lines:
                        # Take the first non-empty line as the description
                        return lines[0].strip()
                
                return f"CLI tool available in the system"
            return None
        except Exception as e:
            self._logger.debug(f"Error getting tool information for {tool}: {str(e)}")
            return None
    
    def _take_context_snapshot(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """
        Take a snapshot of the relevant context information for workflow execution.
        
        Args:
            context: The full context dictionary
            
        Returns:
            A filtered copy of the context with only relevant information
        """
        # Extract only the keys we need for workflow execution
        relevant_keys = [
            "cwd", "project_root", "project_type", "user", "environment",
            "recent_files", "session"
        ]
        
        snapshot = {}
        for key in relevant_keys:
            if key in context:
                # Make a deep copy to avoid modifying the original
                try:
                    # Try to serialize to JSON and back to ensure it's serializable
                    snapshot[key] = json.loads(json.dumps(context[key]))
                except (TypeError, json.JSONDecodeError):
                    # If not JSON serializable, convert to string
                    snapshot[key] = str(context[key])
        
        return snapshot


    
    async def _execute_command_step(
        self, 
        step: WorkflowStep, 
        execution_state: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Execute a command step in a workflow.
        
        Args:
            step: The workflow step
            execution_state: Current execution state
            
        Returns:
            Execution result
        """
        if not step.command:
            return {
                "success": False,
                "error": "Missing command for command step"
            }
        
        # Apply variable substitution to the command
        command = self._substitute_variables(step.command, execution_state["variables"])
        
        self._logger.info(f"Executing command: {command}")
        
        # If dry run, just simulate the command
        if execution_state["dry_run"]:
            return {
                "success": True,
                "command": command,
                "stdout": f"[DRY RUN] Would execute: {command}",
                "stderr": "",
                "return_code": 0,
                "outputs": {}
            }
        
        # Determine working directory
        working_dir = None
        if step.working_dir:
            working_dir = self._substitute_variables(step.working_dir, execution_state["variables"])
        
        # Set up environment variables
        env = {}
        if step.environment:
            for key, value in step.environment.items():
                env[key] = self._substitute_variables(value, execution_state["variables"])
        
        # Execute the command using the adaptive engine
        try:
            # Get adaptive engine from API
            adaptive_engine = get_adaptive_engine()
            
            # Execute using proper engine
            result = await adaptive_engine.execute_command(
                command=command,
                natural_request=f"Workflow step: {step.name}",
                explanation=step.description,
                dry_run=execution_state["dry_run"],
                working_dir=working_dir,
                environment=env if env else None
            )
            
            # Extract outputs from the command result
            outputs = {}
            outputs[f"{step.id}_success"] = result.get("success", False)
            outputs[f"{step.id}_return_code"] = result.get("return_code", -1)
            
            if "stdout" in result:
                outputs[f"{step.id}_stdout"] = result["stdout"]
                # Try to extract variables from stdout
                extracted_vars = self._extract_variables_from_output(result["stdout"])
                outputs.update(extracted_vars)
            
            if "stderr" in result:
                outputs[f"{step.id}_stderr"] = result["stderr"]
            
            return {
                "success": result.get("success", False),
                "command": command,
                "stdout": result.get("stdout", ""),
                "stderr": result.get("stderr", ""),
                "return_code": result.get("return_code", -1),
                "outputs": outputs
            }
            
        except Exception as e:
            self._logger.exception(f"Error executing command step: {str(e)}")
            return {
                "success": False,
                "command": command,
                "error": str(e),
                "outputs": {
                    f"{step.id}_success": False,
                    f"{step.id}_error": str(e)
                }
            }
    
    async def _execute_tool_step(
        self, 
        step: WorkflowStep, 
        execution_state: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Execute a tool step in a workflow using universal_cli_translator.
        
        Args:
            step: The workflow step
            execution_state: Current execution state
            
        Returns:
            Execution result
        """
        if not step.tool:
            return {
                "success": False,
                "error": "Missing tool name for tool step"
            }
        
        # Get tool name and command
        tool = self._substitute_variables(step.tool, execution_state["variables"])
        command = self._substitute_variables(step.command or "", execution_state["variables"])
        
        # Combine tool and command
        full_command = f"{tool} {command}".strip()
        
        self._logger.info(f"Executing tool command: {full_command}")
        
        # If dry run, just simulate the command
        if execution_state["dry_run"]:
            return {
                "success": True,
                "tool": tool,
                "command": full_command,
                "stdout": f"[DRY RUN] Would execute tool: {full_command}",
                "stderr": "",
                "return_code": 0,
                "outputs": {}
            }
        
        # Determine working directory
        working_dir = None
        if step.working_dir:
            working_dir = self._substitute_variables(step.working_dir, execution_state["variables"])
        
        # Set up environment variables
        env = {}
        if step.environment:
            for key, value in step.environment.items():
                env[key] = self._substitute_variables(value, execution_state["variables"])
        
        # Execute the command using the adaptive engine
        try:
            # Get adaptive engine from API
            adaptive_engine = get_adaptive_engine()
            
            # Execute using proper engine
            result = await adaptive_engine.execute_command(
                command=full_command,
                natural_request=f"Workflow tool step: {step.name}",
                explanation=step.description,
                dry_run=execution_state["dry_run"],
                working_dir=working_dir,
                environment=env if env else None
            )
            
            # Extract outputs from the command result
            outputs = {}
            outputs[f"{step.id}_success"] = result.get("success", False)
            outputs[f"{step.id}_return_code"] = result.get("return_code", -1)
            
            if "stdout" in result:
                outputs[f"{step.id}_stdout"] = result["stdout"]
                # Try to extract variables from stdout
                extracted_vars = self._extract_variables_from_output(result["stdout"])
                outputs.update(extracted_vars)
            
            if "stderr" in result:
                outputs[f"{step.id}_stderr"] = result["stderr"]
            
            return {
                "success": result.get("success", False),
                "tool": tool,
                "command": full_command,
                "stdout": result.get("stdout", ""),
                "stderr": result.get("stderr", ""),
                "return_code": result.get("return_code", -1),
                "outputs": outputs
            }
            
        except Exception as e:
            self._logger.exception(f"Error executing tool step: {str(e)}")
            return {
                "success": False,
                "tool": tool,
                "command": full_command,
                "error": str(e),
                "outputs": {
                    f"{step.id}_success": False,
                    f"{step.id}_error": str(e)
                }
            }
    
    async def _execute_api_step(
        self, 
        step: WorkflowStep, 
        execution_state: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Execute an API call step in a workflow.
        
        Args:
            step: The workflow step
            execution_state: Current execution state
            
        Returns:
            Execution result
        """
        if not step.api_url:
            return {
                "success": False,
                "error": "Missing API URL for API step"
            }
        
        # Apply variable substitution
        api_url = self._substitute_variables(step.api_url, execution_state["variables"])
        method = step.api_method or "GET"
        
        # Process headers with variable substitution
        headers = {}
        for key, value in step.api_headers.items():
            headers[key] = self._substitute_variables(value, execution_state["variables"])
        
        # Process data payload with variable substitution
        data = None
        if step.api_data:
            if isinstance(step.api_data, str):
                data = self._substitute_variables(step.api_data, execution_state["variables"])
            elif isinstance(step.api_data, dict):
                data = {}
                for key, value in step.api_data.items():
                    if isinstance(value, str):
                        data[key] = self._substitute_variables(value, execution_state["variables"])
                    else:
                        data[key] = value
        
        self._logger.info(f"Executing API call: {method} {api_url}")
        
        # If dry run, just simulate the API call
        if execution_state["dry_run"]:
            return {
                "success": True,
                "url": api_url,
                "method": method,
                "response": f"[DRY RUN] Would call API: {method} {api_url}",
                "outputs": {}
            }
        
        # Execute the API call
        try:
            import aiohttp
            timeout = aiohttp.ClientTimeout(total=step.timeout or 30)
            
            async with aiohttp.ClientSession(timeout=timeout) as session:
                # Prepare the request
                if method.upper() in ["GET", "DELETE"]:
                    async with session.request(method, api_url, headers=headers) as response:
                        status = response.status
                        response_text = await response.text()
                        try:
                            response_json = await response.json()
                        except:
                            response_json = None
                else:  # POST, PUT, PATCH
                    if headers.get("Content-Type") == "application/json" and data:
                        async with session.request(method, api_url, json=data, headers=headers) as response:
                            status = response.status
                            response_text = await response.text()
                            try:
                                response_json = await response.json()
                            except:
                                response_json = None
                    else:
                        async with session.request(method, api_url, data=data, headers=headers) as response:
                            status = response.status
                            response_text = await response.text()
                            try:
                                response_json = await response.json()
                            except:
                                response_json = None
                
                # Prepare result
                success = 200 <= status < 300
                
                # Extract outputs from the response
                outputs = {}
                outputs[f"{step.id}_status"] = status
                outputs[f"{step.id}_success"] = success
                outputs[f"{step.id}_response_text"] = response_text
                
                if response_json:
                    outputs[f"{step.id}_response_json"] = response_json
                    
                    # Try to extract variables from JSON response
                    if isinstance(response_json, dict):
                        for key, value in response_json.items():
                            outputs[f"{step.id}_{key}"] = value
                
                return {
                    "success": success,
                    "url": api_url,
                    "method": method,
                    "status": status,
                    "response_text": response_text,
                    "response_json": response_json,
                    "outputs": outputs
                }
                
        except Exception as e:
            self._logger.exception(f"Error executing API step: {str(e)}")
            return {
                "success": False,
                "url": api_url,
                "method": method,
                "error": str(e),
                "outputs": {
                    f"{step.id}_success": False,
                    f"{step.id}_error": str(e)
                }
            }
    
    async def _execute_decision_step(
        self, 
        step: WorkflowStep, 
        execution_state: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Execute a decision step in a workflow.
        
        Args:
            step: The workflow step
            execution_state: Current execution state
            
        Returns:
            Execution result with condition evaluation
        """
        if not step.condition:
            return {
                "success": False,
                "error": "Missing condition for decision step"
            }
        
        # Apply variable substitution to the condition
        condition = self._substitute_variables(step.condition, execution_state["variables"])
        
        self._logger.info(f"Evaluating condition: {condition}")
        
        # Evaluate the condition
        try:
            condition_result = await self._evaluate_condition(
                condition, 
                execution_state["variables"]
            )
            
            self._logger.debug(f"Condition evaluated to: {condition_result}")
            
            return {
                "success": True,
                "condition": condition,
                "condition_result": condition_result,
                "outputs": {
                    f"{step.id}_condition": condition,
                    f"{step.id}_result": condition_result
                }
            }
        except Exception as e:
            self._logger.exception(f"Error evaluating condition: {str(e)}")
            return {
                "success": False,
                "condition": condition,
                "error": str(e),
                "outputs": {
                    f"{step.id}_success": False,
                    f"{step.id}_error": str(e)
                }
            }
    
    async def _execute_wait_step(
        self, 
        step: WorkflowStep, 
        execution_state: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Execute a wait step in a workflow.
        
        Args:
            step: The workflow step
            execution_state: Current execution state
            
        Returns:
            Execution result
        """
        # Check for a wait condition
        if step.wait_condition:
            # Apply variable substitution to the condition
            condition = self._substitute_variables(step.wait_condition, execution_state["variables"])
            
            self._logger.info(f"Waiting for condition: {condition}")
            
            # If dry run, just simulate the wait
            if execution_state["dry_run"]:
                return {
                    "success": True,
                    "condition": condition,
                    "message": f"[DRY RUN] Would wait for condition: {condition}",
                    "outputs": {}
                }
            
            # Set up timeout
            timeout = step.timeout or 300  # Default to 5 minutes
            wait_interval = 5  # Check every 5 seconds
            max_attempts = timeout // wait_interval
            
            # Wait for the condition to be true
            for attempt in range(max_attempts):
                try:
                    condition_result = await self._evaluate_condition(
                        condition, 
                        execution_state["variables"]
                    )
                    
                    if condition_result:
                        self._logger.info(f"Wait condition satisfied after {attempt * wait_interval} seconds")
                        return {
                            "success": True,
                            "condition": condition,
                            "wait_time": attempt * wait_interval,
                            "outputs": {
                                f"{step.id}_wait_time": attempt * wait_interval,
                                f"{step.id}_success": True
                            }
                        }
                    
                    # Wait before checking again
                    await asyncio.sleep(wait_interval)
                    
                except Exception as e:
                    self._logger.error(f"Error evaluating wait condition: {str(e)}")
                    # Keep waiting, error may be temporary
            
            # Timeout reached
            self._logger.warning(f"Wait condition timed out after {timeout} seconds")
            return {
                "success": False,
                "condition": condition,
                "error": f"Wait condition timed out after {timeout} seconds",
                "outputs": {
                    f"{step.id}_success": False,
                    f"{step.id}_timed_out": True,
                    f"{step.id}_wait_time": timeout
                }
            }
        else:
            # Fixed time wait
            wait_time = step.timeout or 10  # Default to 10 seconds
            
            self._logger.info(f"Waiting for {wait_time} seconds")
            
            # If dry run, just simulate the wait
            if execution_state["dry_run"]:
                return {
                    "success": True,
                    "wait_time": wait_time,
                    "message": f"[DRY RUN] Would wait for {wait_time} seconds",
                    "outputs": {}
                }
            
            # Wait for the specified time
            await asyncio.sleep(wait_time)
            
            return {
                "success": True,
                "wait_time": wait_time,
                "outputs": {
                    f"{step.id}_wait_time": wait_time,
                    f"{step.id}_success": True
                }
            }





    def _can_execute_in_parallel(self, step_ids: Set[str], steps: Dict[str, WorkflowStep]) -> bool:
        """
        Determine if a set of steps can be executed in parallel.
        
        Args:
            step_ids: Set of step IDs to check
            steps: Dictionary of all workflow steps
            
        Returns:
            True if the steps can be executed in parallel, False otherwise
        """
        # Check for steps that are explicitly marked as not parallel-safe
        for step_id in step_ids:
            step = steps.get(step_id)
            if step and hasattr(step, "parallel_safe") and not step.parallel_safe:
                return False
        
        # Check for steps that might modify the same resources
        resource_map = {}
        
        for step_id in step_ids:
            step = steps.get(step_id)
            if not step:
                continue
                
            # Determine resources affected by this step
            resources = self._get_step_resources(step)
            
            # Check for conflicts with already scheduled steps
            for resource in resources:
                if resource in resource_map:
                    # Another step uses this resource - check if operations are compatible
                    other_ops = resource_map[resource]
                    for op in resources[resource]:
                        if not self._are_operations_compatible(other_ops, op):
                            return False
                        
                    # Add operation to resource map
                    resource_map[resource].update(resources[resource])
                else:
                    # First step to use this resource
                    resource_map[resource] = resources[resource]
        
        return True
    
    def _get_step_resources(self, step: WorkflowStep) -> Dict[str, Set[str]]:
        """
        Determine resources affected by a workflow step.
        
        Args:
            step: The workflow step to analyze
            
        Returns:
            Dictionary mapping resource names to sets of operations
        """
        resources = {}
        
        # Check step type
        if step.type == WorkflowStepType.COMMAND or step.type == WorkflowStepType.TOOL:
            # Extract file paths from command
            command = step.command
            if not command:
                return resources
                
            # Look for file paths in the command
            path_pattern = r'(?:\'|\")([\/\w\.-]+)(?:\'|\")'
            file_paths = re.findall(path_pattern, command)
            
            # Add each file as a resource
            for path in file_paths:
                if os.path.isabs(path) or path.startswith('./') or path.startswith('../'):
                    resources[path] = {"access"}
                    
                    # Infer operation from command
                    if any(x in command for x in ["rm", "del", "remove", "unlink"]):
                        resources[path].add("delete")
                    elif any(x in command for x in ["write", "create", ">", "tee"]):
                        resources[path].add("write")
                    elif any(x in command for x in ["cp", "copy", "mv", "move"]):
                        resources[path].add("write")
                        resources[path].add("read")
                    else:
                        resources[path].add("read")
            
            # Check for database operations
            if any(x in command for x in ["mysql", "psql", "mongo", "sqlite"]):
                db_name = "database"
                resources[db_name] = {"access"}
                
                if any(x in command.lower() for x in ["select", "show", "describe"]):
                    resources[db_name].add("read")
                elif any(x in command.lower() for x in ["insert", "update", "delete", "drop", "create"]):
                    resources[db_name].add("write")
        
        elif step.type == WorkflowStepType.FILE:
            # File operations directly specify the resource
            if hasattr(step, "file_path") and step.file_path:
                path = step.file_path
                resources[path] = {"access"}
                
                # Determine operation from step properties
                operation = getattr(step, "operation", "read")
                if operation in ["write", "create", "append"]:
                    resources[path].add("write")
                elif operation in ["delete", "remove"]:
                    resources[path].add("delete")
                else:
                    resources[path].add("read")
        
        elif step.type == WorkflowStepType.API:
            # API calls are generally independent
            api_url = getattr(step, "api_url", "")
            if api_url:
                resources[api_url] = {"access"}
                
                # Determine operation from HTTP method
                method = getattr(step, "method", "GET").upper()
                if method in ["GET", "HEAD", "OPTIONS"]:
                    resources[api_url].add("read")
                else:
                    resources[api_url].add("write")
        
        return resources
    
    def _are_operations_compatible(self, existing_ops: Set[str], new_op: str) -> bool:
        """
        Determine if operations on the same resource are compatible for parallel execution.
        
        Args:
            existing_ops: Set of existing operations
            new_op: New operation to check
            
        Returns:
            True if operations are compatible, False otherwise
        """
        # Delete is never compatible with anything
        if "delete" in existing_ops or new_op == "delete":
            return False
        
        # Write is not compatible with another write
        if "write" in existing_ops and new_op == "write":
            return False
        
        # Read is compatible with other reads
        return True
    
    async def _execute_parallel_step(
        self, 
        step: WorkflowStep, 
        execution_state: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Execute a parallel step in a workflow.
        
        Args:
            step: The workflow step
            execution_state: Current execution state
            
        Returns:
            Execution result
        """
        if not step.parallel_steps:
            return {
                "success": False,
                "error": "Missing parallel steps for parallel step"
            }
        
        self._logger.info(f"Executing {len(step.parallel_steps)} steps in parallel")
        
        # Check if all referenced steps exist in the workflow
        workflow = self._active_workflows[execution_state["workflow_id"]]["workflow"]
        parallel_steps = []
        
        for parallel_step_id in step.parallel_steps:
            if parallel_step_id in workflow.steps:
                parallel_steps.append(workflow.steps[parallel_step_id])
            else:
                self._logger.warning(f"Referenced parallel step {parallel_step_id} does not exist in workflow")
        
        if not parallel_steps:
            return {
                "success": False,
                "error": "No valid parallel steps found",
                "outputs": {
                    f"{step.id}_success": False,
                    f"{step.id}_error": "No valid parallel steps found"
                }
            }
        
        # If dry run, just simulate the parallel execution
        if execution_state["dry_run"]:
            return {
                "success": True,
                "message": f"[DRY RUN] Would execute {len(parallel_steps)} steps in parallel",
                "parallel_steps": [s.id for s in parallel_steps],
                "outputs": {}
            }
        
        # Create tasks for each parallel step
        tasks = []
        results = {}
        
        for parallel_step in parallel_steps:
            # Create a copy of the execution state for this step
            step_execution_state = execution_state.copy()
            step_execution_state["is_parallel"] = True
            
            # Get the appropriate handler for the step type
            handler = self._step_handlers.get(parallel_step.type)
            if not handler:
                results[parallel_step.id] = {
                    "success": False,
                    "error": f"Unsupported step type: {parallel_step.type}"
                }
                continue
            
            # Create a task for this step
            tasks.append(handler(parallel_step, step_execution_state))
        
        # Execute all tasks in parallel
        try:
            parallel_results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # Process results
            for i, parallel_step in enumerate(parallel_steps):
                result = parallel_results[i]
                
                # Handle exceptions
                if isinstance(result, Exception):
                    results[parallel_step.id] = {
                        "success": False,
                        "error": str(result)
                    }
                else:
                    results[parallel_step.id] = result
                    
                    # Update execution state variables with outputs from this step
                    if result.get("success") and result.get("outputs"):
                        execution_state["variables"].update(result["outputs"])
                
                # Mark the step as completed
                execution_state["completed_steps"].add(parallel_step.id)
            
            # Determine overall success (all steps must succeed)
            success = all(r.get("success", False) for r in results.values())
            
            return {
                "success": success,
                "parallel_steps": [s.id for s in parallel_steps],
                "results": results,
                "outputs": {
                    f"{step.id}_success": success,
                    f"{step.id}_all_succeeded": success,
                    f"{step.id}_completed_count": len(results)
                }
            }
            
        except Exception as e:
            self._logger.exception(f"Error executing parallel steps: {str(e)}")
            return {
                "success": False,
                "error": str(e),
                "outputs": {
                    f"{step.id}_success": False,
                    f"{step.id}_error": str(e)
                }
            }
    
    async def _execute_custom_code_step(
        self, 
        step: WorkflowStep, 
        execution_state: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Execute a custom code step in a workflow.
        
        Args:
            step: The workflow step
            execution_state: Current execution state
            
        Returns:
            Execution result
        """
        if not step.code:
            return {
                "success": False,
                "error": "Missing code for custom code step"
            }
        
        # Apply variable substitution to the code
        code = self._substitute_variables(step.code, execution_state["variables"])
        
        self._logger.info(f"Executing custom code step: {step.id}")
        
        # If dry run, just simulate the code execution
        if execution_state["dry_run"]:
            return {
                "success": True,
                "message": f"[DRY RUN] Would execute custom code: {len(code)} characters",
                "outputs": {}
            }
        
        # Execute code using Python exec()
        # Note: This is potentially unsafe and should be properly sandboxed in a real implementation
        try:
            # Create a safe environment for code execution
            sandbox = {
                "variables": execution_state["variables"].copy(),
                "results": {},
                "os": os,
                "re": re,
                "json": json,
                "Path": Path,
                "datetime": datetime,
                "logging": logging,
                "logger": self._logger,
                "outputs": {}
            }
            
            # Add a print function that redirects to logger
            def safe_print(*args, **kwargs):
                self._logger.info(" ".join(str(arg) for arg in args))
            sandbox["print"] = safe_print
            
            # Execute the code in the sandbox
            exec(code, sandbox)
            
            # Extract any outputs defined by the code
            outputs = sandbox.get("outputs", {})
            
            # Include any updated variables
            for key, value in sandbox.get("variables", {}).items():
                if key not in execution_state["variables"] or execution_state["variables"][key] != value:
                    outputs[key] = value
            
            return {
                "success": True,
                "outputs": outputs,
                "code_execution": "Completed successfully"
            }
            
        except Exception as e:
            self._logger.exception(f"Error executing custom code: {str(e)}")
            return {
                "success": False,
                "code": code,
                "error": str(e),
                "outputs": {
                    f"{step.id}_success": False,
                    f"{step.id}_error": str(e)
                }
            }
    
    async def _execute_notification_step(
        self, 
        step: WorkflowStep, 
        execution_state: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Execute a notification step in a workflow.
        
        Args:
            step: The workflow step
            execution_state: Current execution state
            
        Returns:
            Execution result
        """
        # Apply variable substitution to command or message
        message = step.command or "Workflow notification"
        message = self._substitute_variables(message, execution_state["variables"])
        
        self._logger.info(f"Sending notification: {message}")
        
        # If dry run, just simulate the notification
        if execution_state["dry_run"]:
            return {
                "success": True,
                "message": f"[DRY RUN] Would send notification: {message}",
                "outputs": {}
            }
        
        # In a real implementation, this would integrate with notification services
        # For now, we'll just log the notification
        self._logger.info(f"NOTIFICATION: {message}")
        
        # Print to console using rich
        try:
            from rich.console import Console
            from rich.panel import Panel
            
            console = Console()
            console.print("\n")
            console.print(Panel(
                message,
                title=f"Workflow Notification - {execution_state['workflow_id']}",
                border_style="yellow",
                expand=False
            ))
            
        except ImportError:
            # If rich is not available, just print the notification
            print(f"\n=== WORKFLOW NOTIFICATION ===\n{message}\n=============================\n")
        
        return {
            "success": True,
            "message": message,
            "outputs": {
                f"{step.id}_notification_sent": True,
                f"{step.id}_message": message
            }
        }
    
    async def _execute_validation_step(
        self, 
        step: WorkflowStep, 
        execution_state: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Execute a validation step in a workflow.
        
        Args:
            step: The workflow step
            execution_state: Current execution state
            
        Returns:
            Execution result with validation result
        """
        if not step.condition:
            return {
                "success": False,
                "error": "Missing condition for validation step"
            }
        
        # Apply variable substitution to the condition
        condition = self._substitute_variables(step.condition, execution_state["variables"])
        
        self._logger.info(f"Validating condition: {condition}")
        
        # If dry run, assume validation passes
        if execution_state["dry_run"]:
            return {
                "success": True,
                "condition": condition,
                "validated": True,
                "message": f"[DRY RUN] Would validate condition: {condition}",
                "outputs": {
                    f"{step.id}_validated": True
                }
            }
        
        # Evaluate the condition
        try:
            validation_result = await self._evaluate_condition(
                condition, 
                execution_state["variables"]
            )
            
            # For validation, success means the condition was evaluated without error
            # But validated depends on the condition result
            return {
                "success": True,
                "condition": condition,
                "validated": validation_result,
                "outputs": {
                    f"{step.id}_condition": condition,
                    f"{step.id}_validated": validation_result
                }
            }
        except Exception as e:
            self._logger.exception(f"Error validating condition: {str(e)}")
            return {
                "success": False,
                "condition": condition,
                "error": str(e),
                "outputs": {
                    f"{step.id}_success": False,
                    f"{step.id}_error": str(e)
                }
            }
    
    def _substitute_variables(self, text: str, variables: Dict[str, Any]) -> str:
        """
        Substitute variables in a text string.
        
        Args:
            text: Text with potential variable references
            variables: Dictionary of variables
            
        Returns:
            Text with variables substituted
        """
        if not text or not isinstance(text, str):
            return text
            
        result = text
        
        # Replace ${var} syntax
        for var_name, var_value in variables.items():
            placeholder = f"${{{var_name}}}"
            if placeholder in result:
                result = result.replace(placeholder, str(var_value))
        
        # Replace $var syntax (only for word boundaries to avoid partial replacements)
        var_pattern = r'\$([a-zA-Z0-9_]+)'
        matches = re.findall(var_pattern, result)
        
        for var_name in matches:
            if var_name in variables:
                # Replace with word boundary check
                result = re.sub(r'\$' + var_name + r'\b', str(variables[var_name]), result)
        
        return result
    
    def _extract_variables_from_output(self, output: str) -> Dict[str, Any]:
        """
        Extract variables from command output.
        
        Args:
            output: Command output text
            
        Returns:
            Dictionary of extracted variables
        """
        variables = {}
        
        # Look for lines like "VARIABLE=value" or "export VARIABLE=value"
        lines = output.splitlines()
        for line in lines:
            line = line.strip()
            if "=" in line:
                # Check for export pattern
                if line.startswith("export "):
                    line = line[7:]  # Remove "export "
                
                # Split at first equals sign
                parts = line.split("=", 1)
                if len(parts) == 2:
                    var_name = parts[0].strip()
                    var_value = parts[1].strip()
                    
                    # Remove quotes if present
                    if (var_value.startswith('"') and var_value.endswith('"')) or \
                       (var_value.startswith("'") and var_value.endswith("'")):
                        var_value = var_value[1:-1]
                    
                    variables[var_name] = var_value
        
        # Look for JSON output pattern
        if output.strip().startswith("{") and output.strip().endswith("}"):
            try:
                json_data = json.loads(output)
                if isinstance(json_data, dict):
                    for key, value in json_data.items():
                        variables[key] = value
            except json.JSONDecodeError:
                pass
        
        return variables
    
    async def _evaluate_condition(
        self, 
        condition: str, 
        variables: Dict[str, Any]
    ) -> bool:
        """
        Evaluate a condition expression.
        
        Args:
            condition: The condition expression
            variables: Variables to use in evaluation
            
        Returns:
            Boolean result of the condition
        """
        # Check for simple comparison patterns
        # Variable equals value: $var == value
        var_equals_match = re.search(r'\$\{?([a-zA-Z0-9_]+)\}?\s*==\s*(.+)', condition)
        if var_equals_match:
            var_name = var_equals_match.group(1)
            value_str = var_equals_match.group(2).strip()
            
            # Remove quotes if present
            if (value_str.startswith('"') and value_str.endswith('"')) or \
               (value_str.startswith("'") and value_str.endswith("'")):
                value_str = value_str[1:-1]
            
            # Get variable value
            if var_name in variables:
                var_value = variables[var_name]
                return str(var_value) == value_str
            
            return False
        
        # Variable not equals value: $var != value
        var_not_equals_match = re.search(r'\$\{?([a-zA-Z0-9_]+)\}?\s*!=\s*(.+)', condition)
        if var_not_equals_match:
            var_name = var_not_equals_match.group(1)
            value_str = var_not_equals_match.group(2).strip()
            
            # Remove quotes if present
            if (value_str.startswith('"') and value_str.endswith('"')) or \
               (value_str.startswith("'") and value_str.endswith("'")):
                value_str = value_str[1:-1]
            
            # Get variable value
            if var_name in variables:
                var_value = variables[var_name]
                return str(var_value) != value_str
            
            return True  # Variable doesn't exist, so it's not equal
        
        # Contains pattern: 'x' in $var
        contains_match = re.search(r'[\'"](.+?)[\'"]\s+in\s+\$\{?([a-zA-Z0-9_]+)\}?', condition)
        if contains_match:
            value_str = contains_match.group(1)
            var_name = contains_match.group(2)
            
            if var_name in variables:
                var_value = str(variables[var_name])
                return value_str in var_value
            
            return False
        
        # File exists pattern
        file_exists_match = re.search(r'file\s+exists\s+(.+)', condition, re.IGNORECASE)
        if file_exists_match:
            file_path = file_exists_match.group(1).strip()
            
            # Replace variables in the file path
            file_path = self._substitute_variables(file_path, variables)
            
            # Remove quotes if present
            if (file_path.startswith('"') and file_path.endswith('"')) or \
               (file_path.startswith("'") and file_path.endswith("'")):
                file_path = file_path[1:-1]
            
            return Path(file_path).exists()
        
        # Command success pattern
        command_success_match = re.search(r'command\s+(.+?)\s+succeeds', condition, re.IGNORECASE)
        if command_success_match:
            command = command_success_match.group(1).strip()
            
            # Replace variables in the command
            command = self._substitute_variables(command, variables)
            
            # Remove quotes if present
            if (command.startswith('"') and command.endswith('"')) or \
               (command.startswith("'") and command.endswith("'")):
                command = command[1:-1]
            
            # Execute the command to check if it succeeds
            try:
                process = await asyncio.create_subprocess_shell(
                    command,
                    stdout=asyncio.subprocess.PIPE,
                    stderr=asyncio.subprocess.PIPE
                )
                
                await process.communicate()
                return process.returncode == 0
            except Exception as e:
                self._logger.error(f"Error executing command in condition: {str(e)}")
                return False
        
        # For more complex conditions, a real implementation would use a proper
        # expression evaluator with sandboxing for security
        
        # Default to evaluating the condition string as a boolean value
        return bool(condition and condition.lower() not in ["false", "0", "no", "n", ""])

# Global instance
complex_workflow_planner = ComplexWorkflowPlanner()
</file>

<file path="components/intent/enhanced_task_planner.py">
# angela/intent/enhanced_task_planner.py

"""
Enhanced execution system for complex task orchestration in Angela CLI.

This module extends the TaskPlanner with robust support for advanced execution steps,
including code execution, API integration, looping constructs, and intelligent
data flow between steps.
"""
import os
import re
import json
import shlex
import asyncio
import tempfile
import subprocess
from pathlib import Path
from typing import Dict, Any, List, Tuple, Optional, Set, Union, Callable
from datetime import datetime
import uuid
import logging
import aiohttp
from enum import Enum
import sys

from pydantic import BaseModel, Field, ValidationError, validator

# Updated imports using the new architecture
from angela.components.intent.models import ActionPlan, Intent, IntentType
from angela.api.ai import get_gemini_client, get_gemini_request_class, get_parse_ai_response_func, get_build_prompt_func
from angela.api.context import get_context_manager, get_file_resolver
from angela.api.safety import get_validate_command_safety_func, get_command_risk_classifier
from angela.api.execution import get_execution_engine, get_error_recovery_manager, get_rollback_manager
from angela.core.registry import registry
from angela.utils.logging import get_logger

# Import these directly from the same module to avoid circular imports
from angela.components.intent.planner import (
    PlanStep, TaskPlan, PlanStepType, AdvancedPlanStep, AdvancedTaskPlan,
    TaskPlanner
)

logger = get_logger(__name__)


class StepExecutionContext(BaseModel):
    """Context for step execution with data flow capabilities."""
    step_id: str = Field(..., description="ID of the step being executed")
    plan_id: str = Field(..., description="ID of the plan being executed")
    variables: Dict[str, Any] = Field(default_factory=dict, description="Variables available to the step")
    results: Dict[str, Dict[str, Any]] = Field(default_factory=dict, description="Results of previously executed steps")
    transaction_id: Optional[str] = Field(None, description="Transaction ID for rollback")
    dry_run: bool = Field(False, description="Whether this is a dry run")
    parent_context: Optional[Dict[str, Any]] = Field(None, description="Parent context (e.g., for loops)")
    execution_path: List[str] = Field(default_factory=list, description="Execution path taken so far")

class DataFlowVariable(BaseModel):
    """Model for a variable in the data flow system."""
    name: str = Field(..., description="Name of the variable")
    value: Any = Field(..., description="Value of the variable")
    source_step: Optional[str] = Field(None, description="ID of the step that set this variable")
    timestamp: datetime = Field(default_factory=datetime.now, description="When the variable was set/updated")

class ExecutionResult(BaseModel):
    """Enhanced model for execution results with data flow information."""
    step_id: str = Field(..., description="ID of the executed step")
    type: PlanStepType = Field(..., description="Type of the executed step")
    success: bool = Field(..., description="Whether execution was successful")
    outputs: Dict[str, Any] = Field(default_factory=dict, description="Output values from execution")
    error: Optional[str] = Field(None, description="Error message if execution failed")
    execution_time: float = Field(..., description="Time taken for execution in seconds")
    retried: bool = Field(False, description="Whether the step was retried")
    recovery_applied: bool = Field(False, description="Whether error recovery was applied")
    recovery_strategy: Optional[Dict[str, Any]] = Field(None, description="Recovery strategy that was applied")
    raw_data: Dict[str, Any] = Field(default_factory=dict, description="Raw execution data")

# Data flow operators for variable references
class DataFlowOperator(Enum):
    """Operators for data flow expressions."""
    GET = "get"        # Get a value
    SET = "set"        # Set a value
    CONCAT = "concat"  # Concatenate values
    FORMAT = "format"  # Format a string with values
    JSON = "json"      # Parse or stringify JSON
    REGEX = "regex"    # Apply a regex pattern
    MATH = "math"      # Perform a math operation

# StepStatus enum from the second file - useful for tracking step execution state
class StepStatus(str, Enum):
    """Status of a task step."""
    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    SKIPPED = "skipped"

class CoreEnhancedTaskPlanner:
    """
    Core implementation of enhanced task planning capabilities.
    
    This class provides the actual implementation of advanced execution features
    without causing recursion issues.
    """
    
    def __init__(self):
        """Initialize the enhanced task planner."""
        self._logger = logger
    
        # Initialize to None instead of creating instance directly
        self._error_recovery_manager = None
        
        # Initialize variable store for data flow
        self._variables: Dict[str, DataFlowVariable] = {}
        
        # Track execution statistics
        self._execution_stats = {
            "executed_plans": 0,
            "executed_steps": 0,
            "errors": 0,
            "recoveries": 0,
            "code_executions": 0,
            "api_calls": 0,
            "loops_executed": 0,
        }
        
        # Set up the sandbox environment for code execution
        self._setup_code_sandbox()

    def _get_error_recovery_manager(self):
        """Get or initialize the error recovery manager."""
        if self._error_recovery_manager is None:
            # Use API layer to avoid circular imports
            self._error_recovery_manager = get_error_recovery_manager()
        return self._error_recovery_manager

    
    def _setup_code_sandbox(self):
        """Set up the sandbox environment for code execution."""
        # Create a temp directory for code execution if it doesn't exist
        self._sandbox_dir = Path(tempfile.gettempdir()) / "angela_sandbox"
        self._sandbox_dir.mkdir(exist_ok=True)
        
        # Set up allowed imports for code execution
        self._allowed_imports = {
            # Standard library
            "os", "sys", "re", "json", "csv", "datetime", "math", "random",
            "collections", "itertools", "functools", "pathlib", "uuid",
            "time", "tempfile", "shutil", "hashlib", "base64", "hmac",
            "urllib", "http", "typing",
            
            # Common third-party libs (would need to be installed in the sandbox)
            "requests", "aiohttp", "bs4", "pandas", "numpy", "matplotlib",
        }
        
        # Set up banned function patterns
        self._banned_functions = [
            r"__import__\(",
            r"eval\(",
            r"exec\(",
            r"compile\(",
            r"globals\(\)",
            r"locals\(\)",
            r"getattr\(",
            r"setattr\(",
            r"delattr\(",
            r"subprocess\.",
            r"os\.system",
            r"os\.popen",
            r"open\(.+,\s*['\"]w['\"]",  # Writing to files
        ]
        
        self._logger.debug(f"Code sandbox set up at {self._sandbox_dir}")
    
    async def plan_advanced_task(
        self, 
        request: str, 
        context: Dict[str, Any],
        max_steps: int = 20
    ) -> AdvancedTaskPlan:
        """
        Plan a complex task with branching and conditions based on the request.
        
        This method is integrated from the second file to provide a cleaner
        approach to generating advanced task plans directly from natural language.
        
        Args:
            request: Natural language request
            context: Context information
            max_steps: Maximum number of steps to include
            
        Returns:
            Advanced task plan
        """
        self._logger.info(f"Planning advanced task: {request}")
        
        # Generate plan using AI
        plan_data = await self._generate_plan_data(request, context, max_steps)
        
        # Convert to AdvancedTaskPlan model
        try:
            # Create unique ID for the plan
            plan_id = str(uuid.uuid4())
            
            # Create and validate steps
            steps = {}
            for step_id, step_data in plan_data.get("steps", {}).items():
                # Convert the step data to use the AdvancedPlanStep format
                step_type = self._convert_step_type(step_data.get("type", "command"))
                
                # Create a compatible step object that works with our system
                step_params = {
                    "id": step_id,
                    "type": step_type,
                    "description": step_data.get("description", ""),
                    "command": step_data.get("command"),
                    "code": step_data.get("code"),
                    "dependencies": step_data.get("dependencies", []),
                    "estimated_risk": step_data.get("estimated_risk", 0),
                }
                
                # Add condition-specific fields if present
                if "condition" in step_data:
                    step_params["condition"] = step_data["condition"]
                
                if "true_branch" in step_data:
                    step_params["true_branch"] = step_data["true_branch"]
                
                if "false_branch" in step_data:
                    step_params["false_branch"] = step_data["false_branch"]
                
                # Additional parameters
                if "timeout" in step_data:
                    step_params["timeout"] = step_data["timeout"]
                
                if "retry" in step_data:
                    step_params["retry"] = step_data["retry"]
                
                # Add loop-specific fields if present
                if step_type == PlanStepType.LOOP:
                    step_params["loop_items"] = step_data.get("loop_items", "")
                    step_params["loop_body"] = step_data.get("loop_body", [])
                
                # Create the step object
                step = AdvancedPlanStep(**step_params)
                steps[step_id] = step
            
            # Create the plan
            plan = AdvancedTaskPlan(
                id=plan_id,
                goal=plan_data.get("goal", request),
                description=plan_data.get("description", "Advanced task plan"),
                steps=steps,
                entry_points=plan_data.get("entry_points", []),
                created=datetime.now()
            )
            
            return plan
        except Exception as e:
            self._logger.error(f"Error creating advanced plan: {str(e)}")
            # Fall back to simpler plan structure
            return await self._create_fallback_plan(request, context)
    
    def _convert_step_type(self, type_str: str) -> PlanStepType:
        """
        Convert step type string from the plan data to PlanStepType enum.
        
        Args:
            type_str: Step type as string
            
        Returns:
            PlanStepType enum value
        """
        type_mapping = {
            "command": PlanStepType.COMMAND,
            "condition": PlanStepType.DECISION,
            "branch": PlanStepType.DECISION,
            "loop": PlanStepType.LOOP,
            "python_code": PlanStepType.CODE,
            "javascript_code": PlanStepType.CODE,
            "shell_code": PlanStepType.CODE,
            "decision": PlanStepType.DECISION,
            "file": PlanStepType.FILE,
            "api": PlanStepType.API,
            "code": PlanStepType.CODE,
        }
        
        return type_mapping.get(type_str.lower(), PlanStepType.COMMAND)
    
    async def _generate_plan_data(
        self, 
        request: str, 
        context: Dict[str, Any],
        max_steps: int
    ) -> Dict[str, Any]:
        """
        Generate plan data using AI.
        
        Args:
            request: Natural language request
            context: Context information
            max_steps: Maximum number of steps
            
        Returns:
            Dictionary with plan data
        """
        # Build prompt for AI
        prompt = f"""
You are an expert in creating detailed, executable plans for complex tasks. Break down this request into concrete steps that can be executed programmatically:

"{request}"

Create an advanced execution plan with branching logic, conditions, and dynamic paths.

For context, this plan will be executed in a {context.get('os_type', 'Linux')}-based environment with the current directory set to {context.get('cwd', '/home/user')}.

Return a JSON object with this structure:
```json
{{
  "goal": "High-level goal",
  "description": "Detailed description of what this plan will accomplish",
  "steps": {{
    "step1": {{
      "type": "command",
      "description": "Description of this step",
      "command": "shell command to execute",
      "dependencies": [],
      "estimated_risk": 0,
      "timeout": 30,
      "retry": 0
    }},
    "step2": {{
      "type": "condition",
      "description": "Decision point",
      "condition": "command that returns true/false exit code",
      "true_branch": "step3",
      "false_branch": "step4",
      "dependencies": ["step1"],
      "estimated_risk": 0,
      "timeout": 30
    }},
    "step3": {{
      "type": "python_code",
      "description": "Execute Python code",
      "code": "python code to execute",
      "dependencies": ["step2"],
      "estimated_risk": 0,
      "timeout": 30
    }},
    "step4": {{
      "type": "loop",
      "description": "Repeat operation",
      "loop_step": "step5",
      "loop_max": 5,
      "dependencies": ["step2"],
      "estimated_risk": 0
    }},
    "step5": {{
      "type": "command",
      "description": "Command to execute in loop",
      "command": "shell command",
      "dependencies": ["step4"],
      "estimated_risk": 0,
      "timeout": 30
    }}
  }},
  "entry_points": ["step1"]
}}
```

Valid step types: "command", "condition", "branch", "loop", "python_code", "javascript_code", "shell_code".
Risk levels: 0 (safe) to 4 (high risk).
Each step must have a unique ID. Dependencies must reference existing step IDs.
Entry points are step IDs that should be executed first (typically just one).
Create at most {max_steps} steps, but don't add unnecessary steps.

Ensure the plan handles potential errors and provides clear decision branches for different scenarios.
"""
        
        gemini_client = get_gemini_client()
        GeminiRequest = get_gemini_request_class()
        
        api_request = GeminiRequest(
            prompt=prompt,
            max_tokens=4000,
            temperature=0.2
        )
        
        response = await gemini_client.generate_text(api_request)
        
        # Extract JSON data
        try:
            # Look for JSON block in the response
            json_match = re.search(r'```(?:json)?\s*(.*?)\s*```', response.text, re.DOTALL)
            if json_match:
                json_str = json_match.group(1)
            else:
                # Try to find JSON without code blocks
                json_match = re.search(r'({.*})', response.text, re.DOTALL)
                if json_match:
                    json_str = json_match.group(1)
                else:
                    # Assume the entire response is JSON
                    json_str = response.text
            
            # Parse JSON
            plan_data = json.loads(json_str)
            return plan_data
        
        except (json.JSONDecodeError, IndexError) as e:
            self._logger.error(f"Error parsing AI response: {str(e)}")
            return {
                "goal": request,
                "description": "Fallback plan due to parsing error",
                "steps": {},
                "entry_points": []
            }
    
    async def _create_fallback_plan(
        self, 
        request: str, 
        context: Dict[str, Any]
    ) -> AdvancedTaskPlan:
        """
        Create a fallback plan when advanced plan generation fails.
        
        Args:
            request: Natural language request
            context: Context information
            
        Returns:
            Simplified advanced task plan
        """
        self._logger.info(f"Creating fallback plan for: {request}")
        
        # Generate a simple command for the request
        # Get necessary functions through API layer
        parse_ai_response = get_parse_ai_response_func()
        build_prompt = get_build_prompt_func()
        command_risk_classifier = get_command_risk_classifier()
        
        # Build prompt for AI
        prompt = build_prompt(request, context)
        
        # Get AI client and request class through API layer
        gemini_client = get_gemini_client()
        GeminiRequest = get_gemini_request_class()
        
        # Call AI service
        api_request = GeminiRequest(
            prompt=prompt,
            max_tokens=2000
        )
        
        response = await gemini_client.generate_text(api_request)
        
        # Parse the response to get a command suggestion
        suggestion = parse_ai_response(response.text)
        
        # Create a simple plan with one command step
        plan_id = str(uuid.uuid4())
        step_id = "step1"
        
        # Get risk level
        risk_level, _ = command_risk_classifier(suggestion.command)
        
        # Create the step
        step = AdvancedPlanStep(
            id=step_id,
            type=PlanStepType.COMMAND,
            description=suggestion.explanation or "Execute command",
            command=suggestion.command,
            dependencies=[],
            estimated_risk=risk_level
        )
        
        # Create the plan
        plan = AdvancedTaskPlan(
            id=plan_id,
            goal=request,
            description=f"Execute command: {suggestion.command}",
            steps={step_id: step},
            entry_points=[step_id],
            context={},
            created=datetime.now()
        )
        
        return plan
    
    async def execute_advanced_plan(
        self, 
        plan: AdvancedTaskPlan, 
        dry_run: bool = False,
        transaction_id: Optional[str] = None,
        initial_variables: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Execute an advanced task plan with full support for all step types.
        
        Args:
            plan: The advanced task plan to execute
            dry_run: Whether to simulate execution without making changes
            transaction_id: ID of the transaction this execution belongs to
            initial_variables: Initial variables for data flow
            
        Returns:
            Dictionary with execution results
        """
        # Get error recovery manager through API layer
        error_recovery_manager = self._get_error_recovery_manager()
    
        self._logger.info(f"Executing advanced plan: {plan.goal} (ID: {plan.id})")
        start_time = datetime.now()
        
        # Reset variable store for this execution
        self._variables = {}
        
        # Set initial variables if provided
        if initial_variables:
            for name, value in initial_variables.items():
                self._set_variable(name, value, "initial")
        
        # Initialize execution context
        context = StepExecutionContext(
            step_id="",
            plan_id=plan.id,
            transaction_id=transaction_id,
            dry_run=dry_run,
            results={},
            variables=self._variables.copy() if self._variables else {}
        )
        
        # Initialize execution state
        results = {}
        completed_steps = set()
        pending_steps = {}
        execution_path = []
        
        # Initialize with entry points
        for entry_point in plan.entry_points:
            if entry_point in plan.steps:
                pending_steps[entry_point] = plan.steps[entry_point]
        
        # Execute steps until all are completed or no more can be executed
        while pending_steps:
            # Find steps that can be executed (all dependencies satisfied)
            executable_steps = {}
            for step_id, step in pending_steps.items():
                if all(dep in completed_steps for dep in step.dependencies):
                    executable_steps[step_id] = step
            
            # If no steps can be executed, we're stuck (circular dependencies or missing steps)
            if not executable_steps:
                self._logger.warning("No executable steps found, possible circular dependencies")
                break
            
            # Execute all executable steps
            newly_completed_steps = []
            
            for step_id, step in executable_steps.items():
                self._logger.info(f"Executing step {step_id}: {step.type} - {step.description}")
                
                # Update execution context for this step
                context.step_id = step_id
                context.execution_path = execution_path.copy()
                
                # Execute the step with enhanced error handling
                try:
                    start_step_time = datetime.now()
                    
                    result = await self._execute_advanced_step(
                        step=step,
                        context=context
                    )
                    
                    # Calculate execution time
                    execution_time = (datetime.now() - start_step_time).total_seconds()
                    
                    # Add execution time to result
                    if isinstance(result, dict) and "execution_time" not in result:
                        result["execution_time"] = execution_time
                    
                    # Store the result
                    results[step_id] = result
                    context.results[step_id] = result
                    
                    # Update execution path
                    execution_path.append(step_id)
                    
                    # Check for next steps based on step type
                    if step.type == PlanStepType.DECISION:
                        # Decision step might have conditional branches
                        condition_result = result.get("condition_result", False)
                        branch_key = "true_branch" if condition_result else "false_branch"
                        next_steps = getattr(step, branch_key, [])
                        
                        self._logger.debug(f"Decision step {step_id} evaluated to {condition_result}, following {branch_key}")
                        
                        if next_steps:
                            for next_step_id in next_steps:
                                if next_step_id in plan.steps and next_step_id not in completed_steps:
                                    pending_steps[next_step_id] = plan.steps[next_step_id]
                    
                    elif step.type == PlanStepType.LOOP:
                        # Loop execution will use recursion
                        loop_result = result.get("loop_results", [])
                        self._logger.debug(f"Loop step {step_id} executed {len(loop_result)} iterations")
                    
                    # Mark step as completed
                    completed_steps.add(step_id)
                    newly_completed_steps.append(step_id)
                    
                    # Update execution stats
                    self._execution_stats["executed_steps"] += 1
                    if step.type == PlanStepType.CODE:
                        self._execution_stats["code_executions"] += 1
                    elif step.type == PlanStepType.API:
                        self._execution_stats["api_calls"] += 1
                    elif step.type == PlanStepType.LOOP:
                        self._execution_stats["loops_executed"] += 1
                    
                    # Check if we need to stop due to an error
                    if not result.get("success", False):
                        self._logger.warning(f"Step {step_id} failed with error: {result.get('error', 'Unknown error')}")
                        
                        # Attempt error recovery
                        if not dry_run and error_recovery_manager:
                            recovery_result = await self._attempt_recovery(step, result, context)
                            
                            if recovery_result.get("recovery_success", False):
                                self._logger.info(f"Recovery succeeded for step {step_id}")
                                results[step_id] = recovery_result
                                context.results[step_id] = recovery_result
                                self._execution_stats["recoveries"] += 1
                            else:
                                self._logger.error(f"Recovery failed for step {step_id}")
                                return {
                                    "success": False,
                                    "steps_completed": len(completed_steps),
                                    "steps_total": len(plan.steps),
                                    "failed_step": step_id,
                                    "results": results,
                                    "error": result.get("error", "Unknown error"),
                                    "execution_path": execution_path,
                                    "execution_time": (datetime.now() - start_time).total_seconds()
                                }
                        else:
                            # No recovery attempted, return failure
                            return {
                                "success": False,
                                "steps_completed": len(completed_steps),
                                "steps_total": len(plan.steps),
                                "failed_step": step_id,
                                "results": results,
                                "error": result.get("error", "Unknown error"),
                                "execution_path": execution_path,
                                "execution_time": (datetime.now() - start_time).total_seconds()
                            }
                            
                except Exception as e:
                    self._logger.exception(f"Error executing step {step_id}: {str(e)}")
                    self._execution_stats["errors"] += 1
                    
                    # Record error result
                    error_result = {
                        "step_id": step_id,
                        "type": step.type,
                        "description": step.description,
                        "error": str(e),
                        "success": False,
                        "execution_time": (datetime.now() - start_step_time).total_seconds()
                    }
                    
                    results[step_id] = error_result
                    context.results[step_id] = error_result
                    
                    # Attempt recovery
                    if not dry_run and error_recovery_manager:
                        recovery_result = await self._attempt_recovery(step, error_result, context)
                        
                        if recovery_result.get("recovery_success", False):
                            self._logger.info(f"Recovery succeeded for step {step_id}")
                            results[step_id] = recovery_result
                            context.results[step_id] = recovery_result
                            self._execution_stats["recoveries"] += 1
                        else:
                            self._logger.error(f"Recovery failed for step {step_id}")
                            return {
                                "success": False,
                                "steps_completed": len(completed_steps),
                                "steps_total": len(plan.steps),
                                "failed_step": step_id,
                                "results": results,
                                "error": str(e),
                                "execution_path": execution_path,
                                "execution_time": (datetime.now() - start_time).total_seconds()
                            }
                    else:
                        # No recovery attempted, return failure
                        return {
                            "success": False,
                            "steps_completed": len(completed_steps),
                            "steps_total": len(plan.steps),
                            "failed_step": step_id,
                            "results": results,
                            "error": str(e),
                            "execution_path": execution_path,
                            "execution_time": (datetime.now() - start_time).total_seconds()
                        }
            
            # Remove completed steps from pending steps
            for step_id in newly_completed_steps:
                if step_id in pending_steps:
                    del pending_steps[step_id]
            
            # If we're using sequential execution (i.e., no newly completed steps during the last iteration)
            # update pending_steps with steps that depend on the newly completed steps
            if not newly_completed_steps:
                # Find steps that depend on the newly completed steps
                for step_id, step in plan.steps.items():
                    if step_id not in completed_steps and not step_id in pending_steps:
                        # Check if all dependencies are now satisfied
                        if all(dep in completed_steps for dep in step.dependencies):
                            pending_steps[step_id] = step
        
        # Calculate execution time
        execution_time = (datetime.now() - start_time).total_seconds()
        
        # Update execution stats
        self._execution_stats["executed_plans"] += 1
        
        # Check if all steps were completed
        all_completed = len(completed_steps) == len(plan.steps)
        
        return {
            "success": all_completed,
            "steps_completed": len(completed_steps),
            "steps_total": len(plan.steps),
            "results": results,
            "execution_path": execution_path,
            "execution_time": execution_time,
            "variables": {k: v.dict() for k, v in self._variables.items()}
        }
    
    
    async def _attempt_recovery(
        self, 
        step: Any, 
        error_result: Dict[str, Any], 
        context: StepExecutionContext
    ) -> Dict[str, Any]:
        """
        Attempt to recover from a step execution error.
        
        Args:
            step: The step that failed
            error_result: The execution result with error information
            context: Execution context
            
        Returns:
            Updated execution result with recovery information
        """
        # Get error recovery manager through API layer
        error_recovery_manager = self._get_error_recovery_manager()
        
        if error_recovery_manager:
            # Attempt recovery
            return await error_recovery_manager.handle_error(step, error_result, context)
        else:
            # No recovery manager available
            return {
                "recovery_attempted": False,
                "recovery_success": False,
                "error": "Error recovery manager not available"
            }
    
    
    
    async def _execute_advanced_step(
        self, 
        step: AdvancedPlanStep,
        context: StepExecutionContext
    ) -> Dict[str, Any]:
        """
        Execute a single step of an advanced plan with full support for all step types.
        
        Args:
            step: The step to execute
            context: Execution context with variables and results
            
        Returns:
            Dictionary with execution results
        """
        start_time = datetime.now()
        
        # Prepare common result fields
        result = {
            "step_id": step.id,
            "type": step.type,
            "description": step.description,
            "success": False
        }
        
        try:
            # Process any variable references in parameters
            processed_step = await self._resolve_step_variables(step, context)
            
            # Select appropriate execution method based on step type
            if processed_step.type == PlanStepType.COMMAND:
                step_result = await self._execute_command_step(processed_step, context)
            
            elif processed_step.type == PlanStepType.CODE:
                step_result = await self._execute_code_step(processed_step, context)
            
            elif processed_step.type == PlanStepType.FILE:
                step_result = await self._execute_file_step(processed_step, context)
            
            elif processed_step.type == PlanStepType.DECISION:
                step_result = await self._execute_decision_step(processed_step, context)
            
            elif processed_step.type == PlanStepType.API:
                step_result = await self._execute_api_step(processed_step, context)
            
            elif processed_step.type == PlanStepType.LOOP:
                step_result = await self._execute_loop_step(processed_step, context)
            
            else:
                # Unknown step type
                raise ValueError(f"Unknown step type: {processed_step.type}")
            
            # Merge step-specific results with common fields
            result.update(step_result)
            
            # Extract and store output variables if specified
            if "outputs" in step_result:
                for var_name, var_value in step_result["outputs"].items():
                    self._set_variable(var_name, var_value, step.id)
            
            # Set standard execution time
            result["execution_time"] = (datetime.now() - start_time).total_seconds()
            
            return result
        
        except Exception as e:
            self._logger.exception(f"Error in _execute_advanced_step for {step.id}: {str(e)}")
            
            # Add error information to result
            result["error"] = str(e)
            result["success"] = False
            result["execution_time"] = (datetime.now() - start_time).total_seconds()
            
            # Handle retry if configured
            if step.retry and step.retry > 0:
                result["retry_count"] = 1
                result["retried"] = True
                
                # Attempt retries
                for retry_num in range(1, step.retry + 1):
                    self._logger.info(f"Retrying step {step.id} (attempt {retry_num}/{step.retry})")
                    try:
                        # Wait before retrying with exponential backoff
                        await asyncio.sleep(2 ** retry_num)
                        
                        # Execute retry logic
                        retry_result = await self._execute_advanced_step(step, context)
                        
                        if retry_result.get("success", False):
                            # Retry succeeded
                            retry_result["retry_count"] = retry_num
                            retry_result["retried"] = True
                            return retry_result
                    except Exception as retry_e:
                        self._logger.error(f"Error in retry {retry_num} for step {step.id}: {str(retry_e)}")
                
                # All retries failed
                result["retry_exhausted"] = True
            
            return result
    
    async def _resolve_step_variables(
        self, 
        step: AdvancedPlanStep, 
        context: StepExecutionContext
    ) -> AdvancedPlanStep:
        """
        Resolve variables in step parameters.
        
        Args:
            step: The step with potentially unresolved variables
            context: Execution context with variables
            
        Returns:
            Step with resolved variables
        """
        # Create a copy to avoid modifying the original
        step_dict = step.dict()
        
        # Define a recursive function to process variables in any value
        def process_value(value, path=""):
            if isinstance(value, str):
                # Check for variable references like ${var_name}
                var_pattern = r'\${([^}]+)}'
                matches = re.findall(var_pattern, value)
                
                if matches:
                    result = value
                    for var_name in matches:
                        var_value = self._get_variable_value(var_name, context)
                        if var_value is not None:
                            # Replace the variable reference with its value
                            result = result.replace(f"${{{var_name}}}", str(var_value))
                    return result
                return value
            
            elif isinstance(value, dict):
                return {k: process_value(v, f"{path}.{k}") for k, v in value.items()}
            
            elif isinstance(value, list):
                return [process_value(item, f"{path}[{i}]") for i, item in enumerate(value)]
            
            return value
        
        # Process all fields in the step
        processed_dict = process_value(step_dict)
        
        # Create a new step with processed values
        return AdvancedPlanStep(**processed_dict)
    
    def _get_variable_value(self, var_name: str, context: StepExecutionContext) -> Any:
        """
        Get the value of a variable, supporting both simple names and expressions.
        
        Args:
            var_name: Name of the variable or expression
            context: Execution context
            
        Returns:
            Variable value or None if not found
        """
        # Check for expressions like "result.step1.stdout"
        if "." in var_name:
            parts = var_name.split(".")
            if parts[0] == "result" or parts[0] == "results":
                if len(parts) >= 3:
                    step_id = parts[1]
                    result_field = parts[2]
                    
                    # Get the result for the specified step
                    step_result = context.results.get(step_id)
                    if step_result:
                        # Extract the requested field
                        if result_field in step_result:
                            return step_result[result_field]
                        
                        # Try nested fields
                        if len(parts) > 3:
                            nested_value = step_result
                            for part in parts[2:]:
                                if isinstance(nested_value, dict) and part in nested_value:
                                    nested_value = nested_value[part]
                                else:
                                    return None
                            return nested_value
        
        # Simple variable lookup
        if var_name in self._variables:
            return self._variables[var_name].value
        
        # Check in context variables
        if var_name in context.variables:
            return context.variables[var_name]
        
        return None
    
    def _set_variable(self, name: str, value: Any, source_step: str) -> None:
        """
        Set a variable in the variable store.
        
        Args:
            name: Name of the variable
            value: Value to set
            source_step: ID of the step setting the variable
        """
        self._variables[name] = DataFlowVariable(
            name=name,
            value=value,
            source_step=source_step,
            timestamp=datetime.now()
        )
        self._logger.debug(f"Variable '{name}' set to value from step {source_step}")
    
    # Enhanced variable replacement from the second file
    def _replace_variables(self, text: str, variables: Dict[str, Any]) -> str:
        """
        Replace variables in a text string with a more robust implementation.
        
        This implementation improves handling of both ${var} and $var syntax
        and addresses potential issues with partial matches.
        
        Args:
            text: Text to process
            variables: Dictionary of variables
            
        Returns:
            Text with variables replaced
        """
        if not isinstance(text, str):
            return text
            
        result = text
        
        # Replace ${var} syntax
        for var_name, var_value in variables.items():
            placeholder = f"${{{var_name}}}"
            result = result.replace(placeholder, str(var_value))
        
        # Replace $var syntax
        for var_name, var_value in variables.items():
            placeholder = f"${var_name}"
            
            # Avoid replacing partial matches
            parts = result.split(placeholder)
            if len(parts) > 1:
                new_parts = []
                for i, part in enumerate(parts):
                    new_parts.append(part)
                    if i < len(parts) - 1:
                        # Check if this placeholder is actually part of another variable name
                        if part and part[-1].isalnum() or (i < len(parts) - 1 and parts[i+1] and parts[i+1][0].isalnum()):
                            new_parts.append(placeholder)
                        else:
                            new_parts.append(str(var_value))
                result = "".join(new_parts)
        
        return result
    
    # Enhanced variable extraction from command output
    def _extract_variables_from_output(self, output: str) -> Dict[str, Any]:
        """
        Extract variables from command output with improved pattern detection.
        
        Args:
            output: Command output
            
        Returns:
            Dictionary of extracted variables
        """
        variables = {}
        
        # Look for lines like "VARIABLE=value" or "export VARIABLE=value"
        lines = output.splitlines()
        for line in lines:
            line = line.strip()
            if "=" in line:
                # Check for export pattern
                if line.startswith("export "):
                    line = line[7:]  # Remove "export "
                
                # Split at first equals sign
                parts = line.split("=", 1)
                if len(parts) == 2:
                    var_name = parts[0].strip()
                    var_value = parts[1].strip()
                    
                    # Remove quotes if present
                    if (var_value.startswith('"') and var_value.endswith('"')) or \
                       (var_value.startswith("'") and var_value.endswith("'")):
                        var_value = var_value[1:-1]
                    
                    variables[var_name] = var_value
        
        # Look for JSON output pattern
        if output.strip().startswith("{") and output.strip().endswith("}"):
            try:
                json_data = json.loads(output)
                if isinstance(json_data, dict):
                    for key, value in json_data.items():
                        variables[key] = value
            except json.JSONDecodeError:
                pass
        
        return variables
    
    async def _execute_command_step(
        self, 
        step: AdvancedPlanStep, 
        context: StepExecutionContext
    ) -> Dict[str, Any]:
        """
        Execute a command step.
        
        Args:
            step: The step to execute
            context: Execution context
            
        Returns:
            Dictionary with execution results
        """
        if not step.command:
            return {
                "success": False,
                "error": "Missing command for command step"
            }
        
        self._logger.info(f"Executing command: {step.command}")
        
        if context.dry_run:
            # Simulate command execution
            return {
                "success": True,
                "stdout": f"[DRY RUN] Would execute: {step.command}",
                "stderr": "",
                "return_code": 0,
                "outputs": {
                    f"{step.id}_stdout": f"[DRY RUN] Would execute: {step.command}",
                    f"{step.id}_success": True
                }
            }
        
        # Validate command safety if not specified to skip
        skip_safety = getattr(step, "skip_safety_check", False)
        if not skip_safety:
            # Get validate_command_safety function through API layer
            validate_func = get_validate_command_safety_func()
            if validate_func:
                is_safe, error_message = validate_func(step.command)
                if not is_safe:
                    return {
                        "success": False,
                        "error": f"Command safety validation failed: {error_message}",
                        "command": step.command
                    }
        
        # Execute the command using the execution engine from API layer
        try:
            execution_engine = get_execution_engine()
            stdout, stderr, return_code = await execution_engine.execute_command(
                command=step.command,
                check_safety=not skip_safety
            )
            
            # Create result
            result = {
                "success": return_code == 0,
                "stdout": stdout,
                "stderr": stderr,
                "return_code": return_code,
                "command": step.command,
                "outputs": {
                    f"{step.id}_stdout": stdout,
                    f"{step.id}_stderr": stderr,
                    f"{step.id}_return_code": return_code,
                    f"{step.id}_success": return_code == 0
                }
            }
            
            # Extract variables from output
            extracted_vars = self._extract_variables_from_output(stdout)
            if extracted_vars:
                for var_name, var_value in extracted_vars.items():
                    result["outputs"][var_name] = var_value
            
            # Record command execution in the transaction if successful
            if context.transaction_id and return_code == 0:
                # Get rollback manager through API layer
                rollback_manager = get_rollback_manager()
                if rollback_manager:
                    await rollback_manager.record_command_execution(
                        command=step.command,
                        return_code=return_code,
                        stdout=stdout,
                        stderr=stderr,
                        transaction_id=context.transaction_id,
                        step_id=step.id
                    )
            
            return result
            
        except Exception as e:
            self._logger.exception(f"Error executing command: {str(e)}")
            return {
                "success": False,
                "error": str(e),
                "command": step.command
            }
    
    async def _execute_code_step(
        self, 
        step: AdvancedPlanStep,
        context: StepExecutionContext
    ) -> Dict[str, Any]:
        """
        Execute a code step securely.
        
        Args:
            step: The step to execute
            context: Execution context
            
        Returns:
            Dictionary with execution results
        """
        if not step.code:
            return {
                "success": False,
                "error": "Missing code for code step"
            }
        
        self._logger.info(f"Executing code step: {step.id}")
        
        if context.dry_run:
            # Simulate code execution
            return {
                "success": True,
                "output": f"[DRY RUN] Would execute code: {len(step.code)} characters",
                "outputs": {
                    f"{step.id}_output": f"[DRY RUN] Would execute code: {len(step.code)} characters",
                    f"{step.id}_success": True
                }
            }
        
        try:
            # Validate code for security
            is_safe, validation_error = self._validate_code_security(step.code)
            if not is_safe:
                return {
                    "success": False,
                    "error": f"Code security validation failed: {validation_error}",
                    "code": step.code[:100] + "..." if len(step.code) > 100 else step.code
                }
            
            # Determine code language
            language = getattr(step, "language", "python").lower()
            
            if language == "python":
                # Execute Python code
                code_result = await self._execute_python_code(step.code, context)
            elif language == "javascript" or language == "js":
                # Execute JavaScript code
                code_result = await self._execute_javascript_code(step.code, context)
            elif language == "shell" or language == "bash":
                # Execute shell code
                code_result = await self._execute_shell_code(step.code, context)
            else:
                return {
                    "success": False,
                    "error": f"Unsupported code language: {language}"
                }
            
            # Add step ID to outputs
            if "outputs" in code_result and isinstance(code_result["outputs"], dict):
                prefixed_outputs = {}
                for key, value in code_result["outputs"].items():
                    prefixed_outputs[f"{step.id}_{key}"] = value
                code_result["outputs"] = prefixed_outputs
            
            # Add the code content to result for debugging
            if "code" not in code_result:
                code_short = step.code[:100] + "..." if len(step.code) > 100 else step.code
                code_result["code"] = code_short
            
            return code_result
            
        except Exception as e:
            self._logger.exception(f"Error executing code step: {str(e)}")
            return {
                "success": False,
                "error": str(e),
                "code": step.code[:100] + "..." if len(step.code) > 100 else step.code
            }
    
    def _validate_code_security(self, code: str) -> Tuple[bool, Optional[str]]:
        """
        Validate code for security concerns.
        
        Args:
            code: The code to validate
            
        Returns:
            Tuple of (is_safe, error_message)
        """
        # Check for banned function patterns
        for pattern in self._banned_functions:
            if re.search(pattern, code):
                return False, f"Code contains potentially unsafe pattern: {pattern}"
        
        # Check for potentially harmful import statements
        # This is a simplified check; a real implementation might be more sophisticated
        import_pattern = r'^import\s+([a-zA-Z0-9_.,\s]+)$|^from\s+([a-zA-Z0-9_.]+)\s+import'
        for match in re.finditer(import_pattern, code, re.MULTILINE):
            imports = match.group(1) or match.group(2)
            if imports:
                for imp in re.split(r'[\s,]+', imports):
                    # Get the base module (before the first dot)
                    base_module = imp.split('.')[0].strip()
                    if base_module and base_module not in self._allowed_imports:
                        return False, f"Import of module '{base_module}' is not allowed"
        
        return True, None
    
    async def _execute_python_code(
        self, 
        code: str, 
        context: StepExecutionContext
    ) -> Dict[str, Any]:
        """
        Execute Python code securely.
        
        Args:
            code: The Python code to execute
            context: Execution context
            
        Returns:
            Dictionary with execution results
        """
        # Create a unique identifier for this execution
        execution_id = str(uuid.uuid4())
        
        # Create a temporary file to store the code
        temp_dir = self._sandbox_dir / execution_id
        temp_dir.mkdir(exist_ok=True)
        
        temp_file = temp_dir / "code.py"
        
        # Create a file to store the variables
        variables_file = temp_dir / "variables.json"
        
        try:
            # Prepare context variables
            context_vars = {}
            for var_name, var in self._variables.items():
                context_vars[var_name] = var.value
            
            # Write variables to file (serializing complex objects to JSON)
            with open(variables_file, 'w') as f:
                json.dump(context_vars, f, default=str)
            
            # Add wrapper code to load variables and capture output
            wrapper_code = f'''
# Generated wrapper for secure code execution
import json
import sys
import io
import traceback

# Redirect stdout and stderr
original_stdout = sys.stdout
original_stderr = sys.stderr
sys.stdout = io.StringIO()
sys.stderr = io.StringIO()

# Output dictionary for capturing results
outputs = {{"success": False}}

try:
    # Load variables from file
    with open("{variables_file}", "r") as var_file:
        variables = json.load(var_file)
    
    # Make variables available in execution context
    globals().update(variables)
    
    # Execute the user code
    {code}
    
    # Capture stdout and stderr
    outputs["stdout"] = sys.stdout.getvalue()
    outputs["stderr"] = sys.stderr.getvalue()
    outputs["success"] = True
    
    # If there's a 'result' or 'output' variable, capture it
    if 'result' in locals():
        outputs["result"] = result
    if 'output' in locals():
        outputs["output"] = output
    
    # Capture all locals that don't start with '_'
    outputs["variables"] = {{
        k: v for k, v in locals().items() 
        if not k.startswith('_') and k not in ['variables', 'var_file']
    }}
    
except Exception as e:
    outputs["error"] = str(e)
    outputs["traceback"] = traceback.format_exc()
    outputs["success"] = False

# Restore stdout and stderr
sys.stdout = original_stdout
sys.stderr = original_stderr

# Write outputs to file
with open("{temp_dir / 'output.json'}", "w") as output_file:
    json.dump(outputs, output_file, default=str)
'''
            
            # Write the wrapper code to the temporary file
            with open(temp_file, 'w') as f:
                f.write(wrapper_code)
            
            timeout = 30
            
            process = await asyncio.create_subprocess_exec(
                sys.executable, str(temp_file),
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            
            try:
                # Wait for process with timeout
                stdout, stderr = await asyncio.wait_for(process.communicate(), timeout)
                return_code = process.returncode
                
                # Read output file
                output_file = temp_dir / 'output.json'
                if output_file.exists():
                    with open(output_file, 'r') as f:
                        outputs = json.load(f)
                else:
                    outputs = {
                        "success": False,
                        "error": "Output file was not created"
                    }
                
                # Create the result
                result = {
                    "success": outputs.get("success", False),
                    "outputs": outputs.get("variables", {}),
                    "stdout": outputs.get("stdout", ""),
                    "stderr": outputs.get("stderr", "")
                }
                
                if "error" in outputs:
                    result["error"] = outputs["error"]
                    result["traceback"] = outputs.get("traceback", "")
                
                if "result" in outputs:
                    result["result"] = outputs["result"]
                
                if "output" in outputs:
                    result["output"] = outputs["output"]
                
                return result
                
            except asyncio.TimeoutError:
                # Kill the process if it times out
                process.kill()
                return {
                    "success": False,
                    "error": f"Code execution timed out after {timeout} seconds"
                }
                
        except Exception as e:
            self._logger.exception(f"Error in Python code execution: {str(e)}")
            return {
                "success": False,
                "error": str(e)
            }
        finally:
            # Clean up temporary files
            try:
                import shutil
                shutil.rmtree(temp_dir)
            except Exception as e:
                self._logger.error(f"Error cleaning up temporary files: {str(e)}")
                
                
    async def _execute_javascript_code(
        self, 
        code: str, 
        context: StepExecutionContext
    ) -> Dict[str, Any]:
        """
        Execute JavaScript code securely.
        
        Args:
            code: The JavaScript code to execute
            context: Execution context
            
        Returns:
            Dictionary with execution results
        """
        # Create a unique identifier for this execution
        execution_id = str(uuid.uuid4())
        
        # Create a temporary file to store the code
        temp_dir = self._sandbox_dir / execution_id
        temp_dir.mkdir(exist_ok=True)
        
        temp_file = temp_dir / "code.js"
        
        # Create a file to store the variables
        variables_file = temp_dir / "variables.json"
        
        try:
            # Prepare context variables
            context_vars = {}
            for var_name, var in self._variables.items():
                context_vars[var_name] = var.value
            
            # Write variables to file (serializing complex objects to JSON)
            with open(variables_file, 'w') as f:
                json.dump(context_vars, f, default=str)
            
            # Add wrapper code to load variables and capture output
            wrapper_code = f'''
// Generated wrapper for secure code execution
const fs = require('fs');

// Load variables from file
const variables = JSON.parse(fs.readFileSync("{variables_file}", "utf8"));

// Make variables available in execution context
Object.assign(global, variables);

// Output object for capturing results
const outputs = {{
    success: false,
    stdout: "",
    stderr: "",
    variables: {{}}
}};

// Capture console.log output
const originalLog = console.log;
const originalError = console.error;
const logs = [];
const errors = [];

console.log = (...args) => {{
    const message = args.map(arg => 
        typeof arg === 'object' ? JSON.stringify(arg) : String(arg)
    ).join(' ');
    logs.push(message);
    outputs.stdout += message + "\\n";
    originalLog.apply(console, args);
}};

console.error = (...args) => {{
    const message = args.map(arg => 
        typeof arg === 'object' ? JSON.stringify(arg) : String(arg)
    ).join(' ');
    errors.push(message);
    outputs.stderr += message + "\\n";
    originalError.apply(console, args);
}};

try {{
    // Execute the user code
    {code}
    
    outputs.success = true;
    
    // If there's a 'result' or 'output' variable, capture it
    if (typeof result !== 'undefined') {{
        outputs.result = result;
    }}
    if (typeof output !== 'undefined') {{
        outputs.output = output;
    }}
    
    // Capture all globals that don't start with '_'
    for (const key in global) {{
        if (!key.startsWith('_') && 
            key !== 'variables' && 
            key !== 'outputs' &&
            key !== 'require' &&
            key !== 'module' &&
            key !== 'exports' &&
            key !== 'Buffer' &&
            key !== 'process' &&
            typeof global[key] !== 'function') {{
            outputs.variables[key] = global[key];
        }}
    }}
    
}} catch (error) {{
    outputs.success = false;
    outputs.error = error.message;
    outputs.stack = error.stack;
}}

// Restore console functions
console.log = originalLog;
console.error = originalError;

// Define a replacer function to handle circular references
const seen = new WeakSet();
function replacer(key, value) {{
    if (typeof value === 'object' && value !== null) {{
        if (seen.has(value)) {{
            return '[Circular]';
        }}
        seen.add(value);
        
        // Filter out function properties
        if (Object.prototype.toString.call(value) === '[object Object]') {{
            return Object.fromEntries(
                Object.entries(value).filter(([k, v]) => typeof v !== 'function')
            );
        }}
    }}
    return value;
}}

// Write outputs to file
fs.writeFileSync("{temp_dir / 'output.json'}", JSON.stringify(outputs, replacer, 2));
'''
            
            # Write the wrapper code to the temporary file
            with open(temp_file, 'w') as f:
                f.write(wrapper_code)
            
            # Check if Node.js is available
            try:
                # Try to run a simple Node.js command
                process = await asyncio.create_subprocess_exec(
                    "node", "--version",
                    stdout=asyncio.subprocess.PIPE,
                    stderr=asyncio.subprocess.PIPE
                )
                await process.communicate()
                if process.returncode != 0:
                    return {
                        "success": False,
                        "error": "Node.js is not available for JavaScript execution"
                    }
            except Exception:
                return {
                    "success": False,
                    "error": "Node.js is not available for JavaScript execution"
                }
            
            timeout = 30
            
            process = await asyncio.create_subprocess_exec(
                "node", str(temp_file),
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            
            try:
                # Wait for process with timeout
                stdout, stderr = await asyncio.wait_for(process.communicate(), timeout)
                return_code = process.returncode
                
                # Read output file
                output_file = temp_dir / 'output.json'
                if output_file.exists():
                    with open(output_file, 'r') as f:
                        outputs = json.load(f)
                else:
                    outputs = {
                        "success": False,
                        "error": "Output file was not created"
                    }
                
                # Create the result
                result = {
                    "success": outputs.get("success", False),
                    "outputs": outputs.get("variables", {}),
                    "stdout": outputs.get("stdout", ""),
                    "stderr": outputs.get("stderr", "")
                }
                
                if "error" in outputs:
                    result["error"] = outputs["error"]
                    result["stack"] = outputs.get("stack", "")
                
                if "result" in outputs:
                    result["result"] = outputs["result"]
                
                if "output" in outputs:
                    result["output"] = outputs["output"]
                
                return result
                
            except asyncio.TimeoutError:
                # Kill the process if it times out
                process.kill()
                return {
                    "success": False,
                    "error": f"Code execution timed out after {timeout} seconds"
                }
                
        except Exception as e:
            self._logger.exception(f"Error in JavaScript code execution: {str(e)}")
            return {
                "success": False,
                "error": str(e)
            }
        finally:
            # Clean up temporary files
            try:
                import shutil
                shutil.rmtree(temp_dir)
            except Exception as e:
                self._logger.error(f"Error cleaning up temporary files: {str(e)}")
    
    async def _execute_shell_code(
        self, 
        code: str, 
        context: StepExecutionContext
    ) -> Dict[str, Any]:
        """
        Execute shell code securely.
        
        Args:
            code: The shell code to execute
            context: Execution context
            
        Returns:
            Dictionary with execution results
        """
        # Create a unique identifier for this execution
        execution_id = str(uuid.uuid4())
        
        # Create a temporary file to store the code
        temp_dir = self._sandbox_dir / execution_id
        temp_dir.mkdir(exist_ok=True)
        
        script_file = temp_dir / "script.sh"
        
        try:
            # Write the code to the temporary file
            with open(script_file, 'w') as f:
                f.write("#!/bin/bash\n")
                f.write("set -e\n")  # Exit on error
                
                # Export variables from context
                for var_name, var in self._variables.items():
                    # Only export string variables (shell can't handle complex types)
                    if isinstance(var.value, str) or isinstance(var.value, (int, float, bool)):
                        f.write(f"export {var_name}=\"{str(var.value)}\"\n")
                
                # Add code to capture results
                f.write("OUTPUT_FILE=\"$(mktemp)\"\n")
                f.write("VARS_FILE=\"$(mktemp)\"\n")
                f.write("\n# User code begins\n")
                f.write(code)
                f.write("\n# User code ends\n\n")
                
                # Capture environment variables
                f.write("env > \"$VARS_FILE\"\n")
                
                # Export stdout location
                f.write("echo \"$OUTPUT_FILE\" > " + str(temp_dir / "output_file.txt") + "\n")
                f.write("echo \"$VARS_FILE\" > " + str(temp_dir / "vars_file.txt") + "\n")
            
            # Make the script executable
            script_file.chmod(0o755)
            
            # Execute the script with a timeout
            timeout = 30
            
            process = await asyncio.create_subprocess_exec(
                str(script_file),
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            
            try:
                # Wait for process with timeout
                stdout, stderr = await asyncio.wait_for(process.communicate(), timeout)
                stdout_str = stdout.decode('utf-8', errors='replace')
                stderr_str = stderr.decode('utf-8', errors='replace')
                return_code = process.returncode
                
                # Read output file location
                output_file_location = temp_dir / "output_file.txt"
                vars_file_location = temp_dir / "vars_file.txt"
                
                outputs = {}
                
                if output_file_location.exists():
                    with open(output_file_location, 'r') as f:
                        output_file = f.read().strip()
                        if os.path.exists(output_file):
                            with open(output_file, 'r') as of:
                                try:
                                    outputs = json.load(of)
                                except json.JSONDecodeError:
                                    # Not JSON, treat as plain text
                                    outputs["output"] = of.read()
                
                # Read captured environment variables
                exported_vars = {}
                if vars_file_location.exists():
                    with open(vars_file_location, 'r') as f:
                        vars_file = f.read().strip()
                        if os.path.exists(vars_file):
                            with open(vars_file, 'r') as vf:
                                for line in vf:
                                    if '=' in line:
                                        key, value = line.split('=', 1)
                                        exported_vars[key] = value.strip()
                
                # Create result
                result = {
                    "success": return_code == 0,
                    "stdout": stdout_str,
                    "stderr": stderr_str,
                    "return_code": return_code,
                    "outputs": exported_vars
                }
                
                if "output" in outputs:
                    result["output"] = outputs["output"]
                
                if return_code != 0:
                    result["error"] = f"Shell script failed with return code {return_code}"
                
                return result
                
            except asyncio.TimeoutError:
                # Kill the process if it times out
                process.kill()
                return {
                    "success": False,
                    "error": f"Shell script execution timed out after {timeout} seconds"
                }
                
        except Exception as e:
            self._logger.exception(f"Error in shell code execution: {str(e)}")
            return {
                "success": False,
                "error": str(e)
            }
        finally:
            # Clean up temporary files
            try:
                import shutil
                shutil.rmtree(temp_dir)
            except Exception as e:
                self._logger.error(f"Error cleaning up temporary files: {str(e)}")
    
    async def _execute_file_step(
        self, 
        step: AdvancedPlanStep, 
        context: StepExecutionContext
    ) -> Dict[str, Any]:
        """
        Execute a file operation step.
        
        Args:
            step: The step to execute
            context: Execution context
            
        Returns:
            Dictionary with execution results
        """
        if not step.file_path:
            return {
                "success": False,
                "error": "Missing file path for file step"
            }
        
        self._logger.info(f"Executing file operation on {step.file_path}")
        
        if context.dry_run:
            # Simulate file operation
            operation = "write" if step.file_content else "read"
            return {
                "success": True,
                "message": f"[DRY RUN] Would {operation} file: {step.file_path}",
                "outputs": {
                    f"{step.id}_message": f"[DRY RUN] Would {operation} file: {step.file_path}",
                    f"{step.id}_success": True
                }
            }
        
        try:
            # Determine the operation type
            operation = getattr(step, "operation", "read" if not step.file_content else "write")
            
            # Get filesystem functions through API layer
            from angela.api.execution import get_filesystem_functions
            fs = get_filesystem_functions()
            
            if operation == "read":
                # Read file content
                content = await fs.read_file(step.file_path)
                return {
                    "success": True,
                    "content": content,
                    "outputs": {
                        f"{step.id}_content": content,
                        f"{step.id}_success": True
                    }
                }
                
            elif operation == "write":
                # Create parent directories if needed
                file_path = Path(step.file_path)
                await fs.create_directory(file_path.parent, parents=True)
                
                # Write content to file
                await fs.write_file(step.file_path, step.file_content)
                return {
                    "success": True,
                    "message": f"Content written to {step.file_path}",
                    "outputs": {
                        f"{step.id}_message": f"Content written to {step.file_path}",
                        f"{step.id}_success": True
                    }
                }
                
            elif operation == "delete":
                # Delete file or directory
                if Path(step.file_path).is_dir():
                    await fs.delete_directory(step.file_path)
                    message = f"Directory {step.file_path} deleted"
                else:
                    await fs.delete_file(step.file_path)
                    message = f"File {step.file_path} deleted"
                    
                return {
                    "success": True,
                    "message": message,
                    "outputs": {
                        f"{step.id}_message": message,
                        f"{step.id}_success": True
                    }
                }
                
            elif operation == "copy":
                # Get destination path
                destination = getattr(step, "destination", None)
                if not destination:
                    return {
                        "success": False,
                        "error": "Missing destination for copy operation"
                    }
                
                # Copy file or directory
                await fs.copy_file(step.file_path, destination)
                return {
                    "success": True,
                    "message": f"Copied {step.file_path} to {destination}",
                    "outputs": {
                        f"{step.id}_message": f"Copied {step.file_path} to {destination}",
                        f"{step.id}_source": step.file_path,
                        f"{step.id}_destination": destination,
                        f"{step.id}_success": True
                    }
                }
                
            elif operation == "move":
                # Get destination path
                destination = getattr(step, "destination", None)
                if not destination:
                    return {
                        "success": False,
                        "error": "Missing destination for move operation"
                    }
                
                # Move file or directory
                await fs.move_file(step.file_path, destination)
                return {
                    "success": True,
                    "message": f"Moved {step.file_path} to {destination}",
                    "outputs": {
                        f"{step.id}_message": f"Moved {step.file_path} to {destination}",
                        f"{step.id}_source": step.file_path,
                        f"{step.id}_destination": destination,
                        f"{step.id}_success": True
                    }
                }
                
            else:
                return {
                    "success": False,
                    "error": f"Unsupported file operation: {operation}"
                }
                
        except Exception as e:
            self._logger.exception(f"Error in file operation: {str(e)}")
            return {
                "success": False,
                "error": str(e),
                "file_path": step.file_path
            }
    
    async def _execute_decision_step(
        self, 
        step: AdvancedPlanStep, 
        context: StepExecutionContext
    ) -> Dict[str, Any]:
        """
        Execute a decision step.
        
        Args:
            step: The step to execute
            context: Execution context
            
        Returns:
            Dictionary with execution results
        """
        if not step.condition:
            return {
                "success": False,
                "error": "Missing condition for decision step"
            }
        
        self._logger.info(f"Evaluating condition: {step.condition}")
        
        try:
            # Determine condition evaluation method
            condition_type = getattr(step, "condition_type", "expression")
            
            if condition_type == "expression":
                # Evaluate simple expression
                condition_result = await self._evaluate_expression(step.condition, context)
            elif condition_type == "code":
                # Evaluate code for condition
                condition_code = getattr(step, "condition_code", "")
                if not condition_code:
                    return {
                        "success": False,
                        "error": "Missing condition_code for code-based condition"
                    }
                
                # Create a temporary code step
                code_step = AdvancedPlanStep(
                    id=f"{step.id}_condition",
                    type=PlanStepType.CODE,
                    description=f"Condition evaluation for {step.id}",
                    code=condition_code,
                    dependencies=[],
                    estimated_risk=0
                )
                
                # Execute the code
                code_result = await self._execute_code_step(code_step, context)
                
                # Get condition result from code execution
                condition_result = code_result.get("success", False)
                if "result" in code_result:
                    condition_result = bool(code_result["result"])
            else:
                return {
                    "success": False,
                    "error": f"Unsupported condition_type: {condition_type}"
                }
            
            # Create result
            result = {
                "success": True,
                "condition": step.condition,
                "condition_result": condition_result,
                "next_branch": "true_branch" if condition_result else "false_branch",
                "outputs": {
                    f"{step.id}_condition_result": condition_result,
                    f"{step.id}_next_branch": "true_branch" if condition_result else "false_branch",
                    f"{step.id}_success": True
                }
            }
            
            return result
            
        except Exception as e:
            self._logger.exception(f"Error evaluating condition: {str(e)}")
            return {
                "success": False,
                "error": str(e),
                "condition": step.condition
            }
    
    async def _evaluate_expression(
        self, 
        expression: str, 
        context: StepExecutionContext
    ) -> bool:
        """
        Evaluate a simple condition expression.
        
        Args:
            expression: The condition expression
            context: Execution context
            
        Returns:
            Boolean result of the condition
        """
        # Check for file existence condition
        file_exists_match = re.search(r'file(?:\s+)exists(?:[:=\s]+)(.+)', expression, re.IGNORECASE)
        if file_exists_match:
            file_path = file_exists_match.group(1).strip()
            # Resolve variables in the file path
            file_path = await self._resolve_variables_in_string(file_path, context)
            return Path(file_path).exists()
        
        # Check for command success condition
        cmd_success_match = re.search(r'command(?:\s+)success(?:[:=\s]+)(.+)', expression, re.IGNORECASE)
        if cmd_success_match:
            step_id = cmd_success_match.group(1).strip()
            return context.results.get(step_id, {}).get("success", False)
        
        # Check for output contains condition
        output_contains_match = re.search(r'output(?:\s+)contains(?:[:=\s]+)(.+?)(?:[:=\s]+)in(?:[:=\s]+)(.+)', expression, re.IGNORECASE)
        if output_contains_match:
            pattern = output_contains_match.group(1).strip()
            step_id = output_contains_match.group(2).strip()
            
            # Resolve variables in the pattern
            pattern = await self._resolve_variables_in_string(pattern, context)
            
            # Get output from the specified step
            step_output = context.results.get(step_id, {}).get("stdout", "")
            return pattern in step_output
        
        # Check for variable condition
        var_match = re.search(r'variable(?:\s+)(.+?)(?:\s*)([=!<>]=|[<>])(?:\s*)(.+)', expression, re.IGNORECASE)
        if var_match:
            var_name = var_match.group(1).strip()
            operator = var_match.group(2).strip()
            value = var_match.group(3).strip()
            
            # Get variable value
            var_value = self._get_variable_value(var_name, context)
            if var_value is None:
                return False
            
            # Evaluate comparison
            try:
                # Convert value to appropriate type
                if value.lower() == "true":
                    compare_value = True
                elif value.lower() == "false":
                    compare_value = False
                elif value.isdigit():
                    compare_value = int(value)
                elif re.match(r'^-?\d+(\.\d+)?$', value):  # Fixed regex pattern here
                    compare_value = float(value)
                else:
                    # Try to resolve variables in the value
                    resolved_value = await self._resolve_variables_in_string(value, context)
                    if resolved_value != value:
                        # Value contained variables, use the resolved value
                        compare_value = resolved_value
                    else:
                        # Treat as a string but strip quotes
                        compare_value = value.strip('\'"')
                
                # Compare based on operator
                if operator == "==":
                    return var_value == compare_value
                elif operator == "!=":
                    return var_value != compare_value
                elif operator == "<":
                    return var_value < compare_value
                elif operator == ">":
                    return var_value > compare_value
                elif operator == "<=":
                    return var_value <= compare_value
                elif operator == ">=":
                    return var_value >= compare_value
                else:
                    return False
            except Exception as e:
                self._logger.error(f"Error comparing variable {var_name} with value {value}: {str(e)}")
                return False
        
        # Simple boolean evaluation for unknown conditions
        return bool(expression and expression.lower() not in ['false', '0', 'no', 'n', ''])
    
    async def _resolve_variables_in_string(
        self, 
        text: str, 
        context: StepExecutionContext
    ) -> str:
        """
        Resolve variables in a string.
        
        Args:
            text: The string with potential variable references
            context: Execution context
            
        Returns:
            String with variables resolved
        """
        if not text or "${" not in text:
            return text
        
        result = text
        var_pattern = r'\${([^}]+)}'
        matches = re.findall(var_pattern, text)
        
        for var_name in matches:
            var_value = self._get_variable_value(var_name, context)
            if var_value is not None:
                # Replace the variable reference with its value
                result = result.replace(f"${{{var_name}}}", str(var_value))
        
        return result
    
    async def _resolve_loop_items(
        self, 
        loop_items_expr: str, 
        context: StepExecutionContext
    ) -> List[Any]:
        """
        Resolve loop items from various sources.
        
        Args:
            loop_items_expr: Expression for loop items
            context: Execution context
            
        Returns:
            List of items to loop over
        """
        # Check if loop_items is a variable reference
        var_match = re.match(r'\${([^}]+)}', loop_items_expr)
        if var_match:
            var_name = var_match.group(1)
            var_value = self._get_variable_value(var_name, context)
            
            if var_value is not None:
                if isinstance(var_value, list):
                    return var_value
                elif isinstance(var_value, dict):
                    # For dictionaries, loop over items
                    return list(var_value.items())
                elif isinstance(var_value, str):
                    # For strings, try to parse as JSON
                    try:
                        parsed = json.loads(var_value)
                        if isinstance(parsed, list):
                            return parsed
                    except json.JSONDecodeError:
                        # Not JSON, split by lines
                        return var_value.splitlines()
        
        # Check for range expression: range(start, end, step)
        range_match = re.match(r'range\((\d+)(?:,\s*(\d+))?(?:,\s*(\d+))?\)', loop_items_expr)
        if range_match:
            if range_match.group(2):
                # range(start, end, [step])
                start = int(range_match.group(1))
                end = int(range_match.group(2))
                step = int(range_match.group(3)) if range_match.group(3) else 1
                return list(range(start, end, step))
            else:
                # range(end)
                end = int(range_match.group(1))
                return list(range(end))
        
        # Check for file list: files(pattern)
        files_match = re.match(r'files\(([^)]+)\)', loop_items_expr)
        if files_match:
            pattern = files_match.group(1).strip('"\'')
            
            # Resolve pattern if it contains variables
            resolved_pattern = await self._resolve_variables_in_string(pattern, context)
            
            # Import here to avoid circular imports
            from glob import glob
            
            # Get list of files matching the pattern
            file_list = glob(resolved_pattern)
            return file_list
        
        # Check for JSON array
        if loop_items_expr.startswith('[') and loop_items_expr.endswith(']'):
            try:
                items = json.loads(loop_items_expr)
                if isinstance(items, list):
                    return items
            except json.JSONDecodeError:
                pass
        
        # Check for comma-separated list
        if ',' in loop_items_expr:
            return [item.strip() for item in loop_items_expr.split(',')]
        
        # Default: return as single item
        return [loop_items_expr]
    


# Now define the EnhancedTaskPlanner class that uses the core implementation
class EnhancedTaskPlanner(TaskPlanner):
    """
    Enhanced TaskPlanner with support for advanced execution capabilities.
    
    This class extends the original TaskPlanner with advanced execution capabilities
    while avoiding the infinite recursion problem.
    """
    
    def __init__(self):
        """Initialize the enhanced task planner."""
        super().__init__()
        # Create a CoreEnhancedTaskPlanner instance instead of recursively creating
        # another EnhancedTaskPlanner
        self._core_planner = CoreEnhancedTaskPlanner()
        self._logger = logger
        self._error_recovery_manager = None


    def _get_error_recovery_manager(self):
        """Get or initialize the error recovery manager."""
        if self._error_recovery_manager is None:
            # Get error recovery manager through API layer
            self._error_recovery_manager = get_error_recovery_manager()
        return self._error_recovery_manager
        
            
    async def execute_plan(
        self, 
        plan: Union[TaskPlan, AdvancedTaskPlan], 
        dry_run: bool = False,
        transaction_id: Optional[str] = None,
        initial_variables: Optional[Dict[str, Any]] = None
    ) -> Union[List[Dict[str, Any]], Dict[str, Any]]:
        """
        Execute a task plan with full support for all step types.
        
        Args:
            plan: The plan to execute
            dry_run: Whether to simulate execution without making changes
            transaction_id: ID of the transaction this execution belongs to
            initial_variables: Initial variables for data flow
            
        Returns:
            List of execution results for each step or execution result dict
        """
        if isinstance(plan, AdvancedTaskPlan):
            # Use the core planner for advanced plans
            return await self._core_planner.execute_advanced_plan(
                plan, dry_run, transaction_id, initial_variables
            )
        else:
            # Use the original execution for basic plans
            return await super()._execute_basic_plan(plan, dry_run, transaction_id)
    
    async def plan_task(
        self, 
        request: str, 
        context: Dict[str, Any],
        complexity: str = "auto"
    ) -> Union[TaskPlan, AdvancedTaskPlan]:
        """
        Plan a task by breaking it down into actionable steps.
        
        Enhanced version that can generate advanced task plans directly
        from natural language requests.
        
        Args:
            request: The high-level goal description
            context: Context information
            complexity: Planning complexity level ("simple", "advanced", or "auto")
            
        Returns:
            Either a basic TaskPlan or an advanced AdvancedTaskPlan based on complexity
        """
        self._logger.info(f"Planning task: {request} (complexity: {complexity})")
        
        # Determine planning complexity if auto
        if complexity == "auto":
            complexity = await self._determine_complexity(request)
            self._logger.info(f"Determined complexity: {complexity}")
        
        # Use the appropriate planning strategy
        if complexity == "advanced":
            # Use core planner for advanced planning
            return await self._core_planner.plan_advanced_task(request, context)
        else:
            # Use basic planning for simple tasks
            return await super()._create_basic_plan(request, context)

# Create an instance of the enhanced task planner
enhanced_task_planner = EnhancedTaskPlanner()
</file>

<file path="components/intent/models.py">
# angela/components/intent/models.py
from enum import Enum
from typing import Dict, Any, Optional, List

from pydantic import BaseModel, Field

class IntentType(str, Enum):
    """Enumeration of possible intent types."""
    UNKNOWN = "unknown"
    FILE_SEARCH = "file_search"
    DIRECTORY_LIST = "directory_list"
    FILE_VIEW = "file_view"
    SYSTEM_INFO = "system_info"
    NETWORK_INFO = "network_info"
    FILE_EDIT = "file_edit"  # For future phases
    FILE_CREATE = "file_create"  # For future phases
    GIT_OPERATION = "git_operation"  # For future phases
    DOCKER_OPERATION = "docker_operation"  # For future phases

class Intent(BaseModel):
    """Model for user intent."""
    type: IntentType = Field(..., description="The type of intent")
    entities: Dict[str, Any] = Field(default_factory=dict, description="Entities extracted from the request")
    original_request: str = Field(..., description="The original user request")

class ActionPlan(BaseModel):
    """Model for an action plan derived from intent."""
    intent: Intent = Field(..., description="The intent that led to this plan")
    commands: List[str] = Field(..., description="List of commands to execute")
    explanations: List[str] = Field(..., description="Explanations for each command")
    risk_level: int = Field(0, description="Risk level of the plan (0-4)")
</file>

<file path="components/intent/planner.py">
# angela/components/intent/planner.py
"""
Task planning and goal decomposition for Angela CLI.

This module handles breaking down complex high-level goals into
executable action plans with dependencies and execution flow.
It provides two levels of planning capability:
1. Basic planning - For straightforward sequential tasks
2. Advanced planning - For complex tasks with branching, conditionals, and various step types
"""
import os
import re
import json
import shlex
import asyncio
from pathlib import Path
from typing import Dict, Any, List, Tuple, Optional, Set, Union
from enum import Enum
from datetime import datetime

from pydantic import BaseModel, Field

# Updated imports to use relative imports or API layer
from angela.components.intent.models import ActionPlan, Intent, IntentType
from angela.api.ai import get_gemini_client, get_gemini_request_class
from angela.api.context import get_context_manager
from angela.api.execution import get_execution_engine, get_rollback_manager
from angela.utils.logging import get_logger

logger = get_logger(__name__)

#######################
# Basic Planning Models
#######################

class PlanStep(BaseModel):
    """Model for a single step in a basic plan."""
    command: str = Field(..., description="The command to execute")
    explanation: str = Field(..., description="Explanation of the command")
    dependencies: List[int] = Field(default_factory=list, description="Indices of steps this step depends on")
    estimated_risk: int = Field(0, description="Estimated risk level (0-4)")


class TaskPlan(BaseModel):
    """Model for a complete basic task plan."""
    goal: str = Field(..., description="The original high-level goal")
    steps: List[PlanStep] = Field(..., description="Steps to achieve the goal")
    context: Dict[str, Any] = Field(default_factory=dict, description="Context information")

#######################
# Advanced Planning Models
#######################

class PlanStepType(str, Enum):
    """Types of plan steps for advanced planning."""
    COMMAND = "command"  # Shell command
    CODE = "code"        # Code to execute or save
    FILE = "file"        # File operation
    DECISION = "decision"  # Decision point, may branch execution
    API = "api"          # API call
    LOOP = "loop"        # Looping construct


class AdvancedPlanStep(BaseModel):
    """Model for an advanced plan step with additional capabilities."""
    id: str = Field(..., description="Unique identifier for this step")
    type: PlanStepType = Field(..., description="Type of step")
    description: str = Field(..., description="Human-readable description")
    command: Optional[str] = Field(None, description="Command to execute (for command type)")
    code: Optional[str] = Field(None, description="Code to execute or save (for code type)")
    file_path: Optional[str] = Field(None, description="Path for file operations (for file type)")
    file_content: Optional[str] = Field(None, description="Content for file operations (for file type)")
    condition: Optional[str] = Field(None, description="Condition for decision steps (for decision type)")
    true_branch: Optional[List[str]] = Field(None, description="Steps to execute if condition is true")
    false_branch: Optional[List[str]] = Field(None, description="Steps to execute if condition is false")
    api_url: Optional[str] = Field(None, description="URL for API calls (for api type)")
    api_method: Optional[str] = Field(None, description="HTTP method for API calls (for api type)")
    api_payload: Optional[Dict[str, Any]] = Field(None, description="Payload for API calls (for api type)")
    loop_items: Optional[str] = Field(None, description="Items to loop over (for loop type)")
    loop_body: Optional[List[str]] = Field(None, description="Steps to execute in loop (for loop type)")
    dependencies: List[str] = Field(default_factory=list, description="IDs of steps this step depends on")
    estimated_risk: int = Field(0, description="Estimated risk level (0-4)")
    timeout: Optional[int] = Field(None, description="Timeout in seconds")
    retry: Optional[int] = Field(None, description="Number of retries on failure")
    tags: List[str] = Field(default_factory=list, description="Tags for categorization")


class AdvancedTaskPlan(BaseModel):
    """Model for an advanced task plan with branching and complex steps."""
    id: str = Field(..., description="Unique identifier for this plan")
    goal: str = Field(..., description="The original high-level goal")
    description: str = Field(..., description="Detailed description of the plan")
    steps: Dict[str, AdvancedPlanStep] = Field(..., description="Steps to achieve the goal, indexed by ID")
    entry_points: List[str] = Field(..., description="Step IDs to start execution with")
    context: Dict[str, Any] = Field(default_factory=dict, description="Context information")
    created: datetime = Field(default_factory=datetime.now, description="When the plan was created")


#######################
# Unified Task Planner
#######################

class TaskPlanner:
    """
    Task planner for breaking down complex goals into actionable steps.
    
    This planner can generate two types of plans:
    1. Basic plans (TaskPlan) - For simple sequential tasks
    2. Advanced plans (AdvancedTaskPlan) - For complex tasks with branching execution
    
    The planner automatically determines the appropriate plan type based on the
    complexity of the goal and request context.
    """
    
    def __init__(self):
        """Initialize the task planner."""
        self._logger = logger
    
    async def plan_task(
        self, 
        goal: str, 
        context: Dict[str, Any],
        complexity: str = "auto"
    ) -> Union[TaskPlan, AdvancedTaskPlan]:
        """
        Plan a task by breaking it down into actionable steps.
        
        Args:
            goal: The high-level goal description
            context: Context information
            complexity: Planning complexity level ("simple", "advanced", or "auto")
            
        Returns:
            Either a basic TaskPlan or an advanced AdvancedTaskPlan based on complexity
        """
        self._logger.info(f"Planning task: {goal} (complexity: {complexity})")
        
        # Determine planning complexity if auto
        if complexity == "auto":
            complexity = await self._determine_complexity(goal)
            self._logger.info(f"Determined complexity: {complexity}")
        
        # Use the appropriate planning strategy
        if complexity == "simple":
            # Use basic planning for simple tasks
            return await self._create_basic_plan(goal, context)
        else:
            # Use advanced planning for complex tasks
            return await self._generate_advanced_plan(goal, context)
    
    async def _determine_complexity(self, goal: str) -> str:
        """
        Determine the appropriate planning complexity for a goal.
        
        Args:
            goal: The high-level goal
            
        Returns:
            Complexity level ("simple" or "advanced")
        """
        # Simple heuristics based on goal text
        complexity_indicators = [
            "if", "when", "based on", "for each", "foreach", "loop", "iterate",
            "depending on", "decision", "alternative", "otherwise", "create file",
            "write to file", "dynamic", "api", "request", "conditionally",
            "advanced", "complex", "multiple steps", "error handling"
        ]
        
        # Count indicators
        indicator_count = sum(1 for indicator in complexity_indicators 
                              if indicator in goal.lower())
        
        # Check goal length and complexity indicators
        if len(goal.split()) > 20 or indicator_count >= 2:
            return "advanced"
        else:
            return "simple"
    
    #######################
    # Basic Planning Methods
    #######################
    
    async def _create_basic_plan(
        self, 
        goal: str, 
        context: Dict[str, Any]
    ) -> TaskPlan:
        """
        Create a basic plan for a task.
        
        Args:
            goal: The high-level goal description
            context: Context information
            
        Returns:
            A TaskPlan with steps to achieve the goal
        """
        self._logger.info(f"Creating basic plan for task: {goal}")
        
        # Generate a plan using the AI
        prompt = self._build_planning_prompt(goal, context)
        
        # Get required classes through API layer
        gemini_client = get_gemini_client()
        GeminiRequest = get_gemini_request_class()
        
        # Call AI service
        api_request = GeminiRequest(
            prompt=prompt,
            max_tokens=4000
        )
        
        response = await gemini_client.generate_text(api_request)
        
        # Parse the plan from the response
        plan = self._parse_plan_response(response.text, goal, context)
        
        return plan
    
    def _build_planning_prompt(
        self, 
        goal: str,
        context: Dict[str, Any]
    ) -> str:
        """
        Build a prompt for basic planning.
        
        Args:
            goal: The high-level goal
            context: Context information
            
        Returns:
            A prompt string for the AI service
        """
        # Create context string
        context_str = "Current context:\n"
        
        if context.get("cwd"):
            context_str += f"- Current working directory: {context['cwd']}\n"
        if context.get("project_root"):
            context_str += f"- Project root: {context['project_root']}\n"
        if context.get("project_type"):
            context_str += f"- Project type: {context['project_type']}\n"
        
        # Add files in current directory for context
        if context.get("cwd"):
            try:
                context_manager = get_context_manager()
                dir_contents = context_manager.get_directory_contents(Path(context["cwd"]))
                files_str = "\n".join([f"- {item['name']}" for item in dir_contents])
                context_str += f"\nFiles in current directory:\n{files_str}\n"
            except Exception as e:
                self._logger.error(f"Error getting directory contents: {str(e)}")
        
        # Build the prompt
        prompt = f"""
You are Angela, an AI terminal assistant. Your task is to create a detailed plan for achieving the following goal:

GOAL: {goal}

{context_str}

Break down this goal into a sequence of shell commands that would accomplish it.
For each command, provide:
1. The exact command to run
2. A brief explanation of what the command does
3. Any dependencies (which previous steps must complete first)
4. An estimated risk level (0: SAFE, 1: LOW, 2: MEDIUM, 3: HIGH, 4: CRITICAL)

Format your response as JSON:
{{
  "steps": [
    {{
      "command": "command_1",
      "explanation": "Explanation of command 1",
      "dependencies": [],
      "estimated_risk": 1
    }},
    {{
      "command": "command_2",
      "explanation": "Explanation of command 2",
      "dependencies": [0],
      "estimated_risk": 2
    }},
    ...
  ]
}}

Ensure each command is valid and appropriate for a Linux/Unix shell environment.
Use the most efficient and standard commands to accomplish the task.
Include error handling where appropriate.
"""
        
        return prompt
    
    def _parse_plan_response(self, response: str, goal: str, context: Dict[str, Any]) -> TaskPlan:
        """
        Parse the AI response to extract the basic plan.
        
        Args:
            response: The AI response text
            goal: The original high-level goal
            context: Context information
            
        Returns:
            A TaskPlan object
        """
        try:
            # Extract JSON from the response
            json_match = re.search(r'```(?:json)?\s*(.*?)\s*```', response, re.DOTALL)
            if json_match:
                json_str = json_match.group(1)
            else:
                # Try to find JSON without code blocks
                json_match = re.search(r'({.*})', response, re.DOTALL)
                if json_match:
                    json_str = json_match.group(1)
                else:
                    # Assume the entire response is JSON
                    json_str = response
            
            # Parse the JSON
            plan_data = json.loads(json_str)
            
            # Create a TaskPlan object
            steps = []
            for step_data in plan_data.get("steps", []):
                step = PlanStep(
                    command=step_data["command"],
                    explanation=step_data["explanation"],
                    dependencies=step_data.get("dependencies", []),
                    estimated_risk=step_data.get("estimated_risk", 0)
                )
                steps.append(step)
            
            return TaskPlan(
                goal=goal,
                steps=steps,
                context=context
            )
        
        except Exception as e:
            self._logger.exception(f"Error parsing plan response: {str(e)}")
            # Create a fallback single-step plan
            return TaskPlan(
                goal=goal,
                steps=[
                    PlanStep(
                        command=f"echo 'Unable to create detailed plan for: {goal}'",
                        explanation="Fallback step due to planning error",
                        dependencies=[],
                        estimated_risk=0
                    )
                ],
                context=context
            )
    
    def create_action_plan(self, task_plan: TaskPlan) -> ActionPlan:
        """
        Convert a TaskPlan to an executable ActionPlan.
        
        Args:
            task_plan: The task plan to convert
            
        Returns:
            An ActionPlan ready for execution
        """
        # Create an intent for the action plan
        intent = Intent(
            type=IntentType.UNKNOWN,
            original_request=task_plan.goal
        )
        
        # Extract commands and explanations preserving the order
        commands = []
        explanations = []
        
        # For now, we'll execute steps in the order they appear
        # In the future, we could use the dependencies to create a proper execution order
        for step in task_plan.steps:
            commands.append(step.command)
            explanations.append(step.explanation)
        
        # Determine the overall risk level (maximum of all steps)
        risk_level = max([step.estimated_risk for step in task_plan.steps], default=0)
        
        return ActionPlan(
            intent=intent,
            commands=commands,
            explanations=explanations,
            risk_level=risk_level
        )
    

    async def execute_plan(
        self, 
        plan: Union[TaskPlan, AdvancedTaskPlan], 
        dry_run: bool = False,
        transaction_id: Optional[str] = None
    ) -> Union[List[Dict[str, Any]], Dict[str, Any]]:
        """
        Execute a task plan with transaction support.
        
        Args:
            plan: The plan to execute
            dry_run: Whether to simulate execution without making changes
            transaction_id: ID of the transaction this execution belongs to
            
        Returns:
            List of execution results for each step
        """
        self._logger.info(f"Executing plan: {plan.goal}")
        
        # Check for EnhancedTaskPlanner to delegate advanced plans
        if isinstance(plan, AdvancedTaskPlan):
            # Using API layer to avoid circular imports
            from angela.api.intent import get_enhanced_task_planner
            enhanced_task_planner = get_enhanced_task_planner()
            return await enhanced_task_planner.execute_plan(plan, dry_run, transaction_id)
        else:
            return await self._execute_basic_plan(plan, dry_run, transaction_id)
    
    async def _execute_basic_plan(
        self, 
        plan: TaskPlan, 
        dry_run: bool = False,
        transaction_id: Optional[str] = None
    ) -> List[Dict[str, Any]]:
        """
        Execute a basic task plan with transaction support.
        
        Args:
            plan: The plan to execute
            dry_run: Whether to simulate execution without making changes
            transaction_id: ID of the transaction this execution belongs to
            
        Returns:
            List of execution results for each step
        """
        # Get necessary components through API layer
        execution_engine = get_execution_engine()
        rollback_manager = get_rollback_manager()
        
        results = []
        
        # Execute each step in sequence
        for i, step in enumerate(plan.steps):
            self._logger.info(f"Executing step {i+1}/{len(plan.steps)}: {step.command}")
            
            # Generate a step_id for tracking
            step_id = f"step_{i+1}"
            
            try:
                # Execute the command
                stdout, stderr, return_code = await execution_engine.execute_command(
                    command=step.command,
                    check_safety=True,
                    dry_run=dry_run
                )
                
                # Record command execution in the transaction if successful
                if not dry_run and transaction_id and return_code == 0:
                    await rollback_manager.record_command_execution(
                        command=step.command,
                        return_code=return_code,
                        stdout=stdout,
                        stderr=stderr,
                        transaction_id=transaction_id,
                        step_id=step_id
                    )
                
                # Format the result
                execution_result = {
                    "step": i + 1,
                    "command": step.command,
                    "explanation": step.explanation,
                    "stdout": stdout,
                    "stderr": stderr,
                    "return_code": return_code,
                    "success": return_code == 0
                }
                
                results.append(execution_result)
                
                # Stop execution if a step fails
                if return_code != 0:
                    self._logger.warning(f"Step {i+1} failed with return code {return_code}")
                    break
                    
            except Exception as e:
                self._logger.exception(f"Error executing step {i+1}: {str(e)}")
                
                # Record the error
                results.append({
                    "step": i + 1,
                    "command": step.command,
                    "explanation": step.explanation,
                    "error": str(e),
                    "success": False
                })
                
                # Stop execution on error
                break
        
        return results
    

    #######################
    # Advanced Planning Methods
    
    async def _generate_advanced_plan(
        self, 
        goal: str, 
        context: Dict[str, Any]
    ) -> AdvancedTaskPlan:
        """
        Generate an advanced plan using the AI service.
        
        Args:
            goal: The high-level goal
            context: Context information
            
        Returns:
            An AdvancedTaskPlan with steps to achieve the goal
        """
        # Using API layer to avoid circular imports
        from angela.api.intent import get_enhanced_task_planner
        enhanced_task_planner = get_enhanced_task_planner()
        return await enhanced_task_planner.plan_advanced_task(goal, context)
    
    def _build_advanced_planning_prompt(
        self, 
        goal: str, 
        context: Dict[str, Any]
    ) -> str:
        """
        Build a prompt for generating an advanced plan.
        
        Args:
            goal: The high-level goal
            context: Context information
            
        Returns:
            A prompt string for the AI service
        """
        # Create context string
        context_str = "Current context:\n"
        
        if context.get("cwd"):
            context_str += f"- Current working directory: {context['cwd']}\n"
        if context.get("project_root"):
            context_str += f"- Project root: {context['project_root']}\n"
        if context.get("project_type"):
            context_str += f"- Project type: {context['project_type']}\n"
        
        # Add files in current directory for context
        if context.get("cwd"):
            try:
                context_manager = get_context_manager()
                dir_contents = context_manager.get_directory_contents(Path(context["cwd"]))
                files_str = "\n".join([f"- {item['name']}" for item in dir_contents])
                context_str += f"\nFiles in current directory:\n{files_str}\n"
            except Exception as e:
                self._logger.error(f"Error getting directory contents: {str(e)}")
        
        # Build the prompt
        prompt = f"""
You are Angela, an AI terminal assistant with advanced planning capabilities. Your task is to create a detailed, sophisticated plan for achieving the following complex goal:

GOAL: {goal}

{context_str}

This is a complex goal that may require branching, conditions, loops, or other advanced constructs.

Break down this goal into a comprehensive plan with these advanced features:
1. Different types of steps (commands, code, file operations, API calls, decisions, loops)
2. Branching execution paths based on conditions
3. Dependencies between steps
4. Error recovery strategies
5. Risk assessment for each step

Format your response as JSON:
{{
  "id": "generate a unique plan ID",
  "goal": "the original goal",
  "description": "detailed plan description",
  "steps": {{
    "step1": {{
      "id": "step1",
      "type": "command",
      "description": "Description of step 1",
      "command": "command to execute",
      "dependencies": [],
      "estimated_risk": 1
    }},
    "step2": {{
      "id": "step2",
      "type": "file",
      "description": "Create a file",
      "file_path": "/path/to/file",
      "file_content": "content to write",
      "dependencies": ["step1"],
      "estimated_risk": 2
    }},
    "step3": {{
      "id": "step3",
      "type": "decision",
      "description": "Check if a condition is met",
      "condition": "test condition",
      "true_branch": ["step4a"],
      "false_branch": ["step4b"],
      "dependencies": ["step2"],
      "estimated_risk": 0
    }},
    "step4a": {{
      "id": "step4a",
      "type": "command",
      "description": "Executed if condition is true",
      "command": "command to execute",
      "dependencies": ["step3"],
      "estimated_risk": 1
    }},
    "step4b": {{
      "id": "step4b",
      "type": "command",
      "description": "Executed if condition is false",
      "command": "command to execute",
      "dependencies": ["step3"],
      "estimated_risk": 1
    }},
    "step5": {{
      "id": "step5",
      "type": "loop",
      "description": "Process each item",
      "loop_items": "items to process",
      "loop_body": ["step6"],
      "dependencies": ["step4a", "step4b"],
      "estimated_risk": 2
    }},
    "step6": {{
      "id": "step6",
      "type": "code",
      "description": "Execute some code",
      "code": "code to execute",
      "dependencies": [],
      "estimated_risk": 1
    }}
  }},
  "entry_points": ["step1"]
}}

Ensure each step has a unique ID and clear dependencies. Entry points are the steps that should be executed first.
"""
        
        return prompt
    
    def _parse_advanced_plan_response(
        self, 
        response: str, 
        goal: str, 
        context: Dict[str, Any]
    ) -> AdvancedTaskPlan:
        """
        Parse the AI response into an AdvancedTaskPlan.
        
        Args:
            response: The AI response text
            goal: The original high-level goal
            context: Context information
            
        Returns:
            An AdvancedTaskPlan object
        """
        try:
            # Extract JSON from the response
            json_match = re.search(r'```(?:json)?\s*(.*?)\s*```', response, re.DOTALL)
            if json_match:
                json_str = json_match.group(1)
            else:
                # Try to find JSON without code blocks
                json_match = re.search(r'({.*})', response, re.DOTALL)
                if json_match:
                    json_str = json_match.group(1)
                else:
                    # Assume the entire response is JSON
                    json_str = response
            
            # Parse the JSON
            plan_data = json.loads(json_str)
            
            # Create an AdvancedTaskPlan object
            return AdvancedTaskPlan(
                id=plan_data.get("id", f"plan_{datetime.now().strftime('%Y%m%d%H%M%S')}"),
                goal=goal,
                description=plan_data.get("description", "Advanced plan for " + goal),
                steps=plan_data["steps"],
                entry_points=plan_data.get("entry_points", [next(iter(plan_data["steps"].keys()))]),
                context=context,
                created=datetime.now()
            )
        
        except Exception as e:
            self._logger.exception(f"Error parsing advanced plan response: {str(e)}")
            # Create a fallback plan
            fallback_step_id = "fallback_step"
            return AdvancedTaskPlan(
                id=f"fallback_plan_{datetime.now().strftime('%Y%m%d%H%M%S')}",
                goal=goal,
                description=f"Fallback plan for: {goal}",
                steps={
                    fallback_step_id: AdvancedPlanStep(
                        id=fallback_step_id,
                        type=PlanStepType.COMMAND,
                        description="Fallback step due to planning error",
                        command=f"echo 'Unable to create detailed plan for: {goal}'",
                        dependencies=[],
                        estimated_risk=0
                    )
                },
                entry_points=[fallback_step_id],
                context=context
            )
    
    async def _execute_advanced_plan(
        self, 
        plan: AdvancedTaskPlan, 
        dry_run: bool = False,
        transaction_id: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Execute an advanced task plan with transaction support.
        
        Args:
            plan: The plan to execute
            dry_run: Whether to simulate execution without making changes
            transaction_id: ID of the transaction this execution belongs to
            
        Returns:
            Dictionary with execution results
        """
        self._logger.info(f"Executing advanced plan: {plan.goal}")
        
        # Get execution engine through API layer
        execution_engine = get_execution_engine()
        
        # Initialize execution state
        results = {}
        completed_steps = set()
        pending_steps = {}
        
        # Initialize with entry points
        for entry_point in plan.entry_points:
            if entry_point in plan.steps:
                pending_steps[entry_point] = plan.steps[entry_point]
        
        # Execute steps until all are completed or no more can be executed
        while pending_steps:
            # Find steps that can be executed (all dependencies satisfied)
            executable_steps = {}
            for step_id, step in pending_steps.items():
                if all(dep in completed_steps for dep in step.dependencies):
                    executable_steps[step_id] = step
            
            # If no steps can be executed, we're stuck (circular dependencies or missing steps)
            if not executable_steps:
                self._logger.warning("No executable steps found, possible circular dependencies")
                break
            
            # Execute all executable steps
            for step_id, step in executable_steps.items():
                self._logger.info(f"Executing step {step_id}: {step.command}")
                
                try:
                    # Handle different step types
                    if step.type == "command":
                        # Execute shell command
                        result = await execution_engine.execute_command(
                            command=step.command,
                            check_safety=True,
                            dry_run=dry_run
                        )
                        
                        # Record command execution in the transaction if successful
                        if not dry_run and transaction_id and result[2] == 0:  # return_code == 0
                            # Get rollback manager through API layer
                            rollback_manager = get_rollback_manager()
                            
                            await rollback_manager.record_command_execution(
                                command=step.command,
                                return_code=result[2],
                                stdout=result[0],
                                stderr=result[1],
                                transaction_id=transaction_id,
                                step_id=step_id
                            )
                        
                        # Format the result
                        execution_result = {
                            "step_id": step_id,
                            "type": step.type,
                            "command": step.command,
                            "description": step.description,
                            "stdout": result[0],
                            "stderr": result[1],
                            "return_code": result[2],
                            "success": result[2] == 0
                        }
                        
                    else:
                        # Unsupported step type, mark as failure
                        execution_result = {
                            "step_id": step_id,
                            "type": step.type,
                            "description": step.description,
                            "error": f"Unsupported step type: {step.type}",
                            "success": False
                        }
                    
                    results[step_id] = execution_result
                    
                    # Mark step as completed if successful, otherwise the plan failed
                    if execution_result["success"]:
                        completed_steps.add(step_id)
                    else:
                        self._logger.warning(f"Step {step_id} failed")
                        return {
                            "success": False,
                            "steps_completed": len(completed_steps),
                            "steps_total": len(plan.steps),
                            "failed_step": step_id,
                            "results": results
                        }
                        
                except Exception as e:
                    self._logger.exception(f"Error executing step {step_id}: {str(e)}")
                    
                    # Record the error
                    results[step_id] = {
                        "step_id": step_id,
                        "type": step.type,
                        "description": step.description,
                        "error": str(e),
                        "success": False
                    }
                    
                    # Plan failed
                    return {
                        "success": False,
                        "steps_completed": len(completed_steps),
                        "steps_total": len(plan.steps),
                        "failed_step": step_id,
                        "results": results
                    }
                
                # Remove from pending steps
                del pending_steps[step_id]
        
        # Check if all steps were completed
        all_completed = len(completed_steps) == len(plan.steps)
        
        return {
            "success": all_completed,
            "steps_completed": len(completed_steps),
            "steps_total": len(plan.steps),
            "results": results
        }
    
    
    def _select_next_step(
        self, 
        plan: AdvancedTaskPlan,
        pending_steps: Set[str],
        executed_steps: Set[str]
    ) -> Optional[str]:
        """
        Select the next step to execute in an advanced plan.
        
        Args:
            plan: The plan
            pending_steps: Set of pending step IDs
            executed_steps: Set of executed step IDs
            
        Returns:
            ID of the next step to execute, or None if no steps are ready
        """
        for step_id in pending_steps:
            step = plan.steps[step_id]
            
            # Check if all dependencies are satisfied
            if all(dep in executed_steps for dep in step.dependencies):
                return step_id
        
        return None
    
    async def _execute_step(
        self, 
        step: AdvancedPlanStep,
        previous_results: Dict[str, Dict[str, Any]],
        dry_run: bool
    ) -> Dict[str, Any]:
        """
        Execute a single step of the advanced plan.
        
        Args:
            step: The step to execute
            previous_results: Results of previously executed steps
            dry_run: Whether to simulate execution
            
        Returns:
            Dictionary with step execution results
        """
        # Get execution engine through API layer
        execution_engine = get_execution_engine()
        
        # Prepare base result
        result = {
            "step_id": step.id,
            "type": step.type,
            "description": step.description,
            "dry_run": dry_run
        }
        
        try:
            # Execute based on step type
            if step.type == PlanStepType.COMMAND:
                if step.command:
                    if dry_run:
                        # Simulate command execution
                        result["stdout"] = f"[DRY RUN] Would execute: {step.command}"
                        result["stderr"] = ""
                        result["return_code"] = 0
                        result["success"] = True
                    else:
                        # Execute the command
                        stdout, stderr, return_code = await execution_engine.execute_command(
                            step.command,
                            check_safety=True
                        )
                        result["stdout"] = stdout
                        result["stderr"] = stderr
                        result["return_code"] = return_code
                        result["success"] = return_code == 0
                else:
                    result["error"] = "Missing command for command step"
                    result["success"] = False
            
            elif step.type == PlanStepType.FILE:
                if step.file_path:
                    if dry_run:
                        # Simulate file operation
                        operation = "write" if step.file_content else "read"
                        result["message"] = f"[DRY RUN] Would {operation} file: {step.file_path}"
                        result["success"] = True
                    else:
                        # Execute file operation
                        if step.file_content:
                            # Write to file
                            await self._write_file(step.file_path, step.file_content)
                            result["message"] = f"Wrote content to {step.file_path}"
                            result["success"] = True
                        else:
                            # Read from file
                            content = await self._read_file(step.file_path)
                            result["content"] = content
                            result["success"] = True
                else:
                    result["error"] = "Missing file path for file step"
                    result["success"] = False
            
            elif step.type == PlanStepType.CODE:
                if step.code:
                    if dry_run:
                        # Simulate code execution
                        result["message"] = f"[DRY RUN] Would execute code: {len(step.code)} characters"
                        result["success"] = True
                    else:
                        # Execute the code
                        # This is a simplified implementation - in a real system,
                        # this would use a sandboxed execution environment
                        code_result = await self._execute_code(step.code)
                        result.update(code_result)
                else:
                    result["error"] = "Missing code for code step"
                    result["success"] = False
            
            elif step.type == PlanStepType.DECISION:
                if step.condition:
                    # Evaluate the condition
                    # This is a simplified implementation - in a real system,
                    # this would use a more sophisticated condition evaluation
                    condition_result = await self._evaluate_condition(
                        step.condition, previous_results, dry_run
                    )
                    result["condition"] = step.condition
                    result["condition_result"] = condition_result
                    result["next_branch"] = "true_branch" if condition_result else "false_branch"
                    result["success"] = True
                else:
                    result["error"] = "Missing condition for decision step"
                    result["success"] = False
            
            elif step.type == PlanStepType.API:
                if step.api_url and step.api_method:
                    if dry_run:
                        # Simulate API call
                        result["message"] = f"[DRY RUN] Would call API: {step.api_method} {step.api_url}"
                        result["success"] = True
                    else:
                        # Execute API call
                        api_result = await self._execute_api_call(
                            step.api_url, step.api_method, step.api_payload
                        )
                        result.update(api_result)
                else:
                    result["error"] = "Missing URL or method for API step"
                    result["success"] = False
            
            elif step.type == PlanStepType.LOOP:
                if step.loop_items and step.loop_body:
                    if dry_run:
                        # Simulate loop execution
                        result["message"] = f"[DRY RUN] Would loop over {step.loop_items}"
                        result["success"] = True
                    else:
                        # This is a placeholder for loop execution
                        # In a real system, this would execute the loop body for each item
                        result["message"] = f"Loop execution not implemented: {step.loop_items}"
                        result["success"] = True
                else:
                    result["error"] = "Missing items or body for loop step"
                    result["success"] = False
            
            else:
                result["error"] = f"Unknown step type: {step.type}"
                result["success"] = False
            
        except Exception as e:
            self._logger.exception(f"Error executing step {step.id}: {str(e)}")
            result["error"] = str(e)
            result["success"] = False
            
            # Handle retry if configured
            if step.retry and step.retry > 0:
                result["retry_count"] = 1
                result["retried"] = True
                
                # Attempt retries (simplified)
                for retry_num in range(1, step.retry + 1):
                    self._logger.info(f"Retrying step {step.id} (attempt {retry_num}/{step.retry})")
                    try:
                        # Wait before retrying
                        await asyncio.sleep(1)
                        
                        # Execute retry logic based on step type
                        # This is a simplified implementation
                        retry_result = await self._execute_step(step, previous_results, dry_run)
                        
                        if retry_result.get("success", False):
                            # Retry succeeded
                            retry_result["retry_count"] = retry_num
                            retry_result["retried"] = True
                            return retry_result
                    except Exception as retry_e:
                        self._logger.error(f"Error in retry {retry_num} for step {step.id}: {str(retry_e)}")
                
                # All retries failed
                result["retry_exhausted"] = True
        
        return result
    
    def _update_pending_steps(
        self, 
        plan: AdvancedTaskPlan,
        executed_step: AdvancedPlanStep,
        result: Dict[str, Any],
        pending_steps: Set[str],
        executed_steps: Set[str]
    ) -> None:
        """
        Update the set of pending steps based on execution result.
        
        Args:
            plan: The plan
            executed_step: The step that was just executed
            result: The execution result
            pending_steps: Set of pending step IDs to update
            executed_steps: Set of executed step IDs
        """
        # For decision steps, add the appropriate branch
        if executed_step.type == PlanStepType.DECISION:
            condition_result = result.get("condition_result", False)
            if condition_result and executed_step.true_branch:
                # Add steps from true branch
                for step_id in executed_step.true_branch:
                    if step_id not in executed_steps and step_id in plan.steps:
                        pending_steps.add(step_id)
            elif not condition_result and executed_step.false_branch:
                # Add steps from false branch
                for step_id in executed_step.false_branch:
                    if step_id not in executed_steps and step_id in plan.steps:
                        pending_steps.add(step_id)
        
        # For normal steps, add all steps that depend on this one
        for step_id, step in plan.steps.items():
            if executed_step.id in step.dependencies and step_id not in executed_steps:
                # Check if all dependencies are satisfied
                if all(dep in executed_steps for dep in step.dependencies):
                    pending_steps.add(step_id)
    
    async def _read_file(self, path: str) -> str:
        """Read content from a file."""
        from angela.api.execution import get_filesystem_functions
        fs = get_filesystem_functions()
        return await fs.read_file(path)
    
    async def _write_file(self, path: str, content: str) -> bool:
        """Write content to a file."""
        from angela.api.execution import get_filesystem_functions
        fs = get_filesystem_functions()
        return await fs.write_file(path, content)
    
    async def _execute_code(self, code: str) -> Dict[str, Any]:
        """
        Execute code (placeholder implementation).
        
        In a real system, this would use a sandboxed execution environment.
        """
        return {
            "message": f"Code execution not implemented: {len(code)} characters",
            "success": True
        }
    
    async def _evaluate_condition(
        self, 
        condition: str,
        previous_results: Dict[str, Dict[str, Any]],
        dry_run: bool
    ) -> bool:
        """
        Evaluate a condition (placeholder implementation).
        
        In a real system, this would use a more sophisticated condition evaluation.
        """
        import re
        
        # Look for simple patterns
        if re.search(r'file exists', condition, re.IGNORECASE):
            # Extract file path
            match = re.search(r'file exists[:\s]+([^\s]+)', condition, re.IGNORECASE)
            if match:
                file_path = match.group(1)
                return Path(file_path).exists()
        
        if re.search(r'command success', condition, re.IGNORECASE):
            # Extract step ID
            match = re.search(r'step[:\s]+([^\s]+)', condition, re.IGNORECASE)
            if match:
                step_id = match.group(1)
                return previous_results.get(step_id, {}).get("success", False)
        
        # Default behavior for dry run
        if dry_run:
            return True
        
        # Default for unknown conditions
        return False
    
    async def _execute_api_call(
        self, 
        url: str,
        method: str,
        payload: Optional[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """
        Execute an API call (placeholder implementation).
        
        In a real system, this would use a proper HTTP client.
        """
        return {
            "message": f"API call not implemented: {method} {url}",
            "success": True
        }


task_planner = TaskPlanner()
</file>

<file path="components/intent/semantic_task_planner.py">
# angela/components/intent/semantic_task_planner.py
"""
Semantic task planning and intent decomposition for Angela CLI.

This module extends the enhanced task planner with semantic code understanding
and improved intent decomposition for handling complex, ambiguous, multi-stage goals.
"""
import re
import json
import uuid
import asyncio
from typing import Dict, Any, List, Tuple, Optional, Set, Union
from datetime import datetime
from pathlib import Path

from pydantic import BaseModel, Field, ValidationError

# Updated imports using the new architecture
from angela.api.ai import get_gemini_client, get_gemini_request_class, get_semantic_analyzer
from angela.api.context import get_context_manager, get_project_state_analyzer
from angela.api.shell import get_inline_feedback
from angela.utils.logging import get_logger
from angela.core.registry import registry

# Internal module imports that don't cause circular dependencies
from angela.components.intent.enhanced_task_planner import EnhancedTaskPlanner, AdvancedTaskPlan, PlanStepType

logger = get_logger(__name__)

class IntentClarification(BaseModel):
    """Model for intent clarification information."""
    
    original_request: str = Field(..., description="The original user request")
    ambiguity_type: str = Field(..., description="Type of ambiguity detected")
    ambiguity_details: str = Field(..., description="Details about the ambiguity")
    clarification_question: str = Field(..., description="Question to ask the user")
    options: List[str] = Field(default_factory=list, description="Possible options to present to the user")
    context: Dict[str, Any] = Field(default_factory=dict, description="Context for resolving the ambiguity")


class SemanticTaskPlanner:
    """
    Enhanced task planner with semantic code understanding and improved intent decomposition.
    
    This class extends the existing EnhancedTaskPlanner with:
    1. Integration with semantic code analysis
    2. Improved handling of ambiguous requests
    3. Multi-stage goal decomposition with sub-goals
    4. Interactive clarification for uncertain intents
    """
    
    def __init__(self):
        """Initialize the semantic task planner."""
        self._logger = logger
        self._enhanced_planner = EnhancedTaskPlanner()
        self._clarification_handlers = {
            "file_reference": self._clarify_file_reference,
            "entity_reference": self._clarify_entity_reference,
            "action_type": self._clarify_action_type,
            "operation_scope": self._clarify_operation_scope,
            "step_ordering": self._clarify_step_ordering,
            "parameter_value": self._clarify_parameter_value
        }
    
    async def plan_task(
        self, 
        request: str, 
        context: Dict[str, Any],
        enable_clarification: bool = True,
        semantic_context: bool = True
    ) -> Dict[str, Any]:
        """
        Plan a task with semantic understanding and potential user clarification.
        
        Args:
            request: User request
            context: Task context
            enable_clarification: Whether to enable interactive clarification
            semantic_context: Whether to include semantic code understanding
            
        Returns:
            Dictionary with planning results including a task plan
        """
        self._logger.info(f"Planning semantic task: {request}")
        
        # Enhance context with semantic information if requested
        if semantic_context:
            context = await self._enhance_context_with_semantics(context)
        
        # First, analyze the request for potential ambiguities
        intent_analysis = await self._analyze_intent(request, context)
        
        # Check if clarification is needed and enabled
        if intent_analysis.get("needs_clarification", False) and enable_clarification:
            clarification = await self._create_clarification(request, intent_analysis, context)
            
            if clarification:
                # Get user clarification
                clarified_request = await self._get_user_clarification(clarification)
                
                if clarified_request:
                    # Update the request and intent analysis
                    self._logger.info(f"Using clarified request: {clarified_request}")
                    request = clarified_request
                    intent_analysis = await self._analyze_intent(request, context)
        
        # Decompose the goal into sub-goals if complex
        goal_decomposition = await self._decompose_goal(request, intent_analysis, context)
        
        # Create an execution plan
        plan_result = await self._create_execution_plan(request, goal_decomposition, context)
        
        # Return the planning results
        return {
            "original_request": request,
            "intent_analysis": intent_analysis,
            "goal_decomposition": goal_decomposition,
            "execution_plan": plan_result.get("plan"),
            "plan_type": plan_result.get("plan_type", "simple"),
            "plan_id": plan_result.get("plan_id"),
            "estimated_steps": plan_result.get("estimated_steps", 0),
            "max_risk_level": plan_result.get("max_risk_level", 0),
            "clarification_needed": intent_analysis.get("needs_clarification", False),
            "clarification_performed": intent_analysis.get("needs_clarification", False) and enable_clarification
        }
    
    async def execute_plan(
        self, 
        plan_result: Dict[str, Any],
        context: Dict[str, Any],
        dry_run: bool = False,
        transaction_id: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Execute a task plan generated by the semantic planner.
        
        Args:
            plan_result: Planning results from plan_task
            context: Task context
            dry_run: Whether to perform a dry run without making changes
            transaction_id: Optional transaction ID for rollback
            
        Returns:
            Dictionary with execution results
        """
        self._logger.info(f"Executing semantic task plan: {plan_result.get('original_request')}")
        
        # Get the execution plan
        execution_plan = plan_result.get("execution_plan")
        
        if not execution_plan:
            return {
                "success": False,
                "error": "No execution plan available",
                "plan_result": plan_result
            }
        
        # Execute the plan using the enhanced task planner
        execution_result = await self._enhanced_planner.execute_plan(
            plan=execution_plan,
            dry_run=dry_run,
            transaction_id=transaction_id
        )
        
        # Return the execution results
        return {
            "success": execution_result.get("success", False),
            "execution_result": execution_result,
            "plan_result": plan_result,
            "dry_run": dry_run
        }
    
    async def _analyze_intent(self, request: str, context: Dict[str, Any]) -> Dict[str, Any]:
        """
        Analyze user intent with semantic understanding.
        
        Args:
            request: User request
            context: Task context
            
        Returns:
            Dictionary with intent analysis
        """
        self._logger.debug(f"Analyzing intent for request: {request}")
        
        # Create a prompt for the AI to analyze the intent
        project_context = self._extract_project_context(context)
        
        prompt = f"""
You are an expert assistant analyzing a user's request to an AI terminal assistant that can perform tasks using shell commands and code.

USER REQUEST: "{request}"

CONTEXT:
{project_context}

Provide a detailed analysis of the user's intent, including:
1. The primary goal they're trying to achieve
2. Required sub-tasks or steps
3. Entities involved (files, directories, commands, APIs)
4. Any potential ambiguities that need clarification 
5. Level of complexity (simple/moderate/complex)

Return your analysis as a JSON object with this structure:
```json
{{
  "primary_goal": "Clear description of the main objective",
  "intent_type": "One of: file_operation, code_generation, system_command, project_setup, information_request, refactoring",
  "entities": [
    {{ "type": "file|directory|command|api|code_entity", "name": "entity_name", "confidence": 0.0-1.0 }}
  ],
  "sub_tasks": [
    "Description of sub-task 1",
    "Description of sub-task 2"
  ],
  "needs_clarification": true|false,
  "ambiguities": [
    {{
      "type": "file_reference|entity_reference|action_type|operation_scope|step_ordering|parameter_value",
      "description": "Description of the ambiguity",
      "possible_interpretations": ["interpretation1", "interpretation2"]
    }}
  ],
  "complexity": "simple|moderate|complex",
  "estimated_steps": 1-20,
  "potential_risk": 0-4,
  "confidence": 0.0-1.0
}}
```

Use the highest standards for identifying ambiguities that would benefit from clarification. If anything is unclear, set "needs_clarification" to true and document the ambiguity.
"""
    
        gemini_client = get_gemini_client()
        GeminiRequest = get_gemini_request_class()    

        api_request = GeminiRequest(
            prompt=prompt,
            temperature=0.1,  # Lower temperature for more deterministic analysis
            max_tokens=3000
        )
        
        response = await gemini_client.generate_text(api_request)
        
        # Parse the response
        try:
            # Try to extract JSON from the response
            response_text = response.text
            
            # Look for JSON block
            json_match = re.search(r'```(?:json)?\s*(.*?)\s*```', response_text, re.DOTALL)
            if json_match:
                response_text = json_match.group(1)
            
            # Try to parse the JSON
            intent_analysis = json.loads(response_text)
            
            # Add some metadata to the analysis
            intent_analysis["original_request"] = request
            intent_analysis["analysis_time"] = datetime.now().isoformat()
            
            return intent_analysis
            
        except (json.JSONDecodeError, IndexError) as e:
            self._logger.error(f"Error parsing intent analysis response: {str(e)}")
            
            # Return a basic fallback analysis
            return {
                "primary_goal": request,
                "intent_type": "unknown",
                "entities": [],
                "sub_tasks": [request],
                "needs_clarification": False,
                "ambiguities": [],
                "complexity": "simple",
                "estimated_steps": 1,
                "potential_risk": 0,
                "confidence": 0.5,
                "original_request": request,
                "analysis_time": datetime.now().isoformat(),
                "analysis_error": str(e)
            }
    
    def _extract_project_context(self, context: Dict[str, Any]) -> str:
        """
        Extract relevant project context for intent analysis.
        
        Args:
            context: Task context
            
        Returns:
            String with formatted project context
        """
        lines = []
        
        # Add current working directory
        if "cwd" in context:
            lines.append(f"Current Directory: {context['cwd']}")
        
        # Add project type if available
        if "project_type" in context:
            lines.append(f"Project Type: {context['project_type']}")
        
        # Add project root if available
        if "project_root" in context:
            lines.append(f"Project Root: {context['project_root']}")
        
        # Add enhanced project information if available
        if "enhanced_project" in context:
            enhanced_project = context["enhanced_project"]
            
            if "type" in enhanced_project:
                lines.append(f"Project Type: {enhanced_project['type']}")
            
            if "frameworks" in enhanced_project:
                frameworks = enhanced_project["frameworks"]
                if frameworks:
                    lines.append(f"Frameworks: {', '.join(frameworks.keys())}")
            
            if "dependencies" in enhanced_project and "top_dependencies" in enhanced_project["dependencies"]:
                deps = enhanced_project["dependencies"]["top_dependencies"]
                if deps:
                    lines.append(f"Top Dependencies: {', '.join(deps[:5])}")
        
        # Add recent file context if available
        if "recent_files" in context and "accessed" in context["recent_files"]:
            recent_files = context["recent_files"]["accessed"]
            if recent_files:
                lines.append(f"Recently Accessed Files: {', '.join([Path(f).name for f in recent_files[:3]])}")
        
        # Add resolved file references if available
        if "resolved_files" in context:
            resolved_files = context["resolved_files"]
            if resolved_files:
                lines.append("Resolved File References:")
                for ref_info in resolved_files:
                    lines.append(f"- '{ref_info['reference']}' → {ref_info['path']}")
        
        # Add semantic code information if available
        if "semantic_code" in context:
            semantic_code = context["semantic_code"]
            
            if "modules" in semantic_code:
                module_count = len(semantic_code["modules"])
                lines.append(f"Analyzed Code Modules: {module_count}")
            
            if "key_entities" in semantic_code:
                key_entities = semantic_code["key_entities"]
                if key_entities:
                    lines.append("Key Code Entities:")
                    for entity in key_entities[:5]:
                        lines.append(f"- {entity['type']} '{entity['name']}' in {Path(entity['filename']).name}")
        
        # Add project state information if available
        if "project_state" in context:
            project_state = context["project_state"]
            
            if "git_state" in project_state and project_state["git_state"].get("is_git_repo", False):
                git_state = project_state["git_state"]
                branch = git_state.get("current_branch", "unknown")
                has_changes = git_state.get("has_changes", False)
                
                git_info = f"Git: on branch '{branch}'"
                if has_changes:
                    git_info += " with uncommitted changes"
                
                lines.append(git_info)
            
            if "todo_items" in project_state and project_state["todo_items"]:
                todo_count = len(project_state["todo_items"])
                lines.append(f"Project TODOs: {todo_count} items")
        
        return "\n".join(lines)
    
    async def _create_clarification(
        self, 
        request: str, 
        intent_analysis: Dict[str, Any], 
        context: Dict[str, Any]
    ) -> Optional[IntentClarification]:
        """
        Create a clarification request for ambiguous intent.
        
        Args:
            request: Original user request
            intent_analysis: Intent analysis results
            context: Task context
            
        Returns:
            IntentClarification object or None if no clarification needed
        """
        if not intent_analysis.get("needs_clarification", False):
            return None
        
        ambiguities = intent_analysis.get("ambiguities", [])
        if not ambiguities:
            return None
        
        # Select the most important ambiguity to clarify
        # (for simplicity, just take the first one)
        ambiguity = ambiguities[0]
        
        ambiguity_type = ambiguity.get("type", "unknown")
        description = ambiguity.get("description", "")
        interpretations = ambiguity.get("possible_interpretations", [])
        
        # Select a clarification handler based on ambiguity type
        if ambiguity_type in self._clarification_handlers:
            return await self._clarification_handlers[ambiguity_type](
                request, ambiguity, interpretations, context
            )
        
        # Default clarification
        return IntentClarification(
            original_request=request,
            ambiguity_type=ambiguity_type,
            ambiguity_details=description,
            clarification_question=f"I'm not sure about this part of your request: {description}. Could you clarify?",
            options=interpretations,
            context={"ambiguity": ambiguity}
        )
    
    async def _get_user_clarification(self, clarification: IntentClarification) -> Optional[str]:
        """
        Get clarification from the user.
        
        Args:
            clarification: Clarification object
            
        Returns:
            Clarified request or None if clarification was not provided
        """
        self._logger.info(f"Getting user clarification for: {clarification.ambiguity_type}")
        
        try:
            # Get inline_feedback through API layer
            inline_feedback = get_inline_feedback()
            
            # Check if inline_feedback is available
            if inline_feedback:
                question = clarification.clarification_question
                options = clarification.options
                
                if options:
                    # Present options for selection
                    response = await inline_feedback.ask_question(
                        question, 
                        choices=options,
                        allow_free_text=True
                    )
                else:
                    # Free-form response
                    response = await inline_feedback.ask_question(question)
                
                if response:
                    # Check if we need to create a new request or use the response as is
                    if clarification.ambiguity_type in ["file_reference", "entity_reference", "parameter_value"]:
                        # Just use the clarified response to update the original request
                        return self._update_request_with_clarification(
                            clarification.original_request, 
                            response, 
                            clarification
                        )
                    else:
                        # For other types, assume the response is a complete revised request
                        return response
                
            else:
                self._logger.warning("inline_feedback not available for clarification")
            
            return None
                
        except Exception as e:
            self._logger.error(f"Error getting user clarification: {str(e)}")
            return None
    
    def _update_request_with_clarification(
        self, 
        original_request: str, 
        clarification: str, 
        intent_clarification: IntentClarification
    ) -> str:
        """
        Update the original request with the clarified information.
        
        Args:
            original_request: Original user request
            clarification: User's clarification response
            intent_clarification: Original clarification object
            
        Returns:
            Updated request
        """
        ambiguity_type = intent_clarification.ambiguity_type
        
        if ambiguity_type == "file_reference":
            # Replace the ambiguous file reference with the clarified one
            context = intent_clarification.context
            if "file_reference" in context:
                ambiguous_ref = context["file_reference"]
                # Simple replacement (in practice, you might want a more sophisticated approach)
                return original_request.replace(ambiguous_ref, clarification)
            
        elif ambiguity_type == "entity_reference":
            # Replace the ambiguous entity reference with the clarified one
            context = intent_clarification.context
            if "entity_reference" in context:
                ambiguous_ref = context["entity_reference"]
                return original_request.replace(ambiguous_ref, clarification)
        
        elif ambiguity_type == "parameter_value":
            # Add or update the parameter value
            context = intent_clarification.context
            if "parameter_name" in context:
                param_name = context["parameter_name"]
                # Check if parameter already exists in the request
                if param_name in original_request:
                    # Try to update the existing parameter
                    param_pattern = f"{param_name}\\s*[=:]?\\s*\\S+"
                    updated = re.sub(
                        param_pattern, 
                        f"{param_name}={clarification}", 
                        original_request
                    )
                    if updated != original_request:
                        return updated
                
                # If not found or update failed, append the parameter
                return f"{original_request} {param_name}={clarification}"
        
        # For other types, or if the specific handling failed,
        # just append the clarification to the original request
        return f"{original_request} ({clarification})"
    
    async def _clarify_file_reference(
        self, 
        request: str, 
        ambiguity: Dict[str, Any], 
        interpretations: List[str], 
        context: Dict[str, Any]
    ) -> IntentClarification:
        """
        Create a clarification for ambiguous file references.
        
        Args:
            request: Original user request
            ambiguity: Ambiguity information
            interpretations: Possible interpretations
            context: Task context
            
        Returns:
            IntentClarification object
        """
        description = ambiguity.get("description", "")
        
        # Extract the ambiguous file reference
        file_reference = None
        
        # Try to find the ambiguous reference in the request
        match = re.search(r'(?:file|directory|folder|path|docs?)\s+["\']?([^"\']+)["\']?', description)
        if match:
            file_reference = match.group(1)
        
        # If not found, use a generic approach
        if not file_reference:
            # Use file_resolver to get possible matches
            from angela.api.context import get_file_resolver
            file_resolver = get_file_resolver()
            
            project_root = context.get("project_root")
            if project_root:
                # Get files in the project
                possible_files = await file_resolver.find_files(project_root)
                
                # Filter to the most relevant files based on the request
                relevant_files = []
                
                for file_path in possible_files:
                    # Check if any part of the file path appears in the request
                    file_parts = Path(file_path).parts
                    for part in file_parts:
                        if part in request:
                            relevant_files.append(str(file_path))
                            break
                
                # Limit to 5 options
                interpretations = [Path(f).name for f in relevant_files[:5]]
        
        # Create the clarification
        question = "Which file are you referring to?"
        if file_reference:
            question = f"I'm not sure which file '{file_reference}' refers to. Which one did you mean?"
        
        return IntentClarification(
            original_request=request,
            ambiguity_type="file_reference",
            ambiguity_details=description,
            clarification_question=question,
            options=interpretations,
            context={"file_reference": file_reference}
        )
    
    async def _clarify_entity_reference(
        self, 
        request: str, 
        ambiguity: Dict[str, Any], 
        interpretations: List[str], 
        context: Dict[str, Any]
    ) -> IntentClarification:
        """
        Create a clarification for ambiguous code entity references.
        
        Args:
            request: Original user request
            ambiguity: Ambiguity information
            interpretations: Possible interpretations
            context: Task context
            
        Returns:
            IntentClarification object
        """
        description = ambiguity.get("description", "")
        
        # Extract the ambiguous entity reference
        entity_reference = None
        
        # Try to find the ambiguous reference in the request
        match = re.search(r'(?:function|class|method|variable|object|module|component)\s+["\']?([^"\']+)["\']?', description)
        if match:
            entity_reference = match.group(1)
        
        # If semantic code information is available, try to find similar entities
        if "semantic_code" in context and entity_reference:
            semantic_code = context["semantic_code"]
            
            if "modules" in semantic_code:
                # Get all entities from the modules
                all_entities = []
                
                for module_info in semantic_code["modules"]:
                    # Add functions
                    for func_name in module_info.get("functions", {}):
                        all_entities.append({
                            "name": func_name,
                            "type": "function",
                            "file": module_info.get("filename")
                        })
                    
                    # Add classes
                    for class_name in module_info.get("classes", {}):
                        all_entities.append({
                            "name": class_name,
                            "type": "class",
                            "file": module_info.get("filename")
                        })
                        
                        # Add methods from the class
                        class_info = module_info.get("classes", {}).get(class_name, {})
                        for method_name in class_info.get("methods", {}):
                            all_entities.append({
                                "name": f"{class_name}.{method_name}",
                                "type": "method",
                                "file": module_info.get("filename")
                            })
                
                # Filter to entities with similar names
                import difflib
                
                similar_entities = []
                for entity in all_entities:
                    similarity = difflib.SequenceMatcher(None, entity_reference, entity["name"]).ratio()
                    if similarity > 0.6:
                        similar_entities.append(entity)
                
                # Sort by similarity
                similar_entities.sort(
                    key=lambda e: difflib.SequenceMatcher(None, entity_reference, e["name"]).ratio(),
                    reverse=True
                )
                
                # Update interpretations with the similar entities
                interpretations = [
                    f"{e['name']} ({e['type']} in {Path(e['file']).name})"
                    for e in similar_entities[:5]
                ]
        
        # Create the clarification
        question = "Which code entity are you referring to?"
        if entity_reference:
            question = f"I'm not sure which code entity '{entity_reference}' refers to. Which one did you mean?"
        
        return IntentClarification(
            original_request=request,
            ambiguity_type="entity_reference",
            ambiguity_details=description,
            clarification_question=question,
            options=interpretations,
            context={"entity_reference": entity_reference}
        )
    
    async def _clarify_action_type(
        self, 
        request: str, 
        ambiguity: Dict[str, Any], 
        interpretations: List[str], 
        context: Dict[str, Any]
    ) -> IntentClarification:
        """
        Create a clarification for ambiguous action types.
        
        Args:
            request: Original user request
            ambiguity: Ambiguity information
            interpretations: Possible interpretations
            context: Task context
            
        Returns:
            IntentClarification object
        """
        description = ambiguity.get("description", "")
        
        # If interpretations are provided, use them
        if not interpretations:
            # Default interpretations for action type
            interpretations = [
                "Show/display the content",
                "Edit/modify the content",
                "Create a new file/content",
                "Delete the file/content",
                "Analyze/examine the content"
            ]
        
        # Create the clarification
        question = f"I'm not sure what action you want to perform: {description}. What would you like to do?"
        
        return IntentClarification(
            original_request=request,
            ambiguity_type="action_type",
            ambiguity_details=description,
            clarification_question=question,
            options=interpretations,
            context={}
        )
    
    async def _clarify_operation_scope(
        self, 
        request: str, 
        ambiguity: Dict[str, Any], 
        interpretations: List[str], 
        context: Dict[str, Any]
    ) -> IntentClarification:
        """
        Create a clarification for ambiguous operation scopes.
        
        Args:
            request: Original user request
            ambiguity: Ambiguity information
            interpretations: Possible interpretations
            context: Task context
            
        Returns:
            IntentClarification object
        """
        description = ambiguity.get("description", "")
        
        # If interpretations are provided, use them
        if not interpretations:
            # Default interpretations for operation scope
            interpretations = [
                "Only the current file",
                "The entire directory",
                "All files matching a pattern",
                "The specific files mentioned",
                "The entire project"
            ]
        
        # Create the clarification
        question = f"I'm not sure about the scope of this operation: {description}. What scope do you want to apply?"
        
        return IntentClarification(
            original_request=request,
            ambiguity_type="operation_scope",
            ambiguity_details=description,
            clarification_question=question,
            options=interpretations,
            context={}
        )
    
    async def _clarify_step_ordering(
        self, 
        request: str, 
        ambiguity: Dict[str, Any], 
        interpretations: List[str], 
        context: Dict[str, Any]
    ) -> IntentClarification:
        """
        Create a clarification for ambiguous step ordering.
        
        Args:
            request: Original user request
            ambiguity: Ambiguity information
            interpretations: Possible interpretations
            context: Task context
            
        Returns:
            IntentClarification object
        """
        description = ambiguity.get("description", "")
        
        # Extract the conflicting steps if possible
        step1 = None
        step2 = None
        
        match = re.search(r'([^\s,]+)\s+(?:before|after)\s+([^\s,]+)', description)
        if match:
            step1 = match.group(1)
            step2 = match.group(2)
        
        # Create interpretations if not provided
        if not interpretations and step1 and step2:
            interpretations = [
                f"Do {step1} first, then {step2}",
                f"Do {step2} first, then {step1}",
                f"Do {step1} and {step2} in parallel",
                f"Only do {step1}, skip {step2}",
                f"Only do {step2}, skip {step1}"
            ]
        elif not interpretations:
            # Generic interpretations
            interpretations = [
                "Follow the steps in the order listed",
                "Reverse the order of steps",
                "Only do the first step",
                "Only do the last step",
                "Skip steps that seem risky"
            ]
        
        # Create the clarification
        question = f"I'm not sure about the order of steps: {description}. How should I proceed?"
        
        return IntentClarification(
            original_request=request,
            ambiguity_type="step_ordering",
            ambiguity_details=description,
            clarification_question=question,
            options=interpretations,
            context={"step1": step1, "step2": step2}
        )
    
    async def _clarify_parameter_value(
        self, 
        request: str, 
        ambiguity: Dict[str, Any], 
        interpretations: List[str], 
        context: Dict[str, Any]
    ) -> IntentClarification:
        """
        Create a clarification for ambiguous parameter values.
        
        Args:
            request: Original user request
            ambiguity: Ambiguity information
            interpretations: Possible interpretations
            context: Task context
            
        Returns:
            IntentClarification object
        """
        description = ambiguity.get("description", "")
        
        # Extract the parameter name if available
        param_name = None
        
        match = re.search(r'parameter\s+["\']?([^"\']+)["\']?', description)
        if match:
            param_name = match.group(1)
        
        # If not found through "parameter", try "value"
        if not param_name:
            match = re.search(r'value\s+(?:for|of)\s+["\']?([^"\']+)["\']?', description)
            if match:
                param_name = match.group(1)
        
        # Create the clarification
        question = f"I need a value for the parameter: {param_name or description}. What should it be?"
        
        return IntentClarification(
            original_request=request,
            ambiguity_type="parameter_value",
            ambiguity_details=description,
            clarification_question=question,
            options=interpretations,
            context={"parameter_name": param_name}
        )
    
    async def _decompose_goal(
        self, 
        request: str, 
        intent_analysis: Dict[str, Any], 
        context: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Decompose a complex goal into sub-goals.
        
        Args:
            request: User request
            intent_analysis: Intent analysis results
            context: Task context
            
        Returns:
            Dictionary with goal decomposition
        """
        # If the intent is already simple, no need to decompose
        if intent_analysis.get("complexity", "simple") == "simple":
            return {
                "main_goal": intent_analysis.get("primary_goal", request),
                "complexity": "simple",
                "sub_goals": [intent_analysis.get("primary_goal", request)],
                "sequential": True
            }
        
        # Start with the sub-tasks from the intent analysis
        sub_tasks = intent_analysis.get("sub_tasks", [])
        
        # For complex intentions, do a more detailed decomposition
        if intent_analysis.get("complexity", "simple") == "complex" or len(sub_tasks) < 2:
            # Create a prompt for the AI to decompose the goal
            prompt = f"""
You need to decompose a complex user request into clear, logical sub-goals that can be executed sequentially or in parallel.

USER REQUEST: "{request}"

ANALYZED INTENT:
- Primary Goal: {intent_analysis.get("primary_goal", request)}
- Intent Type: {intent_analysis.get("intent_type", "unknown")}
- Complexity: {intent_analysis.get("complexity", "complex")}

Break this request down into 2-7 clear, logical sub-goals that together will accomplish the main goal.
For each sub-goal, indicate:
1. A clear description of what needs to be done
2. Whether it depends on other sub-goals
3. The estimated complexity (simple, moderate, complex)

Return your decomposition as a JSON object with this structure:
```json
{{
  "main_goal": "Clear description of the main objective",
  "complexity": "complex|moderate",
  "sub_goals": [
    {{
      "id": "sg1",
      "description": "Sub-goal description",
      "dependencies": ["sg0"],
      "complexity": "simple|moderate|complex",
      "estimated_steps": 1-5
    }}
  ],
  "sequential": true|false,
  "explanation": "Brief explanation of the decomposition approach"
}}
```

If the goals must be executed in a specific order, set "sequential" to true and ensure the sub_goals are in the correct execution order.
If some goals can be executed in parallel, set "sequential" to false and use dependencies to indicate required ordering.
"""

            gemini_client = get_gemini_client()
            GeminiRequest = get_gemini_request_class()            

            api_request = GeminiRequest(
                prompt=prompt,
                temperature=0.2,
                max_tokens=3000
            )
            
            response = await gemini_client.generate_text(api_request)
            
            # Parse the response
            try:
                # Try to extract JSON from the response
                response_text = response.text
                
                # Look for JSON block
                json_match = re.search(r'```(?:json)?\s*(.*?)\s*```', response_text, re.DOTALL)
                if json_match:
                    response_text = json_match.group(1)
                
                # Try to parse the JSON
                decomposition = json.loads(response_text)
                
                # Add some metadata to the decomposition
                decomposition["original_request"] = request
                decomposition["intent_type"] = intent_analysis.get("intent_type", "unknown")
                
                return decomposition
                
            except (json.JSONDecodeError, IndexError) as e:
                self._logger.error(f"Error parsing goal decomposition response: {str(e)}")
        
        # If we get here, use a simple decomposition based on the intent analysis
        return {
            "main_goal": intent_analysis.get("primary_goal", request),
            "complexity": intent_analysis.get("complexity", "moderate"),
            "sub_goals": [
                {
                    "id": f"sg{i}",
                    "description": sub_task,
                    "dependencies": [] if i == 0 else [f"sg{i-1}"],
                    "complexity": "simple",
                    "estimated_steps": 1
                }
                for i, sub_task in enumerate(sub_tasks)
            ],
            "sequential": True,
            "explanation": "Simple sequential decomposition based on intent analysis",
            "original_request": request,
            "intent_type": intent_analysis.get("intent_type", "unknown")
        }
    
    async def _create_execution_plan(
        self, 
        request: str, 
        goal_decomposition: Dict[str, Any], 
        context: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Create an execution plan for the decomposed goal.
        
        Args:
            request: User request
            goal_decomposition: Goal decomposition
            context: Task context
            
        Returns:
            Dictionary with plan creation results
        """
        # For simple goals, use basic planning
        if goal_decomposition.get("complexity", "simple") == "simple":
            # Call the enhanced planner directly
            plan = await self._enhanced_planner.plan_task(
                request=request,
                context=context,
                complexity="auto"
            )
            
            return {
                "plan": plan,
                "plan_type": "simple",
                "plan_id": getattr(plan, "id", str(uuid.uuid4())),
                "estimated_steps": len(getattr(plan, "steps", [])),
                "max_risk_level": max(
                    [getattr(step, "estimated_risk", 0) for step in getattr(plan, "steps", {}).values()],
                    default=0
                ) if hasattr(plan, "steps") else 0
            }
        
        # For complex goals, create a more sophisticated plan
        # This is where we'd build a more complex execution plan
        # based on the goal decomposition
        
        # Extract the sub-goals
        sub_goals = goal_decomposition.get("sub_goals", [])
        sequential = goal_decomposition.get("sequential", True)
        
        if sequential:
            # For sequential execution, create a single plan with all steps
            combined_request = f"{request}\n\nExecute these steps in order:\n"
            for i, sub_goal in enumerate(sub_goals):
                combined_request += f"{i+1}. {sub_goal.get('description', '')}\n"
            
            plan = await self._enhanced_planner.plan_task(
                request=combined_request,
                context=context,
                complexity="advanced"
            )
            
            return {
                "plan": plan,
                "plan_type": "advanced_sequential",
                "plan_id": getattr(plan, "id", str(uuid.uuid4())),
                "estimated_steps": len(getattr(plan, "steps", [])),
                "max_risk_level": max(
                    [getattr(step, "estimated_risk", 0) for step in getattr(plan, "steps", {}).values()],
                    default=0
                ) if hasattr(plan, "steps") else 0
            }
            
        else:
            # For non-sequential execution, create a plan with dependencies
            # This requires a more sophisticated planning approach
            # that accounts for the dependencies between sub-goals
            
            # Generate a dependency-aware prompt
            dependency_prompt = f"{request}\n\nExecute these steps with the following dependencies:\n"
            for sub_goal in sub_goals:
                sg_id = sub_goal.get("id", "")
                description = sub_goal.get("description", "")
                dependencies = sub_goal.get("dependencies", [])
                
                if dependencies:
                    dependency_prompt += f"- {description} (depends on: {', '.join(dependencies)})\n"
                else:
                    dependency_prompt += f"- {description} (no dependencies)\n"
            
            plan = await self._enhanced_planner.plan_task(
                request=dependency_prompt,
                context=context,
                complexity="advanced"
            )
            
            return {
                "plan": plan,
                "plan_type": "advanced_dependency",
                "plan_id": getattr(plan, "id", str(uuid.uuid4())),
                "estimated_steps": len(getattr(plan, "steps", [])),
                "max_risk_level": max(
                    [getattr(step, "estimated_risk", 0) for step in getattr(plan, "steps", {}).values()],
                    default=0
                ) if hasattr(plan, "steps") else 0
            }
    
    async def _enhance_context_with_semantics(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """
        Enhance context with semantic code information.
        
        Args:
            context: Task context
            
        Returns:
            Enhanced context
        """
        enhanced_context = dict(context)
        
        # Check if project root is available
        project_root = context.get("project_root")
        if not project_root:
            return enhanced_context
        
        try:
            # Get semantic analyzer and project state analyzer through API layer
            semantic_analyzer = get_semantic_analyzer()
            project_state_analyzer = get_project_state_analyzer()
            
            # Add semantic code information
            semantic_info = {
                "modules": [],
                "key_entities": []
            }
            
            # Get recently accessed files from context
            recent_files = []
            if "recent_files" in context and "accessed" in context["recent_files"]:
                recent_files = context["recent_files"]["accessed"]
            
            # Prioritize recently accessed files for semantic analysis
            for file_path in recent_files[:3]:  # Limit to 3 for performance
                module = await semantic_analyzer.analyze_file(file_path)
                if module:
                    semantic_info["modules"].append(module.get_summary())
                    
                    # Add key entities from this module
                    for func_name, func in module.functions.items():
                        semantic_info["key_entities"].append({
                            "name": func_name,
                            "type": "function",
                            "filename": func.filename,
                            "line_start": func.line_start,
                            "line_end": func.line_end
                        })
                    
                    for class_name, cls in module.classes.items():
                        semantic_info["key_entities"].append({
                            "name": class_name,
                            "type": "class",
                            "filename": cls.filename,
                            "line_start": cls.line_start,
                            "line_end": cls.line_end
                        })
            
            # Add project state information
            project_state = await project_state_analyzer.get_project_state(project_root)
            
            # Add the semantic information and project state to the context
            enhanced_context["semantic_code"] = semantic_info
            enhanced_context["project_state"] = project_state
            
            return enhanced_context
            
        except Exception as e:
            self._logger.error(f"Error enhancing context with semantics: {str(e)}")
            return enhanced_context

# Global semantic task planner instance
semantic_task_planner = SemanticTaskPlanner()
</file>

<file path="components/interfaces/__init__.py">
# angela/components/interfaces/__init__.py
"""Interfaces for Angela CLI components.

This package provides abstract base classes that define standardized interfaces
for various Angela CLI components, enabling separation of interface from implementation
and supporting dependency inversion.
"""

# Export core interfaces
from angela.components.interfaces.execution import CommandExecutor, AdaptiveExecutor
from angela.components.interfaces.safety import SafetyValidator
from angela.core import registry

__all__ = [
    'CommandExecutor',
    'AdaptiveExecutor',
    'SafetyValidator'
]
</file>

<file path="components/interfaces/execution.py">
# angela/components/interfaces/execution.py
"""Interfaces for execution components."""
from abc import ABC, abstractmethod
from typing import Dict, Any, Optional, Tuple

class CommandExecutor(ABC):
    """Interface for command execution."""
    
    @abstractmethod
    async def execute_command(
        self,
        command: str,
        check_safety: bool = True,
        dry_run: bool = False
    ) -> Tuple[str, str, int]:
        """
        Execute a shell command and return its output.
        
        Args:
            command: The shell command to execute
            check_safety: Whether to perform safety checks
            dry_run: Whether to simulate execution
            
        Returns:
            Tuple of (stdout, stderr, return_code)
        """
        pass

class AdaptiveExecutor(ABC):
    """Interface for adaptive command execution."""
    
    @abstractmethod
    async def execute_command(
        self,
        command: str,
        natural_request: str,
        explanation: Optional[str] = None,
        dry_run: bool = False
    ) -> Dict[str, Any]:
        """
        Execute a command with adaptive behavior.
        
        Args:
            command: The command to execute
            natural_request: The original natural language request
            explanation: AI explanation of what the command does
            dry_run: Whether to simulate the command without execution
            
        Returns:
            Dictionary with execution results
        """
        pass
</file>

<file path="components/interfaces/safety.py">
# angela/components/interfaces/safety.pyy
"""Interfaces for safety components."""
from abc import ABC, abstractmethod
from typing import Dict, Any, Tuple

class SafetyValidator(ABC):
    """Interface for safety validation."""
    
    @abstractmethod
    async def check_command_safety(self, command: str, dry_run: bool = False) -> bool:
        """
        Check if a command is safe to execute.
        
        Args:
            command: The shell command to check
            dry_run: Whether this is a dry run
            
        Returns:
            True if the command is safe and confirmed, False otherwise
        """
        pass

    @abstractmethod
    def validate_command_safety(self, command: str) -> Tuple[bool, str]:
        """
        Validate a command against safety rules.
        
        Args:
            command: The shell command to validate
            
        Returns:
            A tuple of (is_valid, error_message or None)
        """
        pass
</file>

<file path="components/monitoring/__init__.py">
# angela/components/monitoring/__init__.py
"""
Monitoring and proactive assistance for Angela CLI.

This package provides background monitoring capabilities that allow Angela
to offer proactive suggestions and assistance based on system state.
"""

from .background import background_monitor
from .network_monitor import network_monitor
from .proactive_assistant import proactive_assistant

__all__ = ['background_monitor', 'network_monitor', 'proactive_assistant']
</file>

<file path="components/monitoring/background.py">
# angela/monitoring/background.py
"""
Background monitoring and proactive suggestions for Angela CLI.

This module provides background monitoring of system state and user activities
to offer proactive assistance and suggestions.
"""
import os
import sys
import asyncio
import time
import re
import signal
import subprocess
from pathlib import Path
from typing import Dict, Any, Optional, List, Set, Callable, Awaitable
from datetime import datetime, timedelta

from angela.api.ai import get_gemini_request_class
from angela.api.context import get_context_manager
from angela.utils.logging import get_logger
from angela.api.shell import get_terminal_formatter
from angela.core.events import event_bus

logger = get_logger(__name__)

class BackgroundMonitor:
    """
    Background monitor for detecting potential issues and offering proactive assistance.
    
    Monitors:
    1. Git status changes
    2. Syntax errors in recently modified files
    3. System resource usage
    4. Process failures
    5. Common error patterns
    """
    
    def __init__(self):
        """Initialize the background monitor."""
        self._logger = logger
        self._monitoring_tasks = set()
        self._monitoring_active = False
        self._suggestions = set()  # To avoid repeating the same suggestions
        self._last_suggestion_time = datetime.now() - timedelta(hours=1)  # Ensure initial delay has passed
        self._suggestion_cooldown = timedelta(minutes=5)  # Minimum time between suggestions
        self._insight_callbacks = []  # Add this line to store callbacks
    
    def start_monitoring(self):
        """Start background monitoring tasks."""
        if self._monitoring_active:
            return
            
        self._monitoring_active = True
        
        # Create and start monitoring tasks
        self._create_monitoring_task(self._monitor_git_status(), "git_status")
        self._create_monitoring_task(self._monitor_file_changes(), "file_changes")
        self._create_monitoring_task(self._monitor_system_resources(), "system_resources")
        
        self._logger.info("Background monitoring started")
    
    def stop_monitoring(self):
        """Stop all background monitoring tasks."""
        if not self._monitoring_active:
            return
            
        self._monitoring_active = False
        
        # Cancel all running tasks
        for task in self._monitoring_tasks:
            if not task.done():
                task.cancel()
                
        self._monitoring_tasks.clear()
        self._logger.info("Background monitoring stopped")
    
    def _create_monitoring_task(self, coro: Awaitable, name: str) -> None:
        """
        Create and start a monitoring task.
        
        Args:
            coro: The coroutine to run as a task
            name: A name for the task (for logging)
        """
        task = asyncio.create_task(self._run_monitoring_task(coro, name))
        self._monitoring_tasks.add(task)
        task.add_done_callback(self._monitoring_tasks.discard)
    
    async def _run_monitoring_task(self, coro: Awaitable, name: str) -> None:
        """
        Run a monitoring task with error handling.
        
        Args:
            coro: The coroutine to run
            name: The task name
        """
        try:
            await coro
        except asyncio.CancelledError:
            self._logger.debug(f"Monitoring task {name} cancelled")
        except Exception as e:
            self._logger.exception(f"Error in monitoring task {name}: {str(e)}")
            
            # Restart the task after a delay
            await asyncio.sleep(30)
            if self._monitoring_active:
                self._logger.info(f"Restarting monitoring task {name}")
                if name == "git_status":
                    self._create_monitoring_task(self._monitor_git_status(), name)
                elif name == "file_changes":
                    self._create_monitoring_task(self._monitor_file_changes(), name)
                elif name == "system_resources":
                    self._create_monitoring_task(self._monitor_system_resources(), name)
    
    async def _monitor_git_status(self) -> None:
        """Monitor Git status in the current project."""
        from angela.api.context import get_context_manager
        from angela.api.shell import get_terminal_formatter
        
        self._logger.debug("Starting Git status monitoring")
        
        while self._monitoring_active:
            try:
                # Check if the current directory is a Git repository
                context = get_context_manager().get_context_dict()
                if not context.get("project_root"):
                    # No project detected, sleep and try again later
                    await asyncio.sleep(60)
                    continue
                
                project_root = Path(context["project_root"])
                git_dir = project_root / ".git"
                
                if not git_dir.exists():
                    # Not a Git repository, sleep and try again later
                    await asyncio.sleep(60)
                    continue
                
                # Check Git status
                result = await self._run_command("git status -s", cwd=str(project_root))
                
                if result["success"] and result["stdout"].strip():
                    # Check if this is different from the last status we saw
                    status_text = result["stdout"].strip()
                    
                    # Count changes
                    modified_count = status_text.count(" M ")
                    untracked_count = status_text.count("?? ")
                    deleted_count = status_text.count(" D ")
                    
                    # Analyze the status and suggest actions
                    if modified_count > 0 or untracked_count > 0 or deleted_count > 0:
                        suggestion_key = f"git_status:{modified_count}:{untracked_count}:{deleted_count}"
                        
                        if suggestion_key not in self._suggestions:
                            # Create a suggestion based on the status
                            suggestion = await self._generate_git_suggestion(
                                modified_count, 
                                untracked_count, 
                                deleted_count
                            )
                            
                            # Display the suggestion if possible
                            if suggestion and self._can_show_suggestion():
                                get_terminal_formatter().print_proactive_suggestion(suggestion, "Git Monitor")
                                self._suggestions.add(suggestion_key)
                                self._last_suggestion_time = datetime.now()
                                
                                await event_bus.publish("monitoring:git_status", {
                                    "suggestion": suggestion,
                                    "modified_count": modified_count,
                                    "untracked_count": untracked_count,
                                    "deleted_count": deleted_count,
                                    "timestamp": datetime.now().isoformat()
                                })        
                                                       
                                await self._notify_insight_callbacks("git_status", {
                                    "suggestion": suggestion,
                                    "modified_count": modified_count,
                                    "untracked_count": untracked_count,
                                    "deleted_count": deleted_count,
                                    "timestamp": datetime.now().isoformat()
                                })                                
                                
                
                # Wait before checking again
                await asyncio.sleep(60)
                
            except Exception as e:
                self._logger.exception(f"Error in Git status monitoring: {str(e)}")
                await asyncio.sleep(60)  # Wait before retrying
    
    async def _monitor_file_changes(self) -> None:
        """Monitor file changes for syntax errors and linting issues."""
        from angela.api.context import get_context_manager
        from angela.api.shell import get_terminal_formatter
        
        self._logger.debug("Starting file changes monitoring")
        
        # Track the last modified time of each file
        last_modified_times = {}
        
        while self._monitoring_active:
            try:
                # Get current project context
                context = get_context_manager().get_context_dict()
                if not context.get("project_root"):
                    # No project detected, sleep and try again later
                    await asyncio.sleep(30)
                    continue
                
                project_root = Path(context["project_root"])
                
                # Scan for files that have changed
                changed_files = []
                
                for file_path in self._find_source_files(project_root):
                    try:
                        mtime = file_path.stat().st_mtime
                        
                        # Check if this file is newly modified
                        if file_path in last_modified_times:
                            if mtime > last_modified_times[file_path]:
                                changed_files.append(file_path)
                                last_modified_times[file_path] = mtime
                        else:
                            # New file we haven't seen before
                            last_modified_times[file_path] = mtime
                    except (FileNotFoundError, PermissionError):
                        # File may have been deleted or is inaccessible
                        if file_path in last_modified_times:
                            del last_modified_times[file_path]
                
                # Check changed files for issues
                for file_path in changed_files:
                    # Get file info
                    file_info = get_context_manager().get_file_info(file_path)
                    
                    # Check file based on language
                    if file_info.get("language") == "Python":
                        await self._check_python_file(file_path)
                    elif file_info.get("language") == "JavaScript":
                        await self._check_javascript_file(file_path)
                    # Add more language checks as needed
                
                # Wait before checking again
                await asyncio.sleep(10)
                
            except Exception as e:
                self._logger.exception(f"Error in file changes monitoring: {str(e)}")
                await asyncio.sleep(30)  # Wait before retrying
    
    async def _monitor_system_resources(self) -> None:
        """Monitor system resources for potential issues."""
        from angela.api.shell import get_terminal_formatter
        
        self._logger.debug("Starting system resources monitoring")
        
        # Last values for comparison
        last_values = {
            "disk_usage": 0,
            "memory_usage": 0,
            "cpu_usage": 0
        }
        
        while self._monitoring_active:
            try:
                # Check disk space
                disk_usage = await self._get_disk_usage()
                if disk_usage > 90 and disk_usage > last_values["disk_usage"] + 5:
                    # Disk usage above 90% and increased by 5%
                    if self._can_show_suggestion():
                        suggestion = f"Your disk space is running low ({disk_usage}% used). Consider cleaning up unused files or moving data to free up space."
                        get_terminal_formatter().print_proactive_suggestion(suggestion, "System Monitor")
                        self._last_suggestion_time = datetime.now()
                        
                        # Publish as an event
                        await event_bus.publish("monitoring:disk_space_low", {
                            "suggestion": suggestion,
                            "disk_usage": disk_usage,
                            "timestamp": datetime.now().isoformat()
                        })
    
    
                        await self._notify_insight_callbacks("disk_space_low", {
                            "suggestion": suggestion,
                            "disk_usage": disk_usage,
                            "timestamp": datetime.now().isoformat()
                        })
    
                        
                last_values["disk_usage"] = disk_usage
                
                # Wait before checking again
                await asyncio.sleep(300)  # Check every 5 minutes
                
            except Exception as e:
                self._logger.exception(f"Error in system resources monitoring: {str(e)}")
                await asyncio.sleep(60)  # Wait before retrying
    
    async def _run_command(self, command: str, cwd: Optional[str] = None) -> Dict[str, Any]:
        """
        Run a shell command and return its output.
        
        Args:
            command: The command to run
            cwd: Optional working directory
            
        Returns:
            Dictionary with command results
        """
        try:
            process = await asyncio.create_subprocess_shell(
                command,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE,
                cwd=cwd
            )
            
            stdout, stderr = await process.communicate()
            
            return {
                "command": command,
                "stdout": stdout.decode('utf-8', errors='replace'),
                "stderr": stderr.decode('utf-8', errors='replace'),
                "return_code": process.returncode,
                "success": process.returncode == 0
            }
        except Exception as e:
            self._logger.error(f"Error running command '{command}': {str(e)}")
            return {
                "command": command,
                "stdout": "",
                "stderr": str(e),
                "return_code": -1,
                "success": False
            }
    
    def _find_source_files(self, base_dir: Path) -> List[Path]:
        """
        Find source code files in a directory.
        
        Args:
            base_dir: The base directory to search
            
        Returns:
            List of file paths
        """
        source_files = []
        
        # File extensions to look for
        extensions = {
            ".py", ".js", ".ts", ".java", ".c", ".cpp", ".h", ".hpp",
            ".rs", ".go", ".rb", ".php", ".html", ".css", ".jsx", ".tsx"
        }
        
        # Directories to ignore
        ignore_dirs = {
            "__pycache__", "node_modules", ".git", "venv", "env",
            "build", "dist", "target", ".idea", ".vscode"
        }
        
        try:
            for root, dirs, files in os.walk(base_dir):
                # Skip ignored directories
                dirs[:] = [d for d in dirs if d not in ignore_dirs]
                
                for file in files:
                    file_ext = os.path.splitext(file)[1]
                    if file_ext in extensions:
                        source_files.append(Path(os.path.join(root, file)))
        except Exception as e:
            self._logger.error(f"Error finding source files: {str(e)}")
        
        return source_files
    
    async def _check_python_file(self, file_path: Path) -> None:
        """
        Check a Python file for syntax errors or linting issues.
        
        Args:
            file_path: Path to the Python file
        """
        from angela.api.shell import get_terminal_formatter
        
        # Check for syntax errors
        result = await self._run_command(f"python -m py_compile {file_path}")
        
        if not result["success"]:
            # Found a syntax error, generate a suggestion
            error_text = result["stderr"]
            suggestion_key = f"python_syntax:{file_path}"
            
            if suggestion_key not in self._suggestions and self._can_show_suggestion():
                # Extract the error message
                match = re.search(r"SyntaxError: (.*)", error_text)
                error_msg = match.group(1) if match else "syntax error"
                
                suggestion = f"Syntax error detected in {file_path.name}: {error_msg}"
                get_terminal_formatter().print_proactive_suggestion(suggestion, "File Monitor")
                self._suggestions.add(suggestion_key)
                self._last_suggestion_time = datetime.now()
                
                # Publish as an event
                await event_bus.publish("monitoring:python_syntax_error", {
                    "suggestion": suggestion,
                    "file_path": str(file_path),
                    "error_message": error_msg,
                    "timestamp": datetime.now().isoformat()
                })
    
                await self._notify_insight_callbacks("python_syntax_error", {
                    "suggestion": suggestion,
                    "file_path": str(file_path),
                    "error_message": error_msg,
                    "timestamp": datetime.now().isoformat()
                })
    
      
        # Check with flake8 if available
        flake8_result = await self._run_command(f"flake8 {file_path}")
        
        if flake8_result["success"] and flake8_result["stdout"].strip():
            # Found linting issues
            suggestion_key = f"python_lint:{file_path}"
            
            if suggestion_key not in self._suggestions and self._can_show_suggestion():
                lint_issues = flake8_result["stdout"].strip().count('\n') + 1
                suggestion = f"Found {lint_issues} linting issues in {file_path.name}"
                get_terminal_formatter().print_proactive_suggestion(suggestion, "File Monitor")
                self._suggestions.add(suggestion_key)
                self._last_suggestion_time = datetime.now()
                
                # Publish as an event
                await event_bus.publish("monitoring:python_lint_issue", {
                    "suggestion": suggestion,
                    "file_path": str(file_path),
                    "lint_issues": lint_issues,
                    "timestamp": datetime.now().isoformat()
                })
    
    async def _check_javascript_file(self, file_path: Path) -> None:
        """
        Check a JavaScript file for syntax errors or linting issues.
        
        Args:
            file_path: Path to the JavaScript file
        """
        from angela.api.shell import get_terminal_formatter
        
        # Check for syntax errors with Node.js
        result = await self._run_command(f"node --check {file_path}")
        
        if not result["success"]:
            # Found a syntax error, generate a suggestion
            error_text = result["stderr"]
            suggestion_key = f"js_syntax:{file_path}"
            
            if suggestion_key not in self._suggestions and self._can_show_suggestion():
                suggestion = f"Syntax error detected in {file_path.name}"
                get_terminal_formatter().print_proactive_suggestion(suggestion, "File Monitor")
                self._suggestions.add(suggestion_key)
                self._last_suggestion_time = datetime.now()
                
                # Publish as an event
                await event_bus.publish("monitoring:javascript_syntax_error", {
                    "suggestion": suggestion,
                    "file_path": str(file_path),
                    "error_text": error_text,
                    "timestamp": datetime.now().isoformat()
                })
        
        # Check with ESLint if available
        eslint_result = await self._run_command(f"eslint {file_path}")
        
        if eslint_result["success"] and eslint_result["stdout"].strip():
            # Found linting issues
            suggestion_key = f"js_lint:{file_path}"
            
            if suggestion_key not in self._suggestions and self._can_show_suggestion():
                lint_issues = eslint_result["stdout"].strip().count('\n') + 1
                suggestion = f"Found {lint_issues} linting issues in {file_path.name}"
                get_terminal_formatter().print_proactive_suggestion(suggestion, "File Monitor")
                self._suggestions.add(suggestion_key)
                self._last_suggestion_time = datetime.now()
                
                # Publish as an event
                await event_bus.publish("monitoring:javascript_lint_issue", {
                    "suggestion": suggestion,
                    "file_path": str(file_path),
                    "lint_issues": lint_issues,
                    "timestamp": datetime.now().isoformat()
                })
    
    async def _get_disk_usage(self) -> float:
        """
        Get disk usage percentage for the current directory.
        
        Returns:
            Disk usage percentage (0-100)
        """
        if sys.platform == "win32":
            # Windows
            result = await self._run_command("wmic logicaldisk get freespace,size")
            if result["success"]:
                lines = result["stdout"].strip().split('\n')
                if len(lines) >= 2:
                    values = lines[1].split()
                    if len(values) >= 2:
                        try:
                            free_space = int(values[0])
                            total_size = int(values[1])
                            return 100 - (free_space / total_size * 100)
                        except (ValueError, IndexError):
                            pass
            return 0
        else:
            # Unix-like
            result = await self._run_command("df -h .")
            if result["success"]:
                lines = result["stdout"].strip().split('\n')
                if len(lines) >= 2:
                    values = lines[1].split()
                    if len(values) >= 5:
                        try:
                            usage = values[4].rstrip('%')
                            return float(usage)
                        except (ValueError, IndexError):
                            pass
            return 0
    
    async def _generate_git_suggestion(
        self, 
        modified_count: int, 
        untracked_count: int, 
        deleted_count: int
    ) -> Optional[str]:
        """
        Generate a suggestion based on Git status.
        
        Args:
            modified_count: Number of modified files
            untracked_count: Number of untracked files
            deleted_count: Number of deleted files
            
        Returns:
            A suggestion string, or None if no suggestion is needed
        """
        total_changes = modified_count + untracked_count + deleted_count
        
        if total_changes <= 0:
            return None
            
        if total_changes > 10:
            return f"You have {total_changes} uncommitted changes in your Git repository. Consider committing your changes to avoid losing work."
            
        if modified_count > 0 and untracked_count > 0:
            return f"You have {modified_count} modified files and {untracked_count} untracked files. Consider using 'git add' to stage changes and 'git commit' to save your work."
            
        if modified_count > 0:
            return f"You have {modified_count} modified files that aren't committed. Use 'git commit' to save your changes."
            
        if untracked_count > 0:
            return f"You have {untracked_count} untracked files. Use 'git add' to begin tracking them."
            
        if deleted_count > 0:
            return f"You have {deleted_count} deleted files that haven't been committed. Use 'git commit' to record these deletions."
            
        return None
    
    def _can_show_suggestion(self) -> bool:
        """
        Check if we can show a suggestion now (respecting cooldown period).
        
        Returns:
            True if a suggestion can be shown, False otherwise
        """
        now = datetime.now()
        return (now - self._last_suggestion_time) >= self._suggestion_cooldown

    def register_insight_callback(self, callback):
        """
        Register a callback function to be called when an insight is generated.
        
        Args:
            callback: Async function to call with insight_type and insight_data
        """
        self._logger.debug(f"Registering insight callback: {callback.__name__}")
        self._insight_callbacks.append(callback)
    
    def unregister_insight_callback(self, callback):
        """
        Unregister a previously registered callback function.
        
        Args:
            callback: The callback function to unregister
        """
        if callback in self._insight_callbacks:
            self._logger.debug(f"Unregistering insight callback: {callback.__name__}")
            self._insight_callbacks.remove(callback)
    
    async def _notify_insight_callbacks(self, insight_type, insight_data):
        """
        Notify all registered callbacks about a new insight.
        
        Args:
            insight_type: Type of insight
            insight_data: Insight data
        """
        for callback in self._insight_callbacks:
            try:
                await callback(insight_type, insight_data)
            except Exception as e:
                self._logger.error(f"Error in insight callback: {str(e)}")


# Global background monitor instance
background_monitor = BackgroundMonitor()
</file>

<file path="components/monitoring/network_monitor.py">
# angela/monitoring/network_monitor.py

import asyncio
import time
import socket
import os
import re
import json
from pathlib import Path
from typing import Dict, Any, List, Set, Optional, Tuple
from datetime import datetime, timedelta

from angela.config import config_manager
from angela.utils.logging import get_logger
from angela.api.shell import get_terminal_formatter
from angela.api.context import get_context_manager

logger = get_logger(__name__)

class NetworkMonitor:
    """
    Network monitoring for services, dependencies, and connections.
    
    Monitors:
    1. Local service health (e.g., web servers, databases)
    2. External API status
    3. Network connectivity
    4. Dependency update availability
    """
    
    def __init__(self):
        """Initialize the network monitor."""
        self._logger = logger
        self._monitoring_tasks = set()
        self._monitoring_active = False
        self._suggestions = set()
        self._last_suggestion_time = datetime.now() - timedelta(hours=1)
        self._suggestion_cooldown = timedelta(minutes=15)
        self._insight_callbacks = []  
        
    def start_monitoring(self):
        """Start network monitoring tasks."""
        if self._monitoring_active:
            return
            
        self._monitoring_active = True
        
        # Create and start monitoring tasks
        self._create_monitoring_task(self._monitor_local_services(), "local_services")
        self._create_monitoring_task(self._monitor_dependency_updates(), "dependency_updates")
        self._create_monitoring_task(self._monitor_network_connectivity(), "network_connectivity")
        
        self._logger.info("Network monitoring started")
    
    def stop_monitoring(self):
        """Stop all network monitoring tasks."""
        if not self._monitoring_active:
            return
            
        self._monitoring_active = False
        
        # Cancel all running tasks
        for task in self._monitoring_tasks:
            if not task.done():
                task.cancel()
                
        self._monitoring_tasks.clear()
        self._logger.info("Network monitoring stopped")
    
    def _create_monitoring_task(self, coro, name):
        """Create and start a monitoring task."""
        task = asyncio.create_task(self._run_monitoring_task(coro, name))
        self._monitoring_tasks.add(task)
        task.add_done_callback(self._monitoring_tasks.discard)
    
    async def _run_monitoring_task(self, coro, name):
        """Run a monitoring task with error handling."""
        try:
            await coro
        except asyncio.CancelledError:
            self._logger.debug(f"Network monitoring task {name} cancelled")
        except Exception as e:
            self._logger.exception(f"Error in network monitoring task {name}: {str(e)}")
            
            # Restart the task after a delay
            await asyncio.sleep(60)
            if self._monitoring_active:
                self._logger.info(f"Restarting network monitoring task {name}")
                if name == "local_services":
                    self._create_monitoring_task(self._monitor_local_services(), name)
                elif name == "dependency_updates":
                    self._create_monitoring_task(self._monitor_dependency_updates(), name)
                elif name == "network_connectivity":
                    self._create_monitoring_task(self._monitor_network_connectivity(), name)
    
    async def _monitor_local_services(self) -> None:
        """Monitor local services like web servers and databases."""
        from angela.api.context import get_context_manager
        from angela.api.shell import get_terminal_formatter
        
        self._logger.debug("Starting local services monitoring")
        
        # Track service status to detect changes
        service_status = {}
        
        while self._monitoring_active:
            try:
                # Get current project context
                context = get_context_manager().get_context_dict()
                project_type = context.get("project_type")
                
                # Detect potential services based on project type
                services_to_check = self._detect_project_services(project_type)
                
                # Check each service
                for service_name, service_info in services_to_check.items():
                    status = await self._check_service_status(service_info)
                    
                    # Compare with previous status
                    prev_status = service_status.get(service_name, {}).get("status")
                    if prev_status is not None and prev_status != status["status"]:
                        # Status changed
                        if status["status"] == "down" and self._can_show_suggestion():
                            suggestion = f"Service '{service_name}' appears to be down. {status.get('message', '')}"
                            get_terminal_formatter().print_proactive_suggestion(suggestion, "Network Monitor")
                            self._last_suggestion_time = datetime.now()
                    
                    # Update status
                    service_status[service_name] = status
                
                # Wait before checking again
                await asyncio.sleep(60)
                
            except Exception as e:
                self._logger.exception(f"Error monitoring local services: {str(e)}")
                await asyncio.sleep(120)  # Wait before retrying
    
    async def _monitor_dependency_updates(self) -> None:
        """Monitor for available updates to project dependencies."""
        from angela.api.context import get_context_manager
        from angela.api.shell import get_terminal_formatter
        
        self._logger.debug("Starting dependency updates monitoring")
        
        # Track which dependencies we've already notified about
        notified_updates = set()
        
        while self._monitoring_active:
            try:
                # Get current project context
                context = get_context_manager().get_context_dict()
                project_root = context.get("project_root")
                project_type = context.get("project_type")
                
                if not project_root:
                    # No project detected, sleep and try again later
                    await asyncio.sleep(3600)  # Check every hour
                    continue
                
                # Check dependencies based on project type
                if project_type == "python":
                    updates = await self._check_python_dependencies(Path(project_root))
                elif project_type == "node":
                    updates = await self._check_node_dependencies(Path(project_root))
                else:
                    # Unknown project type, sleep and try again later
                    await asyncio.sleep(3600)
                    continue
                
                # Notify about new updates
                if updates and self._can_show_suggestion():
                    # Filter out already notified updates
                    new_updates = [u for u in updates if f"{u['name']}:{u['new_version']}" not in notified_updates]
                    
                    if new_updates:
                        count = len(new_updates)
                        pkg_list = ", ".join([f"{u['name']} ({u['current_version']} → {u['new_version']})" 
                                             for u in new_updates[:3]])
                        more = f" and {count - 3} more" if count > 3 else ""
                        
                        suggestion = f"Found {count} dependency updates available: {pkg_list}{more}"
                        get_terminal_formatter().print_proactive_suggestion(suggestion, "Dependency Monitor")
                        
                        # Mark as notified
                        for update in new_updates:
                            notified_updates.add(f"{update['name']}:{update['new_version']}")
                        
                        self._last_suggestion_time = datetime.now()
                
                # Wait before checking again (dependencies don't change often)
                await asyncio.sleep(86400)  # Check once per day
                
            except Exception as e:
                self._logger.exception(f"Error monitoring dependency updates: {str(e)}")
                await asyncio.sleep(3600)  # Wait before retrying
    
    async def _monitor_network_connectivity(self) -> None:
        """Monitor network connectivity to important services."""
        from angela.api.shell import get_terminal_formatter
        
        self._logger.debug("Starting network connectivity monitoring")
        
        # Track connectivity status to detect changes
        connectivity_status = {
            "internet": True,  # Assume connected initially
            "last_check": datetime.now()
        }
        
        while self._monitoring_active:
            try:
                # Check internet connectivity
                internet_status = await self._check_internet_connectivity()
                
                # Check if status changed
                if connectivity_status["internet"] != internet_status["connected"]:
                    if not internet_status["connected"] and self._can_show_suggestion():
                        suggestion = f"Internet connectivity appears to be down. {internet_status.get('message', '')}"
                        get_terminal_formatter().print_proactive_suggestion(suggestion, "Network Monitor")
                        self._last_suggestion_time = datetime.now()
                    elif internet_status["connected"] and not connectivity_status["internet"]:
                        # Internet connection restored
                        elapsed = datetime.now() - connectivity_status["last_check"]
                        if elapsed > timedelta(minutes=5) and self._can_show_suggestion():
                            suggestion = "Internet connectivity has been restored."
                            get_terminal_formatter().print_proactive_suggestion(suggestion, "Network Monitor")
                            self._last_suggestion_time = datetime.now()
                
                # Update status
                connectivity_status["internet"] = internet_status["connected"]
                connectivity_status["last_check"] = datetime.now()
                
                # Wait before checking again
                await asyncio.sleep(30)
                
            except Exception as e:
                self._logger.exception(f"Error monitoring network connectivity: {str(e)}")
                await asyncio.sleep(60)  # Wait before retrying
                
    def _detect_project_services(self, project_type: Optional[str]) -> Dict[str, Dict[str, Any]]:
        """
        Detect services to monitor based on project type.
        
        Args:
            project_type: The type of project
            
        Returns:
            Dictionary of service information
        """
        services = {}
        
        # Default services to check
        services["localhost:8000"] = {
            "host": "localhost",
            "port": 8000,
            "name": "Web Server (8000)",
            "type": "http"
        }
        
        # Add services based on project type
        if project_type == "node":
            services["localhost:3000"] = {
                "host": "localhost",
                "port": 3000,
                "name": "Node.js Server",
                "type": "http"
            }
        elif project_type == "python":
            services["localhost:5000"] = {
                "host": "localhost",
                "port": 5000,
                "name": "Flask Server",
                "type": "http"
            }
            services["localhost:8000"] = {
                "host": "localhost",
                "port": 8000,
                "name": "Django Server",
                "type": "http"
            }
        
        # Always check database ports
        services["localhost:5432"] = {
            "host": "localhost",
            "port": 5432,
            "name": "PostgreSQL",
            "type": "tcp"
        }
        services["localhost:3306"] = {
            "host": "localhost",
            "port": 3306,
            "name": "MySQL",
            "type": "tcp"
        }
        services["localhost:27017"] = {
            "host": "localhost",
            "port": 27017,
            "name": "MongoDB",
            "type": "tcp"
        }
        services["localhost:6379"] = {
            "host": "localhost",
            "port": 6379,
            "name": "Redis",
            "type": "tcp"
        }
        
        return services
    
    async def _check_service_status(self, service_info: Dict[str, Any]) -> Dict[str, Any]:
        """
        Check the status of a service.
        
        Args:
            service_info: Service information
            
        Returns:
            Status information
        """
        host = service_info.get("host", "localhost")
        port = service_info.get("port", 80)
        service_type = service_info.get("type", "tcp")
        
        # Basic port check
        try:
            # Create a socket
            sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            sock.settimeout(2.0)  # 2 second timeout
            
            # Try to connect
            result = sock.connect_ex((host, port))
            sock.close()
            
            if result == 0:
                # Port is open
                if service_type == "http":
                    # For HTTP services, try to get a response
                    try:
                        import aiohttp
                        async with aiohttp.ClientSession() as session:
                            async with session.get(f"http://{host}:{port}/", timeout=5) as response:
                                if response.status < 400:
                                    return {"status": "up", "message": f"HTTP status: {response.status}"}
                                else:
                                    return {"status": "error", "message": f"HTTP error: {response.status}"}
                    except Exception as e:
                        return {"status": "error", "message": f"HTTP error: {str(e)}"}
                else:
                    # TCP service is up
                    return {"status": "up", "message": "Port is open"}
            else:
                # Port is closed
                return {"status": "down", "message": "Port is closed"}
                
        except Exception as e:
            return {"status": "error", "message": f"Error checking service: {str(e)}"}
    
    async def _check_python_dependencies(self, project_root: Path) -> List[Dict[str, Any]]:
        """
        Check for updates to Python dependencies.
        
        Args:
            project_root: The project root directory
            
        Returns:
            List of available updates
        """
        requirements_path = project_root / "requirements.txt"
        
        if not requirements_path.exists():
            return []
        
        try:
            # Run pip list --outdated
            result = await self._run_command("pip list --outdated --format=json")
            
            if not result["success"]:
                return []
                
            # Parse the output
            outdated = json.loads(result["stdout"])
            
            # Format the updates
            updates = []
            for pkg in outdated:
                updates.append({
                    "name": pkg["name"],
                    "current_version": pkg["version"],
                    "new_version": pkg["latest_version"],
                    "type": "python"
                })
                
            return updates
            
        except Exception as e:
            self._logger.error(f"Error checking Python dependencies: {str(e)}")
            return []
    
    async def _check_node_dependencies(self, project_root: Path) -> List[Dict[str, Any]]:
        """
        Check for updates to Node.js dependencies.
        
        Args:
            project_root: The project root directory
            
        Returns:
            List of available updates
        """
        package_json_path = project_root / "package.json"
        
        if not package_json_path.exists():
            return []
        
        try:
            # Run npm outdated --json
            result = await self._run_command("npm outdated --json", cwd=str(project_root))
            
            if not result["success"] and not result["stdout"]:
                return []
                
            # Parse the output
            try:
                outdated = json.loads(result["stdout"])
            except json.JSONDecodeError:
                # npm outdated returns non-zero exit code when updates are available
                if not result["stdout"]:
                    return []
                outdated = {}
            
            # Format the updates
            updates = []
            for pkg_name, pkg_info in outdated.items():
                updates.append({
                    "name": pkg_name,
                    "current_version": pkg_info.get("current", "unknown"),
                    "new_version": pkg_info.get("latest", "unknown"),
                    "type": "npm"
                })
                
            return updates
            
        except Exception as e:
            self._logger.error(f"Error checking Node.js dependencies: {str(e)}")
            return []
    
    async def _check_internet_connectivity(self) -> Dict[str, Any]:
        """
        Check internet connectivity.
        
        Returns:
            Status information
        """
        # List of reliable domains to check
        check_domains = [
            "google.com",
            "cloudflare.com",
            "amazon.com",
            "microsoft.com"
        ]
        
        successes = 0
        failures = 0
        
        for domain in check_domains:
            try:
                # Try to resolve the domain
                await asyncio.get_event_loop().getaddrinfo(domain, 80)
                successes += 1
            except socket.gaierror:
                failures += 1
        
        # Consider internet connected if at least half of the checks succeeded
        connected = successes >= len(check_domains) / 2
        
        return {
            "connected": connected,
            "message": f"{successes}/{len(check_domains)} connectivity checks succeeded"
        }
    
    async def _run_command(self, command: str, cwd: Optional[str] = None) -> Dict[str, Any]:
        """
        Run a shell command and return its output.
        
        Args:
            command: The command to run
            cwd: Optional working directory
            
        Returns:
            Dictionary with command results
        """
        try:
            process = await asyncio.create_subprocess_shell(
                command,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE,
                cwd=cwd
            )
            
            stdout, stderr = await process.communicate()
            
            return {
                "command": command,
                "stdout": stdout.decode('utf-8', errors='replace'),
                "stderr": stderr.decode('utf-8', errors='replace'),
                "return_code": process.returncode,
                "success": process.returncode == 0
            }
        except Exception as e:
            self._logger.error(f"Error running command '{command}': {str(e)}")
            return {
                "command": command,
                "stdout": "",
                "stderr": str(e),
                "return_code": -1,
                "success": False
            }
    
    def _can_show_suggestion(self) -> bool:
        """
        Check if we can show a suggestion now (respecting cooldown period).
        
        Returns:
            True if a suggestion can be shown, False otherwise
        """
        now = datetime.now()
        return (now - self._last_suggestion_time) >= self._suggestion_cooldown


    def register_insight_callback(self, callback):
        """
        Register a callback function to be called when an insight is generated.
        
        Args:
            callback: Async function to call with insight_type and insight_data
        """
        self._logger.debug(f"Registering insight callback: {callback.__name__}")
        self._insight_callbacks.append(callback)
    
    def unregister_insight_callback(self, callback):
        """
        Unregister a previously registered callback function.
        
        Args:
            callback: The callback function to unregister
        """
        if callback in self._insight_callbacks:
            self._logger.debug(f"Unregistering insight callback: {callback.__name__}")
            self._insight_callbacks.remove(callback)
    
    async def _notify_insight_callbacks(self, insight_type, insight_data):
        """
        Notify all registered callbacks about a new insight.
        
        Args:
            insight_type: Type of insight
            insight_data: Insight data
        """
        for callback in self._insight_callbacks:
            try:
                await callback(insight_type, insight_data)
            except Exception as e:
                self._logger.error(f"Error in insight callback: {str(e)}")




# Global network monitor instance
network_monitor = NetworkMonitor()
</file>

<file path="components/monitoring/notification_handler.py">
# angela/monitoring/notification_handler.py

"""
Handles notifications from shell hooks for Angela CLI.
"""
import asyncio
import re
from typing import Dict, Any, List, Optional
from pathlib import Path
import logging
import sys

from angela.utils.logging import get_logger
from angela.api.context import get_context_manager
from angela.api.context import get_session_manager
from angela.api.context import get_file_activity_tracker
from angela.api.monitoring import get_background_monitor
from angela.api.shell import get_inline_feedback

logger = get_logger(__name__)

class NotificationHandler:
    """
    Handles notifications from shell hooks.
    
    This class processes notifications sent by the shell integration hooks like
    command pre-execution, post-execution, and directory changes.
    """
    
    def __init__(self):
        """Initialize the notification handler."""
        self._logger = logger
        # Store currently running long commands for monitoring
        self._running_commands = {}
        # Track command execution times for performance insights
        self._command_times = {}
        # Track recent directories for context enhancement
        self._recent_directories = []
        # Maximum number of directories to track
        self._max_directories = 10
        # Track commonly failing commands for better suggestions
        self._command_errors = {}
        # Maximum command errors to track
        self._max_errors_per_command = 5
    
    async def handle_notification(self, notification_type: str, *args) -> None:
        """
        Handle a notification from the shell hooks.
        
        Args:
            notification_type: Type of notification (pre_exec, post_exec, dir_change)
            args: Additional arguments for the notification
        """
        self._logger.debug(f"Received notification: {notification_type} with args: {args}")
        
        if notification_type == "pre_exec":
            await self._handle_pre_exec(args[0] if args else "")
        elif notification_type == "post_exec":
            await self._handle_post_exec(
                command=args[0] if len(args) > 0 else "",
                exit_code=int(args[1]) if len(args) > 1 else 0,
                duration=int(args[2]) if len(args) > 2 else 0,
                stderr=args[3] if len(args) > 3 else ""
            )
        elif notification_type == "dir_change":
            await self._handle_dir_change(args[0] if args else "")
    
    async def _handle_pre_exec(self, command: str) -> None:
        """
        Handle command pre-execution notification.
        
        Args:
            command: The command about to be executed
        """
        from angela.api.context import get_session_manager, get_context_manager
        from angela.api.monitoring import get_background_monitor
        
        if not command:
            return
            
        # Update session context
        get_session_manager().add_entity("current_command", "command", command)
        
        # Record start time for performance tracking
        self._running_commands[command] = {
            "start_time": asyncio.get_event_loop().time(),
            "cwd": str(get_context_manager().cwd)
        }
        
        # Log the command for later analysis
        self._logger.info(f"Command started: {command}")
        
        # If it's a potentially long-running command, start monitoring
        if _is_long_running_command(command):
            get_background_monitor().start_command_monitoring(command)
    
    async def _handle_post_exec(self, command: str, exit_code: int, duration: int, stderr: str = "") -> None:
        """
        Handle command post-execution notification.
        
        Args:
            command: The executed command
            exit_code: The command's exit code
            duration: Execution duration in seconds
            stderr: Standard error output (if available)
        """
        from angela.api.context import get_session_manager
        from angela.api.monitoring import get_background_monitor
        from angela.api.shell import get_inline_feedback
        
        if not command:
            return
            
        # Update command statistics
        cmd_base = _extract_base_command(command)
        if cmd_base not in self._command_times:
            self._command_times[cmd_base] = {"count": 0, "total_time": 0, "failures": 0}
        
        self._command_times[cmd_base]["count"] += 1
        self._command_times[cmd_base]["total_time"] += duration
        if exit_code != 0:
            self._command_times[cmd_base]["failures"] += 1
            
            # Track error information for this command
            if cmd_base not in self._command_errors:
                self._command_errors[cmd_base] = []
                
            # Store error information with limited history
            self._command_errors[cmd_base].append({
                "command": command,
                "stderr": stderr,
                "exit_code": exit_code,
                "timestamp": asyncio.get_event_loop().time()
            })
            
            # Trim error history if needed
            if len(self._command_errors[cmd_base]) > self._max_errors_per_command:
                self._command_errors[cmd_base].pop(0)
        
        # Clean up running commands
        if command in self._running_commands:
            del self._running_commands[command]
        
        # Stop monitoring if it was a long-running command
        if _is_long_running_command(command):
            get_background_monitor().stop_command_monitoring(command)
        
        # Add to session recent commands
        get_session_manager().add_command(command)
        get_session_manager().add_entity("last_exit_code", "exit_code", exit_code)
        if stderr:
            get_session_manager().add_entity("last_stderr", "error_output", stderr)
        
        # If command failed, store it for potential automatic fixes
        if exit_code != 0:
            get_session_manager().add_entity("last_failed_command", "command", command)
            
            # Analyze the failed command and show proactive suggestions
            fix_suggestion = await self._analyze_failed_command(command, stderr)
            if fix_suggestion:
                # Trigger a proactive suggestion in the terminal
                await get_inline_feedback().show_message(
                    f"Command failed. Suggestion: {fix_suggestion}",
                    message_type="warning",
                    timeout=15
                )
    
    async def _handle_dir_change(self, new_dir: str) -> None:
        """
        Handle directory change notification.
        
        Args:
            new_dir: The new current directory
        """
        from angela.api.context import get_context_manager
        from angela.api.context import get_session_manager
        from angela.api.shell import get_inline_feedback
        
        if not new_dir:
            return
            
        # Update context manager with new directory
        get_context_manager().refresh_context()
        
        # Add to recent directories
        if new_dir not in self._recent_directories:
            self._recent_directories.insert(0, new_dir)
            # Trim to max size
            if len(self._recent_directories) > self._max_directories:
                self._recent_directories = self._recent_directories[:self._max_directories]
        
        # Update session context
        get_session_manager().add_entity("current_directory", "directory", new_dir)
        get_session_manager().add_entity("recent_directories", "directories", self._recent_directories)
        
        # If moving to a project directory, refresh project context
        project_root = get_context_manager().project_root
        if project_root:
            get_session_manager().add_entity("project_root", "directory", str(project_root))
            
            # If this is a new project, show a helpful message
            if str(project_root) not in self._recent_directories[1:]:
                project_type = get_context_manager().project_type
                if project_type:
                    await get_inline_feedback().show_message(
                        f"Detected {project_type.capitalize()} project. Type 'angela help-with {project_type}' for project-specific assistance.",
                        message_type="info",
                        timeout=5
                    )

    async def _analyze_failed_command(self, command: str, stderr: str = "") -> Optional[str]:
        """
        Analyze a failed command to generate a fix suggestion.
        
        Args:
            command: The failed command
            stderr: Standard error output (if available)
            
        Returns:
            A suggestion string or None if no suggestion is available
        """
        from angela.api.context import get_session_manager, get_history_manager
        
        # Get exit code from session if stderr not provided
        if not stderr:
            stderr_entity = get_session_manager().get_entity("last_stderr")
            stderr = stderr_entity.get("value", "") if stderr_entity else ""
        
        # Common error patterns and suggested fixes
        error_patterns = [
            # Git errors
            {
                "pattern": "fatal: could not read Username",
                "command_pattern": "git push",
                "suggestion": "Try setting up SSH keys or use a credential helper: git config --global credential.helper cache"
            },
            {
                "pattern": "fatal: not a git repository",
                "command_pattern": "git",
                "suggestion": "Initialize a git repository first: git init"
            },
            {
                "pattern": "error: failed to push some refs",
                "command_pattern": "git push",
                "suggestion": "Pull changes first: git pull --rebase"
            },
            {
                "pattern": "CONFLICT",
                "command_pattern": "git merge",
                "suggestion": "Resolve merge conflicts and then commit the changes"
            },
            {
                "pattern": "error: Your local changes to the following files would be overwritten by merge",
                "command_pattern": "git pull",
                "suggestion": "Stash your changes first: git stash"
            },
            
            # Python/pip errors
            {
                "pattern": "No module named",
                "command_pattern": "python",
                "suggestion": "Install the missing module with pip: pip install [module_name]"
            },
            {
                "pattern": "ModuleNotFoundError",
                "command_pattern": "python",
                "suggestion": "Install the missing module with pip: pip install [module_name]"
            },
            {
                "pattern": "Could not find a version that satisfies the requirement",
                "command_pattern": "pip install",
                "suggestion": "Check the package name or try with a specific version"
            },
            {
                "pattern": "SyntaxError",
                "command_pattern": "python",
                "suggestion": "Fix the syntax error in your Python file"
            },
            
            # NPM errors
            {
                "pattern": "npm ERR! code ENOENT",
                "command_pattern": "npm",
                "suggestion": "Check if package.json exists in the current directory"
            },
            {
                "pattern": "npm ERR! code E404",
                "command_pattern": "npm install",
                "suggestion": "Package not found. Check the package name and registry"
            },
            {
                "pattern": "Missing script:",
                "command_pattern": "npm run",
                "suggestion": "The script does not exist in package.json. Check available scripts with: npm run"
            },
            
            # Docker errors
            {
                "pattern": "Error response from daemon",
                "command_pattern": "docker",
                "suggestion": "Check if docker daemon is running: systemctl start docker"
            },
            {
                "pattern": "image not found",
                "command_pattern": "docker",
                "suggestion": "Pull the image first: docker pull [image_name]"
            },
            
            # Permission errors
            {
                "pattern": "Permission denied",
                "suggestion": "Try running with sudo or check file permissions"
            },
            
            # Make errors
            {
                "pattern": "No rule to make target",
                "command_pattern": "make",
                "suggestion": "Check your Makefile for the correct target names"
            },
            
            # Generic command not found
            {
                "pattern": "command not found",
                "suggestion": "Install the required package or check the command spelling"
            }
        ]
        
        # Check for specific error patterns in stderr
        for pattern in error_patterns:
            error_pattern = pattern["pattern"]
            cmd_pattern = pattern.get("command_pattern", "")
            
            # Skip if command pattern doesn't match
            if cmd_pattern and cmd_pattern not in command:
                continue
                
            if error_pattern in stderr:
                suggestion = pattern["suggestion"]
                
                # Extract module name if applicable
                if "[module_name]" in suggestion and "No module named " in stderr:
                    module_match = re.search(r"No module named '([^']+)'", stderr)
                    if module_match:
                        module_name = module_match.group(1)
                        suggestion = suggestion.replace("[module_name]", module_name)
                
                # Extract image name if applicable
                if "[image_name]" in suggestion and "image not found" in stderr:
                    image_match = re.search(r"[Ee]rror.*image (.*): not found", stderr)
                    if image_match:
                        image_name = image_match.group(1)
                        suggestion = suggestion.replace("[image_name]", image_name)
                
                return suggestion
        
        # Learning from command history
        cmd_base = _extract_base_command(command)
        
        # Check if we have similar past failures
        if cmd_base in self._command_errors:
            # Look for successful commands that followed the same failed command
            similar_commands = get_history_manager().search_similar_command(command)
            if similar_commands and similar_commands.get("success", False):
                return f"Previously successful command: {similar_commands['command']}"
        
        # Fallback suggestions based on command type
        fallback_suggestions = {
            "git": "Check git status and repository configuration",
            "npm": "Verify package.json is valid and node_modules is not corrupted",
            "pip": "Check your Python environment and package requirements",
            "python": "Verify your Python code syntax and imported modules",
            "docker": "Ensure Docker daemon is running and you have sufficient permissions",
            "make": "Check Makefile syntax and required dependencies",
            "yarn": "Verify package.json and yarn.lock files",
            "cargo": "Check your Rust project configuration",
            "mvn": "Verify your Maven configuration and dependencies"
        }
        
        # Return fallback suggestion if available
        if cmd_base in fallback_suggestions:
            return fallback_suggestions[cmd_base]
        
        # No suggestion available
        return None

def _extract_base_command(command: str) -> str:
    """
    Extract the base command from a full command string.
    
    Args:
        command: The full command string
        
    Returns:
        The base command (first word or git subcommand)
    """
    parts = command.strip().split()
    if not parts:
        return ""
    
    # Handle git subcommands as a special case
    if parts[0] == "git" and len(parts) > 1:
        return f"git {parts[1]}"
    
    # Handle npm subcommands
    if parts[0] == "npm" and len(parts) > 1:
        return f"npm {parts[1]}"
    
    # Handle docker subcommands
    if parts[0] == "docker" and len(parts) > 1:
        return f"docker {parts[1]}"
    
    return parts[0]

def _is_long_running_command(command: str) -> bool:
    """
    Check if a command is potentially long-running.
    
    Args:
        command: The command to check
        
    Returns:
        True if the command is potentially long-running
    """
    long_running_patterns = [
        "npm install", "pip install", "apt", "brew", "docker build", 
        "docker-compose up", "make", "cmake", "gcc", "g++", "mvn", "gradle",
        "cargo build", "test", "pytest", "yarn", "sleep", "find", "grep -r",
        "ffmpeg", "convert", "zip", "tar", "unzip", "gunzip", "rsync"
    ]
    
    return any(pattern in command for pattern in long_running_patterns)

def _has_known_fix_pattern(command: str) -> bool:
    """
    Check if a command has a known fix pattern.
    
    Args:
        command: The command to check
        
    Returns:
        True if there's a known fix pattern for this command
    """
    known_patterns = [
        "git push", "git pull", "git commit", "git merge", "git checkout",
        "npm install", "npm run", "npm start", "npm test", "npm build",
        "pip install", "pip uninstall", "python", "pytest", "python -m",
        "docker", "docker-compose", "docker run", "docker build",
        "cargo", "cargo build", "cargo test", "cargo run",
        "make", "make clean", "make install", 
        "mvn", "gradle", "yarn", "bundle"
    ]
    
    return any(pattern in command for pattern in known_patterns)

# Global instance
notification_handler = NotificationHandler()
</file>

<file path="components/monitoring/proactive_assistant.py">
# angela/monitoring/proactive_assistant.py
"""
Proactive Assistance V2 for Angela CLI.

This module provides enhanced monitoring and proactive suggestions 
based on combined context and system states. It integrates various
sources of information to offer intelligent, contextual assistance.
"""
import asyncio
import re
import time
import json
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, Any, List, Optional, Set, Tuple, Union, Callable, Awaitable
from enum import Enum

from angela.core.registry import registry
from angela.utils.logging import get_logger
from angela.core.events import event_bus
from angela.api.shell import get_terminal_formatter
from angela.api.context import get_context_manager
from angela.api.context import get_session_manager
from angela.api.context import get_history_manager
from angela.api.ai import get_gemini_client, get_gemini_request_class
from angela.api.monitoring import get_background_monitor
from angela.api.execution import get_execution_hooks

logger = get_logger(__name__)

class AssistanceType(str, Enum):
    """Types of proactive assistance."""
    SUGGESTION = "suggestion"      # Suggest an action
    WARNING = "warning"            # Warn about a potential issue
    INSIGHT = "insight"            # Provide an insight or information
    OPTIMIZATION = "optimization"  # Suggest an optimization
    RECOVERY = "recovery"          # Suggest a recovery action
    AUTOMATION = "automation"      # Suggest automation or workflow

class AssistanceTrigger(str, Enum):
    """Triggers for proactive assistance."""
    ERROR = "error"                # Command error
    FILE_CHANGE = "file_change"    # File system change
    RESOURCE_USAGE = "resource"    # Resource usage threshold
    TIME_BASED = "time"            # Time-based trigger
    PATTERN = "pattern"            # Pattern in output or history
    COMPOUND = "compound"          # Multiple conditions

class ProactiveAssistant:
    """
    Enhanced proactive assistance system that monitors various aspects of
    system and user activity to provide timely, contextual suggestions.
    """
    
    def __init__(self):
        """Initialize the proactive assistant."""
        self._logger = logger
        self._recent_suggestions = set()  # To avoid repeating suggestions
        self._last_suggestion_time = datetime.now() - timedelta(hours=1)
        self._suggestion_cooldown = timedelta(minutes=3)
        
        # Registered insight handlers
        self._insight_handlers = {
            "git_status": self._handle_git_status_insight,
            "file_syntax_error": self._handle_file_syntax_error,
            "disk_space_low": self._handle_disk_space_insight,
            "python_syntax_error": self._handle_python_syntax_error,
            "javascript_syntax_error": self._handle_javascript_syntax_error,
            "test_failure": self._handle_test_failure_insight,
            "build_failure": self._handle_build_failure_insight,
            "deployment_issue": self._handle_deployment_issue_insight,
            "network_issue": self._handle_network_issue_insight,
            "security_alert": self._handle_security_alert_insight,
            "performance_issue": self._handle_performance_issue_insight,
            "dependency_update": self._handle_dependency_update_insight,
        }
        
        # Event tracking
        self._command_error_history = []
        self._pattern_detectors = []
        self._active_listening = False
        
        # Initialize pattern detectors
        self._setup_pattern_detectors()
    
    def start(self):
        """Start the proactive assistant."""
        from angela.api.monitoring import get_background_monitor
        from angela.api.execution import get_execution_hooks
        
        if self._active_listening:
            return
        
        self._active_listening = True
        
        # Subscribe to events
        event_bus.subscribe("monitoring:*", self._handle_monitoring_event)
        event_bus.subscribe("command:error", self._handle_command_error)
        event_bus.subscribe("command:executed", self._handle_command_executed)
        
        # Register with background monitor
        get_background_monitor().register_insight_callback(self._handle_monitor_insight)
        
        # Register with execution hooks - with error handling
        try:
            execution_hooks = get_execution_hooks()
            execution_hooks.register_hook("post_execute_command", self._post_execute_command_hook)
            self._logger.debug("Successfully registered post_execute_command hook")
        except Exception as e:
            self._logger.error(f"Error registering with execution hooks: {str(e)}")
        
        self._logger.info("Proactive assistant started")
    
    def stop(self):
        """Stop the proactive assistant."""
        from angela.api.monitoring import get_background_monitor
        from angela.api.execution import get_execution_hooks
        
        if not self._active_listening:
            return
        
        self._active_listening = False
        
        # Unsubscribe from events
        event_bus.unsubscribe("monitoring:*", self._handle_monitoring_event)
        event_bus.unsubscribe("command:error", self._handle_command_error)
        event_bus.unsubscribe("command:executed", self._handle_command_executed)
        
        # Unregister from background monitor
        get_background_monitor().unregister_insight_callback(self._handle_monitor_insight)
        
        # Unregister from execution hooks
        get_execution_hooks().unregister_hook("post_execute_command", self._post_execute_command_hook)
        
        self._logger.info("Proactive assistant stopped")
    
    def _setup_pattern_detectors(self):
        """Set up pattern detectors for command output."""
        # Add pattern detectors
        self._pattern_detectors = [
            {
                "name": "missing_dependency",
                "pattern": r"(?:command not found|No module named|cannot find module|Cannot find module|npm ERR! missing)",
                "handler": self._handle_missing_dependency_pattern
            },
            {
                "name": "permission_denied",
                "pattern": r"(?:Permission denied|EACCES|access is denied)",
                "handler": self._handle_permission_denied_pattern
            },
            {
                "name": "port_in_use",
                "pattern": r"(?:port \d+ is already in use|address already in use|EADDRINUSE)",
                "handler": self._handle_port_in_use_pattern
            },
            {
                "name": "api_rate_limit",
                "pattern": r"(?:rate limit exceeded|too many requests|429 Too Many Requests)",
                "handler": self._handle_api_rate_limit_pattern
            },
            {
                "name": "disk_full",
                "pattern": r"(?:No space left on device|disk quota exceeded|ENOSPC)",
                "handler": self._handle_disk_full_pattern
            },
            {
                "name": "network_unreachable",
                "pattern": r"(?:network is unreachable|could not resolve host|connection refused|ECONNREFUSED)",
                "handler": self._handle_network_unreachable_pattern
            },
            {
                "name": "outdated_cli",
                "pattern": r"(?:newer version|update available|outdated|deprecated)",
                "handler": self._handle_outdated_cli_pattern
            },
        ]
    
    async def _handle_monitoring_event(self, event_name: str, event_data: Dict[str, Any]):
        """
        Handle events from the monitoring system.
        
        Args:
            event_name: Name of the event
            event_data: Event data
        """
        self._logger.debug(f"Received monitoring event: {event_name}")
        
        # Extract insight type from event name
        parts = event_name.split(":")
        if len(parts) >= 2:
            insight_type = parts[1]
            
            # Check if we have a handler for this insight type
            handler = self._insight_handlers.get(insight_type)
            if handler:
                await handler(event_data)
    
    async def _handle_command_error(self, command: str, error: str, return_code: int):
        """
        Handle command error events.
        
        Args:
            command: The command that failed
            error: Error message
            return_code: Return code
        """
        self._logger.debug(f"Received command error event: {command}")
        
        # Add to error history
        self._command_error_history.append({
            "command": command,
            "error": error,
            "return_code": return_code,
            "timestamp": datetime.now()
        })
        
        # Limit history size
        if len(self._command_error_history) > 10:
            self._command_error_history = self._command_error_history[-10:]
        
        # Check for patterns in the error message
        for detector in self._pattern_detectors:
            if re.search(detector["pattern"], error, re.IGNORECASE):
                await detector["handler"](command, error, return_code)
    
    async def _handle_command_executed(self, command: str, output: str, return_code: int):
        """
        Handle command executed events.
        
        Args:
            command: The executed command
            output: Command output
            return_code: Return code
        """
        self._logger.debug(f"Received command executed event: {command}")
        
        # Check for successful test/build/deploy commands
        if return_code == 0:
            # Check for test command patterns
            test_patterns = [
                r"(?:pytest|unittest|jest|npm test|go test|rspec|gradle test|mvn test)",
                r"(?:run tests|running tests|test suite)"
            ]
            
            is_test_command = any(re.search(pattern, command, re.IGNORECASE) for pattern in test_patterns)
            
            if is_test_command:
                await self._handle_successful_tests(command, output)
            
            # Check for build command patterns
            build_patterns = [
                r"(?:npm run build|yarn build|go build|mvn package|gradle build)",
                r"(?:docker build|make build)"
            ]
            
            is_build_command = any(re.search(pattern, command, re.IGNORECASE) for pattern in build_patterns)
            
            if is_build_command:
                await self._handle_successful_build(command, output)
            
            # Check for deploy command patterns
            deploy_patterns = [
                r"(?:deploy|publish|release)",
                r"(?:kubectl apply|helm install|terraform apply)",
                r"(?:aws|gcloud|az)\s+(?:deploy|app|function)",
                r"(?:firebase deploy|vercel deploy|netlify deploy)"
            ]
            
            is_deploy_command = any(re.search(pattern, command, re.IGNORECASE) for pattern in deploy_patterns)
            
            if is_deploy_command:
                await self._handle_successful_deploy(command, output)
        
        # Check for patterns in the output
        for detector in self._pattern_detectors:
            if re.search(detector["pattern"], output, re.IGNORECASE):
                await detector["handler"](command, output, return_code)
    
    async def _post_execute_command_hook(self, command: str, result: Dict[str, Any], context: Dict[str, Any]):
        """
        Hook called after a command is executed.
        
        Args:
            command: The executed command
            result: Execution result
            context: Execution context
        """
        self._logger.debug(f"Post-execute command hook: {command}")
        
        # Check for success/failure
        success = result.get("success", False)
        stdout = result.get("stdout", "")
        stderr = result.get("stderr", "")
        
        if success:
            # Publish successful command event
            await event_bus.publish("command:executed", {
                "command": command,
                "output": stdout,
                "return_code": result.get("return_code", 0)
            })
        else:
            # Publish command error event
            await event_bus.publish("command:error", {
                "command": command,
                "error": stderr,
                "return_code": result.get("return_code", -1)
            })
        
        # Check for optimization opportunities in successful commands
        if success and self._can_show_suggestion():
            # Check for repeated commands
            await self._check_for_repeated_commands(command)
            
            # Check for inefficient command patterns
            await self._check_for_inefficient_patterns(command)
    
    async def _handle_monitor_insight(self, insight_type: str, insight_data: Dict[str, Any]):
        """
        Handle insights from background monitoring.
        
        Args:
            insight_type: Type of insight
            insight_data: Insight data
        """
        self._logger.debug(f"Received monitor insight: {insight_type}")
        
        # Check if we have a handler for this insight type
        handler = self._insight_handlers.get(insight_type)
        if handler:
            await handler(insight_data)
    
    async def _handle_git_status_insight(self, insight_data: Dict[str, Any]):
        """
        Handle git status insights.
        
        Args:
            insight_data: Insight data
        """
        from angela.api.shell import get_terminal_formatter
        
        if not self._can_show_suggestion():
            return
        
        suggestion = insight_data.get("suggestion")
        if not suggestion:
            return
        
        # Check if we've seen this suggestion recently
        suggestion_key = f"git_status:{insight_data.get('modified_count', 0)}:{insight_data.get('untracked_count', 0)}"
        
        if suggestion_key in self._recent_suggestions:
            return
        
        # Show the suggestion
        get_terminal_formatter().print_proactive_suggestion(suggestion, "Git Status")
        
        # Remember this suggestion
        self._recent_suggestions.add(suggestion_key)
        self._last_suggestion_time = datetime.now()
    
    async def _handle_file_syntax_error(self, insight_data: Dict[str, Any]):
        """
        Handle file syntax error insights.
        
        Args:
            insight_data: Insight data
        """
        from angela.api.shell import get_terminal_formatter
        
        if not self._can_show_suggestion():
            return
        
        file_path = insight_data.get("file_path")
        error_message = insight_data.get("error_message", "syntax error")
        
        if not file_path:
            return
        
        # Check if we've seen this error recently
        suggestion_key = f"syntax_error:{file_path}"
        
        if suggestion_key in self._recent_suggestions:
            return
        
        # Generate suggestion
        suggestion = f"Syntax error detected in {Path(file_path).name}: {error_message}"
        
        # Show the suggestion
        get_terminal_formatter().print_proactive_suggestion(
            suggestion, 
            "File Error", 
            "Consider fixing this error to avoid compilation/runtime issues."
        )
        
        # Remember this suggestion
        self._recent_suggestions.add(suggestion_key)
        self._last_suggestion_time = datetime.now()
    
    async def _handle_disk_space_insight(self, insight_data: Dict[str, Any]):
        """
        Handle disk space insights.
        
        Args:
            insight_data: Insight data
        """
        if not self._can_show_suggestion():
            return
        
        suggestion = insight_data.get("suggestion")
        if not suggestion:
            return
        
        # Check if we've seen this suggestion recently
        disk_usage = insight_data.get("disk_usage", 0)
        suggestion_key = f"disk_space:{int(disk_usage // 5) * 5}"  # Round to nearest 5%
        
        if suggestion_key in self._recent_suggestions:
            return
        
        # Show the suggestion
        terminal_formatter.print_proactive_suggestion(
            suggestion, 
            "Disk Space Warning", 
            "You might want to free up space to avoid issues."
        )
        
        # Remember this suggestion
        self._recent_suggestions.add(suggestion_key)
        self._last_suggestion_time = datetime.now()
    
    async def _handle_python_syntax_error(self, insight_data: Dict[str, Any]):
        """
        Handle Python syntax error insights.
        
        Args:
            insight_data: Insight data
        """
        if not self._can_show_suggestion():
            return
        
        suggestion = insight_data.get("suggestion")
        if not suggestion:
            return
        
        file_path = insight_data.get("file_path")
        if not file_path:
            return
        
        # Check if we've seen this error recently
        suggestion_key = f"python_syntax:{file_path}"
        
        if suggestion_key in self._recent_suggestions:
            return
        
        # Show the suggestion
        terminal_formatter.print_proactive_suggestion(
            suggestion, 
            "Python Syntax Error", 
            "Fix this error to ensure your Python code runs correctly."
        )
        
        # Remember this suggestion
        self._recent_suggestions.add(suggestion_key)
        self._last_suggestion_time = datetime.now()
    
    async def _handle_javascript_syntax_error(self, insight_data: Dict[str, Any]):
        """
        Handle JavaScript syntax error insights.
        
        Args:
            insight_data: Insight data
        """
        if not self._can_show_suggestion():
            return
        
        suggestion = insight_data.get("suggestion")
        if not suggestion:
            return
        
        file_path = insight_data.get("file_path")
        if not file_path:
            return
        
        # Check if we've seen this error recently
        suggestion_key = f"js_syntax:{file_path}"
        
        if suggestion_key in self._recent_suggestions:
            return
        
        # Show the suggestion
        terminal_formatter.print_proactive_suggestion(
            suggestion, 
            "JavaScript Syntax Error", 
            "Fix this error to ensure your JavaScript code runs correctly."
        )
        
        # Remember this suggestion
        self._recent_suggestions.add(suggestion_key)
        self._last_suggestion_time = datetime.now()
    
        await self._notify_insight_callbacks("javascript_syntax_error", insight_data)    
    
    async def _handle_test_failure_insight(self, insight_data: Dict[str, Any]):
        """
        Handle test failure insights.
        
        Args:
            insight_data: Insight data
        """
        if not self._can_show_suggestion():
            return
        
        suggestion = insight_data.get("suggestion")
        if not suggestion:
            return
        
        # Check if we've seen this suggestion recently
        suggestion_key = f"test_failure:{insight_data.get('test_file', 'unknown')}"
        
        if suggestion_key in self._recent_suggestions:
            return
        
        # Get recent commits to enhance suggestion
        recent_commit_message = None
        try:
            process = await asyncio.create_subprocess_exec(
                "git", "log", "-1", "--pretty=%s",
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            stdout, _ = await process.communicate()
            if process.returncode == 0:
                recent_commit_message = stdout.decode('utf-8').strip()
        except Exception:
            pass
        
        if recent_commit_message:
            enhanced_suggestion = f"{suggestion}\n\nTests failed after your recent commit: '{recent_commit_message}'"
            suggestion = enhanced_suggestion
        
        # Show the suggestion
        terminal_formatter.print_proactive_suggestion(
            suggestion, 
            "Test Failure", 
            "Would you like me to help troubleshoot this test failure?"
        )
        
        # Remember this suggestion
        self._recent_suggestions.add(suggestion_key)
        self._last_suggestion_time = datetime.now()
  
        await self._notify_insight_callbacks("test_failure", insight_data)  
  
    
    async def _handle_build_failure_insight(self, insight_data: Dict[str, Any]):
        """
        Handle build failure insights.
        
        Args:
            insight_data: Insight data
        """
        if not self._can_show_suggestion():
            return
        
        suggestion = insight_data.get("suggestion")
        if not suggestion:
            return
        
        # Check if we've seen this suggestion recently
        suggestion_key = f"build_failure:{insight_data.get('build_id', 'unknown')}"
        
        if suggestion_key in self._recent_suggestions:
            return
        
        # Show the suggestion
        terminal_formatter.print_proactive_suggestion(
            suggestion, 
            "Build Failure", 
            "Need help debugging this build issue?"
        )
        
        # Remember this suggestion
        self._recent_suggestions.add(suggestion_key)
        self._last_suggestion_time = datetime.now()

        await self._notify_insight_callbacks("build_failure", insight_data)
    
    async def _handle_deployment_issue_insight(self, insight_data: Dict[str, Any]):
        """
        Handle deployment issue insights.
        
        Args:
            insight_data: Insight data
        """
        if not self._can_show_suggestion():
            return
        
        suggestion = insight_data.get("suggestion")
        if not suggestion:
            return
        
        # Check if we've seen this suggestion recently
        suggestion_key = f"deployment_issue:{insight_data.get('deployment_id', 'unknown')}"
        
        if suggestion_key in self._recent_suggestions:
            return
        
        # Show the suggestion
        terminal_formatter.print_proactive_suggestion(
            suggestion, 
            "Deployment Issue", 
            "I can help diagnose and fix this deployment problem."
        )
        
        # Remember this suggestion
        self._recent_suggestions.add(suggestion_key)
        self._last_suggestion_time = datetime.now()

        await self._notify_insight_callbacks("deployment_issue", insight_data)

    
    async def _handle_network_issue_insight(self, insight_data: Dict[str, Any]):
        """
        Handle network issue insights.
        
        Args:
            insight_data: Insight data
        """
        if not self._can_show_suggestion():
            return
        
        suggestion = insight_data.get("suggestion")
        if not suggestion:
            return
        
        # Check if we've seen this suggestion recently
        suggestion_key = f"network_issue:{insight_data.get('service', 'unknown')}"
        
        if suggestion_key in self._recent_suggestions:
            return
        
        # Show the suggestion
        terminal_formatter.print_proactive_suggestion(
            suggestion, 
            "Network Issue", 
            "Would you like me to diagnose the network problem?"
        )
        
        # Remember this suggestion
        self._recent_suggestions.add(suggestion_key)
        self._last_suggestion_time = datetime.now()
        
        await self._notify_insight_callbacks("network_issue", insight_data)
    
    async def _handle_security_alert_insight(self, insight_data: Dict[str, Any]):
        """
        Handle security alert insights.
        
        Args:
            insight_data: Insight data
        """
        if not self._can_show_suggestion():
            return
        
        suggestion = insight_data.get("suggestion")
        if not suggestion:
            return
        
        # Security alerts are important, always show them
        # with a shorter cooldown
        has_shorter_cooldown = (datetime.now() - self._last_suggestion_time) >= timedelta(seconds=30)
        
        if not has_shorter_cooldown:
            return
        
        # Check if we've seen this suggestion recently
        suggestion_key = f"security_alert:{insight_data.get('issue_type', 'unknown')}"
        
        if suggestion_key in self._recent_suggestions:
            return
        
        # Show the suggestion with high visibility
        terminal_formatter.print_proactive_suggestion(
            suggestion, 
            "⚠️ Security Alert ⚠️", 
            "This security issue requires immediate attention!",
            style="bold red on yellow"
        )
        
        # Remember this suggestion
        self._recent_suggestions.add(suggestion_key)
        self._last_suggestion_time = datetime.now()

        await self._notify_insight_callbacks("security_alert", insight_data)
    
    async def _handle_performance_issue_insight(self, insight_data: Dict[str, Any]):
        """
        Handle performance issue insights.
        
        Args:
            insight_data: Insight data
        """
        if not self._can_show_suggestion():
            return
        
        suggestion = insight_data.get("suggestion")
        if not suggestion:
            return
        
        # Check if we've seen this suggestion recently
        suggestion_key = f"performance_issue:{insight_data.get('resource', 'unknown')}"
        
        if suggestion_key in self._recent_suggestions:
            return
        
        # Show the suggestion
        terminal_formatter.print_proactive_suggestion(
            suggestion, 
            "Performance Issue", 
            "I can suggest optimizations to improve performance."
        )
        
        # Remember this suggestion
        self._recent_suggestions.add(suggestion_key)
        self._last_suggestion_time = datetime.now()


        await self._notify_insight_callbacks("performance_issue", insight_data)
    
    async def _handle_dependency_update_insight(self, insight_data: Dict[str, Any]):
        """
        Handle dependency update insights.
        
        Args:
            insight_data: Insight data
        """
        if not self._can_show_suggestion():
            return
        
        suggestion = insight_data.get("suggestion")
        if not suggestion:
            return
        
        # Check if we've seen this suggestion recently
        suggestion_key = f"dependency_update:{insight_data.get('package', 'unknown')}"
        
        if suggestion_key in self._recent_suggestions:
            return
        
        # Show the suggestion
        terminal_formatter.print_proactive_suggestion(
            suggestion, 
            "Dependency Update Available", 
            "Would you like me to update this dependency for you?"
        )
        
        # Remember this suggestion
        self._recent_suggestions.add(suggestion_key)
        self._last_suggestion_time = datetime.now()

        await self._notify_insight_callbacks("dependency_update", insight_data)

    
    async def _handle_missing_dependency_pattern(self, command: str, output: str, return_code: int):
        """
        Handle missing dependency pattern.
        
        Args:
            command: The command that was executed
            output: Command output
            return_code: Return code
        """
        if not self._can_show_suggestion():
            return
        
        # Check if we've seen this pattern recently
        suggestion_key = f"missing_dependency:{command}"
        
        if suggestion_key in self._recent_suggestions:
            return
        
        # Try to extract the missing dependency
        dependency = None
        
        # Common patterns for different package managers
        dependency_patterns = [
            # npm/yarn
            r"npm ERR! missing:.*?([a-zA-Z0-9_\-\./@]+)@",
            # pip/python
            r"No module named ['\"]([a-zA-Z0-9_\-\.]+)['\"]",
            # Command not found
            r"command not found: ([a-zA-Z0-9_\-\.]+)",
            # Go
            r"cannot find package \"([a-zA-Z0-9_\-\./]+)\"",
            # General
            r"(?:missing|not found|cannot find).*?([a-zA-Z0-9_\-\./@:]+)"
        ]
        
        for pattern in dependency_patterns:
            match = re.search(pattern, output, re.IGNORECASE)
            if match:
                dependency = match.group(1)
                break
        
        # Generate suggestion based on the command and output
        if dependency:
            suggestion = f"It looks like you're missing the dependency: {dependency}"
            
            # Try to determine the command to install it
            install_cmd = None
            
            if "npm" in command.lower() or "node" in command.lower():
                install_cmd = f"npm install {dependency}"
            elif "yarn" in command.lower():
                install_cmd = f"yarn add {dependency}"
            elif "pip" in command.lower() or "python" in command.lower():
                install_cmd = f"pip install {dependency}"
            elif "go" in command.lower():
                install_cmd = f"go get {dependency}"
            
            if install_cmd:
                suggestion += f"\n\nYou can install it with:\n{install_cmd}"
        else:
            # Generic suggestion
            suggestion = "It looks like you're missing a dependency needed for this command."
        
        # Show the suggestion
        terminal_formatter.print_proactive_suggestion(
            suggestion, 
            "Missing Dependency", 
            "Would you like me to install the missing dependency?"
        )
        
        # Remember this suggestion
        self._recent_suggestions.add(suggestion_key)
        self._last_suggestion_time = datetime.now()
    
    async def _handle_permission_denied_pattern(self, command: str, output: str, return_code: int):
        """
        Handle permission denied pattern.
        
        Args:
            command: The command that was executed
            output: Command output
            return_code: Return code
        """
        if not self._can_show_suggestion():
            return
        
        # Check if we've seen this pattern recently
        suggestion_key = f"permission_denied:{command}"
        
        if suggestion_key in self._recent_suggestions:
            return
        
        # Try to extract the file/directory with permission issue
        path = None
        
        path_pattern = r"[Pp]ermission denied.*?['\"]?([/\w\-\.]+)['\"]?"
        match = re.search(path_pattern, output, re.IGNORECASE)
        if match:
            path = match.group(1)
        
        # Generate suggestion based on the command and output
        if path:
            suggestion = f"Permission denied for: {path}"
            suggestion += "\n\nYou might need to run the command with sudo or adjust permissions."
            suggestion += f"\nTry: sudo {command}"
            
            if "r" in output.lower():
                suggestion += f"\n\nOr change permissions: chmod +r {path}"
            
            if "w" in output.lower():
                suggestion += f"\n\nOr change permissions: chmod +w {path}"
            
            if "x" in output.lower():
                suggestion += f"\n\nOr change permissions: chmod +x {path}"
        else:
            # Generic suggestion
            suggestion = "You don't have sufficient permissions to run this command."
            suggestion += f"\n\nYou might need to use sudo: sudo {command}"
        
        # Show the suggestion
        terminal_formatter.print_proactive_suggestion(
            suggestion, 
            "Permission Denied", 
            "Need help fixing this permission issue?"
        )
        
        # Remember this suggestion
        self._recent_suggestions.add(suggestion_key)
        self._last_suggestion_time = datetime.now()
    
    async def _handle_port_in_use_pattern(self, command: str, output: str, return_code: int):
        """
        Handle port in use pattern.
        
        Args:
            command: The command that was executed
            output: Command output
            return_code: Return code
        """
        if not self._can_show_suggestion():
            return
        
        # Try to extract the port number
        port = None
        
        port_pattern = r"port (\d+)|:(\d+).*?(?:already in use|EADDRINUSE)"
        match = re.search(port_pattern, output, re.IGNORECASE)
        if match:
            port = match.group(1) or match.group(2)
        
        # Check if we've seen this pattern recently
        suggestion_key = f"port_in_use:{port or 'unknown'}"
        
        if suggestion_key in self._recent_suggestions:
            return
        
        # Generate suggestion based on the command and output
        if port:
            suggestion = f"Port {port} is already in use by another process."
            suggestion += "\n\nYou can find the process using this port with:"
            
            if "linux" in sys.platform or "darwin" in sys.platform:
                suggestion += f"\nlsof -i :{port}"
            elif "win" in sys.platform:
                suggestion += f"\nnetstat -ano | findstr :{port}"
            
            suggestion += "\n\nOr you can use a different port in your command."
        else:
            # Generic suggestion
            suggestion = "A port is already in use by another process."
            suggestion += "\n\nYou can try using a different port in your command."
        
        # Show the suggestion
        terminal_formatter.print_proactive_suggestion(
            suggestion, 
            "Port Already In Use", 
            "Need help resolving this port conflict?"
        )
        
        # Remember this suggestion
        self._recent_suggestions.add(suggestion_key)
        self._last_suggestion_time = datetime.now()
    
    async def _handle_api_rate_limit_pattern(self, command: str, output: str, return_code: int):
        """
        Handle API rate limit pattern.
        
        Args:
            command: The command that was executed
            output: Command output
            return_code: Return code
        """
        if not self._can_show_suggestion():
            return
        
        # Check if we've seen this pattern recently
        suggestion_key = "api_rate_limit"
        
        if suggestion_key in self._recent_suggestions:
            return
        
        # Generate suggestion
        suggestion = "You've hit an API rate limit."
        suggestion += "\n\nTips for handling rate limits:"
        suggestion += "\n1. Wait and retry later"
        suggestion += "\n2. Implement exponential backoff"
        suggestion += "\n3. Check if authentication would increase your rate limit"
        suggestion += "\n4. Consider caching responses to reduce API calls"
        
        # Show the suggestion
        terminal_formatter.print_proactive_suggestion(
            suggestion, 
            "API Rate Limit Exceeded", 
            "Would you like me to create a retry script with backoff?"
        )
        
        # Remember this suggestion
        self._recent_suggestions.add(suggestion_key)
        self._last_suggestion_time = datetime.now()
    
    async def _handle_disk_full_pattern(self, command: str, output: str, return_code: int):
        """
        Handle disk full pattern.
        
        Args:
            command: The command that was executed
            output: Command output
            return_code: Return code
        """
        if not self._can_show_suggestion():
            return
        
        # Check if we've seen this pattern recently
        suggestion_key = "disk_full"
        
        if suggestion_key in self._recent_suggestions:
            return
        
        # Generate suggestion
        suggestion = "You've run out of disk space."
        suggestion += "\n\nCommands to help free up space:"
        suggestion += "\n1. df -h (check disk usage)"
        suggestion += "\n2. du -h --max-depth=1 (find large directories)"
        suggestion += "\n3. find . -type f -size +100M (find large files)"
        suggestion += "\n4. rm -rf ~/.cache/* (clear cache files)"
        
        # Show the suggestion
        terminal_formatter.print_proactive_suggestion(
            suggestion, 
            "Disk Full", 
            "Would you like me to help you free up disk space?"
        )
        
        # Remember this suggestion
        self._recent_suggestions.add(suggestion_key)
        self._last_suggestion_time = datetime.now()
    
    async def _handle_network_unreachable_pattern(self, command: str, output: str, return_code: int):
        """
        Handle network unreachable pattern.
        
        Args:
            command: The command that was executed
            output: Command output
            return_code: Return code
        """
        if not self._can_show_suggestion():
            return
        
        # Check if we've seen this pattern recently
        suggestion_key = "network_unreachable"
        
        if suggestion_key in self._recent_suggestions:
            return
        
        # Try to extract the host/URL
        host = None
        
        host_pattern = r"(?:host|resolve|connect to|unreachable)[:\s]+([a-zA-Z0-9\-\.]+\.[a-zA-Z]{2,})"
        match = re.search(host_pattern, output, re.IGNORECASE)
        if match:
            host = match.group(1)
        
        # Generate suggestion
        if host:
            suggestion = f"Unable to reach {host}. This might be due to network connectivity issues."
        else:
            suggestion = "You're experiencing network connectivity issues."
        
        suggestion += "\n\nCommands to diagnose network problems:"
        suggestion += "\n1. ping google.com (check basic connectivity)"
        suggestion += "\n2. nslookup example.com (check DNS resolution)"
        suggestion += "\n3. traceroute example.com (check routing)"
        suggestion += "\n4. curl -I https://example.com (check HTTP connectivity)"
        
        # Show the suggestion
        terminal_formatter.print_proactive_suggestion(
            suggestion, 
            "Network Connectivity Issue", 
            "Would you like me to help diagnose the network problem?"
        )
        
        # Remember this suggestion
        self._recent_suggestions.add(suggestion_key)
        self._last_suggestion_time = datetime.now()
    
    async def _handle_outdated_cli_pattern(self, command: str, output: str, return_code: int):
        """
        Handle outdated CLI tool pattern.
        
        Args:
            command: The command that was executed
            output: Command output
            return_code: Return code
        """
        if not self._can_show_suggestion():
            return
        
        # Extract tool name from command
        tool = command.split()[0] if command else "tool"
        
        # Check if we've seen this pattern recently
        suggestion_key = f"outdated_cli:{tool}"
        
        if suggestion_key in self._recent_suggestions:
            return
        
        # Generate suggestion
        suggestion = f"Your {tool} CLI tool has an update available."
        suggestion += "\n\nCommon update commands:"
        
        if tool == "npm":
            suggestion += "\nnpm install -g npm@latest"
        elif tool == "pip":
            suggestion += "\npip install --upgrade pip"
        elif tool == "yarn":
            suggestion += "\nnpm install -g yarn@latest"
        elif tool == "docker":
            suggestion += "\nRefer to https://docs.docker.com/engine/install/ for update instructions"
        elif tool == "kubectl":
            suggestion += "\nRefer to https://kubernetes.io/docs/tasks/tools/install-kubectl/ for update instructions"
        elif tool == "aws":
            suggestion += "\npip install --upgrade awscli"
        elif tool == "gcloud":
            suggestion += "\ngcloud components update"
        else:
            suggestion += f"\nCheck the {tool} documentation for update instructions"
        
        # Show the suggestion
        terminal_formatter.print_proactive_suggestion(
            suggestion, 
            "Update Available", 
            f"Would you like me to update the {tool} CLI tool?"
        )
        
        # Remember this suggestion
        self._recent_suggestions.add(suggestion_key)
        self._last_suggestion_time = datetime.now()
    
    async def _handle_successful_tests(self, command: str, output: str):
        """
        Handle successful test command execution.
        
        Args:
            command: The test command
            output: Command output
        """
        if not self._can_show_suggestion():
            return
        
        # Check if we've seen this pattern recently
        suggestion_key = "successful_tests"
        
        if suggestion_key in self._recent_suggestions:
            return
        
        # Extract test statistics if possible
        test_count = None
        time_taken = None
        coverage = None
        
        # Common patterns for test output
        test_count_pattern = r"(?:Ran|Running|Executed) (\d+) tests?"
        time_pattern = r"(?:in|finished in|took) ([0-9\.]+)s?"
        coverage_pattern = r"(?:Coverage|coverage)[^0-9]*?([0-9\.]+%)"
        
        test_match = re.search(test_count_pattern, output, re.IGNORECASE)
        time_match = re.search(time_pattern, output, re.IGNORECASE)
        coverage_match = re.search(coverage_pattern, output, re.IGNORECASE)
        
        if test_match:
            test_count = test_match.group(1)
        if time_match:
            time_taken = time_match.group(1)
        if coverage_match:
            coverage = coverage_match.group(1)
        
        # Generate suggestion
        suggestion = "🎉 All tests passed successfully!"
        
        if test_count:
            suggestion += f"\n\nRan {test_count} tests"
        if time_taken:
            suggestion += f" in {time_taken}s"
        if coverage:
            suggestion += f"\nTest coverage: {coverage}"
        
        # Add project context if CI/CD is not set up
        context = context_manager.get_context_dict()
        project_type = context.get("project_type")
        
        ci_files = [
            ".github/workflows", ".gitlab-ci.yml", "Jenkinsfile", 
            ".travis.yml", ".circleci"
        ]
        
        has_ci = any(Path(context.get("project_root", ".")).glob(ci_file) for ci_file in ci_files)
        
        if not has_ci and project_type:
            suggestion += "\n\nConsider setting up CI/CD to automate testing."
            suggestion += "\nI can help you create a CI/CD configuration for GitHub Actions, GitLab CI, or Jenkins."
        
        # Show the suggestion
        terminal_formatter.print_proactive_suggestion(
            suggestion, 
            "Tests Passed", 
            "Great job! Your code is working as expected."
        )
        
        # Remember this suggestion
        self._recent_suggestions.add(suggestion_key)
        self._last_suggestion_time = datetime.now()
    
    async def _handle_successful_build(self, command: str, output: str):
        """
        Handle successful build command execution.
        
        Args:
            command: The build command
            output: Command output
        """
        if not self._can_show_suggestion():
            return
        
        # Check if we've seen this pattern recently
        suggestion_key = "successful_build"
        
        if suggestion_key in self._recent_suggestions:
            return
        
        # Generate suggestion
        suggestion = "🔨 Build completed successfully!"
        
        # Extract build time if available
        time_pattern = r"(?:in|finished in|took|time) ([0-9\.]+)s?"
        time_match = re.search(time_pattern, output, re.IGNORECASE)
        
        if time_match:
            suggestion += f"\nBuild took {time_match.group(1)}s"
        
        # Check if tests were run as part of the build
        has_tests = "test" in output.lower() or "spec" in output.lower()
        
        if has_tests:
            suggestion += "\nTests were included in the build and passed"
        
        # Check if deployment might be next
        context = context_manager.get_context_dict()
        recent_commands = context.get("session", {}).get("recent_commands", [])
        
        has_recent_test = any("test" in cmd for cmd in recent_commands[-5:]) if recent_commands else False
        
        if has_recent_test:
            suggestion += "\n\nYour tests and build have both succeeded. Would you like to deploy?"
        
        # Show the suggestion
        terminal_formatter.print_proactive_suggestion(
            suggestion, 
            "Build Succeeded", 
            "Your code has built successfully."
        )
        
        # Remember this suggestion
        self._recent_suggestions.add(suggestion_key)
        self._last_suggestion_time = datetime.now()
    
    async def _handle_successful_deploy(self, command: str, output: str):
        """
        Handle successful deploy command execution.
        
        Args:
            command: The deploy command
            output: Command output
        """
        if not self._can_show_suggestion():
            return
        
        # Check if we've seen this pattern recently
        suggestion_key = "successful_deploy"
        
        if suggestion_key in self._recent_suggestions:
            return
        
        # Try to extract the deployment URL or environment
        url_pattern = r"(?:deployed to|available at|url|http)[:\s]+(https?://[a-zA-Z0-9\-\.]+\.[a-zA-Z]{2,}[^\s]*)"
        env_pattern = r"(?:deployed to|environment|env)[:\s]+([a-zA-Z0-9\-\_]+)"
        
        url_match = re.search(url_pattern, output, re.IGNORECASE)
        env_match = re.search(env_pattern, output, re.IGNORECASE)
        
        # Generate suggestion
        suggestion = "🚀 Deployment completed successfully!"
        
        if url_match:
            url = url_match.group(1)
            suggestion += f"\n\nYour application is available at: {url}"
        
        if env_match:
            env = env_match.group(1)
            suggestion += f"\n\nDeployed to environment: {env}"
        
        # Check if monitoring should be suggested
        has_monitoring_terms = any(term in output.lower() for term in ["monitor", "logging", "metrics", "health"])
        
        if not has_monitoring_terms:
            suggestion += "\n\nDon't forget to monitor your deployment for any issues."
            suggestion += "\nI can help you set up monitoring tools if needed."
        
        # Show the suggestion
        terminal_formatter.print_proactive_suggestion(
            suggestion, 
            "Deployment Succeeded", 
            "Your application has been deployed successfully."
        )
        
        # Remember this suggestion
        self._recent_suggestions.add(suggestion_key)
        self._last_suggestion_time = datetime.now()
    
    async def _check_for_repeated_commands(self, command: str):
        """
        Check for repeated commands and suggest workflow automation.
        
        Args:
            command: The command that was executed
        """
        from angela.api.context import get_history_manager
        from angela.api.shell import get_terminal_formatter
        
        # Get recent commands
        recent_commands = get_history_manager().get_recent_commands(limit=20)
        if not recent_commands:
            return
        
        # Count occurrences of the current command
        command_count = sum(1 for cmd in recent_commands if cmd == command)
        
        # If command has been repeated multiple times, suggest workflow
        if command_count >= 3:
            # Check if we've seen this pattern recently
            suggestion_key = f"repeated_command:{command}"
            
            if suggestion_key in self._recent_suggestions:
                return
            
            # Generate suggestion
            suggestion = f"I noticed you've run this command {command_count} times:"
            suggestion += f"\n{command}"
            suggestion += "\n\nWould you like me to create a workflow for this task?"
            suggestion += "\nYou can then run it with a simple command like: angela run my-workflow"
            
            # Show the suggestion
            get_terminal_formatter().print_proactive_suggestion(
                suggestion, 
                "Workflow Opportunity", 
                "I can automate this repeated command for you."
            )
            
            # Remember this suggestion
            self._recent_suggestions.add(suggestion_key)
            self._last_suggestion_time = datetime.now()
    
    async def _check_for_inefficient_patterns(self, command: str):
        """
        Check for inefficient command patterns and suggest optimizations.
        
        Args:
            command: The command that was executed
        """
        # Define patterns to check and their optimizations
        inefficient_patterns = [
            {
                "pattern": r"find\s+.+?\s+-name\s+.+?\s+\|\s+xargs",
                "suggestion": "You can use 'find -exec' instead of piping to xargs for better handling of filenames with spaces:\nfind . -name '*.txt' -exec command {} \\;",
                "key": "find_xargs"
            },
            {
                "pattern": r"grep\s+.+?\s+\|\s+grep",
                "suggestion": "You can combine multiple grep patterns with the -e option:\ngrep -e 'pattern1' -e 'pattern2' file.txt",
                "key": "grep_pipe"
            },
            {
                "pattern": r"cat\s+.+?\s+\|\s+grep",
                "suggestion": "You can use grep directly on the file for better performance:\ngrep 'pattern' file.txt",
                "key": "cat_grep"
            },
            {
                "pattern": r"sort\s+.+?\s+\|\s+uniq",
                "suggestion": "You can use 'sort -u' to sort and get unique lines in one command:\nsort -u file.txt",
                "key": "sort_uniq"
            }
        ]
        
        # Check each pattern
        for pattern_info in inefficient_patterns:
            if re.search(pattern_info["pattern"], command, re.IGNORECASE):
                # Check if we've seen this pattern recently
                suggestion_key = f"inefficient_pattern:{pattern_info['key']}"
                
                if suggestion_key in self._recent_suggestions:
                    continue
                
                # Generate suggestion
                suggestion = "I noticed a command pattern that could be optimized:"
                suggestion += f"\n{command}"
                suggestion += f"\n\n{pattern_info['suggestion']}"
                
                # Show the suggestion
                terminal_formatter.print_proactive_suggestion(
                    suggestion, 
                    "Command Optimization", 
                    "Here's a more efficient way to achieve the same result."
                )
                
                # Remember this suggestion
                self._recent_suggestions.add(suggestion_key)
                self._last_suggestion_time = datetime.now()
                
                # Only show one suggestion at a time
                break
    
    def _can_show_suggestion(self) -> bool:
        """
        Check if we can show a suggestion right now (respecting cooldown).
        
        Returns:
            True if a suggestion can be shown, False otherwise
        """
        return (datetime.now() - self._last_suggestion_time) >= self._suggestion_cooldown
    
    def get_suggestion_opportunity(
        self, 
        request: str, 
        context: Dict[str, Any]
    ) -> Optional[Dict[str, Any]]:
        """
        Check if there's an opportunity for a proactive suggestion.
        
        This method is called by the orchestrator before processing a request
        to see if there's a proactive suggestion that could be offered.
        
        Args:
            request: The user request
            context: Context information
            
        Returns:
            Suggestion opportunity data or None
        """
        # Implement proactive suggestion opportunities
        # For example, suggesting CI/CD setup for repositories without it
        # This would require access to the project context, history, etc.
        return None

# Global instance
proactive_assistant = ProactiveAssistant()
</file>

<file path="components/review/__init__.py">
# angela/review/__init__.py
"""
Review components for Angela CLI.

This package provides functionality for reviewing, diffing, and applying 
feedback to code and text content.
"""

from .diff_manager import diff_manager
from .feedback import feedback_manager

__all__ = ['diff_manager', 'feedback_manager']
</file>

<file path="components/review/diff_manager.py">
# angela/review/diff_manager.py
"""
Diff management for Angela CLI.

This module provides functionality for managing and presenting diffs
between original and modified code.
"""
import os
import difflib
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple, Union

from angela.utils.logging import get_logger

logger = get_logger(__name__)

class DiffManager:
    """
    Manager for generating and displaying code diffs.
    """
    
    def __init__(self):
        """Initialize the diff manager."""
        self._logger = logger
    
    def generate_diff(
        self, 
        original: str, 
        modified: str, 
        context_lines: int = 3
    ) -> str:
        """
        Generate a unified diff between original and modified content.
        
        Args:
            original: Original content
            modified: Modified content
            context_lines: Number of context lines to include
            
        Returns:
            Unified diff string
        """
        self._logger.debug("Generating diff")
        
        # Split content into lines
        original_lines = original.splitlines(keepends=True)
        modified_lines = modified.splitlines(keepends=True)
        
        # Generate unified diff
        diff = difflib.unified_diff(
            original_lines, 
            modified_lines,
            fromfile='original',
            tofile='modified',
            n=context_lines
        )
        
        return ''.join(diff)
    
    def generate_html_diff(
        self, 
        original: str, 
        modified: str, 
        context_lines: int = 3
    ) -> str:
        """
        Generate an HTML diff between original and modified content.
        
        Args:
            original: Original content
            modified: Modified content
            context_lines: Number of context lines to include
            
        Returns:
            HTML diff string
        """
        self._logger.debug("Generating HTML diff")
        
        # Split content into lines
        original_lines = original.splitlines()
        modified_lines = modified.splitlines()
        
        # Generate HTML diff
        diff = difflib.HtmlDiff().make_file(
            original_lines, 
            modified_lines,
            fromdesc='Original',
            todesc='Modified',
            context=True,
            numlines=context_lines
        )
        
        return diff
    
    def generate_file_diff(
        self, 
        original_file: Union[str, Path], 
        modified_file: Union[str, Path],
        context_lines: int = 3
    ) -> str:
        """
        Generate a unified diff between original and modified files.
        
        Args:
            original_file: Path to original file
            modified_file: Path to modified file
            context_lines: Number of context lines to include
            
        Returns:
            Unified diff string
        """
        self._logger.debug(f"Generating diff between {original_file} and {modified_file}")
        
        # Read file contents
        try:
            with open(original_file, 'r', encoding='utf-8', errors='replace') as f:
                original_content = f.read()
            
            with open(modified_file, 'r', encoding='utf-8', errors='replace') as f:
                modified_content = f.read()
            
            # Generate diff
            return self.generate_diff(
                original_content, 
                modified_content,
                context_lines
            )
        except Exception as e:
            self._logger.error(f"Error generating file diff: {str(e)}")
            return f"Error generating diff: {str(e)}"
    
    def generate_directory_diff(
        self, 
        original_dir: Union[str, Path], 
        modified_dir: Union[str, Path],
        context_lines: int = 3
    ) -> Dict[str, str]:
        """
        Generate diffs for all files in two directories.
        
        Args:
            original_dir: Path to original directory
            modified_dir: Path to modified directory
            context_lines: Number of context lines to include
            
        Returns:
            Dictionary mapping file paths to diffs
        """
        self._logger.debug(f"Generating diffs between {original_dir} and {modified_dir}")
        
        original_dir = Path(original_dir)
        modified_dir = Path(modified_dir)
        
        # Check if directories exist
        if not original_dir.exists() or not original_dir.is_dir():
            self._logger.error(f"Original directory does not exist: {original_dir}")
            return {}
        
        if not modified_dir.exists() or not modified_dir.is_dir():
            self._logger.error(f"Modified directory does not exist: {modified_dir}")
            return {}
        
        # Find all files in both directories
        original_files = set()
        modified_files = set()
        
        for root, _, files in os.walk(original_dir):
            for file in files:
                file_path = Path(root) / file
                rel_path = file_path.relative_to(original_dir)
                original_files.add(str(rel_path))
        
        for root, _, files in os.walk(modified_dir):
            for file in files:
                file_path = Path(root) / file
                rel_path = file_path.relative_to(modified_dir)
                modified_files.add(str(rel_path))
        
        # Generate diffs for all files
        diffs = {}
        
        # Files in both directories
        for rel_path in original_files.intersection(modified_files):
            original_file = original_dir / rel_path
            modified_file = modified_dir / rel_path
            
            try:
                diff = self.generate_file_diff(
                    original_file, 
                    modified_file,
                    context_lines
                )
                
                # Only include if there are differences
                if diff:
                    diffs[rel_path] = diff
            except Exception as e:
                self._logger.error(f"Error generating diff for {rel_path}: {str(e)}")
        
        # Files only in original directory (deleted)
        for rel_path in original_files - modified_files:
            original_file = original_dir / rel_path
            
            try:
                with open(original_file, 'r', encoding='utf-8', errors='replace') as f:
                    content = f.read()
                
                # Generate diff showing deletion
                diff = self.generate_diff(content, '')
                diffs[rel_path] = diff
            except Exception as e:
                self._logger.error(f"Error generating diff for {rel_path}: {str(e)}")
        
        # Files only in modified directory (added)
        for rel_path in modified_files - original_files:
            modified_file = modified_dir / rel_path
            
            try:
                with open(modified_file, 'r', encoding='utf-8', errors='replace') as f:
                    content = f.read()
                
                # Generate diff showing addition
                diff = self.generate_diff('', content)
                diffs[rel_path] = diff
            except Exception as e:
                self._logger.error(f"Error generating diff for {rel_path}: {str(e)}")
        
        return diffs
    
    def apply_diff(
        self, 
        original: str, 
        diff: str
    ) -> Tuple[str, bool]:
        """
        Apply a unified diff to original content.
        
        Args:
            original: Original content
            diff: Unified diff string
            
        Returns:
            Tuple of (modified_content, success)
        """
        self._logger.debug("Applying diff")
        
        try:
            # Parse the diff
            lines = diff.splitlines()
            
            # Skip header lines (starting with ---, +++, @@)
            i = 0
            while i < len(lines) and (lines[i].startswith('---') or lines[i].startswith('+++') or lines[i].startswith('@@')):
                i += 1
            
            # Apply changes
            result = []
            original_lines = original.splitlines()
            
            line_num = 0
            while line_num < len(original_lines):
                if i < len(lines):
                    if lines[i].startswith('-'):
                        # Line removed, skip in original
                        if not original_lines[line_num] == lines[i][1:]:
                            # Mismatch, can't apply diff
                            return original, False
                        
                        line_num += 1
                        i += 1
                    elif lines[i].startswith('+'):
                        # Line added
                        result.append(lines[i][1:])
                        i += 1
                    elif lines[i].startswith(' '):
                        # Context line
                        if not original_lines[line_num] == lines[i][1:]:
                            # Mismatch, can't apply diff
                            return original, False
                        
                        result.append(original_lines[line_num])
                        line_num += 1
                        i += 1
                    else:
                        # Unknown line in diff
                        return original, False
                else:
                    # No more diff lines, copy remaining original lines
                    result.extend(original_lines[line_num:])
                    break
            
            # Return the modified content
            return '\n'.join(result), True
        except Exception as e:
            self._logger.error(f"Error applying diff: {str(e)}")
            return original, False

# Global diff manager instance
diff_manager = DiffManager()
</file>

<file path="components/review/feedback.py">
# angela/review/feedback.py
"""
Feedback processing for Angela CLI.

This module provides functionality for processing user feedback
on generated code and refining code based on feedback.
"""
import os
import re
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple, Union
import json

from angela.api.ai import get_gemini_client, get_gemini_request_class
from angela.api.review import get_diff_manager
from angela.utils.logging import get_logger

logger = get_logger(__name__)

gemini_client = get_gemini_client()
GeminiRequest = get_gemini_request_class()

class FeedbackManager:
    """
    Manager for processing user feedback and refining code.
    """
    
    def __init__(self):
        """Initialize the feedback manager."""
        self._logger = logger
    
    async def process_feedback(
        self, 
        feedback: str,
        original_code: str,
        file_path: Optional[str] = None,
        context: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Process feedback on code and generate improved version.
        
        Args:
            feedback: User feedback
            original_code: Original code to improve
            file_path: Optional path to the file
            context: Optional additional context
            
        Returns:
            Dictionary with the improved code and other information
        """
        self._logger.info("Processing feedback for code improvement")
        
        # Extract file extension for language detection
        language = None
        if file_path:
            _, ext = os.path.splitext(file_path)
            language = self._get_language_from_extension(ext)
        
        # Build prompt for code improvement
        prompt = self._build_improvement_prompt(
            feedback, 
            original_code, 
            language,
            file_path,
            context
        )
        
        # Call AI service using API
        api_request = GeminiRequest(
            prompt=prompt,
            max_tokens=16000,  # Large token limit for code
            temperature=0.2
        )
        
        response = await gemini_client.generate_text(api_request)
        
        # Extract improved code and explanation
        improved_code, explanation = self._extract_improved_code(response.text, original_code)
        
        # Get diff_manager through API
        diff_manager = get_diff_manager()
        
        # Generate diff
        diff = diff_manager.generate_diff(original_code, improved_code)
        
        return {
            "original_code": original_code,
            "improved_code": improved_code,
            "explanation": explanation,
            "diff": diff,
            "file_path": file_path,
            "language": language,
            "feedback": feedback
        }
    
    async def refine_project(
        self, 
        project_dir: Union[str, Path],
        feedback: str,
        focus_files: Optional[List[str]] = None,
        context: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Refine an entire project based on feedback.
        
        Args:
            project_dir: Path to the project directory
            feedback: User feedback
            focus_files: Optional list of files to focus on
            context: Optional additional context
            
        Returns:
            Dictionary with the refinement results
        """
        self._logger.info(f"Refining project in {project_dir} based on feedback")
        
        project_dir = Path(project_dir)
        
        # Check if directory exists
        if not project_dir.exists() or not project_dir.is_dir():
            return {
                "success": False,
                "error": f"Project directory does not exist: {project_dir}",
                "feedback": feedback
            }
        
        # Get list of files to refine
        files_to_refine = []
        
        if focus_files:
            # Refine specific files
            for file_pattern in focus_files:
                # Handle glob patterns
                if '*' in file_pattern or '?' in file_pattern:
                    matches = list(project_dir.glob(file_pattern))
                    for match in matches:
                        if match.is_file():
                            files_to_refine.append(match)
                else:
                    # Direct file path
                    file_path = project_dir / file_pattern
                    if file_path.is_file():
                        files_to_refine.append(file_path)
        else:
            # Auto-detect files to refine based on feedback
            files = self._find_relevant_files(project_dir, feedback)
            files_to_refine.extend(files)
        
        # Process each file
        results = []
        
        for file_path in files_to_refine:
            try:
                with open(file_path, 'r', encoding='utf-8', errors='replace') as f:
                    original_code = f.read()
                
                # Process feedback for this file
                file_result = await self.process_feedback(
                    feedback,
                    original_code,
                    str(file_path.relative_to(project_dir)),
                    context
                )
                
                results.append({
                    "file_path": str(file_path.relative_to(project_dir)),
                    "has_changes": original_code != file_result["improved_code"],
                    "diff": file_result["diff"],
                    "explanation": file_result["explanation"]
                })
            except Exception as e:
                self._logger.error(f"Error processing {file_path}: {str(e)}")
                results.append({
                    "file_path": str(file_path.relative_to(project_dir)),
                    "error": str(e)
                })
        
        return {
            "success": True,
            "project_dir": str(project_dir),
            "feedback": feedback,
            "results": results,
            "files_processed": len(results)
        }
    
    async def apply_refinements(
        self, 
        refinements: Dict[str, Any],
        backup: bool = True
    ) -> Dict[str, Any]:
        """
        Apply refinements to files.
        
        Args:
            refinements: Refinement results from refine_project
            backup: Whether to create backup files
            
        Returns:
            Dictionary with the application results
        """
        self._logger.info("Applying refinements to files")
        
        # Extract project directory and results
        project_dir = Path(refinements["project_dir"])
        results = refinements["results"]
        
        # Get diff_manager through API
        diff_manager = get_diff_manager()
        
        # Apply changes to each file
        applied_results = []
        
        for result in results:
            file_path = project_dir / result["file_path"]
            
            # Skip files with errors
            if "error" in result:
                applied_results.append({
                    "file_path": result["file_path"],
                    "applied": False,
                    "error": result["error"]
                })
                continue
            
            # Skip files without changes
            if not result.get("has_changes", False):
                applied_results.append({
                    "file_path": result["file_path"],
                    "applied": False,
                    "message": "No changes to apply"
                })
                continue
            
            try:
                # Read original content
                with open(file_path, 'r', encoding='utf-8', errors='replace') as f:
                    original_content = f.read()
                    
                    
                if backup:
                    backup_path = file_path.with_suffix(file_path.suffix + '.bak')
                    with open(backup_path, 'w', encoding='utf-8') as f:
                        f.write(original_content)
                
                # Apply diff
                new_content, success = diff_manager.apply_diff(original_content, result["diff"])
                
                if not success:
                    # If diff application fails, regenerate the improved code
                    new_content = self._regenerate_improved_code(original_content, result["diff"])
                
                # Write the improved content
                with open(file_path, 'w', encoding='utf-8') as f:
                    f.write(new_content)
                
                applied_results.append({
                    "file_path": result["file_path"],
                    "applied": True,
                    "backup": str(backup_path) if backup else None,
                    "explanation": result.get("explanation", "")
                })
            except Exception as e:
                self._logger.error(f"Error applying changes to {file_path}: {str(e)}")
                applied_results.append({
                    "file_path": result["file_path"],
                    "applied": False,
                    "error": str(e)
                })
        
        return {
            "success": True,
            "project_dir": str(project_dir),
            "results": applied_results,
            "files_processed": len(applied_results),
            "files_changed": sum(1 for r in applied_results if r.get("applied", False))
        }
    
    def _get_language_from_extension(self, extension: str) -> Optional[str]:
        """
        Get programming language from file extension.
        
        Args:
            extension: File extension (with dot)
            
        Returns:
            Language name or None if unknown
        """
        # Map of extensions to languages
        extension_map = {
            '.py': 'Python',
            '.js': 'JavaScript',
            '.jsx': 'JavaScript (React)',
            '.ts': 'TypeScript',
            '.tsx': 'TypeScript (React)',
            '.html': 'HTML',
            '.css': 'CSS',
            '.java': 'Java',
            '.c': 'C',
            '.cpp': 'C++',
            '.h': 'C/C++ Header',
            '.rb': 'Ruby',
            '.go': 'Go',
            '.rs': 'Rust',
            '.php': 'PHP',
            '.swift': 'Swift',
            '.kt': 'Kotlin',
            '.md': 'Markdown',
            '.json': 'JSON',
            '.xml': 'XML',
            '.yaml': 'YAML',
            '.yml': 'YAML',
            '.toml': 'TOML',
            '.sh': 'Shell',
            '.bash': 'Bash',
            '.sql': 'SQL'
        }
        
        return extension_map.get(extension.lower())
    
    def _build_improvement_prompt(
        self, 
        feedback: str,
        original_code: str,
        language: Optional[str],
        file_path: Optional[str],
        context: Optional[Dict[str, Any]] = None
    ) -> str:
        """
        Build a prompt for code improvement.
        
        Args:
            feedback: User feedback
            original_code: Original code to improve
            language: Programming language
            file_path: Path to the file
            context: Additional context
            
        Returns:
            Prompt for the AI service
        """
        # Add language context
        language_str = f"Language: {language}" if language else "Language: Unknown"
        
        # Add file path context
        file_context = f"File: {file_path}" if file_path else ""
        
        # Add additional context if provided
        context_str = ""
        if context:
            context_str = "Additional context:\n"
            for key, value in context.items():
                context_str += f"- {key}: {value}\n"
        
        # Build the prompt
        prompt = f"""
You are an expert software developer helping to improve code based on user feedback.

{language_str}
{file_context}
{context_str}

User feedback:
{feedback}

Your task is to refine the code according to the feedback while preserving the original functionality.
Provide both the improved code and an explanation of the changes you made.

Original code:
{original_code}

Provide your response in this format:
1. First, the full improved code block
2. Then, a detailed explanation of the changes you made

Improved code:
// Your improved code here

Explanation:
// Your explanation here
"""
        
        return prompt
    
    def _extract_improved_code(
        self, 
        response: str, 
        original_code: str
    ) -> Tuple[str, str]:
        """
        Extract improved code and explanation from AI response.
        
        Args:
            response: AI response
            original_code: Original code (fallback)
            
        Returns:
            Tuple of (improved_code, explanation)
        """
        # Try to extract code block
        code_match = re.search(r'```(?:\w*\n)?(.*?)```', response, re.DOTALL)
        
        if code_match:
            code = code_match.group(1).strip()
        else:
            # Fallback: look for "Improved code:" section
            code_section_match = re.search(r'Improved code:\s*(.*?)(?:\n\n|$)', response, re.DOTALL)
            if code_section_match:
                code = code_section_match.group(1).strip()
            else:
                # No clear code section, use original
                code = original_code
        
        # Try to extract explanation
        explanation_match = re.search(r'Explanation:\s*(.*?)(?:\n\n|$)', response, re.DOTALL)
        
        if explanation_match:
            explanation = explanation_match.group(1).strip()
        else:
            # Fallback: anything after the code block
            if code_match:
                parts = response.split('```', 2)
                if len(parts) > 2:
                    explanation = parts[2].strip()
                else:
                    explanation = "No explanation provided."
            else:
                explanation = "No explanation provided."
        
        return code, explanation
    
    def _find_relevant_files(
        self, 
        project_dir: Path, 
        feedback: str
    ) -> List[Path]:
        """
        Find files relevant to the user feedback.
        
        Args:
            project_dir: Project directory
            feedback: User feedback
            
        Returns:
            List of relevant file paths
        """
        relevant_files = []
        
        # Extract potential file references from feedback
        file_mentions = set()
        
        # Look for explicit file references
        file_patterns = [
            r'file[s]?\s+(?:"|\')?([^"\'\s]+)(?:"|\')?',
            r'in\s+(?:"|\')?([^"\'\s]+)(?:"|\')?',
            r'(?:"|\')?([^"\'\s]+\.(?:py|js|java|html|css|cpp|h|go|rb))(?:"|\')?'
        ]
        
        for pattern in file_patterns:
            for match in re.finditer(pattern, feedback, re.IGNORECASE):
                file_mentions.add(match.group(1))
        
        # Check if mentioned files exist
        for mention in file_mentions:
            # Check for exact path
            file_path = project_dir / mention
            if file_path.exists() and file_path.is_file():
                relevant_files.append(file_path)
                continue
            
            # Check for glob pattern
            if '*' in mention or '?' in mention:
                matches = list(project_dir.glob(mention))
                for match in matches:
                    if match.is_file():
                        relevant_files.append(match)
                continue
            
            # Check for just the filename (could be in any directory)
            for root, _, files in os.walk(project_dir):
                if mention in files:
                    relevant_files.append(Path(root) / mention)
        
        # If no specific files mentioned, return all source code files
        if not relevant_files:
            for root, _, files in os.walk(project_dir):
                for file in files:
                    # Skip common non-source files and directories
                    if any(excluded in root for excluded in ['.git', 'node_modules', '__pycache__', '.venv']):
                        continue
                    
                    # Check if it's a source file
                    _, ext = os.path.splitext(file)
                    if ext.lower() in ['.py', '.js', '.jsx', '.ts', '.tsx', '.java', '.c', '.cpp', '.h', '.go', '.rb', '.php', '.swift']:
                        relevant_files.append(Path(root) / file)
        
        return relevant_files
    
    def _regenerate_improved_code(
        self, 
        original_content: str, 
        diff: str
    ) -> str:
        """
        Regenerate improved code from original and diff when apply_diff fails.
        
        This is a fallback method when the diff can't be applied cleanly.
        It uses simple heuristics to apply changes.
        
        Args:
            original_content: Original content
            diff: Unified diff string
            
        Returns:
            Regenerated improved content
        """
        # Simple heuristic: if diff shows additions, add them at the end
        # if diff shows deletions, try to find and remove them
        
        lines = diff.splitlines()
        adds = []
        removes = []
        
        for line in lines:
            if line.startswith('+') and not line.startswith('+++'):
                adds.append(line[1:])
            elif line.startswith('-') and not line.startswith('---'):
                removes.append(line[1:])
        
        # Start with original content
        result = original_content
        
        # Try to remove lines
        for remove in removes:
            result = result.replace(remove + '\n', '')
            result = result.replace(remove, '')
        
        # Add new lines at the end
        if adds:
            if not result.endswith('\n'):
                result += '\n'
            result += '\n'.join(adds)
        
        return result

# Global feedback manager instance
feedback_manager = FeedbackManager()
</file>

<file path="components/safety/__init__.py">
# angela/components/safety/__init__.py
"""
Safety validation for Angela CLI operations.

This package provides functionality to validate and confirm potentially risky
operations before execution, including command classification, preview generation,
and adaptive confirmation based on user preferences.
"""
from .classifier import command_risk_classifier
from .validator import validate_command_safety, validate_operation
from .preview import generate_preview
from .confirmation import get_confirmation, requires_confirmation
from .adaptive_confirmation import get_adaptive_confirmation, offer_command_learning


def classify_command_risk(command: str):
    return command_risk_classifier.classify(command)

def analyze_command_impact(command: str): 
    return command_risk_classifier.analyze_impact(command)
    
    
# Define the main safety check function
async def check_command_safety(command: str, dry_run: bool = False) -> bool:
    is_valid, error_message = validate_command_safety(command)
    if not is_valid:
        logger.warning(f"Command validation failed: {error_message}")
        return False

    risk_level, risk_reason = command_risk_classifier.classify(command) # Use instance
    impact = command_risk_classifier.analyze_impact(command)          # Use instance
    preview_text = await generate_preview(command)

    confirmed = await get_confirmation(
        command=command, risk_level=risk_level, risk_reason=risk_reason,
        impact=impact, preview=preview_text, dry_run=dry_run
    )
    if not confirmed:
        logger.info(f"Command execution cancelled by user: {command}")
        return False
    return True

async def check_operation_safety(operation_type: str, params: dict, dry_run: bool = False) -> bool:
    is_valid, error_message = validate_operation(operation_type, params)
    if not is_valid:
        logger.warning(f"Operation validation failed: {error_message}")
        return False

    if operation_type == 'execute_command':
        return await check_command_safety(params.get('command', ''), dry_run)

    from angela.constants import RISK_LEVELS
    risk_level = RISK_LEVELS["MEDIUM"]
    risk_reason = f"File operation: {operation_type}"

    if operation_type in ['delete_file', 'delete_directory']:
        risk_level = RISK_LEVELS["HIGH"]
    elif operation_type in ['create_file', 'create_directory']:
        risk_level = RISK_LEVELS["LOW"]

    impact = {
        "operations": [operation_type],
        "affected_files": [params.get('path')] if 'path' in params else [],
        "affected_dirs": [params.get('path')] if operation_type.endswith('directory') and 'path' in params else [],
        "destructive": operation_type.startswith('delete'),
        "creates_files": operation_type.startswith('create'),
        "modifies_files": operation_type in ['write_file', 'move_file', 'copy_file']
    }
    confirmed = await get_confirmation(
        command=f"{operation_type}: {params}", risk_level=risk_level, risk_reason=risk_reason,
        impact=impact, preview=None, dry_run=dry_run
    )
    if not confirmed:
        logger.info(f"Operation cancelled by user: {operation_type} {params}")
        return False
    return True

def register_safety_functions():
    registry.register("check_command_safety", check_command_safety)
    registry.register("validate_command_safety", validate_command_safety)
    registry.register("check_operation_safety", check_operation_safety)
    logger.debug("Safety functions registered.")

__all__ = [
    'check_command_safety', 'check_operation_safety',
    'validate_command_safety', 'validate_operation',
    'classify_command_risk',      # Exporting the new function defined in this __init__.py
    'analyze_command_impact',     # Exporting the new function defined in this __init__.py
    'command_risk_classifier',    # Still exporting the instance for direct use if any component needs it
    'generate_preview', 'get_confirmation', 'requires_confirmation',
    'get_adaptive_confirmation', 'offer_command_learning',
    'register_safety_functions'
]

def register_safety_functions():
    """Register safety functions to the registry to avoid circular imports."""
    # Import inside function to avoid circular imports
    from angela.core.registry import registry
    registry.register("check_command_safety", check_command_safety)
    registry.register("validate_command_safety", validate_command_safety)
    registry.register("check_operation_safety", check_operation_safety)
</file>

<file path="components/safety/adaptive_confirmation.py">
# angela/components/safety/adaptive_confirmation.py

import asyncio
from typing import Dict, Any, Optional, List, Tuple

from rich.console import Console
from rich.panel import Panel
from rich.syntax import Syntax
from rich.text import Text
from rich.table import Table

from angela.constants import RISK_LEVELS
from angela.api.context import get_history_manager, get_preferences_manager
from angela.utils.logging import get_logger

logger = get_logger(__name__)

# Create a console for rich output
console = Console()

# Risk level names
RISK_LEVEL_NAMES = {v: k for k, v in RISK_LEVELS.items()}

# Rich-compatible color mapping for risk levels
RISK_COLORS = {
    RISK_LEVELS["SAFE"]: "green",
    RISK_LEVELS["LOW"]: "blue",
    RISK_LEVELS["MEDIUM"]: "yellow",
    RISK_LEVELS["HIGH"]: "red",
    RISK_LEVELS["CRITICAL"]: "dark_red",
}

async def get_adaptive_confirmation(
    command: str, 
    risk_level: int, 
    risk_reason: str,
    impact: Dict[str, Any],
    preview: Optional[str] = None,
    explanation: Optional[str] = None,
    natural_request: Optional[str] = None,
    dry_run: bool = False,
    confidence_score: Optional[float] = None,
    command_info: Optional[Dict[str, Any]] = None
) -> bool:
    """
    Get user confirmation for a command based on risk level and user history.
    
    Args:
        command: The command to be executed
        risk_level: The risk level of the command
        risk_reason: The reason for the risk classification
        impact: The impact analysis dictionary
        preview: Optional preview of command results
        explanation: AI explanation of what the command does
        natural_request: The original natural language request
        dry_run: Whether this is a dry run
        confidence_score: Optional confidence score for the command
        command_info: Optional command information dictionary
        
    Returns:
        True if confirmed or dry_run is True, False otherwise
    """
    # Get managers from API
    preferences_manager = get_preferences_manager()
    history_manager = get_history_manager()
    
    # Check if auto-execution is enabled for this risk level and command
    if preferences_manager.should_auto_execute(risk_level, command):
        # Get command frequency and success rate
        frequency = history_manager.get_command_frequency(command)
        success_rate = history_manager.get_command_success_rate(command)
        
        # For frequently used commands with high success rate, auto-execute
        if frequency >= 5 and success_rate > 0.8:
            logger.info(f"Auto-executing command with high trust: {command}")
            await _show_auto_execution_notice(command, risk_level, preview)
            return True
    
    # Skip confirmation for dry runs
    if dry_run:
        # Get terminal formatter from API
        from angela.api.shell import get_terminal_formatter
        terminal_formatter = get_terminal_formatter()
        
        await terminal_formatter.display_pre_confirmation_info(
            command=command,
            risk_level=risk_level,
            risk_reason=risk_reason,
            impact=impact,
            explanation=explanation,
            preview=preview,
            confidence_score=confidence_score
        )
        
        console.print(Panel(
            "[bold blue]This is a dry run.[/bold blue] No changes will be made.",
            border_style="blue",
            expand=False
        ))
        
        return False
    
    # For all other cases, get explicit confirmation
    risk_name = RISK_LEVEL_NAMES.get(risk_level, "UNKNOWN")
    
    # For high-risk operations, use a more detailed confirmation dialog
    if risk_level >= RISK_LEVELS["HIGH"]:
        return await _get_detailed_confirmation(
            command, risk_level, risk_reason, impact, preview, explanation, confidence_score
        )
    
    # For medium and lower risk operations, use a simpler confirmation
    return await _get_simple_confirmation(
        command, risk_level, risk_reason, preview, explanation, confidence_score
    )


async def _show_auto_execution_notice(
    command: str, 
    risk_level: int,
    preview: Optional[str]
) -> None:
    """Show a notice for auto-execution."""
    risk_name = RISK_LEVEL_NAMES.get(risk_level, "UNKNOWN")
    preferences_manager = get_preferences_manager()
    
    # Get terminal formatter from API
    from angela.api.shell import display_auto_execution_notice
    await display_auto_execution_notice(command, risk_level, preview)
    


async def _get_simple_confirmation(
    command: str, 
    risk_level: int, 
    risk_reason: str,
    preview: Optional[str],
    explanation: Optional[str],
    confidence_score: Optional[float] = None
) -> bool:
    """Get a simple confirmation for medium/low risk operations."""
    # Get terminal formatter from API
    from angela.api.shell import get_terminal_formatter
    terminal_formatter = get_terminal_formatter()
    
    # Risk name for display
    risk_name = RISK_LEVEL_NAMES.get(risk_level, "UNKNOWN")
    
    # Display all information
    await terminal_formatter.display_pre_confirmation_info(
        command=command,
        risk_level=risk_level,
        risk_reason=risk_reason,
        impact={"operations": [risk_reason]},  # Simple impact for low-risk operations
        explanation=explanation,
        preview=preview,
        confidence_score=confidence_score
    )
    
    # Ask for confirmation with inline prompt
    prompt_text = f"Proceed with this {risk_name} risk operation?"
    return await terminal_formatter.display_inline_confirmation(prompt_text)


async def _get_detailed_confirmation(
    command: str, 
    risk_level: int, 
    risk_reason: str,
    impact: Dict[str, Any],
    preview: Optional[str],
    explanation: Optional[str],
    confidence_score: Optional[float] = None
) -> bool:
    """Get a detailed confirmation for high/critical risk operations."""
    # Get terminal formatter from API
    from angela.api.shell import get_terminal_formatter
    terminal_formatter = get_terminal_formatter()
    
    # Risk name for display
    risk_name = RISK_LEVEL_NAMES.get(risk_level, "UNKNOWN")
    risk_color = RISK_COLORS.get(risk_level, "red")
    
    # Display all information
    await terminal_formatter.display_pre_confirmation_info(
        command=command,
        risk_level=risk_level,
        risk_reason=risk_reason,
        impact=impact,
        explanation=explanation,
        preview=preview,
        confidence_score=confidence_score
    )
    
    # For critical operations, add an extra warning
    if risk_level >= RISK_LEVELS["CRITICAL"]:
        console.print(Panel(
            f"[bold red]⚠️  This is a {risk_name} RISK operation  ⚠️[/bold red]\n"
            "It may cause significant changes to your system or data that cannot be easily undone.",
            border_style="red",
            expand=False
        ))
    
    # Ask for confirmation with additional warning
    prompt_text = f"⚠️ Proceed with this {risk_name} RISK operation? ⚠️"
    confirmed = await terminal_formatter.display_inline_confirmation(prompt_text)
    
    # If confirmed for a high-risk operation, offer to add to trusted commands
    if confirmed and risk_level >= RISK_LEVELS["HIGH"]:
        # Get preferences manager from API
        from angela.api.context import get_preferences_manager
        preferences_manager = get_preferences_manager()
        
        # Ask if the user wants to trust this command
        trust_prompt = "Add to trusted commands for future auto-execution?"
        add_to_trusted = await terminal_formatter.display_inline_confirmation(trust_prompt)
        
        if add_to_trusted:
            preferences_manager.add_trusted_command(command)
            console.print(f"[green]Added command to trusted list. It will execute automatically in the future.[/green]")
    
    return confirmed


async def offer_command_learning(command: str) -> None:
    """
    After a successful execution, offer to add the command to trusted commands.
    
    Args:
        command: The command that was executed
    """
    # Get managers from API
    preferences_manager = get_preferences_manager()
    history_manager = get_history_manager()
    
    # Check if the command should be offered for learning
    base_command = history_manager._extract_base_command(command)
    pattern = history_manager._patterns.get(base_command)
    
    # Only offer for commands used a few times but not yet trusted
    if pattern and pattern.count >= 2 and command not in preferences_manager.preferences.trust.trusted_commands:
        # Check if user has previously rejected this command
        rejection_count = preferences_manager.get_command_rejection_count(command)
        
        # Determine if we should show the prompt based on rejection count
        threshold = 2  # Base threshold
        if rejection_count > 0:
            # Progressive threshold: 2, 5, 7, 9, 11, etc.
            threshold = 2 + (rejection_count * 2)
        
        if pattern.count >= threshold:
            # Get terminal formatter for inline confirmation
            from angela.api.shell import get_terminal_formatter, display_command_learning, display_trust_added_message
            terminal_formatter = get_terminal_formatter()
            
            # Create a fancy learning prompt with purple styling
            await display_command_learning(base_command, pattern.count)
            
            # Use the new custom y/n formatting
            prompt_text = f"Would you like to auto-execute this command in the future?"
            add_to_trusted = await terminal_formatter.display_inline_confirmation(prompt_text)
            
            if add_to_trusted:
                preferences_manager.add_trusted_command(command)
                await display_trust_added_message(command)
            else:
                # Record the rejection to increase the threshold for next time
                preferences_manager.increment_command_rejection_count(command)
                console.print(f"[dim]You'll be asked again after using this command {threshold + 2} more times.[/dim]")


# Export for API access
adaptive_confirmation = get_adaptive_confirmation
</file>

<file path="components/safety/classifier.py">
# angela/safety/classifier.py
"""
Command and operation risk classification system for Angela CLI.

This module is responsible for determining the risk level of commands 
and operations to ensure appropriate confirmation and safety measures.
"""
import re
import shlex
from typing import List, Dict, Tuple, Set, Optional

from angela.constants import RISK_LEVELS
from angela.utils.logging import get_logger

logger = get_logger(__name__)

# Define risk patterns for shell commands
RISK_PATTERNS = {
    # Critical risk - destructive operations
    RISK_LEVELS["CRITICAL"]: [
        # rm with recursive or force flags
        (r"^rm\s+.*((-r|-rf|--recursive|-f|--force)\b|--)", "File deletion with dangerous flags"),
        # Disk formatting
        (r"^(mkfs|fdisk|dd)\b", "Disk formatting/partitioning"), 
        # Systemwide configuration changes
        (r"^(sudo|pkexec|su)\s+", "Privileged operation"),
        # Direct writes to device files
        (r">\s*/dev/", "Direct write to device file"),
    ],
    
    # High risk - significant changes
    RISK_LEVELS["HIGH"]: [
        # Regular file deletion
        (r"^rm\s+", "File deletion"),
        # Moving files
        (r"^mv\s+", "File movement"),
        # Installing packages
        (r"^(apt(-get)?|yum|pacman|dnf|brew)\s+(install|remove|purge)\b", "Package management"),
        # Changing permissions
        (r"^chmod\s+", "Changing file permissions"),
        # Changing ownership
        (r"^chown\s+", "Changing file ownership"),
    ],
    
    # Medium risk - file modifications
    RISK_LEVELS["MEDIUM"]: [
        # Writing to files
        (r"(>|>>)\s*[\w\./-]+", "Writing to files"),
        # Editing files
        (r"^(nano|vim|vi|emacs|sed)\s+", "File editing"),
        # Creating symbolic links
        (r"^ln\s+(-s|--symbolic)\s+", "Creating symbolic links"),
        # Transferring files remotely
        (r"^(scp|rsync)\s+", "File transfer"),
    ],
    
    # Low risk - creating files/dirs without overwriting
    RISK_LEVELS["LOW"]: [
        # Making directories
        (r"^mkdir\s+", "Creating directory"),
        # Touching files
        (r"^touch\s+", "Creating/updating file timestamp"),
        # Copying files
        (r"^cp\s+", "Copying files"),
    ],
    
    # Safe - read-only operations
    RISK_LEVELS["SAFE"]: [
        # Listing files
        (r"^ls\s+", "Listing files"),
        # Reading files
        (r"^(cat|less|more|head|tail)\s+", "Reading file content"),
        # Finding files
        (r"^find\s+", "Finding files"),
        # Viewing disk usage
        (r"^du\s+", "Checking disk usage"),
        # Getting working directory
        (r"^pwd\s*$", "Printing working directory"),
        # Checking file status
        (r"^(stat|file)\s+", "Checking file information"),
    ],
}

# Special case patterns that override normal classification
OVERRIDE_PATTERNS = {
    # Force certain grep operations to be safe
    "SAFE": [
        r"^grep\s+(-r|--recursive)?\s+[\w\s]+\s+[\w\s\./-]+$",  # Basic grep with fixed strings
        r"^find\s+[\w\s\./-]+\s+-name\s+[\w\s\*\./-]+$",  # Basic find by name
    ],
    # Operations that should always be considered critical regardless of base command
    "CRITICAL": [
        r"[\s;|`]+rm\s+(-r|-f|--recursive|--force)\s+[~/]",  # rm commands affecting home or root
        r"[\s;|`]+dd\s+",  # dd embedded in a command chain
        r">/dev/null\s+2>&1",  # Redirecting errors (often hiding destructive operations)
    ],
}



class CommandRiskClassifier: # New Class
    def classify(self, command: str) -> Tuple[int, str]: # Renamed from classify_command_risk
        """
        Classify the risk level of a shell command.
        """
        if not command.strip():
            return RISK_LEVELS["SAFE"], "Empty command"

        for level_name, patterns in OVERRIDE_PATTERNS.items():
            for pattern in patterns:
                if re.search(pattern, command):
                    level = RISK_LEVELS[level_name]
                    return level, f"Matched override pattern for {level_name} risk"

        for level, patterns in sorted(RISK_PATTERNS.items(), key=lambda x: x[0], reverse=True):
            for pattern, reason in patterns:
                if re.search(pattern, command.strip()):
                    return level, reason

        return RISK_LEVELS["MEDIUM"], "Unrecognized command type"

    def analyze_impact(self, command: str) -> Dict[str, any]: # Renamed from analyze_command_impact
        """
        Analyze the potential impact of a command.
        """
        impact = {
            "affected_files": set(),
            "affected_dirs": set(),
            "operations": [],
            "destructive": False,
            "creates_files": False,
            "modifies_files": False,
        }
        try:
            tokens = shlex.split(command)
            if not tokens:
                return impact
            base_cmd = tokens[0]
            args = tokens[1:]
            for arg in args:
                if arg.startswith('-'):
                    continue
                if arg in ['>', '>>', '<', '|']:
                    continue
                if '/' in arg or '.' in arg or not arg.startswith('-'):
                    if base_cmd in ['rm', 'mv', 'rmdir']:
                        impact["destructive"] = True
                    if base_cmd in ['mkdir']:
                        impact["affected_dirs"].add(arg)
                        impact["creates_files"] = True
                    else:
                        impact["affected_files"].add(arg)
                    if base_cmd in ['cp', 'mv', 'touch', 'mkdir', 'ln']:
                        impact["creates_files"] = True
                    if base_cmd in ['vim', 'nano', 'sed', 'cp', 'mv']:
                        impact["modifies_files"] = True
            if base_cmd in ['ls', 'find', 'grep', 'cat', 'less', 'more', 'tail', 'head']:
                impact["operations"].append("read")
            elif base_cmd in ['rm', 'rmdir']:
                impact["operations"].append("delete")
            elif base_cmd in ['mv']:
                impact["operations"].append("move")
            elif base_cmd in ['cp']:
                impact["operations"].append("copy")
            elif base_cmd in ['touch', 'mkdir']:
                impact["operations"].append("create")
            elif base_cmd in ['chmod', 'chown']:
                impact["operations"].append("change_attributes")
            else:
                impact["operations"].append("unknown")
        
        except Exception as e:
            logger.exception(f"Error analyzing command impact for '{command}': {str(e)}")
        
        # Convert sets to lists for easier serialization
        impact["affected_files"] = list(impact["affected_files"])
        impact["affected_dirs"] = list(impact["affected_dirs"])
        
        return impact
        
       
command_risk_classifier = CommandRiskClassifier()
</file>

<file path="components/safety/confirmation.py">
# angela/components/safety/confirmation.py

import sys
from typing import Dict, Any, Optional, List, Tuple

from rich.console import Console
from rich.panel import Panel
from rich.syntax import Syntax
from rich.table import Table
from rich.text import Text

from angela.constants import RISK_LEVELS, DEFAULT_CONFIRMATION_REQUIREMENTS
from angela.config import config_manager
from angela.utils.logging import get_logger

logger = get_logger(__name__)
console = Console()

# Risk level color mapping
RISK_COLORS = {
    RISK_LEVELS["SAFE"]: "green",
    RISK_LEVELS["LOW"]: "blue", 
    RISK_LEVELS["MEDIUM"]: "yellow",
    RISK_LEVELS["HIGH"]: "bright_red",  
    RISK_LEVELS["CRITICAL"]: "red",
}

# Risk level names for display
RISK_LEVEL_NAMES = {v: k for k, v in RISK_LEVELS.items()}


def requires_confirmation(risk_level: int) -> bool:
    """
    Determine if a risk level requires confirmation based on configuration.
    
    Args:
        risk_level: The risk level to check.
        
    Returns:
        True if confirmation is required, False otherwise.
    """
    # If confirm_all_actions is set, always confirm
    if config_manager.config.user.confirm_all_actions:
        return True
    
    # Otherwise, check the default requirements
    return DEFAULT_CONFIRMATION_REQUIREMENTS.get(risk_level, True)


def format_impact_analysis(impact: Dict[str, Any]) -> Table:
    """
    Format the command impact analysis into a rich Table.
    
    Args:
        impact: The impact analysis dictionary.
        
    Returns:
        A rich Table object with the formatted impact analysis.
    """
    table = Table(expand=True)
    
    table.add_column("Aspect", style="bold cyan")
    table.add_column("Details", style="white")
    
    # Add operation types
    operations = ", ".join(impact.get("operations", ["unknown"]))
    table.add_row("Operations", operations)
    
    # Add destructive warning if applicable
    if impact.get("destructive", False):
        table.add_row("⚠️ Warning", "[bold red]This operation may delete or overwrite files[/bold red]")
    
    # Add file creation info
    if impact.get("creates_files", False):
        table.add_row("Creates Files", "Yes")
    
    # Add file modification info
    if impact.get("modifies_files", False):
        table.add_row("Modifies Files", "Yes")
    
    # Add affected files
    affected_files = impact.get("affected_files", [])
    if affected_files:
        file_list = "\n".join(affected_files[:5])
        if len(affected_files) > 5:
            file_list += f"\n...and {len(affected_files) - 5} more"
        table.add_row("Affected Files", file_list)
    
    # Add affected directories
    affected_dirs = impact.get("affected_dirs", [])
    if affected_dirs:
        dir_list = "\n".join(affected_dirs[:5])
        if len(affected_dirs) > 5:
            dir_list += f"\n...and {len(affected_dirs) - 5} more"
        table.add_row("Affected Directories", dir_list)
    
    return table


async def get_confirmation(
    command: str, 
    risk_level: int, 
    risk_reason: str,
    impact: Dict[str, Any],
    preview: Optional[str] = None,
    dry_run: bool = False,
    explanation: Optional[str] = None,
    confidence_score: Optional[float] = None
) -> bool:
    """
    Get user confirmation for a command based on its risk level.
    
    Args:
        command: The command to be executed.
        risk_level: The risk level of the command.
        risk_reason: The reason for the risk classification.
        impact: The impact analysis dictionary.
        preview: Optional preview of command results.
        dry_run: Whether this is a dry run.
        explanation: Explanation of what the command does.
        confidence_score: Confidence score for the command.
        
    Returns:
        True if the user confirms, False otherwise.
    """
    # If safety checks are not required, return True
    if not requires_confirmation(risk_level) and not dry_run:
        return True
    
    # Get the terminal formatter
    from angela.api.shell import get_terminal_formatter
    terminal_formatter = get_terminal_formatter()
    
    # For dry run, just show the information without asking for confirmation
    if dry_run:
        await terminal_formatter.display_pre_confirmation_info(
            command=command,
            risk_level=risk_level,
            risk_reason=risk_reason,
            impact=impact,
            explanation=explanation,
            preview=preview,
            confidence_score=confidence_score
        )
        
        console.print(Panel(
            "[bold blue]This is a dry run.[/bold blue] No changes will be made.",
            border_style="blue",
            expand=False
        ))
        
        return False
    
    # Display all the information
    await terminal_formatter.display_pre_confirmation_info(
        command=command,
        risk_level=risk_level,
        risk_reason=risk_reason,
        impact=impact,
        explanation=explanation,
        preview=preview,
        confidence_score=confidence_score
    )
    
    # Ask for confirmation
    prompt_text = "Proceed with execution?"
    if risk_level >= RISK_LEVELS["HIGH"]:
        prompt_text = f"Proceed with this {RISK_LEVEL_NAMES.get(risk_level, 'HIGH')} risk operation?"
        
    return await terminal_formatter.display_inline_confirmation(prompt_text)
</file>

<file path="components/safety/preview.py">
# angela/components/safety/preview.py
"""
Preview generator for command execution.

This module generates previews of what commands will do before they are executed,
helping users make informed decisions about risky operations.
"""
import os
import re
import shlex
import glob
import tempfile
from pathlib import Path
from typing import Dict, Any, Optional, List, Tuple

from angela.api.execution import get_execution_engine
from angela.utils.logging import get_logger

logger = get_logger(__name__)



async def preview_mkdir(command: str, tokens: List[str]) -> str:
    """Generate a preview for mkdir command."""
    # Parse flags and paths
    paths = []
    recursive = '-p' in tokens or '--parents' in tokens
    
    for arg in tokens[1:]:
        if not arg.startswith('-'):
            paths.append(arg)
    
    result = []
    for path in paths:
        path_obj = Path(path)
        if path_obj.exists():
            result.append(f"⚠️ Path already exists: {path}")
        elif path_obj.parent.exists() or recursive:
            result.append(f"✓ Will create directory: {path}")
        else:
            result.append(f"❌ Parent directory does not exist: {path.parent}")
    
    if not result:
        return "No directories specified to create."
    
    return "\n".join(result)


async def preview_touch(command: str, tokens: List[str]) -> str:
    """Generate a preview for touch command."""
    # Parse flags and paths
    paths = []
    
    for arg in tokens[1:]:
        if not arg.startswith('-'):
            paths.append(arg)
    
    result = []
    for path in paths:
        path_obj = Path(path)
        if path_obj.exists():
            result.append(f"Will update timestamp: {path}")
        elif path_obj.parent.exists():
            result.append(f"Will create empty file: {path}")
        else:
            result.append(f"❌ Parent directory does not exist: {path_obj.parent}")
    
    if not result:
        return "No files specified to touch."
    
    return "\n".join(result)


async def preview_rm(command: str, tokens: List[str]) -> str:
    """Generate a preview for rm command."""
    # Parse flags and paths
    paths = []
    recursive = '-r' in tokens or '--recursive' in tokens or '-rf' in tokens
    force = '-f' in tokens or '--force' in tokens or '-rf' in tokens
    
    for arg in tokens[1:]:
        if not arg.startswith('-'):
            paths.append(arg)
    
    # Expand any glob patterns
    expanded_paths = []
    for path in paths:
        if '*' in path or '?' in path or '[' in path:
            # Use glob to expand wildcards
            expanded = glob.glob(path)
            if expanded:
                expanded_paths.extend(expanded)
            else:
                expanded_paths.append(f"{path} (no matches)")
        else:
            expanded_paths.append(path)
    
    result = []
    for path in expanded_paths:
        path_obj = Path(path)
        if not path_obj.exists():
            if force:
                continue  # With force flag, non-existent files are silently ignored
            else:
                result.append(f"❌ Not found: {path}")
        elif path_obj.is_dir() and not recursive:
            result.append(f"❌ Cannot remove directory without -r flag: {path}")
        elif path_obj.is_dir():
            file_count = sum(1 for _ in path_obj.glob('**/*'))
            result.append(f"⚠️ Will remove directory containing {file_count} files: {path}")
        else:
            result.append(f"Will remove file: {path}")
    
    if not result:
        return "No files specified to remove or all paths are invalid."
    
    return "\n".join(result)


async def preview_cp(command: str, tokens: List[str]) -> str:
    """Generate a preview for cp command."""
    # This is a simplified preview that doesn't handle all cp options
    
    # Need at least 3 tokens: cp source dest
    if len(tokens) < 3:
        return "Invalid cp command: missing source or destination"
    
    # Last argument is the destination
    destination = tokens[-1]
    # All arguments except the command and destination are sources
    sources = [arg for arg in tokens[1:-1] if not arg.startswith('-')]
    
    recursive = '-r' in tokens or '--recursive' in tokens
    
    result = []
    for source in sources:
        source_path = Path(source)
        
        if not source_path.exists():
            result.append(f"❌ Source does not exist: {source}")
            continue
        
        if source_path.is_dir() and not recursive:
            result.append(f"❌ Cannot copy directory without -r flag: {source}")
            continue
        
        # Determine the destination path
        dest_path = Path(destination)
        if len(sources) > 1 or dest_path.is_dir():
            # Multiple sources or destination is a directory
            if not dest_path.exists():
                if dest_path.name.endswith('/'):  # Explicitly specified as directory
                    result.append(f"Will create directory: {destination}")
                else:
                    result.append(f"Will copy {source} to {destination}")
            else:
                if dest_path.is_dir():
                    result.append(f"Will copy {source} to {destination}/{source_path.name}")
                else:
                    result.append(f"⚠️ Cannot copy multiple sources to a single file: {destination}")
        else:
            # Single source to destination
            if dest_path.exists() and dest_path.is_file():
                result.append(f"⚠️ Will overwrite: {destination}")
            else:
                result.append(f"Will copy {source} to {destination}")
    
    if not result:
        return "No files specified to copy."
    
    return "\n".join(result)


async def preview_mv(command: str, tokens: List[str]) -> str:
    """Generate a preview for mv command."""
    # This is a simplified preview that doesn't handle all mv options
    
    # Need at least 3 tokens: mv source dest
    if len(tokens) < 3:
        return "Invalid mv command: missing source or destination"
    
    # Last argument is the destination
    destination = tokens[-1]
    # All arguments except the command and destination are sources
    sources = [arg for arg in tokens[1:-1] if not arg.startswith('-')]
    
    result = []
    for source in sources:
        source_path = Path(source)
        
        if not source_path.exists():
            result.append(f"❌ Source does not exist: {source}")
            continue
        
        # Determine the destination path
        dest_path = Path(destination)
        if len(sources) > 1 or dest_path.is_dir():
            # Multiple sources or destination is a directory
            if not dest_path.exists():
                if dest_path.name.endswith('/'):  # Explicitly specified as directory
                    result.append(f"Will create directory: {destination}")
                else:
                    result.append(f"Will move {source} to {destination}")
            else:
                if dest_path.is_dir():
                    result.append(f"Will move {source} to {destination}/{source_path.name}")
                else:
                    result.append(f"⚠️ Cannot move multiple sources to a single file: {destination}")
        else:
            # Single source to destination
            if dest_path.exists() and dest_path.is_file():
                result.append(f"⚠️ Will overwrite: {destination}")
            else:
                result.append(f"Will move {source} to {destination}")
    
    if not result:
        return "No files specified to move."
    
    return "\n".join(result)


async def preview_ls(command: str, tokens: List[str]) -> str:
    """Generate a preview for ls command."""
    # Extract the paths from the command
    paths = []
    for arg in tokens[1:]:
        if not arg.startswith('-'):
            paths.append(arg)
    
    # If no paths specified, use current directory
    if not paths:
        paths = ['.']
    
    result = []
    for path in paths:
        try:
            path_obj = Path(path)
            if not path_obj.exists():
                result.append(f"❌ Path does not exist: {path}")
                continue
            
            if path_obj.is_dir():
                # Just count files rather than listing them all
                file_count = sum(1 for _ in path_obj.iterdir())
                result.append(f"Will list directory: {path} (contains {file_count} entries)")
            else:
                result.append(f"Will show file information: {path}")
        except Exception as e:
            result.append(f"Error analyzing {path}: {str(e)}")
    
    return "\n".join(result)


async def preview_cat(command: str, tokens: List[str]) -> str:
    """Generate a preview for cat command."""
    # Extract the paths from the command
    paths = []
    for arg in tokens[1:]:
        if not arg.startswith('-'):
            paths.append(arg)
    
    if not paths:
        return "No files specified to display."
    
    result = []
    for path in paths:
        path_obj = Path(path)
        if not path_obj.exists():
            result.append(f"❌ File does not exist: {path}")
        elif path_obj.is_dir():
            result.append(f"❌ Cannot display directory content: {path}")
        else:
            # Get file size
            size = path_obj.stat().st_size
            size_str = f"{size} bytes"
            if size > 1024:
                size_str = f"{size/1024:.1f} KB"
            if size > 1024 * 1024:
                size_str = f"{size/(1024*1024):.1f} MB"
            
            # Try to determine if it's a text file
            try:
                with open(path_obj, 'rb') as f:
                    is_text = True
                    for block in iter(lambda: f.read(1024), b''):
                        if b'\0' in block:
                            is_text = False
                            break
                
                if is_text:
                    # Count lines
                    with open(path_obj, 'r', errors='replace') as f:
                        line_count = sum(1 for _ in f)
                    result.append(f"Will display text file: {path} ({size_str}, {line_count} lines)")
                else:
                    result.append(f"⚠️ Will display binary file: {path} ({size_str})")
            except Exception as e:
                result.append(f"Error analyzing {path}: {str(e)}")
    
    return "\n".join(result)


async def preview_grep(command: str, tokens: List[str]) -> str:
    """Generate a preview for grep command."""
    # This is a simplified preview that doesn't handle all grep options
    
    # Need at least 3 tokens: grep pattern file
    if len(tokens) < 3:
        return "Invalid grep command: missing pattern or file"
    
    pattern = None
    files = []
    recursive = '-r' in tokens or '--recursive' in tokens
    
    # Simple parsing to extract pattern and files
    pattern_found = False
    for arg in tokens[1:]:
        if arg.startswith('-'):
            continue
        
        if not pattern_found:
            pattern = arg
            pattern_found = True
        else:
            files.append(arg)
    
    if not pattern:
        return "No pattern specified for grep."
    
    if not files:
        if recursive:
            files = ['.']
        else:
            return "No files specified for grep."
    
    result = []
    for file_path in files:
        path_obj = Path(file_path)
        if not path_obj.exists():
            result.append(f"❌ Path does not exist: {file_path}")
        elif path_obj.is_dir() and not recursive:
            result.append(f"❌ Cannot grep directory without -r flag: {file_path}")
        elif path_obj.is_dir() and recursive:
            # Count files in directory
            file_count = sum(1 for _ in path_obj.glob('**/*') if Path(_).is_file())
            result.append(f"Will search for '{pattern}' in directory: {file_path} "
                         f"(contains {file_count} files)")
        else:
            # Try to count occurrences in file
            try:
                with open(path_obj, 'r', encoding='utf-8', errors='replace') as f:
                    content = f.read()
                    count = len(re.findall(pattern, content))
                    result.append(f"Will search for '{pattern}' in {file_path} "
                                 f"(potentially {count} matches)")
            except Exception as e:
                result.append(f"Will search in {file_path}, but preview failed: {str(e)}")
    
    return "\n".join(result)


async def preview_find(command: str, tokens: List[str]) -> str:
    """Generate a preview for find command."""
    # Extract directories to search from the command
    # This is a simple implementation that doesn't handle all find options
    
    dirs = []
    name_pattern = None
    type_filter = None
    
    # Find the directories (arguments before the first option)
    for i, arg in enumerate(tokens[1:], 1):
        if arg.startswith('-'):
            break
        dirs.append(arg)
    
    # If no directories specified, use current directory
    if not dirs:
        dirs = ['.']
    
    # Try to extract name pattern if present
    for i, arg in enumerate(tokens):
        if arg == '-name' and i + 1 < len(tokens):
            name_pattern = tokens[i + 1]
        elif arg == '-type' and i + 1 < len(tokens):
            type_filter = tokens[i + 1]
    
    result = []
    for directory in dirs:
        dir_path = Path(directory)
        if not dir_path.exists():
            result.append(f"❌ Directory does not exist: {directory}")
            continue
        
        if not dir_path.is_dir():
            result.append(f"❌ Not a directory: {directory}")
            continue
        
        # Count files and directories in the search path
        file_count = sum(1 for _ in dir_path.glob('**/*') if Path(_).is_file())
        dir_count = sum(1 for _ in dir_path.glob('**/*') if Path(_).is_dir())
        
        search_desc = f"Will search in: {directory} ({file_count} files, {dir_count} directories)"
        
        if name_pattern:
            search_desc += f"\nLooking for files matching: {name_pattern}"
        
        if type_filter:
            type_desc = {'f': 'files', 'd': 'directories', 'l': 'symbolic links'}.get(type_filter, type_filter)
            search_desc += f"\nFiltering by type: {type_desc}"
        
        result.append(search_desc)
    
    return "\n".join(result)

# Commands that can be simulated with more specific previews
PREVIEWABLE_COMMANDS = {
    'mkdir': preview_mkdir,
    'touch': preview_touch,
    'rm': preview_rm,
    'cp': preview_cp,
    'mv': preview_mv,
    'ls': preview_ls,
    'cat': preview_cat,
    'grep': preview_grep,
    'find': preview_find,
}

async def generate_preview(command: str) -> Optional[str]:
    """
    Generate a preview of what a command will do.
    
    Args:
        command: The shell command to preview.
        
    Returns:
        A string containing the preview, or None if preview is not available.
    """
    try:
        # Parse the command
        tokens = shlex.split(command)
        if not tokens:
            return None
        
        base_cmd = tokens[0]
        
        # Check if we have a specific preview function for this command
        if base_cmd in PREVIEWABLE_COMMANDS:
            return await PREVIEWABLE_COMMANDS[base_cmd](command, tokens)
        
        # For other commands, try to use --dry-run or similar flags if available
        return await generic_preview(command)
    
    except Exception as e:
        logger.exception(f"Error generating preview for '{command}': {str(e)}")
        return f"Preview generation failed: {str(e)}"


async def generic_preview(command: str) -> Optional[str]:
    """
    Generate a generic preview for commands without specific implementations.
    Attempts to use --dry-run flags when available.
    
    Args:
        command: The shell command to preview.
        
    Returns:
        A string containing the preview, or None if preview is not available.
    """
    # List of commands that support --dry-run or similar
    dry_run_commands = {
        'rsync': '--dry-run',
        'apt': '--dry-run',
        'apt-get': '--dry-run',
        'dnf': '--dry-run',
        'yum': '--dry-run',
        'pacman': '--print',
    }
    
    tokens = shlex.split(command)
    base_cmd = tokens[0]
    
    if base_cmd in dry_run_commands:
        # Add the dry run flag
        dry_run_flag = dry_run_commands[base_cmd]
        
        # Check if the flag is already in the command
        if dry_run_flag not in command:
            modified_command = f"{command} {dry_run_flag}"
        else:
            modified_command = command
        
        # Execute the command with the dry run flag
        # Get execution engine from API
        execution_engine = get_execution_engine()
        stdout, stderr, return_code = await execution_engine.execute_command(modified_command)
        
        if return_code == 0:
            return f"Dry run output:\n{stdout}"
        else:
            return f"Dry run failed with error:\n{stderr}"
    
    # For commands without dry run support, return a generic message
    return "Preview not available for this command type. Use --dry-run to simulate."
    
    
class CommandPreviewGenerator:
    """Generator for command previews."""
    
    async def generate_preview(self, command: str) -> Optional[str]:
        """
        Generate a preview of what a command will do.
        
        Args:
            command: The shell command to preview.
            
        Returns:
            A string containing the preview, or None if preview is not available.
        """
        # This simply delegates to the existing function
        preview_text = await generate_preview(command)
        
        return preview_text

# Create a global instance of the generator class
command_preview_generator = CommandPreviewGenerator()
</file>

<file path="components/safety/validator.py">
# angela/safety/validator.py
"""
Safety validation for operations.

This module validates operations against safety policies and constraints
before they are executed.
"""
import os
import re
import shlex
from pathlib import Path
from typing import Dict, Any, Optional, List, Tuple

from angela.constants import RISK_LEVELS
from angela.utils.logging import get_logger

logger = get_logger(__name__)

# Define dangerous patterns that should be blocked
DANGEROUS_PATTERNS = [
    # Remove critical system directories
    (r"rm\s+(-r|-f|--recursive|--force)\s+(/|/boot|/etc|/bin|/sbin|/lib|/usr|/var|~)",
     "Removing critical system directories is not allowed"),
    
    # Format disk operations
    (r"(mkfs|fdisk|dd|shred)\s+.*(/dev/sd[a-z]|/dev/nvme[0-9])",
     "Disk formatting operations are not allowed"),
    
    # Critical system commands
    (r"(shutdown|reboot|halt|poweroff|init\s+0|init\s+6)",
     "System power commands are not allowed"),
    
    # Chmod 777 recursively
    (r"chmod\s+(-R|--recursive)\s+777",
     "Setting recursive 777 permissions is not allowed"),
    
    # Network disruption
    (r"(ifconfig|ip)\s+.*down",
     "Network interface disabling is not allowed"),
    
    # Dangerous redirects
    (r">\s*(/etc/passwd|/etc/shadow|/etc/sudoers)",
     "Writing directly to critical system files is not allowed"),
    
    # Hidden command execution
    (r";\s*rm\s+",
     "Hidden deletion commands are not allowed"),
    
    # Web download + execute
    (r"(curl|wget).*\|\s*(bash|sh)",
     "Downloading and executing scripts is not allowed"),
    
    # Disk full attack
    (r"(dd|fallocate)\s+.*if=/dev/zero",
     "Creating large files that may fill disk space is not allowed"),
    
    # Dangerous shell loops
    (r"for\s+.*\s+in\s+.*;.*rm\s+",
     "Shell loops with file deletion are not allowed"),
]

# Define patterns that would require root/sudo access
ROOT_PATTERNS = [
    r"^sudo\s+",
    r"^pkexec\s+",
    r"^su\s+(-|--|-c|\w+)\s+",
    r"(chmod|chown|chgrp)\s+.*(/usr/|/etc/|/bin/|/sbin/|/lib/|/var/)",
    r"(touch|rm|mv|cp)\s+.*(/usr/|/etc/|/bin/|/sbin/|/lib/|/var/)",
    r">\s*(/usr/|/etc/|/bin/|/sbin/|/lib/|/var/)",
]

class ValidationError(Exception):
    """Exception raised when a command fails validation."""
    pass


def validate_command_safety(command: str) -> Tuple[bool, Optional[str]]:
    """
    Validate a command against safety rules.
    
    Args:
        command: The shell command to validate.
        
    Returns:
        A tuple of (is_valid, error_message). If is_valid is False,
        error_message will contain the reason.
    """
    if not command.strip():
        return True, None
    
    # Check against dangerous patterns
    for pattern, message in DANGEROUS_PATTERNS:
        if re.search(pattern, command):
            logger.warning(f"Command '{command}' blocked: {message}")
            return False, message
    
    # Check permission requirements
    if not is_superuser() and requires_superuser(command):
        logger.warning(f"Command '{command}' requires superuser privileges")
        return False, "This command requires superuser privileges, which Angela CLI doesn't have."
    
    return True, None


def requires_superuser(command: str) -> bool:
    """
    Check if a command requires superuser privileges.
    
    Args:
        command: The shell command to check.
        
    Returns:
        True if the command requires superuser privileges, False otherwise.
    """
    for pattern in ROOT_PATTERNS:
        if re.search(pattern, command):
            return True
    
    return False


def is_superuser() -> bool:
    """
    Check if the current process has superuser privileges.
    
    Returns:
        True if running as superuser, False otherwise.
    """
    return os.geteuid() == 0 if hasattr(os, 'geteuid') else False


def check_file_permission(path: Path, require_write: bool = False) -> Tuple[bool, Optional[str]]:
    """
    Check if a file has the required permissions.
    
    Args:
        path: The path to check.
        require_write: Whether write permission is required.
        
    Returns:
        A tuple of (has_permission, error_message). If has_permission is False,
        error_message will contain the reason.
    """
    try:
        if not path.exists():
            # If the file doesn't exist, check if the parent directory is writable
            if require_write:
                parent = path.parent
                if not parent.exists():
                    return False, f"Parent directory {parent} does not exist"
                if not os.access(parent, os.W_OK):
                    return False, f"No write permission for directory {parent}"
            return True, None
        
        if not os.access(path, os.R_OK):
            return False, f"No read permission for {path}"
        
        if require_write and not os.access(path, os.W_OK):
            return False, f"No write permission for {path}"
        
        return True, None
    
    except Exception as e:
        logger.exception(f"Error checking permissions for {path}: {str(e)}")
        return False, f"Permission check failed: {str(e)}"


def validate_operation(operation_type: str, params: Dict[str, Any]) -> Tuple[bool, Optional[str]]:
    """
    Validate a high-level operation against safety rules.
    
    Args:
        operation_type: The type of operation (e.g., 'create_file', 'delete_file').
        params: Parameters for the operation.
        
    Returns:
        A tuple of (is_valid, error_message). If is_valid is False,
        error_message will contain the reason.
    """
    try:
        if operation_type == 'create_file':
            path = Path(params.get('path', ''))
            return check_file_permission(path, require_write=True)
        
        elif operation_type == 'write_file':
            path = Path(params.get('path', ''))
            return check_file_permission(path, require_write=True)
        
        elif operation_type == 'read_file':  # Add this handler
            path = Path(params.get('path', ''))
            return check_file_permission(path, require_write=False)
            
        elif operation_type == 'delete_file':
            path = Path(params.get('path', ''))
            # Check if this is a system file
            system_dirs = ['/bin', '/sbin', '/lib', '/usr', '/etc', '/var']
            if any(str(path).startswith(dir) for dir in system_dirs):
                return False, f"Deleting system files is not allowed: {path}"
            
            return check_file_permission(path, require_write=True)
        
        elif operation_type == 'create_directory':
            path = Path(params.get('path', ''))
            if path.exists():
                return False, f"Path already exists: {path}"
            return check_file_permission(path.parent, require_write=True)
        
        elif operation_type == 'delete_directory':
            path = Path(params.get('path', ''))
            # Check if this is a system directory
            system_dirs = ['/bin', '/sbin', '/lib', '/usr', '/etc', '/var']
            if any(str(path).startswith(dir) for dir in system_dirs):
                return False, f"Deleting system directories is not allowed: {path}"
            
            return check_file_permission(path, require_write=True)
        
        elif operation_type == 'copy_file':  # Add this handler
            source = Path(params.get('source', ''))
            destination = Path(params.get('destination', ''))
            
            # Check if source exists
            if not source.exists():
                return False, f"Source file does not exist: {source}"
                
            # Check destination permissions
            return check_file_permission(destination.parent, require_write=True)
            
        elif operation_type == 'move_file':  # Add this handler
            source = Path(params.get('source', ''))
            destination = Path(params.get('destination', ''))
            
            # Check if source exists
            if not source.exists():
                return False, f"Source file does not exist: {source}"
                
            # Check permissions for both source and destination
            source_ok, source_err = check_file_permission(source, require_write=True)
            if not source_ok:
                return False, source_err
                
            return check_file_permission(destination.parent, require_write=True)
            
        elif operation_type == 'execute_command':
            command = params.get('command', '')
            return validate_command_safety(command)
        
        # Unknown operation type
        logger.warning(f"Unknown operation type: {operation_type}")
        return False, f"Unknown operation type: {operation_type}"
    
    except Exception as e:
        logger.exception(f"Error validating operation {operation_type}: {str(e)}")
        return False, f"Validation failed: {str(e)}"
</file>

<file path="components/shell/__init__.py">
# angela/shell/__init__.py
"""
Shell integration and terminal formatting for Angela CLI.

This package provides shell hook scripts, terminal formatting utilities,
interactive feedback mechanisms, and command completion functionality.
"""

# Export main components that other modules will need
from .formatter import terminal_formatter
from .inline_feedback import inline_feedback
from .completion import completion_handler

# Ensure these exports are already defined before any code tries to use them
__all__ = ['terminal_formatter', 'inline_feedback', 'completion_handler']

# Advanced formatter will modify terminal_formatter after import,
# so we need to import it after terminal_formatter is defined and exported
# But we'll use try/except to avoid blocking initialization if there's an issue
try:
    # Import this separately so if it fails, the rest of the module is still usable
    from . import advanced_formatter
    # No need to add to __all__ as it's meant for internal use
except Exception as e:
    from angela.utils.logging import get_logger
    logger = get_logger(__name__)
    logger.warning(f"Failed to import advanced_formatter: {str(e)}")
    # Continue without the advanced formatter - the basic formatter will still work
</file>

<file path="components/shell/advanced_formatter.py">
# angela/components/shell/advanced_formatter.py
"""
Terminal formatter extensions for displaying advanced task plans.

This module extends the terminal_formatter to properly display advanced
task plans with all step types, data flow, and execution results.
"""
import os
from pathlib import Path
from typing import Dict, Any, List, Optional, Union
from datetime import datetime

from rich.console import Console
from rich.panel import Panel
from rich.table import Table
from rich.syntax import Syntax
from rich.tree import Tree
from rich.markdown import Markdown
from rich.text import Text
from rich import box
from angela.api.intent import get_advanced_task_plan_class, get_plan_step_type_enum
from angela.components.shell.formatter import terminal_formatter, OutputType

from typing import TYPE_CHECKING
if TYPE_CHECKING:
    from angela.api.intent import get_advanced_task_plan_class, get_plan_step_type_enum

from angela.utils.logging import get_logger

# Get the classes after import to avoid circular imports
AdvancedTaskPlan = get_advanced_task_plan_class()
PlanStepType = get_plan_step_type_enum()

logger = get_logger(__name__)

console = Console()

async def display_advanced_plan(plan: Any) -> None:
    """
    Display an advanced task plan with rich formatting.
    
    Args:
        plan: The advanced task plan to display
    """
    # Import inside the function through API layer
    from angela.api.intent import get_advanced_task_plan_class, get_plan_step_type_enum
    
    # Get the classes after import
    AdvancedTaskPlan = get_advanced_task_plan_class()
    PlanStepType = get_plan_step_type_enum()
    
    header = Panel(
        f"[bold]{plan.description}[/bold]\n\n{plan.goal}",
        title=f"Advanced Plan: {plan.id}",
        border_style="blue"
    )
    console.print(header)
    
    # Create a table for the steps
    table = Table(
        title="Execution Steps",
        box=box.ROUNDED,
        header_style="bold cyan",
        expand=True
    )
    
    # Add columns
    table.add_column("ID", style="cyan")
    table.add_column("Type", style="magenta")
    table.add_column("Description", style="green")
    table.add_column("Details", style="yellow")
    table.add_column("Risk", style="red")
    table.add_column("Dependencies", style="blue")
    
    # Add rows for each step
    for step_id, step in plan.steps.items():
        # Format risk level
        risk_colors = ["green", "green", "yellow", "red", "red bold"]
        risk_level = min(step.estimated_risk, 4)  # Cap at 4
        risk_text = f"[{risk_colors[risk_level]}]{risk_level}[/{risk_colors[risk_level]}]"
        
        # Format details based on step type
        details = ""
        if step.type == PlanStepType.COMMAND:
            if step.command:
                details = f"Command: {step.command}"
        elif step.type == PlanStepType.CODE:
            details = f"Code: {len(step.code)} chars"
        elif step.type == PlanStepType.FILE:
            if step.file_path:
                operation = getattr(step, "operation", "read/write")
                details = f"{operation.capitalize()}: {step.file_path}"
        elif step.type == PlanStepType.DECISION:
            if step.condition:
                details = f"Condition: {step.condition}"
        elif step.type == PlanStepType.API:
            if step.api_url:
                details = f"{step.api_method} {step.api_url}"
        elif step.type == PlanStepType.LOOP:
            if step.loop_items:
                details = f"Items: {step.loop_items}"
        
        # Format dependencies
        deps = ", ".join(step.dependencies) if step.dependencies else "None"
        
        # Add row
        table.add_row(
            step_id,
            str(step.type.value),
            step.description,
            details,
            risk_text,
            deps
        )
    
    console.print(table)
    
    # Show execution flow with entry points
    flow_panel = Panel(
        f"Entry Points: [bold cyan]{', '.join(plan.entry_points)}[/bold cyan]",
        title="Execution Flow",
        border_style="green"
    )
    console.print(flow_panel)
    
    # Show dependencies as a tree for visualization
    console.print("\n[bold]Dependency Tree:[/bold]")
    
    # Build dependency tree
    tree = Tree("🔄 [bold]Execution Plan[/bold]")
    
    # Add entry points
    entry_node = tree.add("🚀 [bold cyan]Entry Points[/bold cyan]")
    for entry in plan.entry_points:
        entry_branch = entry_node.add(f"[cyan]{entry}[/cyan]")
        _build_dependency_tree(entry_branch, entry, plan)
    
    console.print(tree)

def _build_dependency_tree(node, step_id, plan):
    """
    Recursively build a dependency tree for visualization.
    
    Args:
        node: The current tree node
        step_id: Current step ID
        plan: The advanced task plan
    """
    # Import inside the function through API layer
    from angela.api.intent import get_plan_step_type_enum
    
    # Get the enum after import
    PlanStepType = get_plan_step_type_enum()
    
    # Find steps that depend on this step
    for next_id, next_step in plan.steps.items():
        if step_id in next_step.dependencies:
            step_node = node.add(f"[yellow]{next_id}[/yellow]: {next_step.description}")
            # Recursively build the tree
            _build_dependency_tree(step_node, next_id, plan)
    
    # Special handling for decision steps (branches)
    if step_id in plan.steps and plan.steps[step_id].type == PlanStepType.DECISION:
        step = plan.steps[step_id]
        
        if step.true_branch:
            true_node = node.add("[green]True Branch[/green]")
            for branch_step in step.true_branch:
                branch_text = f"[green]{branch_step}[/green]"
                if branch_step in plan.steps:
                    branch_text += f": {plan.steps[branch_step].description}"
                true_node.add(branch_text)
        
        if step.false_branch:
            false_node = node.add("[red]False Branch[/red]")
            for branch_step in step.false_branch:
                branch_text = f"[red]{branch_step}[/red]"
                if branch_step in plan.steps:
                    branch_text += f": {plan.steps[branch_step].description}"
                false_node.add(branch_text)
    
    # Special handling for loop steps
    if step_id in plan.steps and plan.steps[step_id].type == PlanStepType.LOOP:
        step = plan.steps[step_id]
        
        if step.loop_body:
            loop_node = node.add("[blue]Loop Body[/blue]")
            for body_step in step.loop_body:
                body_text = f"[blue]{body_step}[/blue]"
                if body_step in plan.steps:
                    body_text += f": {plan.steps[body_step].description}"
                loop_node.add(body_text)

async def display_execution_results(
    plan: AdvancedTaskPlan, 
    results: Dict[str, Any]
) -> None:
    """
    Display execution results for an advanced task plan.
    
    Args:
        plan: The executed advanced task plan
        results: The execution results
    """
    # Import inside the function through API layer
    from angela.api.intent import get_advanced_task_plan_class, get_plan_step_type_enum
    
    # Get the classes after import
    AdvancedTaskPlan = get_advanced_task_plan_class()
    PlanStepType = get_plan_step_type_enum()

    logger.debug(f"Displaying execution results for plan: {plan.id}")
    
    # Create a header
    success = results.get("success", False)
    success_text = "[bold green]SUCCESS[/bold green]" if success else "[bold red]FAILED[/bold red]"
    
    header = Panel(
        f"Plan execution {success_text}\n\n"
        f"Steps completed: [bold]{results.get('steps_completed', 0)}[/bold] / {results.get('steps_total', len(plan.steps))}\n"
        f"Execution time: [bold]{results.get('execution_time', 0):.2f}[/bold] seconds",
        title=f"Execution Results: {plan.id}",
        border_style="green" if success else "red"
    )
    console.print(header)
    
    # Display execution path
    if "execution_path" in results:
        path_text = " → ".join(results["execution_path"])
        path_panel = Panel(
            path_text,
            title="Execution Path",
            border_style="blue"
        )
        console.print(path_panel)
    
    # Display step results in a table
    if "results" in results:
        step_results = results["results"]
        
        # Create table
        table = Table(
            title="Step Results",
            box=box.ROUNDED,
            header_style="bold cyan",
            expand=True
        )
        
        # Add columns
        table.add_column("Step", style="cyan")
        table.add_column("Type", style="magenta")
        table.add_column("Status", style="green")
        table.add_column("Output", style="yellow")
        table.add_column("Time (s)", style="blue")
        
        # Add rows for each step in execution order
        execution_order = results.get("execution_path", list(step_results.keys()))
        
        for step_id in execution_order:
            if step_id not in step_results:
                continue
                
            result = step_results[step_id]
            
            # Format status
            status = "[green]Success[/green]" if result.get("success", False) else "[red]Failed[/red]"
            
            # Handle retry/recovery
            if result.get("retried", False):
                status += " [yellow](Retried)[/yellow]"
            if result.get("recovery_applied", False):
                status += " [blue](Recovered)[/blue]"
            
            # Format output based on step type
            output = ""
            # Assuming PlanStepType is an Enum and result["type"] is a string matching an enum value
            step_type_str = result.get("type", "") 
            
            if step_type_str == PlanStepType.COMMAND.value:
                # Get first few lines of stdout
                stdout = result.get("stdout", "").strip()
                if stdout:
                    lines = stdout.split("\n")
                    output = lines[0]
                    if len(lines) > 1:
                        output += f"... (+{len(lines)-1} lines)"
            elif step_type_str == PlanStepType.CODE.value:
                # Show execution result or console output
                if "result" in result:
                    output = f"Result: {result['result']}"
                elif "stdout" in result and result["stdout"]:
                    lines = result["stdout"].strip().split("\n")
                    output = lines[0]
                    if len(lines) > 1:
                        output += f"... (+{len(lines)-1} lines)"
            elif step_type_str == PlanStepType.FILE.value:
                # Show operation result
                output = result.get("message", "")
            elif step_type_str == PlanStepType.DECISION.value:
                # Show condition result
                condition_result = result.get("condition_result", False)
                output = f"Condition: [green]True[/green]" if condition_result else "Condition: [red]False[/red]"
                output += f" → {result.get('next_branch', '')}"
            elif step_type_str == PlanStepType.API.value:
                # Show status code and response summary
                status_code = result.get("status_code", 0)
                output = f"Status: {status_code}"
                
                # Add response summary
                if "json" in result:
                    output += f" (JSON response)"
                elif "text" in result:
                    text = result["text"]
                    if len(text) > 30:
                        text = text[:30] + "..."
                    output += f" Response: {text}"
            elif step_type_str == PlanStepType.LOOP.value:
                # Show loop iteration count
                iterations = result.get("iterations", 0)
                output = f"Iterations: {iterations}"
            
            # Format execution time
            exec_time = result.get("execution_time", 0)
            time_text = f"{exec_time:.2f}"
            
            # Determine display string for step_type for the table
            # This part of the original code correctly handles if step_type_str is an enum or string for display
            display_step_type = step_type_str 
            try:
                # If step_type_str matches an enum value, display the value
                # This assumes PlanStepType can be iterated or accessed by value
                # For simplicity, we'll just use step_type_str, assuming it's descriptive enough
                # Or, if PlanStepType was passed around as enum members:
                # actual_enum_member = result.get("type_enum_member_if_available") 
                # display_step_type = str(actual_enum_member.value) if isinstance(actual_enum_member, PlanStepType) else str(step_type_str)
                pass # Using step_type_str directly for the table is often fine.
            except: # Be careful with bare except
                pass


            table.add_row(
                step_id,
                display_step_type, # Use the string type identifier for display
                status,
                output,
                time_text
            )
        
        console.print(table)
    
    # Display detailed outputs for steps of interest
    if "results" in results:
        step_results = results["results"]
        
        # First show any failed step in detail
        failed_step = results.get("failed_step")
        if failed_step and failed_step in step_results:
            console.print(f"\n[bold red]Failed Step: {failed_step}[/bold red]")
            await display_step_details(failed_step, step_results[failed_step], plan)
        
        # Show details for specific step types that typically have interesting output
        for step_id, result in step_results.items():
            if step_id == failed_step:
                continue  # Already displayed
                
            step_type_str = result.get("type", "")
            
            # Show all API responses, loop results, and code outputs with results
            if step_type_str == PlanStepType.API.value or \
               step_type_str == PlanStepType.LOOP.value or \
               (step_type_str == PlanStepType.CODE.value and ("result" in result or "output" in result)): # Check "output" key for code output
                console.print(f"\n[bold cyan]Step Details: {step_id}[/bold cyan]")
                await display_step_details(step_id, result, plan)
    
    # Show variables at the end of execution if available
    if "variables" in results:
        console.print("\n[bold]Final Variables:[/bold]")
        var_table = Table(title="Data Flow Variables", box=box.ROUNDED)
        var_table.add_column("Name", style="cyan")
        var_table.add_column("Value", style="green")
        var_table.add_column("Source", style="blue")
        
        for var_name, var_info in results["variables"].items():
            # Format value based on type
            value = var_info.get("value", "")
            if isinstance(value, dict) or isinstance(value, list):
                import json
                value_str = json.dumps(value, indent=2)
                if len(value_str) > 50:
                    value_str = value_str[:50] + "..."
            else:
                value_str = str(value)
                if len(value_str) > 50:
                    value_str = value_str[:50] + "..."
            
            var_table.add_row(
                var_name,
                value_str,
                var_info.get("source_step", "initial")
            )
        
        console.print(var_table)

async def display_step_details(
    step_id: str, 
    result: Dict[str, Any],
    plan: Optional[AdvancedTaskPlan] = None
) -> None:
    """
    Display detailed results for a specific step.
    
    Args:
        step_id: ID of the step
        result: The step's execution result
        plan: Optional plan for context
    """
    # Import inside the function through API layer
    from angela.api.intent import get_plan_step_type_enum
    
    # Get the enum after import
    PlanStepType = get_plan_step_type_enum()
    
    # Get the step type as a string from the result
    step_type_str = result.get("type", "")
    
    # Get step description from plan if available
    description = ""
    if plan and step_id in plan.steps:
        description = plan.steps[step_id].description
        # If description is available, print it for context
        if description:
            console.print(f"[bold]Description:[/bold] {description}")

    # Format output based on step type string
    if step_type_str == PlanStepType.COMMAND.value:
        # Show command and output
        console.print(f"[bold]Command:[/bold] {result.get('command', '')}")
        
        if result.get("stdout", "").strip():
            syntax = Syntax(
                result["stdout"],
                "bash-session",
                theme="monokai",
                line_numbers=True,
                word_wrap=True
            )
            console.print(Panel(syntax, title="Standard Output", border_style="green"))
        
        if result.get("stderr", "").strip():
            syntax = Syntax(
                result["stderr"],
                "bash-session",
                theme="monokai",
                line_numbers=True,
                word_wrap=True
            )
            console.print(Panel(syntax, title="Error Output", border_style="red"))
    
    elif step_type_str == PlanStepType.CODE.value:
        # Show code and output
        if "code" in result:
            lang = "python"  # Default to Python
            if plan and step_id in plan.steps:
                # Ensure plan.steps[step_id] exists and has 'language' attribute
                step_details = plan.steps.get(step_id)
                if step_details:
                    lang = getattr(step_details, "language", "python")
            
            syntax = Syntax(
                result["code"],
                lang,
                theme="monokai",
                line_numbers=True,
                word_wrap=True
            )
            console.print(Panel(syntax, title="Code", border_style="blue"))
        
        if "stdout" in result and result["stdout"].strip():
            console.print(Panel(result["stdout"], title="Standard Output", border_style="green"))
        
        if "result" in result:
            import json
            if isinstance(result["result"], (dict, list)):
                result_str = json.dumps(result["result"], indent=2)
            else:
                result_str = str(result["result"])
            
            console.print(Panel(result_str, title="Result", border_style="cyan"))
        
        if "error" in result:
            console.print(Panel(result["error"], title="Error", border_style="red"))
            if "traceback" in result:
                console.print(Panel(result["traceback"], title="Traceback", border_style="red"))
    
    elif step_type_str == PlanStepType.FILE.value:
        # Show file operation details
        console.print(f"[bold]File Path:[/bold] {result.get('file_path', '')}")
        operation = "read/write" # Default operation
        if plan and step_id in plan.steps:
            step_details = plan.steps.get(step_id)
            if step_details:
                operation = getattr(step_details, 'operation', 'read/write')
        console.print(f"[bold]Operation:[/bold] {operation}")
        
        if "content" in result:
            # For read operations
            content = result["content"]
            if len(content) > 500:
                content = content[:500] + "...\n(truncated)"
            
            syntax = Syntax(
                content,
                "text", # Assuming text, could be improved if file type is known
                theme="monokai",
                line_numbers=True,
                word_wrap=True
            )
            console.print(Panel(syntax, title="File Content", border_style="green"))
        
        if "message" in result:
            console.print(f"[bold]Result:[/bold] {result['message']}")
    
    elif step_type_str == PlanStepType.DECISION.value:
        # Show decision details
        console.print(f"[bold]Condition:[/bold] {result.get('condition', '')}")
        condition_result = result.get("condition_result", False)
        console.print(f"[bold]Evaluated:[/bold] {'[green]True[/green]' if condition_result else '[red]False[/red]'}")
        
        if plan and step_id in plan.steps:
            step = plan.steps.get(step_id)
            if step:
                if condition_result and step.true_branch:
                    console.print(f"[bold]True Branch taken:[/bold] {', '.join(step.true_branch)}")
                elif not condition_result and step.false_branch:
                    console.print(f"[bold]False Branch taken:[/bold] {', '.join(step.false_branch)}")
    
    elif step_type_str == PlanStepType.API.value:
        # Show API call details
        console.print(f"[bold]URL:[/bold] {result.get('url', '')}")
        console.print(f"[bold]Method:[/bold] {result.get('method', 'GET')}")
        console.print(f"[bold]Status Code:[/bold] {result.get('status_code', 0)}")
        
        # Show headers
        if "headers" in result and isinstance(result["headers"], dict):
            header_table = Table(title="Response Headers", box=box.SIMPLE)
            header_table.add_column("Header", style="cyan")
            header_table.add_column("Value", style="green")
            
            for header, value in result["headers"].items():
                header_table.add_row(header, str(value))
            
            console.print(header_table)
        
        # Show JSON response if available
        if "json" in result:
            import json
            json_str = json.dumps(result["json"], indent=2)
            
            syntax = Syntax(
                json_str,
                "json",
                theme="monokai",
                line_numbers=True,
                word_wrap=True
            )
            console.print(Panel(syntax, title="JSON Response", border_style="green"))
        elif "text" in result:
            # Show text response
            text = result["text"]
            
            # Try to detect content type
            is_json_like = text.strip().startswith('{') and text.strip().endswith('}')
            is_xml_like = text.strip().startswith('<') and text.strip().endswith('>')
            is_html_like = '<html' in text.lower() and '</html>' in text.lower()
            
            syntax_type = "json" if is_json_like else "xml" if is_xml_like or is_html_like else "text"
            
            syntax = Syntax(
                text,
                syntax_type,
                theme="monokai",
                line_numbers=True,
                word_wrap=True
            )
            console.print(Panel(syntax, title="Response Body", border_style="green"))
    
    elif step_type_str == PlanStepType.LOOP.value:
        # Show loop details
        loop_items_desc = ""
        if plan and step_id in plan.steps:
            step_details = plan.steps.get(step_id)
            if step_details:
                loop_items_desc = getattr(step_details, 'loop_items', '')
        console.print(f"[bold]Loop Items Description:[/bold] {loop_items_desc}")
        console.print(f"[bold]Iterations Executed:[/bold] {result.get('iterations', 0)}")
        
        if "loop_results" in result and isinstance(result["loop_results"], list):
            loop_results = result["loop_results"]
            
            # Create a table for iteration results
            loop_table = Table(title="Loop Iterations", box=box.SIMPLE)
            loop_table.add_column("Index", style="cyan")
            loop_table.add_column("Item", style="green")
            loop_table.add_column("Status", style="yellow")
            
            for iteration in loop_results:
                if not isinstance(iteration, dict): continue # Skip malformed iteration results

                # Format item based on type
                item = iteration.get("item", "")
                if isinstance(item, (dict, list)):
                    import json
                    item_str = json.dumps(item)
                    if len(item_str) > 30:
                        item_str = item_str[:27] + "..."
                else:
                    item_str = str(item)
                    if len(item_str) > 30:
                        item_str = item_str[:27] + "..."
                
                # Format status
                status = "[green]Success[/green]" if iteration.get("success", False) else "[red]Failed[/red]"
                
                loop_table.add_row(
                    str(iteration.get("index", "?")), # Use '?' if index is missing
                    item_str,
                    status
                )
            
            if loop_table.rows:
                console.print(loop_table)
            else:
                console.print("[italic]No iteration results to display.[/italic]")
        else:
            console.print("[italic]No loop iteration results available.[/italic]")
    else:
        # Fallback for unknown or unhandled step types
        console.print(f"[yellow]Details for step type '{step_type_str}' are not specifically formatted.[/yellow]")
        # Generic display of result dictionary for debugging
        import json
        try:
            result_dump = json.dumps(result, indent=2, default=str) # Use default=str for non-serializable items
            console.print(Panel(result_dump, title=f"Raw Result Data for Step {step_id}", border_style="yellow"))
        except Exception as e:
            console.print(f"[red]Could not serialize raw result data: {e}[/red]")

async def display_step_error(
    step_id: str,
    error: str,
    step_type: str,
    description: str
) -> None:
    """
    Display an error that occurred during step execution.
    
    Args:
        step_id: ID of the failed step
        error: Error message
        step_type: Type of the step
        description: Step description
    """
    error_panel = Panel(
        f"[bold]{description}[/bold]\n\n"
        f"Step Type: {step_type}\n"
        f"Error: {error}",
        title=f"Step Error: {step_id}",
        border_style="red"
    )
    console.print(error_panel)

# Add the new methods to terminal_formatter
terminal_formatter.display_advanced_plan = display_advanced_plan
terminal_formatter.display_execution_results = display_execution_results
terminal_formatter.display_step_details = display_step_details
terminal_formatter.display_step_error = display_step_error


logger.info("Extended terminal formatter with advanced task plan display capabilities")
</file>

<file path="components/shell/angela_enhanced.bash">
#!/bin/bash
# Angela CLI Enhanced Bash Integration

# Global variables for tracking
ANGELA_LAST_COMMAND=""
ANGELA_LAST_COMMAND_RESULT=$?
ANGELA_LAST_PWD="$PWD"
ANGELA_COMMAND_START_TIME=0

# Pre-command execution hook
angela_pre_exec() {
    # Capture the command
    ANGELA_LAST_COMMAND="$BASH_COMMAND"
    ANGELA_COMMAND_START_TIME=$(date +%s)
    
    # Send notification to Angela's monitoring system
    if [[ ! "$ANGELA_LAST_COMMAND" =~ ^angela ]]; then
        # Only track non-angela commands to avoid recursion
        (angela --notify pre_exec "$ANGELA_LAST_COMMAND" &>/dev/null &)
    fi
}

# Post-command execution hook
angela_post_exec() {
    local exit_code=$?
    ANGELA_LAST_COMMAND_RESULT=$exit_code
    local duration=$(($(date +%s) - ANGELA_COMMAND_START_TIME))
    
    # Check for directory change
    if [[ "$PWD" != "$ANGELA_LAST_PWD" ]]; then
        # Directory changed, update context
        ANGELA_LAST_PWD="$PWD"
        (angela --notify dir_change "$PWD" &>/dev/null &)
    fi
    
    # Send post-execution notification for non-angela commands
    if [[ ! "$ANGELA_LAST_COMMAND" =~ ^angela ]]; then
        # Pass execution result to Angela
        (angela --notify post_exec "$ANGELA_LAST_COMMAND" $exit_code $duration &>/dev/null &)
        
        # Check if we should offer assistance based on exit code and command pattern
        if [[ $exit_code -ne 0 ]]; then
            angela_check_command_suggestion "$ANGELA_LAST_COMMAND" $exit_code
        fi
    fi
}

# Function to check if Angela should offer command suggestions
angela_check_command_suggestion() {
    local command="$1"
    local exit_code=$2
    
    # Check for common error patterns
    case "$command" in
        git*)
            # For git commands with errors, offer assistance
            if [[ $exit_code -ne 0 ]]; then
                echo -e "\033[33m[Angela] I noticed your git command failed. Need help? Try: angela fix-git\033[0m"
            fi
            ;;
        python*|pip*)
            # For Python-related errors
            if [[ $exit_code -ne 0 ]]; then
                echo -e "\033[33m[Angela] Python command failed. For assistance, try: angela fix-python\033[0m"
            fi
            ;;
    esac
    
    # Match other patterns that might benefit from Angela's assistance
    if [[ "$command" =~ "commit -m" ]]; then
        # Offer to enhance commit messages
        echo -e "\033[33m[Angela] I can help with more descriptive commit messages. Try: angela enhance-commit\033[0m"
    fi
}

# Install hooks
trap angela_pre_exec DEBUG
PROMPT_COMMAND="angela_post_exec${PROMPT_COMMAND:+;$PROMPT_COMMAND}"

# Main Angela function
angela() {
    # Check if no arguments or help requested
    if [ $# -eq 0 ] || [ "$1" = "--help" ] || [ "$1" = "-h" ]; then
        python -m angela --help
        return
    fi

    # Handle notify subcommand (used by hooks)
    if [ "$1" = "--notify" ]; then
        # This is a notification from the hooks, handle silently
        python -m angela --notify "${@:2}" &>/dev/null &
        return
    fi

    # Handle version flag
    if [ "$1" = "--version" ] || [ "$1" = "-v" ]; then
        python -m angela --version
        return
    fi

    # Handle debug flag
    if [ "$1" = "--debug" ] || [ "$1" = "-d" ]; then
        DEBUG_FLAG="--debug"
        shift  # Remove the debug flag from arguments
    else
        DEBUG_FLAG=""
    fi

    # Handle natural language commands (implicit invocation)
    if [ "$1" = "fix" ] || [ "$1" = "explain" ] || [ "$1" = "help-with" ]; then
        # These are common natural language commands
        python -m angela $DEBUG_FLAG request "$@"
        return
    fi

    # Handle specific command (init, etc.)
    if [ "$1" = "init" ]; then
        python -m angela $DEBUG_FLAG init
        return
    fi

    # Process as a request for anything else
    python -m angela $DEBUG_FLAG request "$@"
}

# Register completion for angela
_angela_completion() {
    local cur prev opts
    COMPREPLY=()
    cur="${COMP_WORDS[COMP_CWORD]}"
    prev="${COMP_WORDS[COMP_CWORD-1]}"
    
    # Get dynamic completions from Angela
    local completions=$(angela --completions "${COMP_WORDS[@]:1}" 2>/dev/null)
    
    # If completions were returned, use them
    if [ -n "$completions" ]; then
        COMPREPLY=( $(compgen -W "$completions" -- "$cur") )
        return 0
    fi
    
    # Fallback static completions
    opts="init status shell files workflows generate rollback fix explain help-with"
    
    # Complete based on the current argument
    if [[ ${prev} == "angela" ]]; then
        COMPREPLY=( $(compgen -W "${opts}" -- ${cur}) )
        return 0
    fi
}
complete -F _angela_completion angela
</file>

<file path="components/shell/angela_enhanced.zsh">
#!/bin/zsh
# Angela CLI Enhanced Zsh Integration

# Global variables for tracking
ANGELA_LAST_COMMAND=""
ANGELA_LAST_COMMAND_RESULT=0
ANGELA_LAST_PWD="$PWD"
ANGELA_COMMAND_START_TIME=0

# Pre-command execution hook (before command runs)
angela_preexec() {
    # Capture the command
    ANGELA_LAST_COMMAND="$1"
    ANGELA_COMMAND_START_TIME=$(date +%s)
    
    # Send notification to Angela's monitoring system
    if [[ ! "$ANGELA_LAST_COMMAND" =~ ^angela ]]; then
        # Only track non-angela commands to avoid recursion
        (angela --notify pre_exec "$ANGELA_LAST_COMMAND" &>/dev/null &)
    fi
}

# Pre-prompt hook (after command runs, before showing next prompt)
angela_precmd() {
    local exit_code=$?
    ANGELA_LAST_COMMAND_RESULT=$exit_code
    local duration=$(($(date +%s) - ANGELA_COMMAND_START_TIME))
    
    # Check for directory change
    if [[ "$PWD" != "$ANGELA_LAST_PWD" ]]; then
        # Directory changed, update context
        ANGELA_LAST_PWD="$PWD"
        (angela --notify dir_change "$PWD" &>/dev/null &)
    fi
    
    # Send post-execution notification for non-angela commands
    if [[ ! "$ANGELA_LAST_COMMAND" =~ ^angela ]]; then
        # Pass execution result to Angela
        (angela --notify post_exec "$ANGELA_LAST_COMMAND" $exit_code $duration &>/dev/null &)
        
        # Check if we should offer assistance based on exit code and command pattern
        if [[ $exit_code -ne 0 ]]; then
            angela_check_command_suggestion "$ANGELA_LAST_COMMAND" $exit_code
        fi
    fi
}

# Function to check if Angela should offer command suggestions
angela_check_command_suggestion() {
    local command="$1"
    local exit_code=$2
    
    # Check for common error patterns
    case "$command" in
        git*)
            # For git commands with errors, offer assistance
            if [[ $exit_code -ne 0 ]]; then
                echo -e "\033[33m[Angela] I noticed your git command failed. Need help? Try: angela fix-git\033[0m"
            fi
            ;;
        python*|pip*)
            # For Python-related errors
            if [[ $exit_code -ne 0 ]]; then
                echo -e "\033[33m[Angela] Python command failed. For assistance, try: angela fix-python\033[0m"
            fi
            ;;
    esac
    
    # Match other patterns that might benefit from Angela's assistance
    if [[ "$command" =~ "commit -m" ]]; then
        # Offer to enhance commit messages
        echo -e "\033[33m[Angela] I can help with more descriptive commit messages. Try: angela enhance-commit\033[0m"
    fi
}

# Register the hooks with Zsh
autoload -Uz add-zsh-hook
add-zsh-hook preexec angela_preexec
add-zsh-hook precmd angela_precmd

# Main Angela function
angela() {
    # Check if no arguments or help requested
    if [[ $# -eq 0 || "$1" = "--help" || "$1" = "-h" ]]; then
        python -m angela --help
        return
    fi

    # Handle notify subcommand (used by hooks)
    if [[ "$1" = "--notify" ]]; then
        # This is a notification from the hooks, handle silently
        python -m angela --notify "${@:2}" &>/dev/null &
        return
    fi

    # Handle version flag
    if [[ "$1" = "--version" || "$1" = "-v" ]]; then
        python -m angela --version
        return
    fi

    # Handle debug flag
    if [[ "$1" = "--debug" || "$1" = "-d" ]]; then
        DEBUG_FLAG="--debug"
        shift  # Remove the debug flag from arguments
    else
        DEBUG_FLAG=""
    fi

    # Handle natural language commands (implicit invocation)
    if [[ "$1" = "fix" || "$1" = "explain" || "$1" = "help-with" ]]; then
        # These are common natural language commands
        python -m angela $DEBUG_FLAG request "$@"
        return
    fi

    # Handle specific command (init, etc.)
    if [[ "$1" = "init" ]]; then
        python -m angela $DEBUG_FLAG init
        return
    fi

    # Process as a request for anything else
    python -m angela $DEBUG_FLAG request "$@"
}

# ZSH completion function for angela
# ZSH completion function for angela
_angela_completion() {
    local curcontext="$curcontext" state line
    typeset -A opt_args
    
    # Get dynamic completions from Angela (expected: one completion candidate per line)
    local -a compl_array
    compl_array=("${(@f)$(angela --completions "${words[@]:1}" 2>/dev/null)}")
    
    # If completions were returned as an array, use them
    if (( ${#compl_array[@]} > 0 )); then
        # Pass the array of completions. Each element is a candidate.
        # The '*: :(...)' tells _arguments to offer these for any argument.
        _arguments '*: :($compl_array)'
        return
    fi
    
    # Fallback static completions if dynamic ones fail or return empty
    _arguments \
        '1: :(init status shell files workflows generate rollback fix explain help-with)' \
        '*::arg:->args'
}
compdef _angela_completion angela
</file>

<file path="components/shell/angela.bash">
#!/bin/bash
# Angela CLI Bash Integration

# Function to handle Angela CLI requests
angela() {
    # Check if no arguments or help requested
    if [ $# -eq 0 ] || [ "$1" = "--help" ] || [ "$1" = "-h" ]; then
        python -m angela --help
        return
    fi

    # Handle version flag
    if [ "$1" = "--version" ] || [ "$1" = "-v" ]; then
        python -m angela --version
        return
    fi

    # Handle debug flag
    if [ "$1" = "--debug" ] || [ "$1" = "-d" ]; then
        DEBUG_FLAG="--debug"
        shift  # Remove the debug flag from arguments
    else
        DEBUG_FLAG=""
    fi

    # Handle specific command (init, etc.)
    if [ "$1" = "init" ]; then
        python -m angela $DEBUG_FLAG init
        return
    fi

    # Process as a request for anything else
    python -m angela $DEBUG_FLAG request "$@"
}

# Enable command completion for angela function
# This will be implemented in a future phase
</file>

<file path="components/shell/angela.tmux">
#!/bin/bash
# Angela CLI Tmux Integration

# Define Angela status indicator for Tmux status bar
angela_tmux_status() {
    # Check if Angela is enabled
    if [ -f "$HOME/.config/angela/enabled" ]; then
        echo "#[fg=green]◉ Angela#[fg=default]"
    else
        echo "#[fg=red]◯ Angela#[fg=default]"
    fi
}

# Register Angela key bindings
angela_tmux_bindings() {
    # Bind Alt+A to activate Angela
    tmux bind-key -n M-a run-shell "angela status"
    
    # Bind Alt+C to send current pane command to Angela
    tmux bind-key -n M-c run-shell "tmux capture-pane -p | tail -n 1 | sed 's/^[^#]*#//' | angela request"
    
    # Bind Alt+H for Angela help
    tmux bind-key -n M-h run-shell "angela --help"
}

# Setup Angela integration in Tmux
angela_tmux_setup() {
    # Add Angela status to right status bar
    tmux set-option -g status-right "#{?window_zoomed_flag,#[fg=yellow]Z#[fg=default] ,}#[fg=blue]#(angela_tmux_status) | %H:%M %d-%b-%y"
    
    # Register key bindings
    angela_tmux_bindings
    
    # Set the status update interval to update Angela status
    tmux set-option -g status-interval 5
}

# Main function
main() {
    # Check if we're running inside tmux
    if [ -n "$TMUX" ]; then
        angela_tmux_setup
        echo "Angela Tmux integration enabled"
    else
        echo "Error: Not running inside tmux session"
        exit 1
    fi
}

# Run main function
main "$@"
</file>

<file path="components/shell/angela.zsh">
#!/bin/zsh
# Angela CLI Zsh Integration

# Function to handle Angela CLI requests
angela() {
    # Check if arguments were provided
    if [ $# -eq 0 ]; then
        # No arguments, show help
        python -m angela --help
    else
        # Capture the current working directory
        local current_dir=$(pwd)
        
        # Process the request
        python -m angela request "$@"
        
        # Note: In future phases, we'll add support for command execution,
        # directory changing, etc. For now, this is just a simple pass-through.
    fi
}

# Enable command completion for angela function
# This will be implemented in a future phase
</file>

<file path="components/shell/completion.py">
# angela/shell/completion.py
"""
AI-powered contextual auto-completion for Angela CLI.
"""
import asyncio
from typing import List, Dict, Any, Optional, Set
import os
from pathlib import Path
import re

from angela.utils.logging import get_logger
from angela.api.context import get_context_manager
from angela.api.context import get_file_activity_tracker
from angela.api.context import get_session_manager
from angela.api.context import get_history_manager
from angela.api.ai import get_gemini_client, get_gemini_request_class

logger = get_logger(__name__)

class CompletionHandler:
    """
    Provides contextually relevant completions for the Angela CLI.
    
    This class generates completions based on command history, project context,
    recent file activity, and current input state.
    """
    
    def __init__(self):
        """Initialize the completion handler."""
        self._logger = logger
        
        # Cache common completions to avoid repeated calculation
        self._completion_cache = {}
        self._cache_ttl = 300  # Cache lifetime in seconds
        self._cache_last_update = {}
        
        # Static completions for common commands
        self._static_completions = {
            "init": [],
            "status": [],
            "shell": [],
            "files": ["ls", "mkdir", "rm", "cat", "write", "find", "info", "rollback"],
            "workflows": ["list", "create", "run", "delete", "show", "export", "import"],
            "generate": ["create-project", "add-feature", "refine-code", "refine-project", "generate-ci", "generate-tests"],
            "rollback": ["list", "operation", "transaction", "last"],
        }
        
        # Common file extensions for different contexts
        self._file_extensions = {
            "python": [".py", ".json", ".yml", ".yaml", ".txt", ".md"],
            "javascript": [".js", ".json", ".ts", ".jsx", ".tsx", ".html", ".css"],
            "web": [".html", ".css", ".js", ".svg", ".png", ".jpg"],
            "data": [".csv", ".json", ".xml", ".yaml", ".sql"],
            "docs": [".md", ".txt", ".pdf", ".docx"],
        }
    
    async def get_completions(self, args: List[str]) -> List[str]:
        """
        Get completions for the current command line.
        
        Args:
            args: The current command line arguments
            
        Returns:
            List of completions
        """
        self._logger.debug(f"Generating completions for args: {args}")
        
        if not args:
            # No args yet, return top-level commands
            return self._get_top_level_completions()
        
        # Handle subcommand completions
        main_command = args[0]
        
        # Check static completions first
        if main_command in self._static_completions and len(args) == 1:
            return self._static_completions[main_command]
        
        # Handle specific completion contexts
        if main_command == "files":
            return await self._get_files_completions(args[1:] if len(args) > 1 else [])
        elif main_command == "workflows":
            return await self._get_workflow_completions(args[1:] if len(args) > 1 else [])
        elif main_command == "generate":
            return await self._get_generate_completions(args[1:] if len(args) > 1 else [])
        elif main_command == "rollback":
            return await self._get_rollback_completions(args[1:] if len(args) > 1 else [])
        elif main_command in ["fix", "explain", "help-with"]:
            # Natural language commands get context-aware completions
            return await self._get_contextual_completions(main_command, args[1:] if len(args) > 1 else [])
        
        # Default to empty list for unknown commands
        return []
    
    def _get_top_level_completions(self) -> List[str]:
        """
        Get top-level command completions.
        
        Returns:
            List of top-level commands
        """
        # Standard commands
        commands = list(self._static_completions.keys())
        
        # Add natural language command prefixes
        commands.extend(["fix", "explain", "help-with"])
        
        # Sort and return
        return sorted(commands)
    
    async def _get_files_completions(self, args: List[str]) -> List[str]:
        """
        Get completions for file-related commands.
        
        Args:
            args: The command arguments after 'files'
            
        Returns:
            List of completions
        """
        if not args:
            # No subcommand yet, return available subcommands
            return self._static_completions["files"]
        
        subcommand = args[0]
        
        # For commands that take file paths, provide file path completions
        if subcommand in ["ls", "cat", "rm", "find", "info"] and len(args) <= 2:
            return await self._get_file_path_completions(args[1] if len(args) > 1 else "")
        
        return []
    
    async def _get_file_path_completions(self, partial_path: str) -> List[str]:
        """
        Get completions for file paths.
        
        Args:
            partial_path: The partial file path to complete
            
        Returns:
            List of matching file paths
        """
        # Convert to Path object for easier handling
        path = Path(partial_path) if partial_path else Path(".")
        
        # Check if it's a directory prefix
        if not partial_path.endswith("/") and path.is_dir():
            # Return the directory with a trailing slash
            return [f"{partial_path}/"]
        
        # Get the directory to search in
        directory = path.parent if partial_path and not partial_path.endswith("/") else path
        prefix = path.name if partial_path and not partial_path.endswith("/") else ""
        
        try:
            # List directory contents matching the prefix
            if not directory.exists():
                return []
                
            completions = []
            for item in directory.iterdir():
                if prefix and not item.name.startswith(prefix):
                    continue
                    
                # Handle directories
                if item.is_dir():
                    completions.append(f"{item.name}/")
                else:
                    completions.append(item.name)
            
            # Return with proper prefix
            prefix_dir = str(directory) if str(directory) != "." else ""
            if prefix_dir and not prefix_dir.endswith("/"):
                prefix_dir += "/"
                
            return [f"{prefix_dir}{c}" for c in completions]
            
        except Exception as e:
            self._logger.error(f"Error getting file path completions: {str(e)}")
            return []
    
    async def _get_workflow_completions(self, args: List[str]) -> List[str]:
        """
        Get completions for workflow-related commands.
        
        Args:
            args: The command arguments after 'workflows'
            
        Returns:
            List of completions
        """
        if not args:
            # No subcommand yet, return available subcommands
            return self._static_completions["workflows"]
        
        subcommand = args[0]
        
        # For commands that take workflow names, provide workflow name completions
        if subcommand in ["run", "delete", "show", "export"] and len(args) <= 2:
            # This would be replaced with actual workflow names from the workflow manager
            return ["workflow1", "workflow2", "backup", "deploy"]
        
        return []
    
    async def _get_generate_completions(self, args: List[str]) -> List[str]:
        """
        Get completions for code generation commands.
        
        Args:
            args: The command arguments after 'generate'
            
        Returns:
            List of completions
        """
        if not args:
            # No subcommand yet, return available subcommands
            return self._static_completions["generate"]
        
        subcommand = args[0]
        
        # Handle specific generate subcommands
        if subcommand == "generate-ci" and len(args) <= 2:
            return ["github_actions", "gitlab_ci", "jenkins", "travis", "circle_ci"]
        
        return []
    

    
    async def _get_rollback_completions(self, args: List[str]) -> List[str]:
        """
        Get completions for rollback commands.
        
        Args:
            args: The command arguments after 'rollback'
            
        Returns:
            List of completions
        """
        if not args:
            # No subcommand yet, return available subcommands
            return self._static_completions["rollback"]
        
        subcommand = args[0]
        
        # For commands that take operation or transaction IDs
        if subcommand in ["operation", "transaction"] and len(args) <= 2:
            # Get IDs from rollback manager
            try:
                # Import here to avoid circular imports
                from angela.execution.rollback import rollback_manager
                
                if subcommand == "operation":
                    # Get recent operations
                    operations = await rollback_manager.get_recent_operations(limit=10)
                    return [str(op["id"]) for op in operations if op.get("can_rollback", False)]
                else:  # transaction
                    # Get recent transactions
                    transactions = await rollback_manager.get_recent_transactions(limit=10)
                    return [str(tx["id"]) for tx in transactions if tx.get("can_rollback", False)]
            except Exception as e:
                self._logger.error(f"Error fetching rollback IDs: {str(e)}")
                return []
        
        # For the "list" command with options
        elif subcommand == "list" and len(args) <= 2:
            return ["--transactions", "--operations", "--limit"]
        
        # For the "last" command with options
        elif subcommand == "last" and len(args) <= 2:
            return ["--transaction", "--force"]
        
        return []
    
    def _get_fix_completions(self, context: Dict[str, Any]) -> List[str]:
        """
        Get completions for the 'fix' command.
        
        Args:
            context: The completion context
            
        Returns:
            List of completions
        """
        completions = []
        
        # Add completions based on last failed command
        last_failed = context.get("last_failed_command")
        if last_failed:
            base_command = last_failed.split()[0] if last_failed.strip() else ""
            completions.append(f"last {base_command} command")
            completions.append(f"last command")
        
        # Add project-specific fix suggestions
        project_type = context.get("project_type")
        if project_type == "python":
            completions.extend([
                "import errors",
                "python syntax",
                "pip dependencies",
                "missing module"
            ])
        elif project_type == "node":
            completions.extend([
                "npm dependencies",
                "webpack config",
                "typescript errors",
                "node version"
            ])
        
        # Add general fix suggestions
        completions.extend([
            "git conflicts",
            "git merge issues",
            "build errors",
            "path issues"
        ])
        
        return completions
    
    def _get_explain_completions(self, context: Dict[str, Any]) -> List[str]:
        """
        Get completions for the 'explain' command.
        
        Args:
            context: The completion context
            
        Returns:
            List of completions
        """
        completions = []
        
        # Add completions for recent files
        recent_files = context.get("recent_files", [])
        for file in recent_files[:3]:  # Limit to 3 recent files
            completions.append(f"file {file}")
        
        # Add completions for recent commands
        recent_commands = context.get("recent_commands", [])
        for cmd in recent_commands[:3]:  # Limit to 3 recent commands
            # Extract base command
            base_cmd = cmd.split()[0] if cmd.strip() else ""
            if base_cmd:
                completions.append(f"command {base_cmd}")
        
        # Add project-specific explain suggestions
        project_type = context.get("project_type")
        if project_type == "python":
            completions.extend([
                "virtual environments",
                "python packaging",
                "project structure"
            ])
        elif project_type == "node":
            completions.extend([
                "package.json",
                "nodejs modules",
                "npm scripts"
            ])
        
        return completions
    
    def _get_help_completions(self, context: Dict[str, Any]) -> List[str]:
        """
        Get completions for the 'help-with' command.
        
        Args:
            context: The completion context
            
        Returns:
            List of completions
        """
        completions = []
        
        # Add project-specific help suggestions
        project_type = context.get("project_type")
        if project_type == "python":
            completions.extend([
                "setting up pytest",
                "creating a Python package",
                "using virtual environments",
                "debugging Python code"
            ])
        elif project_type == "node":
            completions.extend([
                "creating a React component",
                "setting up webpack",
                "optimizing npm builds",
                "debugging JavaScript"
            ])
        
        # Add general help suggestions
        completions.extend([
            "git workflow",
            "project structure",
            "optimizing performance",
            "writing documentation"
        ])
        
        return completions
    
    async def _get_ai_completions(
        self,
        command: str,
        partial: str,
        context: Dict[str, Any]
    ) -> List[str]:
        """
        Get AI-powered completions for natural language commands.
        
        Args:
            command: The main command (fix, explain, help-with)
            partial: The partial natural language input
            context: The completion context
            
        Returns:
            List of AI-suggested completions
        """
        from angela.api.ai import get_gemini_client, get_gemini_request_class
        
        try:
            # Build a prompt for the AI
            prompt = f"""
    You are suggesting auto-completions for the Angela CLI's "{command}" command.
    The user has typed: "{command} {partial}"
    Context information:
    Project type: {context.get('project_type', 'unknown')}
    Recent files: {', '.join(context.get('recent_files', [])[:3])}
    Recent commands: {', '.join(context.get('recent_commands', [])[:3])}
    Last failed command: {context.get('last_failed_command', 'none')}
    Suggest 3-5 natural language completions that would be helpful continuations of what the user is typing.
    Each completion should be the FULL text that would follow "{command} ", not just the part after "{partial}".
    Completions should be relevant to the current context and project type.
    Respond with ONLY a JSON array of strings, like:
    ["completion 1", "completion 2", "completion 3"
    """
            
            GeminiRequest = get_gemini_request_class()
            api_request = GeminiRequest(
                prompt=prompt,
                max_tokens=200,
                temperature=0.1  # Low temperature for more deterministic completions
            )
            
            response = await get_gemini_client().generate_text(api_request)
            
            # Parse the response to extract completions
            import json
            import re
            
            # Try to find JSON in the response
            json_match = re.search(r'\[.*\]', response.text, re.DOTALL)
            if json_match:
                try:
                    completions = json.loads(json_match.group(0))
                    if isinstance(completions, list): # Ensure it's a list
                        return completions
                    else:
                        self._logger.warning(f"AI completion response was valid JSON but not a list: {json_match.group(0)}")
                        return []
                except json.JSONDecodeError as e:
                    self._logger.error(f"Error decoding AI completions JSON: {str(e)} - Response part: {json_match.group(0)}")
                    return []
            
            # If we couldn't parse JSON or find a match, return empty list
            self._logger.debug(f"No valid JSON array found in AI completion response: {response.text}")
            return []
            
        except Exception as e:
            self._logger.error(f"Error getting AI completions: {str(e)}")
            return []
    
    def _build_completion_context(self) -> Dict[str, Any]:
        """
        Build a context dictionary for completions.
        
        Returns:
            Context dictionary
        """
        from angela.api.context import get_context_manager
        from angela.api.context import get_file_activity_tracker
        from angela.api.context import get_session_manager
        
        context = {}
        
        # Add project information
        context["project_type"] = get_context_manager().project_type
        
        # Add recent files
        recent_activities = get_file_activity_tracker().get_recent_activities(max_count=5)
        context["recent_files"] = [str(activity.file_path) for activity in recent_activities 
                                  if activity.file_path is not None]
        
        # Add recent commands from session
        session_context = get_session_manager().get_context()
        context["recent_commands"] = session_context.get("recent_commands", [])
        
        # Add last failed command if available
        last_failed = get_session_manager().get_entity("last_failed_command")
        if last_failed:
            context["last_failed_command"] = last_failed.get("value", "")
        
        return context

    
    def _build_completion_context(self) -> Dict[str, Any]:
        """
        Build a context dictionary for completions.
        
        Returns:
            Context dictionary
        """
        context = {}
        
        # Add project information
        context["project_type"] = context_manager.project_type
        
        # Add recent files
        recent_activities = file_activity_tracker.get_recent_activities(max_count=5)
        context["recent_files"] = [str(activity.file_path) for activity in recent_activities 
                                  if activity.file_path is not None]
        
        # Add recent commands from session
        session_context = session_manager.get_context()
        context["recent_commands"] = session_context.get("recent_commands", [])
        
        # Add last failed command if available
        last_failed = session_manager.get_entity("last_failed_command")
        if last_failed:
            context["last_failed_command"] = last_failed.get("value", "")
        
        return context

# Global instance
completion_handler = CompletionHandler()
</file>

<file path="components/shell/formatter.py">
"""
Enhanced terminal formatter for Angela CLI with improved layout and consistent styling.

This module provides responsive terminal output formatting with 
symmetric layouts, proper content sizing, and a consistent color scheme.
"""
import asyncio
import sys
import time
import random
from typing import Optional, List, Dict, Any, Callable, Awaitable, Tuple, Set
from enum import Enum
from pathlib import Path
import textwrap

from rich.console import Console, Group
from rich.panel import Panel
from rich.syntax import Syntax
from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn, TimeElapsedColumn
from rich.table import Table
from rich.live import Live
from rich.text import Text
from rich.layout import Layout
from rich.tree import Tree
from rich.spinner import Spinner
from rich.markdown import Markdown
from rich.columns import Columns
from rich import box
from rich.style import Style
from rich.align import Align

from angela.api.intent import get_advanced_task_plan_class, get_plan_step_type_enum
from angela.utils.logging import get_logger
from angela.constants import RISK_LEVELS

# ════════════════════════════════════════════════
# Consistent box style for visual unity
# ════════════════════════════════════════════════
DEFAULT_BOX = box.ROUNDED

# ════════════════════════════════════════════════
# Cohesive color palette with consistent theme
# ════════════════════════════════════════════════

COLOR_PALETTE = {
    "border": "#ff0055",           # Red border for all panels
    "text": "#00c8ff",             # Blue text for panel content
    "confirmation": "#8a2be2",     # Purple for confirmation panels
    "confirmation_text": "#ff0055", # Red text for confirmation panels
    "success": "#00ff99",          # Success green
    "warning": "#ffcc00",          # Warning yellow
    "error": "#ff3355",            # Error red
    "info": "#00c8ff",             # Information blue
    "subtle": "#6c7280",           # Subdued color for less important elements
    "white": "#ffffff",            # White for high-contrast elements
}

# ════════════════════════════════════════════════
# Simple ASCII decorators for visual markers
# ════════════════════════════════════════════════

ASCII_DECORATIONS = {
    "command": "⚡",
    "output": "◈",
    "error": "⚠",
    "confirmation": "◈",
    "success": "✓",
    "warning": "⚠",
}

# Get risk level names mapping for display
RISK_LEVEL_NAMES = {v: k for k, v in RISK_LEVELS.items()}

# Risk styling with consistent color scheme
RISK_COLORS = {
    RISK_LEVELS["SAFE"]: COLOR_PALETTE["success"],
    RISK_LEVELS["LOW"]: COLOR_PALETTE["info"],
    RISK_LEVELS["MEDIUM"]: COLOR_PALETTE["warning"],
    RISK_LEVELS["HIGH"]: COLOR_PALETTE["error"],
    RISK_LEVELS["CRITICAL"]: COLOR_PALETTE["border"],
}

RISK_ICONS = {
    RISK_LEVELS["SAFE"]: "✓",
    RISK_LEVELS["LOW"]: "⚡",
    RISK_LEVELS["MEDIUM"]: "⚠",
    RISK_LEVELS["HIGH"]: "❗",
    RISK_LEVELS["CRITICAL"]: "⛔",
}

AdvancedTaskPlan = get_advanced_task_plan_class()
PlanStepType = get_plan_step_type_enum()

_console = Console(record=True, width=100)

logger = get_logger(__name__)

class OutputType(Enum):
    """Types of command output."""
    STDOUT = "stdout"
    STDERR = "stderr"
    INFO = "info"
    SUCCESS = "success"
    WARNING = "warning"
    ERROR = "error"
    PROGRESS = "progress"

class TerminalFormatter:
    """
    Rich terminal formatter with responsive layout and consistent styling.
    """

    # Philosophy quotes for loading screens
    PHILOSOPHY_QUOTES = [
        # Aristotle quotes
        "We are what we repeatedly do. Excellence, then, is not an act, but a habit. - Aristotle",
        "The whole is greater than the sum of its parts. - Aristotle",
        "Knowing yourself is the beginning of all wisdom. - Aristotle",
        "It is the mark of an educated mind to be able to entertain a thought without accepting it. - Aristotle",
        "Happiness depends upon ourselves. - Aristotle",
        
        # Socrates quotes
        "The unexamined life is not worth living. - Socrates",
        "I know that I am intelligent, because I know that I know nothing. - Socrates",
        "The secret of change is to focus all of your energy not on fighting the old, but on building the new. - Socrates",
        "Be kind, for everyone you meet is fighting a hard battle. - Socrates",
        "Wonder is the beginning of wisdom. - Socrates",
        
        # Plato quotes
        "At the touch of love everyone becomes a poet. - Plato",
        "We can easily forgive a child who is afraid of the dark; the real tragedy of life is when men are afraid of the light. - Plato",
        "Be kind, for everyone you meet is fighting a harder battle. - Plato",
        "The measure of a man is what he does with power. - Plato",
        "Wise men speak because they have something to say; fools because they have to say something. - Plato",
        
        # Sun Tzu quotes
        "Appear weak when you are strong, and strong when you are weak. - Sun Tzu",
        "The supreme art of war is to subdue the enemy without fighting. - Sun Tzu",
        "Let your plans be dark and impenetrable as night, and when you move, fall like a thunderbolt. - Sun Tzu",
        "If you know the enemy and know yourself, you need not fear the result of a hundred battles. - Sun Tzu",
        "Victorious warriors win first and then go to war, while defeated warriors go to war first and then seek to win. - Sun Tzu",
        
        # Hypatia quotes
        "Reserve your right to think, for even to think wrongly is better than not to think at all. - Hypatia",
        "Life is an unfoldment, and the further we travel the more truth we can comprehend. - Hypatia",
        "To teach superstitions as truth is a most terrible thing. - Hypatia",
        "All formal dogmatic religions are fallacious and must never be accepted by self-respecting persons as final. - Hypatia",
        
        # Sextus Empiricus quotes
        "For every argument, there is a counter-argument of equal weight. - Sextus Empiricus",
        "The wise man suspends judgment; he recognizes that nothing is certain. - Sextus Empiricus",
        "Appearances are our only guide in life. - Sextus Empiricus",
        "The goal of the skeptic is tranquility of mind. - Sextus Empiricus",
        
        # Pythagoras quotes
        "The highest form of pure thought is in mathematics. - Pythagoras",
        "Number is the ruler of forms and ideas, and the cause of gods and demons. - Pythagoras",
        "Concern should drive us into action and not into a depression. - Pythagoras",
        "Above all things, reverence yourself. - Pythagoras",
        
        # Xenophanes quotes
        "If cattle and horses had hands, they would draw the forms of gods like cattle and horses. - Xenophanes",
        "There is one god, greatest among gods and men, similar to mortals neither in shape nor in thought. - Xenophanes",
        "The gods did not reveal all things to men at the start; but as time goes on, by searching, they discover more. - Xenophanes",
        "Men create the gods in their own image. - Xenophanes",
        
        # Additional philosophical quotes
        "The only true wisdom is in knowing you know nothing. - Socrates",
        "Man is the measure of all things. - Protagoras",
        "I think, therefore I am. - René Descartes",
        "He who has a why to live can bear almost any how. - Friedrich Nietzsche",
        "One cannot step twice in the same river. - Heraclitus",
        "The function of prayer is not to influence God, but rather to change the nature of the one who prays. - Søren Kierkegaard",
        "What is rational is actual and what is actual is rational. - G.W.F. Hegel"
    ]
    
    def __init__(self):
        """Initialize the terminal formatter."""
        self._console = Console()
        self._logger = logger
        self._active_displays = set()
        
    def _get_quantum_vortex_spinner(self, elapsed: float) -> Text:
        """Create a mesmerizing quantum vortex spinner animation."""
        import math
        spinner_text = Text()
        
        # Outer vortex ring with color cycling
        outer_symbols = ["◜", "◠", "◝", "◞", "◡", "◟"]
        outer_idx = int(elapsed * 6) % len(outer_symbols)
        outer_char = outer_symbols[outer_idx]
        
        # Color cycle through spectrum
        hue = int(elapsed * 36) % 360
        # Create RGB colors that cycle
        r = int(abs(math.sin(hue/60)) * 255)
        g = int(abs(math.sin((hue+120)/60)) * 255)
        b = int(abs(math.sin((hue+240)/60)) * 255)
        color_hex = f"#{r:02x}{g:02x}{b:02x}"
        
        # Inner particle with opposite rotation
        inner_symbols = ["•", "◦", "●", "○", "◎", "◉"]
        inner_idx = int(elapsed * 8) % len(inner_symbols)
        inner_idx_reverse = (len(inner_symbols) - 1 - inner_idx)  # Reverse direction
        inner_char = inner_symbols[inner_idx_reverse]
        
        # Pulsing effect for center particle
        pulse_size = 1 + 0.5 * math.sin(elapsed * 3)
        if pulse_size > 1.25:
            center_style = "bold bright_white"
        else:
            center_style = "dim white"
        
        # Assemble the spinner components
        spinner_text.append(outer_char, style=color_hex)
        spinner_text.append(inner_char, style=center_style)
        
        # Quantum particle trails
        particle_pos = int(elapsed * 5) % 3
        trail = " " * particle_pos + "∴" + " " * (2 - particle_pos)
        spinner_text.append(trail, style=COLOR_PALETTE["info"])
        
        return spinner_text
    
    def _get_elemental_cascade_spinner(self, elapsed: float) -> Text:
        """Create a spinner that cycles through elemental themes."""
        import math
        spinner_text = Text()
        
        # Determine current element based on time
        element_cycle = int(elapsed * 2) % 4  # Changes every 0.5 seconds
        
        # Element-specific animations
        if element_cycle == 0:  # Fire
            fire_symbols = ["🔥", "🔥", "🔥", "🔥", "💥", "✨", "🔥", "🔥"]
            fire_idx = int(elapsed * 12) % len(fire_symbols)
            
            # Flame intensity changes
            flame_chars = ["═", "╪", "╫", "╬", "╬", "╫", "╪"]
            flame_idx = int(elapsed * 14) % len(flame_chars)
            
            spinner_text.append(fire_symbols[fire_idx]) # Appending a plain character
            spinner_text.append(flame_chars[flame_idx], style=COLOR_PALETTE["border"])
            spinner_text.append("~", style=COLOR_PALETTE["warning"])
            
        elif element_cycle == 1:  # Water
            water_symbols = ["≈", "≋", "≈", "∽", "∿", "≈"]
            water_idx = int(elapsed * 12) % len(water_symbols)
            
            # Wave effect
            wave_level = int(2 + 2 * math.sin(elapsed * 6))
            waves = "~" * wave_level
            
            spinner_text.append(water_symbols[water_idx], style=COLOR_PALETTE["info"])
            spinner_text.append(waves, style=COLOR_PALETTE["info"])
            spinner_text.append("○", style=COLOR_PALETTE["info"])
            
        elif element_cycle == 2:  # Earth
            earth_symbols = ["◦", "•", "●", "◎", "◉", "⦿", "◉", "◎", "●", "•"]
            earth_idx = int(elapsed * 10) % len(earth_symbols)
            
            # Growth effect
            growth = [".", "․", "‥", "…", "⁘", "⁙"]
            growth_idx = int(elapsed * 6) % len(growth)
            
            spinner_text.append(earth_symbols[earth_idx], style=COLOR_PALETTE["success"])
            spinner_text.append(growth[growth_idx], style=COLOR_PALETTE["success"])
            spinner_text.append("⏣", style=COLOR_PALETTE["success"])
            
        else:  # Air
            air_symbols = ["≋", "≈", "≋", "≈", "≋", "≈"]
            air_idx = int(elapsed * 8) % len(air_symbols)
            
            # Wind effect
            wind_dir = int(elapsed * 4) % 2
            if wind_dir == 0:
                wind = "»»»"
            else:
                wind = "«««"
                
            spinner_text.append(air_symbols[air_idx], style=COLOR_PALETTE["white"])
            spinner_text.append(wind, style=COLOR_PALETTE["white"])
            spinner_text.append("◌", style=COLOR_PALETTE["info"])
        
        return spinner_text
    
    def _get_interstellar_warp_spinner(self, elapsed: float) -> Text:
        """Create a mind-blowing warp drive animation effect."""
        import math
        spinner_text = Text()
        
        # Warp ship core with pulsing energy
        energy_level = int(3 + 3 * math.sin(elapsed * 8))
        core_symbols = ["▮", "▰", "█", "▰", "▮", "▯", "▮", "▰", "█"]
        core_idx = int(elapsed * 12) % len(core_symbols)
        core_char = core_symbols[core_idx]
        
        # Warp field effect with varying lengths based on warp speed
        warp_factor = 1 + int(elapsed * 10) % 9  # Warp factors 1-9
        warp_speed = min(4, int(1 + warp_factor/2))  # Max length of 4
        
        # Center of animation
        spinner_text.append("=", style=COLOR_PALETTE["info"])
        spinner_text.append(core_char * energy_level, style=COLOR_PALETTE["warning"])
        spinner_text.append("=", style=COLOR_PALETTE["info"])
        
        # Starfield effect - stars zooming past at different speeds
        star_positions = []
        for i in range(5):  # Generate 5 stars
            # Each star moves at different speeds
            pos = (elapsed * (5 + i)) % 15
            intensity = min(1.0, 15 - pos) / 1.0  # Fade based on position
            
            if intensity > 0.7:
                style = COLOR_PALETTE["white"]
            elif intensity > 0.4:
                style = COLOR_PALETTE["text"]
            else:
                style = COLOR_PALETTE["subtle"]
                
            # Convert position to integer for display
            pos_int = int(pos)
            
            # Star character depends on position (moving away = smaller)
            if pos_int < 3:
                star = "*"
            elif pos_int < 7:
                star = "·"
            else:
                star = "."
                
            # Add to positions list
            star_positions.append((pos_int, star, style))
        
        # Sort stars by position for layering
        star_positions.sort()
        
        # Create starfield
        starfield_markup_list = [""] * 15  # Initialize empty spaces with placeholder
        for pos, star, style in star_positions:
            if 0 <= pos < 15:  # Ensure within bounds
                # Store as markup string in the list
                starfield_markup_list[pos] = f"[{style}]{star}[/{style}]"
        
        # Add leading stars
        for i in range(warp_speed):
            # Join the markup strings from the list for the current segment
            starfield_segment_markup = "".join(starfield_markup_list[i*3:(i+1)*3])
            if starfield_segment_markup.strip():  # Only add if there's visible content
                # Append the composed markup string, ensuring it's parsed by Text.from_markup
                spinner_text.append(Text.from_markup(starfield_segment_markup))
        
        # Add warp drive energy fluctuation
        fluctuation = int(elapsed * 20) % 3
        if fluctuation == 0:
            spinner_text.append("⚡", style=COLOR_PALETTE["info"])
        elif fluctuation == 1:
            spinner_text.append("⚡", style=COLOR_PALETTE["confirmation"])
        else:
            spinner_text.append("⚡", style=COLOR_PALETTE["warning"])
        
        return spinner_text
    
    def _get_random_spinner(self, elapsed: float) -> Text:
        """Get a random spinner based on object ID determinism."""
        import random
        
        # We'll use the hash of the elapsed time's integer part to select a spinner,
        # but only hash it once at the beginning of a loading session
        if not hasattr(self, '_current_spinner_choice'):
            # Initialize a random spinner for this loading session
            self._current_spinner_choice = random.randint(1, 3)
            self._logger.debug(f"Selected spinner animation: {self._current_spinner_choice}")
        
        # Use the selected spinner
        if self._current_spinner_choice == 1:
            return self._get_quantum_vortex_spinner(elapsed)
        elif self._current_spinner_choice == 2:
            return self._get_elemental_cascade_spinner(elapsed)
        else:
            return self._get_interstellar_warp_spinner(elapsed)

    def print_command(self, command: str, title: Optional[str] = None) -> None:
        """
        Display a command with syntax highlighting.
        
        Args:
            command: The command to display
            title: Optional title for the panel
        """
        title = title or f"{ASCII_DECORATIONS['command']} Command {ASCII_DECORATIONS['command']}"
        
        # Create a syntax object with proper styling
        syntax = Syntax(
            command, 
            "bash", 
            theme="monokai", 
            word_wrap=True,
            background_color="default"
        )
        
        # Create a panel with the syntax
        panel = Panel(
            syntax,
            title=f"[bold {COLOR_PALETTE['border']}]{title}[/bold {COLOR_PALETTE['border']}]",
            border_style=COLOR_PALETTE["border"],
            box=DEFAULT_BOX,
            expand=False,  # Don't expand beyond content
            padding=(1, 2)
        )
        
        self._console.print("")
        self._console.print(panel)
    
    def print_output(
        self, 
        output: str, 
        output_type: OutputType = OutputType.STDOUT,
        title: Optional[str] = None
    ) -> None:
        """
        Display command output with appropriate formatting.
        
        Args:
            output: The output text
            output_type: Type of output
            title: Optional title for the panel
        """
        if not output:
            return
            
        # Set styling based on output type
        if output_type == OutputType.STDERR or output_type == OutputType.ERROR:
            style = COLOR_PALETTE["error"]
            title = title or f"{ASCII_DECORATIONS['error']} Error {ASCII_DECORATIONS['error']}"
        elif output_type == OutputType.WARNING:
            style = COLOR_PALETTE["warning"]
            title = title or f"{ASCII_DECORATIONS['warning']} Warning {ASCII_DECORATIONS['warning']}"
        elif output_type == OutputType.SUCCESS:
            style = COLOR_PALETTE["success"]
            title = title or f"{ASCII_DECORATIONS['success']} Success {ASCII_DECORATIONS['success']}"
        elif output_type == OutputType.INFO:
            style = COLOR_PALETTE["info"]
            title = title or f"{ASCII_DECORATIONS['info']} Info {ASCII_DECORATIONS['info']}"
        else:  # Default for STDOUT
            style = COLOR_PALETTE["text"]
            title = title or f"{ASCII_DECORATIONS['output']} Output {ASCII_DECORATIONS['output']}"
        
        # Clean up the output - remove any trailing blank lines
        cleaned_output = output.rstrip()
        
        # Create panel with output
        # Use the consistent color scheme - red border, blue text
        text = Text(cleaned_output, style=COLOR_PALETTE["text"])
        
        panel = Panel(
            text,
            title=f"[bold {style}]{title}[/bold {style}]",
            border_style=COLOR_PALETTE["border"],  # Always use red for border
            box=DEFAULT_BOX,
            expand=False,  # Don't expand beyond content
            padding=(1, 2)
        )
        
        self._console.print("")
        self._console.print(panel)
    
    def print_error_analysis(self, analysis: Dict[str, Any]) -> None:
        """
        Display error analysis with fix suggestions.
        
        Args:
            analysis: The error analysis dictionary
        """
        # Create a table for the error analysis 
        table = Table(
            title=f"[bold {COLOR_PALETTE['error']}]Error Analysis[/bold {COLOR_PALETTE['error']}]",
            box=DEFAULT_BOX,
            border_style=COLOR_PALETTE["border"],  # Consistent red border
            highlight=True,
            expand=False
        )
        
        table.add_column("Aspect", style=COLOR_PALETTE["confirmation"], justify="right")
        table.add_column("Details", style=COLOR_PALETTE["text"])  # Consistent blue text
        
        # Add error summary
        error_text = Text(analysis.get("error_summary", "Unknown error"), style=COLOR_PALETTE["error"])
        table.add_row("Error", error_text)
        
        # Add possible cause
        table.add_row("Possible Cause", analysis.get("possible_cause", "Unknown"))
        
        # Add command issues
        if analysis.get("command_issues"):
            issues = "\n".join(f"• {issue}" for issue in analysis["command_issues"])
            table.add_row("Command Issues", issues)
        
        # Add file issues
        if analysis.get("file_issues"):
            file_issues = []
            for issue in analysis["file_issues"]:
                path = issue.get("path", "unknown")
                if "suggestion" in issue:
                    file_issues.append(f"• {path}: {issue['suggestion']}")
                if "similar_files" in issue:
                    similar = ", ".join(issue["similar_files"])
                    file_issues.append(f"  Did you mean: {similar}?")
            
            if file_issues:
                table.add_row("File Issues", "\n".join(file_issues))
        
        # Display the table
        self._console.print("")
        self._console.print(table)
        
        # Display fix suggestions if available
        if analysis.get("fix_suggestions"):
            suggestions = analysis["fix_suggestions"]
            if suggestions:
                # Create a suggestions panel with consistent styling
                suggestion_text = []
                for i, suggestion in enumerate(suggestions, 1):
                    suggestion_text.append(f"[{COLOR_PALETTE['text']}]• {suggestion}[/{COLOR_PALETTE['text']}]")
                
                self._console.print(Panel(
                    "\n".join(suggestion_text),
                    title=f"[bold {COLOR_PALETTE['success']}]Fix Suggestions[/bold {COLOR_PALETTE['success']}]",
                    border_style=COLOR_PALETTE["border"],  # Consistent red border
                    box=DEFAULT_BOX,
                    expand=False,
                    padding=(1, 2)
                ))

    async def display_pre_confirmation_info(
        self,
        command: str,
        risk_level: int,
        risk_reason: str,
        impact: Dict[str, Any],
        explanation: Optional[str] = None,
        preview: Optional[str] = None,
        confidence_score: Optional[float] = None,
        execution_time: Optional[float] = None
    ) -> None:
        """
        Display a symmetrical and properly aligned pre-confirmation layout.
        
        Args:
            command: The command to be executed
            risk_level: Risk level (0-4)
            risk_reason: Reason for the risk assessment
            impact: Impact analysis dictionary
            explanation: Optional explanation of the command
            preview: Optional preview of command execution
            confidence_score: Optional AI confidence score (0-1)
            execution_time: Optional execution time if this is post-execution
        """
        # Get console width for proper layout
        console_width = self._console.width
        
        # Risk level styling
        risk_name = RISK_LEVEL_NAMES.get(risk_level, "UNKNOWN")
        risk_icon = RISK_ICONS.get(risk_level, "⚠")
        risk_color = RISK_COLORS.get(risk_level, COLOR_PALETTE["warning"])
        
        # Calculate main panel width - make sure it's centered
        main_panel_width = min(console_width - 10, 80)  # Max 80 chars or less depending on terminal
        
        # 1. Command panel (top)
        command_panel = Panel(
            Syntax(command, "bash", theme="monokai", word_wrap=True),
            title=f"[bold {risk_color}]{risk_icon} Execute [{risk_name} Risk][/bold {risk_color}]",
            border_style=COLOR_PALETTE["border"],
            box=DEFAULT_BOX,
            width=main_panel_width,
            padding=(1, 2)
        )
        
        # 2. Explanation and confidence score panels (side by side)
        explanation_text = explanation or "No explanation available for this command."
        explanation_panel = Panel(
            Text(explanation_text, style=COLOR_PALETTE["text"]),
            title=f"[bold {COLOR_PALETTE['text']}]✧ Command Insight ✧[/bold {COLOR_PALETTE['text']}]",
            border_style=COLOR_PALETTE["border"],
            box=DEFAULT_BOX,
            width=main_panel_width // 2 - 1,  # Divide available space
            padding=(1, 2)
        )
        
        # Confidence panel creation
        if confidence_score is not None:
            confidence_stars = int(confidence_score * 5)
            confidence_display = "★" * confidence_stars + "☆" * (5 - confidence_stars)
            
            if confidence_score > 0.8:
                confidence_color = COLOR_PALETTE["success"]
            elif confidence_score > 0.6:
                confidence_color = COLOR_PALETTE["info"]
            else:
                confidence_color = COLOR_PALETTE["error"]
            
            confidence_panel = Panel(
                Group(
                    Text("Score:", style=f"bold {COLOR_PALETTE['text']}", justify="center"),
                    Text(f"{confidence_score:.2f}", style=f"bold {confidence_color}", justify="center"),
                    Text(confidence_display, style=confidence_color, justify="center"),
                    Text("", justify="center"),  # Empty line for spacing
                    Text("(AI confidence in", style="dim", justify="center"),
                    Text("command accuracy)", style="dim", justify="center")
                ),
                title=f"[bold {COLOR_PALETTE['text']}]✧ AI Confidence ✧[/bold {COLOR_PALETTE['text']}]",
                border_style=COLOR_PALETTE["border"],
                box=DEFAULT_BOX,
                width=main_panel_width // 2 - 1,  # Divide available space
                padding=(1, 2)
            )
        else:
            confidence_panel = Panel(
                Text("No confidence score available.", style="dim", justify="center"),
                title=f"[bold {COLOR_PALETTE['text']}]✧ AI Confidence ✧[/bold {COLOR_PALETTE['text']}]",
                border_style=COLOR_PALETTE["border"],
                box=DEFAULT_BOX,
                width=main_panel_width // 2 - 1,
                padding=(1, 2)
            )
        
        # 3. Preview panel (if available)
        if preview:
            preview_panel = Panel(
                Text(preview, style=COLOR_PALETTE["text"]),
                title=f"[bold {COLOR_PALETTE['text']}]⚡ Command Preview ⚡[/bold {COLOR_PALETTE['text']}]",
                border_style=COLOR_PALETTE["border"],
                box=DEFAULT_BOX,
                width=main_panel_width,
                padding=(1, 2)
            )
        
        # 4. Risk assessment panel
        impact_summary = []
        if impact.get("operations"):
            ops = ", ".join(impact["operations"])
            impact_summary.append(f"Operations: {ops}")
        
        if impact.get("affected_files"):
            files = ", ".join(Path(f).name for f in impact["affected_files"])
            impact_summary.append(f"Files: {files}")
        
        if impact.get("affected_dirs"):
            dirs = ", ".join(Path(d).name for d in impact["affected_dirs"])
            impact_summary.append(f"Directories: {dirs}")
        
        impact_text = "\n".join(impact_summary) if impact_summary else "No detailed impact available."
        
        risk_panel = Panel(
            Group(
                Text(f"Level: {risk_name}", style=risk_color),
                Text(f"Reason: {risk_reason}", style=COLOR_PALETTE["text"]),
                Text("", justify="center"),  # Empty line for spacing
                Text("Impact Assessment:", style=f"bold {COLOR_PALETTE['text']}"),
                Text(impact_text, style=COLOR_PALETTE["text"])
            ),
            title=f"[bold {risk_color}]⚠ Risk Assessment[/bold {risk_color}]",
            border_style=COLOR_PALETTE["border"],
            box=DEFAULT_BOX,
            width=main_panel_width,
            padding=(1, 2)
        )
        
        # Now actually display everything in the right order and centered properly
        
        # 1. Display command panel centered
        self._console.print()
        self._console.print(Align.center(command_panel))
        
        # 2. Display explanation and confidence panels side by side
        columns = Columns([explanation_panel, confidence_panel], equal=False, padding=0)
        self._console.print(Align.center(columns, width=main_panel_width + 2))  # +2 for borders
        
        # 3. Display preview panel if available
        if preview:
            self._console.print(Align.center(preview_panel))
        
        # 4. Display risk panel
        self._console.print(Align.center(risk_panel))
        self._console.print()  # Add some space at the end

    async def display_inline_confirmation(
        self,
        prompt_text: str = "Proceed with execution?"
    ) -> bool:
        """
        Display an inline confirmation prompt and get user input.
        
        Args:
            prompt_text: The confirmation prompt text
            
        Returns:
            True if confirmed, False otherwise
        """
        # Create a confirmation prompt with consistent styling
        # Purple background with red text as requested
        panel_content = Group(
            Text(prompt_text, style=COLOR_PALETTE["confirmation_text"], justify="center"),
            Text("", justify="center"),  # Empty line for spacing
            Text.from_markup(f"([bold {COLOR_PALETTE['success']}]y[/bold {COLOR_PALETTE['success']}]/[bold {COLOR_PALETTE['error']}]n[/bold {COLOR_PALETTE['error']}])", justify="center")
        )
        
        confirmation_panel = Panel(
            panel_content,
            title=f"[bold {COLOR_PALETTE['confirmation']}]{ASCII_DECORATIONS['confirmation']} Awaiting Confirmation {ASCII_DECORATIONS['confirmation']}[/bold {COLOR_PALETTE['confirmation']}]",
            border_style=COLOR_PALETTE["confirmation"],  # Purple border for confirmation
            box=DEFAULT_BOX,
            expand=False,
            padding=(1, 2)
        )
        
        self._console.print("")
        self._console.print(confirmation_panel)
        
        # Get the user's response with consistent styling
        self._console.print(f"[bold {COLOR_PALETTE['confirmation']}]>>> [/bold {COLOR_PALETTE['confirmation']}]", end="")
        response = input().strip().lower()
        
        # Consider empty response or y/yes as "yes"
        if not response or response in ("y", "yes"):
            return True
        
        # Everything else is "no"
        return False

    async def display_execution_timer(
        self,
        command: str,
        with_philosophy: bool = True
    ) -> Tuple[str, str, int, float]:
        """
        Display a command execution timer with philosophy quotes.
        
        Args:
            command: The command being executed
            with_philosophy: Whether to display philosophy quotes
            
        Returns:
            Tuple of (stdout, stderr, return_code, execution_time)
        """
        import random
        import time
        import asyncio
        from rich.live import Live
        from rich.panel import Panel
        from rich.text import Text
        from rich.console import Group
        
        # Ensure no active live displays
        self._ensure_no_active_live()
        
        # Reset spinner choice for new execution session
        if hasattr(self, '_current_spinner_choice'):
            delattr(self, '_current_spinner_choice')
        
        start_time = time.time()
        
        # Choose a random philosophy quote
        quote = random.choice(self.PHILOSOPHY_QUOTES) if with_philosophy else ""
        
        # Create a layout for execution display
        def get_layout():
            elapsed = time.time() - start_time
            
            # Get a random but consistent spinner for this execution session
            spinner = self._get_random_spinner(elapsed)
            
            # Add execution message
            spinner_with_text = Text()
            spinner_with_text.append(spinner)
            spinner_with_text.append(" ")
            spinner_with_text.append(f"{elapsed:.2f}s", style=f"bold {COLOR_PALETTE['text']}")
            spinner_with_text.append(" - Executing command...")
            
            if with_philosophy:
                # For the philosophy quote
                quote_text = Text(quote, style=f"italic {COLOR_PALETTE['text']}")
                
                # Add an empty line for spacing
                spacer = Text("")
                
                # Group them together with proper spacing
                content = Group(quote_text, spacer, spinner_with_text)
            else:
                content = spinner_with_text
            
            panel = Panel(
                content,
                title="Command Execution",
                border_style=COLOR_PALETTE["border"],  # Consistent red border
                box=DEFAULT_BOX,
                padding=(1, 2)
            )
            
            return panel
        
        # Use asyncio.create_subprocess_shell to execute the command
        process = await asyncio.create_subprocess_shell(
            command,
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE
        )
        
        # Collect output
        stdout_chunks = []
        stderr_chunks = []
        
        # Set up tasks to read output
        async def read_stream(stream, is_stdout: bool):
            while True:
                line = await stream.readline()
                if not line:
                    break
                    
                try:
                    line_str = line.decode('utf-8', errors='replace')
                    
                    # Store the output
                    if is_stdout:
                        stdout_chunks.append(line_str)
                    else:
                        stderr_chunks.append(line_str)
                        
                except Exception as e:
                    self._logger.error(f"Error processing output: {str(e)}")
        
        # Create tasks for stdout and stderr
        stdout_task = asyncio.create_task(read_stream(process.stdout, True))
        stderr_task = asyncio.create_task(read_stream(process.stderr, False))
        
        # Display the live progress with stunning visuals
        try:
            with Live(get_layout(), refresh_per_second=20, console=self._console) as live:
                # Wait for the command to complete while updating the display
                return_code = await process.wait()
                
                # Wait for the streams to complete
                await stdout_task
                await stderr_task
                
                execution_time = time.time() - start_time
                
                # Create a visually stunning completion panel
                completed_panel = Panel(
                    Text(f"Execution completed in {execution_time:.6f}s", style=COLOR_PALETTE["success"], justify="center"),
                    title=f"[bold {COLOR_PALETTE['success']}]✓ Angela Initialized ✓[/bold {COLOR_PALETTE['success']}]",
                    border_style=COLOR_PALETTE["success"],
                    box=DEFAULT_BOX,
                    expand=False,
                    padding=(1, 2)
                )
                
                live.update(completed_panel)
                
                # Brief pause to show completion
                await asyncio.sleep(0.5)
        except Exception as e:
            self._logger.error(f"Error in execution timer: {str(e)}")
            # Ensure we still wait for the process
            if process.returncode is None: # Check if process might still be running
                try:
                    # Wait for process with a timeout to avoid hanging indefinitely
                    await asyncio.wait_for(process.wait(), timeout=5.0) 
                    return_code = process.returncode if process.returncode is not None else -1
                except asyncio.TimeoutError:
                    self._logger.error("Timeout waiting for process to complete after error.")
                    if process.returncode is None: # if still none after timeout, try to kill
                        try:
                            process.kill()
                            await process.wait() # ensure it's reaped
                        except ProcessLookupError:
                            pass # process might have already exited
                        except Exception as kill_e:
                            self._logger.error(f"Error trying to kill process: {kill_e}")
                    return_code = -1 
                except Exception as proc_e:
                    self._logger.error(f"Further error waiting for process: {proc_e}")
                    return_code = -1
            elif process.returncode is not None: # Process already finished, just get its code
                return_code = process.returncode
            else: # Fallback if process object is in an unexpected state
                return_code = -1

            # Wait for the streams to complete, even in error cases
            # Use try-except for each to ensure one doesn't prevent the other
            try:
                await asyncio.wait_for(stdout_task, timeout=2.0)
            except asyncio.TimeoutError:
                self._logger.error("Timeout waiting for stdout_task to complete after error.")
            except Exception as stream_e:
                self._logger.error(f"Error waiting for stdout_task: {stream_e}")
            
            try:
                await asyncio.wait_for(stderr_task, timeout=2.0)
            except asyncio.TimeoutError:
                self._logger.error("Timeout waiting for stderr_task to complete after error.")
            except Exception as stream_e:
                self._logger.error(f"Error waiting for stderr_task: {stream_e}")
            
            # Recalculate execution_time up to the point of error handling completion
            execution_time = time.time() - start_time
        
        # Return the results
        return (
            "".join(stdout_chunks),
            "".join(stderr_chunks),
            return_code if isinstance(return_code, int) else -1, # Ensure return_code is an int
            execution_time
        )


    async def display_loading_timer(
        self,
        message: str,
        with_philosophy: bool = True
    ) -> None:
        """
        Display a loading timer with optional philosophy quotes.
        
        Args:
            message: The loading message to display
            with_philosophy: Whether to display philosophy quotes
        """
        import random
        import time
        import asyncio
        from rich.live import Live
        from rich.panel import Panel
        from rich.text import Text
        from rich.console import Group
        
        # Ensure no active live displays
        self._ensure_no_active_live()
        
        # Reset spinner choice for new loading session
        if hasattr(self, '_current_spinner_choice'):
            delattr(self, '_current_spinner_choice')
        
        start_time = time.time()
        
        # Choose a random philosophy quote
        quote = random.choice(self.PHILOSOPHY_QUOTES) if with_philosophy else ""
        
        # Create a layout function that properly handles the spinner
        def get_layout():
            elapsed = time.time() - start_time
            
            # Get a random but consistent spinner for this loading session
            spinner = self._get_random_spinner(elapsed)
            
            # Add timer and message to the spinner
            spinner_with_text = Text()
            spinner_with_text.append(spinner)
            spinner_with_text.append(" ")
            spinner_with_text.append(f"{elapsed:.2f}s", style=f"bold {COLOR_PALETTE['text']}")
            spinner_with_text.append(f" - {message}")
            
            if with_philosophy:
                # For the philosophy quote
                quote_text = Text(quote, style=f"italic {COLOR_PALETTE['text']}")
                
                # Add an empty line for spacing
                spacer = Text("")
                
                # Group them together with proper spacing
                content = Group(quote_text, spacer, spinner_with_text)
            else:
                content = spinner_with_text
            
            panel = Panel(
                content,
                title="Angela Initializing...",
                border_style=COLOR_PALETTE["border"],  # Consistent red border
                box=DEFAULT_BOX,
                padding=(1, 2)
            )
            
            return panel
        
        # Use try-except with asyncio.sleep to make it cancellable
        try:
            with Live(get_layout(), refresh_per_second=20, console=self._console, transient=True) as live:
                try:
                    while True:
                        await asyncio.sleep(0.03)  # Smaller sleep for smoother animation
                        live.update(get_layout())
                except asyncio.CancelledError:
                    # Handle cancellation gracefully
                    self._logger.debug("Loading display cancelled")
                    raise  # Re-raise to ensure proper cleanup
        except asyncio.CancelledError:
            # Expected when cancelled from outside
            pass
        except Exception as e:
            self._logger.error(f"Error displaying loading timer: {str(e)}")
    
    def _ensure_no_active_live(self):
        """Ensure no active Live displays by checking and resetting console state."""
        # Access the internal console state to check if it has an active Live
        if hasattr(self._console, "_live") and self._console._live:
            self._logger.warning("Detected active Live display. Attempting cleanup.")
            # Try to gracefully close any existing live display
            try:
                try:
                    # First try to stop it properly
                    self._console._live.stop()
                except Exception:
                    # If that fails, just set it to None
                    pass
                self._console._live = None
                # Reset other potentially problematic console state
                if hasattr(self._console, "_buffer"):
                    self._console._buffer = []
            except Exception as e:
                self._logger.error(f"Error cleaning up console state: {str(e)}")

    async def display_result_summary(self, result: Dict[str, Any]) -> None:
        """
        Display a summary of a command execution result without duplicating explanations.
        
        Args:
            result: The command execution result
        """
        # Extract data from the result
        command = result.get("command", "")
        stdout = result.get("stdout", "")
        stderr = result.get("stderr", "")
        success = result.get("success", False)
        
        # Only display output, not the command or explanations again
        if stdout.strip():
            self.print_output(stdout.strip(), OutputType.STDOUT)
        
        if stderr.strip():
            self.print_output(stderr.strip(), OutputType.STDERR)
        
        # Display overall status
        if success:
            if not stdout.strip() and not stderr.strip():
                # If no output, show a success message
                self._console.print("")
                self._console.print(f"[bold {COLOR_PALETTE['success']}]{ASCII_DECORATIONS['success']} Command executed successfully [/bold {COLOR_PALETTE['success']}]")
        else:
            if not stderr.strip():
                # If no error output but command failed
                self._console.print("")
                self._console.print(f"[bold {COLOR_PALETTE['error']}]{ASCII_DECORATIONS['error']} Command failed with exit code {result.get('return_code', 1)}[/bold {COLOR_PALETTE['error']}]")


    async def display_command_learning(
        self, 
        base_command: str, 
        count: int
    ) -> None:
        """
        Display a notification that a command has been used multiple times.
        
        Args:
            base_command: The command that was executed
            count: Number of times the command has been used
        """
        # Use a fancy learning prompt with purple styling (consistent with your palette)
        self._console.print(Panel(
            f"I noticed you've used [bold cyan]{base_command}[/bold cyan] {count} times.",
            title="Command Learning",
            border_style=COLOR_PALETTE["confirmation"],  # Purple border for consistency
            expand=False
        ))
    
    async def display_auto_execution_notice(
        self,
        command: str, 
        risk_level: int,
        preview: Optional[str]
    ) -> None:
        """
        Show a notice for auto-execution with consistent styling.
        
        Args:
            command: The command being auto-executed
            risk_level: Risk level of the command
            preview: Optional preview of what the command will do
        """
        from angela.api.context import get_preferences_manager
        preferences_manager = get_preferences_manager()
        
        # Use a more subtle notification for auto-execution with your consistent styling
        self._console.print("\n")
        self._console.print(Panel(
            Syntax(command, "bash", theme="monokai", word_wrap=True),
            title="Auto-Executing Trusted Command",
            border_style=COLOR_PALETTE["success"],  # Use success color from your palette
            expand=False
        ))
        
        # Only show preview if it's enabled in preferences
        if preview and preferences_manager.preferences.ui.show_command_preview:
            self._console.print(Panel(
                preview,
                title=f"[bold {COLOR_PALETTE['text']}]⚡ Command Preview ⚡[/bold {COLOR_PALETTE['text']}]",
                border_style=COLOR_PALETTE["border"],  # Consistent red border
                box=DEFAULT_BOX,
                expand=False
            ))
        
        # Create the loading task with your spinner styling
        loading_task = asyncio.create_task(
            self.display_loading_timer("Auto-executing trusted command...", with_philosophy=True)
        )
        
        try:
            # Wait a minimum amount of time for visual feedback
            await asyncio.sleep(0.5)
            
            # Now we're ready to continue, cancel the loading task
            loading_task.cancel()
            try:
                await loading_task
            except asyncio.CancelledError:
                pass  # Expected
        except Exception as e:
            logger.error(f"Error managing loading display: {str(e)}")
            # Ensure the task is cancelled
            if not loading_task.done():
                loading_task.cancel()
    
    async def display_command_preview(
        self,
        command: str, 
        preview: str
    ) -> None:
        """
        Display a command preview with consistent styling.
        
        Args:
            command: The command being previewed
            preview: Preview of what the command will do
        """
        self._console.print(Panel(
            preview,
            title=f"[bold {COLOR_PALETTE['text']}]⚡ Command Preview ⚡[/bold {COLOR_PALETTE['text']}]",
            border_style=COLOR_PALETTE["border"],  # Consistent red border
            box=DEFAULT_BOX,
            expand=False
        ))
    
    async def display_trust_added_message(
        self,
        command: str
    ) -> None:
        """
        Display a message when a command is added to trusted list.
        
        Args:
            command: The command that was trusted
        """
        base_command = command.split()[0] if command.split() else command
        self._console.print(f"Added [green]{base_command}[/green] to trusted commands.")

    async def display_command_summary(
        self,
        command: str,
        success: bool,
        stdout: str,
        stderr: str,
        return_code: int = 0,
        execution_time: Optional[float] = None
    ) -> None:
        """
        Display a comprehensive command execution summary.
        
        Args:
            command: The executed command
            success: Whether the command was successful
            stdout: Standard output from the command
            stderr: Standard error from the command
            return_code: Command return code
            execution_time: Execution time in seconds
        """
        # Command panel
        self.print_command(command, title="Command")
        
        # Output panel with styling
        if stdout.strip():
            # Create a styled panel for stdout
            stdout_panel = Panel(
                Text(stdout.strip(), style=COLOR_PALETTE["text"]),
                title=f"[bold {COLOR_PALETTE['success']}]{ASCII_DECORATIONS['output']} Output {ASCII_DECORATIONS['output']}[/bold {COLOR_PALETTE['success']}]",
                border_style=COLOR_PALETTE["border"],
                box=DEFAULT_BOX,
                expand=False,
                padding=(1, 2)
            )
            self._console.print("")
            self._console.print(stdout_panel)
        elif success:
            # Create a styled panel for success with no output
            success_panel = Panel(
                Text("Command executed successfully.", style=COLOR_PALETTE["success"]),
                title=f"[bold {COLOR_PALETTE['success']}]{ASCII_DECORATIONS['success']} Success {ASCII_DECORATIONS['success']}[/bold {COLOR_PALETTE['success']}]",
                border_style=COLOR_PALETTE["success"],
                box=DEFAULT_BOX,
                expand=False,
                padding=(1, 2)
            )
            self._console.print("")
            self._console.print(success_panel)
        
        # Error panel if command failed
        if not success:
            # Create a styled error panel
            if stderr.strip():
                error_panel = Panel(
                    Text(stderr.strip(), style=COLOR_PALETTE["error"]),
                    title=f"[bold {COLOR_PALETTE['error']}]{ASCII_DECORATIONS['error']} Error {ASCII_DECORATIONS['error']}[/bold {COLOR_PALETTE['error']}]",
                    border_style=COLOR_PALETTE["error"],
                    box=DEFAULT_BOX,
                    expand=False,
                    padding=(1, 2)
                )
                self._console.print("")
                self._console.print(error_panel)
            else:
                # Error with no stderr output
                error_panel = Panel(
                    Text(f"Command failed with exit code {return_code}", style=COLOR_PALETTE["error"]),
                    title=f"[bold {COLOR_PALETTE['error']}]{ASCII_DECORATIONS['error']} Error {ASCII_DECORATIONS['error']}[/bold {COLOR_PALETTE['error']}]",
                    border_style=COLOR_PALETTE["error"],
                    box=DEFAULT_BOX,
                    expand=False,
                    padding=(1, 2)
                )
                self._console.print("")
                self._console.print(error_panel)
        

# Global formatter instance
terminal_formatter = TerminalFormatter()
</file>

<file path="components/shell/inline_feedback.py">
import sys
import os
import asyncio
from typing import Dict, Any, List, Optional, Callable, Awaitable
import time
import threading

from angela.utils.logging import get_logger
from angela.api.context import get_session_manager

logger = get_logger(__name__)

class InlineFeedback:
    """
    Provides inline feedback and interaction capabilities.
    
    This class allows Angela to display feedback, ask questions, and
    interact with the user directly within the terminal session.
    """
    
    def __init__(self):
        """Initialize the inline feedback system."""
        self._logger = logger
        self._active_prompts = {}
        self._last_message_time = 0
        self._message_cooldown = 5  # Seconds between automatic messages
        self._prompt_id_counter = 0
        self._active_threads = {}
        self._active_messages = {}  # Track messages that might need to be cleared
        self.loop: Optional[asyncio.AbstractEventLoop] = None

    async def _ensure_loop(self) -> bool:
        """Ensures self.loop is set to the current running asyncio event loop."""
        if self.loop is None:
            try:
                self.loop = asyncio.get_running_loop()
            except RuntimeError:
                self._logger.error("InlineFeedback: Could not get running asyncio event loop. Threaded operations might fail.")
                return False
        return True

    def _set_future_result_threadsafe(self, future: asyncio.Future, result: Any) -> None:
        """Synchronous method to set future result, intended for call_soon_threadsafe."""
        if self.loop and not future.done(): # Check loop just in case, though call_soon_threadsafe needs it
            self.loop.call_soon_threadsafe(future.set_result, result)
        elif not future.done(): # Fallback if loop somehow not set, though this path is less ideal
            future.set_result(result)


    async def show_message(
        self, 
        message: str, 
        message_type: str = "info",
        timeout: float = 0
    ) -> None:
        """
        Display a message inline in the terminal.
        
        Args:
            message: The message to display
            message_type: Type of message (info, warning, error, success)
            timeout: Auto-clear message after this many seconds (0 = no auto-clear)
        """
        current_time = time.time()
        if current_time - self._last_message_time < self._message_cooldown:
            self._logger.debug(f"Skipping message due to cooldown: {message}")
            return
        
        self._last_message_time = current_time
        
        color_code = {
            "info": "\033[34m",
            "warning": "\033[33m",
            "error": "\033[31m",
            "success": "\033[32m",
        }.get(message_type, "\033[34m")
        
        reset_code = "\033[0m"
        formatted_message = f"\n{color_code}[Angela] {message}{reset_code}"
        message_id = str(time.time())
        self._active_messages[message_id] = formatted_message
        
        print(formatted_message, file=sys.stderr)
        
        if timeout > 0:
            asyncio.create_task(self._clear_message_after_timeout(message_id, timeout))
    
    async def suggest_command(
        self, 
        command: str,
        explanation: str,
        confidence: float = 0.8,
        execute_callback: Optional[Callable[[], Awaitable[None]]] = None
    ) -> bool:
        """
        Suggest a command and offer to execute it with enhanced visuals.
        
        Args:
            command: The command to suggest
            explanation: Explanation of what the command does
            confidence: Confidence score for the suggestion
            execute_callback: Callback to execute the command
            
        Returns:
            True if command was executed, False otherwise
        """
        from angela.api.context import get_session_manager
        from rich.console import Console
        from rich.panel import Panel
        from rich.syntax import Syntax
        
        console = Console()
        
        # Calculate visual confidence representation
        confidence_stars = int(confidence * 5)
        confidence_display = "★" * confidence_stars + "☆" * (5 - confidence_stars)
        confidence_color = "green" if confidence > 0.8 else "yellow" if confidence > 0.6 else "red"
        
        # Display command with rich formatting
        console.print(Panel(
            Syntax(command, "bash", theme="monokai", word_wrap=True),
            title="Suggested Command",
            border_style="blue",
            expand=False
        ))
        
        # Display confidence score
        console.print(Panel(
            f"[bold]Confidence Score:[/bold] [{confidence_color}]{confidence:.2f}[/{confidence_color}] {confidence_display}\n"
            "[dim](Confidence indicates how sure Angela is that this command matches your request)[/dim]",
            title="AI Confidence",
            border_style=confidence_color,
            expand=False
        ))
        
        # Display explanation
        console.print(Panel(
            explanation,
            title="Explanation",
            border_style="blue",
            expand=False
        ))
        
        # Create execution options
        console.print("[bold cyan]┌───────────────────────────────────────┐[/bold cyan]")
        console.print("[bold cyan]│[/bold cyan] Execute? ([green]y[/green]/[red]n[/red]/[yellow]e[/yellow] - where 'e' will edit) [bold cyan]│[/bold cyan]")
        console.print("[bold cyan]└───────────────────────────────────────┘[/bold cyan]")
        console.print("[bold cyan]▶[/bold cyan] ", end="")
        
        # Get user's response
        response = input().strip().lower()
        
        if response == "y" or response == "yes" or response == "":
            if execute_callback:
                await execute_callback()
            return True
        elif response == "e" or response == "edit":
            edited_command = await self._get_edited_command(command)
            if edited_command and execute_callback:
                get_session_manager().add_entity("edited_command", "command", edited_command)
                await execute_callback()
                return True
        return False
    
    async def _get_edited_command(self, original_command: str) -> Optional[str]:
        """
        Allow the user to edit a command with enhanced visual presentation.
        
        Args:
            original_command: The command to edit
            
        Returns:
            The edited command or None if cancelled
        """
        if not await self._ensure_loop() or not self.loop:
            self._logger.error("Cannot edit command: Event loop not available.")
            return None
    
        input_future = self.loop.create_future()
        
        from rich.console import Console
        from rich.panel import Panel
        from rich.syntax import Syntax
        
        console = Console()
    
        try:
            # Try to use prompt_toolkit for a better editing experience
            from prompt_toolkit import prompt
            from prompt_toolkit.history import InMemoryHistory
            from prompt_toolkit.auto_suggest import AutoSuggestFromHistory
            from prompt_toolkit.formatted_text import HTML
            from prompt_toolkit.key_binding import KeyBindings
            
            # Show editing instructions
            console.print(Panel(
                "Edit the command below.\n"
                "Press [bold]Enter[/bold] to confirm or [bold]Esc[/bold] to cancel.",
                title="Command Editor",
                border_style="cyan",
                expand=False
            ))
            
            def get_prompt_toolkit_input():
                try:
                    kb = KeyBindings()
                    @kb.add('escape')
                    def _(event): event.app.exit(result=None)
                    
                    history = InMemoryHistory()
                    history.append_string(original_command)
                    
                    result = prompt(
                        HTML("<ansiblue>Edit the command: </ansiblue>"),
                        default=original_command, history=history,
                        auto_suggest=AutoSuggestFromHistory(), key_bindings=kb,
                        enable_history_search=True, enable_system_prompt=True,
                        enable_suspend=True, mouse_support=True
                    )
                    self._set_future_result_threadsafe(input_future, result)
                except (KeyboardInterrupt, EOFError):
                    self._set_future_result_threadsafe(input_future, None)
                except Exception as e_thread:
                    self._logger.error(f"Error in prompt_toolkit thread: {e_thread}")
                    self._set_future_result_threadsafe(input_future, None)
    
            thread = threading.Thread(target=get_prompt_toolkit_input)
            thread.daemon = True
            thread.start()
            
        except ImportError:
            # Fallback to basic input if prompt_toolkit is not available
            self._logger.warning("prompt_toolkit not available, using basic input for command edit.")
            
            # Display the original command
            console.print(Panel(
                Syntax(original_command, "bash", theme="monokai", word_wrap=True),
                title="Original Command",
                border_style="blue",
                expand=False
            ))
            
            console.print("[bold cyan]┌────────────────────────────────┐[/bold cyan]")
            console.print("[bold cyan]│[/bold cyan] [bold]Edit command:[/bold] [dim](Ctrl+C to cancel)[/dim] [bold cyan]│[/bold cyan]")
            console.print("[bold cyan]└────────────────────────────────┘[/bold cyan]")
            console.print("[bold cyan]▶[/bold cyan] ", end="")
            
            def get_basic_input():
                try:
                    user_input = input()
                    self._set_future_result_threadsafe(input_future, user_input)
                except (KeyboardInterrupt, EOFError):
                    self._set_future_result_threadsafe(input_future, None)
                except Exception as e_thread:
                    self._logger.error(f"Error in basic input thread: {e_thread}")
                    self._set_future_result_threadsafe(input_future, None)
            
            thread = threading.Thread(target=get_basic_input)
            thread.daemon = True
            thread.start()
    
        try:
            edited_command = await asyncio.wait_for(input_future, timeout=60)
            
            # Display the edited command for confirmation
            if edited_command is not None and edited_command != original_command:
                console.print(Panel(
                    Syntax(edited_command, "bash", theme="monokai", word_wrap=True),
                    title="Edited Command",
                    border_style="green",
                    expand=False
                ))
                
            return edited_command
        except asyncio.TimeoutError:
            console.print("\n[yellow]Edit timed out[/yellow]")
            if not input_future.done():
                self._set_future_result_threadsafe(input_future, None)
            return None
        except Exception as e:
            self._logger.error(f"Error in command editor: {str(e)}")
            if not input_future.done():
                self._set_future_result_threadsafe(input_future, None)
            return None
    
    def _input_thread(
        self, 
        prompt_id: int,
        formatted_question: str,
        options: List[str],
        default_option: Optional[str],
        timeout: int,
        future: asyncio.Future
    ) -> None:
        """
        Thread function to handle user input for a prompt.
        
        Args:
            prompt_id: ID of the prompt
            formatted_question: Formatted question text
            options: Valid options
            default_option: Default option
            timeout: Timeout in seconds
            future: Future to set with the result
        """
        try:
            if os.name == "posix":
                import select
                i, _, _ = select.select([sys.stdin], [], [], timeout)
                if i:
                    user_input = sys.stdin.readline().strip().lower()
                    if user_input in options: 
                        result = user_input
                    elif user_input == "" and default_option: 
                        result = default_option
                    else:
                        result = default_option
                else:
                    # Timeout occurred
                    result = default_option
            else: 
                # Fallback for non-Unix (e.g. Windows)
                user_input = input().strip().lower()
                if user_input in options: 
                    result = user_input
                elif user_input == "" and default_option: 
                    result = default_option
                else:
                    result = default_option
        except (KeyboardInterrupt, EOFError):
            # User cancelled, result remains default_option or None if no default
            result = default_option
        except Exception as e:
            self._logger.error(f"Error in input thread: {str(e)}")
            result = default_option
        
        self._set_future_result_threadsafe(future, result)
    
    async def _clear_message_after_timeout(self, message_id: str, timeout: float) -> None:
        """
        Clear a message after a timeout.
        """
        await asyncio.sleep(timeout)
        if message_id not in self._active_messages: return
        message = self._active_messages[message_id]
        try:
            lines = message.count('\n') + 1
            up_sequence = f"\033[{lines}A"
            clear_sequence = "\033[K"
            clear_command = up_sequence
            for _ in range(lines):
                clear_command += clear_sequence + "\033[1B"
            clear_command += f"\033[{lines}A"
            sys.stderr.write(clear_command)
            sys.stderr.flush()
            del self._active_messages[message_id]
            self._logger.debug(f"Cleared message after timeout: {message_id}")
        except Exception as e:
            self._logger.error(f"Error clearing message: {str(e)}")
    
    def _get_next_prompt_id(self) -> int:
        """Get the next prompt ID."""
        self._prompt_id_counter += 1
        return self._prompt_id_counter

# Global instance
inline_feedback = InlineFeedback()
</file>

<file path="components/toolchain/__init__.py">
# angela/components/toolchain/__init__.py
"""
Toolchain components for Angela CLI.

This package provides integrations with various development tools
including package managers, Git, Docker, CI/CD systems, and universal
CLI translation capabilities.
"""

# These imports don't need to be changed as they're importing from within the same package
from .git import git_integration
from .package_managers import package_manager_integration
from .docker import docker_integration
from .universal_cli import universal_cli_translator
from .ci_cd import ci_cd_integration
from .enhanced_universal_cli import enhanced_universal_cli
from .cross_tool_workflow_engine import cross_tool_workflow_engine
from .test_frameworks import test_framework_integration

# Define the public API
__all__ = [
    'git_integration',
    'package_manager_integration', 
    'docker_integration',
    'universal_cli_translator',
    'ci_cd_integration',
    'enhanced_universal_cli',
    'cross_tool_workflow_engine',
    'test_framework_integration'
]
</file>

<file path="components/toolchain/ci_cd.py">
# angela/components/toolchain/ci_cd.py
"""
CI/CD configuration generation for Angela CLI.

This module provides functionality for generating CI/CD configurations
for common CI platforms.
"""
import os
from pathlib import Path
from typing import Dict, Any, List, Optional, Union
import yaml
import json
import re
from collections.abc import MutableMapping, Sequence

from angela.utils.logging import get_logger
# Updated imports to use API layer
from angela.api.context import get_context_manager
from angela.api.execution import get_execution_engine
from angela.api.safety import get_command_validator

logger = get_logger(__name__)

# Common deep merge utility for all configuration merging
def deep_update(d, u):
    """
    Recursively update a nested dictionary structure.
    
    Args:
        d: The original dictionary to update
        u: The dictionary with updates to apply
        
    Returns:
        Updated dictionary
    """
    result = d.copy()
    for k, v in u.items():
        if isinstance(v, MutableMapping) and isinstance(result.get(k, {}), MutableMapping):
            result[k] = deep_update(result.get(k, {}), v)
        elif isinstance(v, (list, tuple)) and isinstance(result.get(k, []), (list, tuple)):
            # Check for __REPLACE__ marker for list replacement
            if v and isinstance(v, list) and v[0] == "__REPLACE__":
                result[k] = v[1:]
            else:
                result[k] = result.get(k, []) + list(v)
        else:
            result[k] = v
    return result

class CiCdIntegration:
    """
    Integration for CI/CD platforms.
    """
    
    def __init__(self):
        """Initialize the CI/CD integration."""
        self._logger = logger
        
        # Supported CI/CD platforms
        self._supported_platforms = [
            "github_actions",
            "gitlab_ci",
            "jenkins",
            "travis",
            "circle_ci",
            "azure_pipelines",  # Additional platform
            "bitbucket_pipelines"  # Additional platform
        ]
    
    async def detect_project_type(
        self, 
        path: Union[str, Path]
    ) -> Dict[str, Any]:
        """
        Detect the project type for CI/CD configuration.
        
        Args:
            path: Path to the project
            
        Returns:
            Dictionary with the detected project info
        """
        self._logger.info(f"Detecting project type in {path}")
        
        path_obj = Path(path)
        
        # Check if path exists
        if not path_obj.exists() or not path_obj.is_dir():
            return {
                "detected": False,
                "error": f"Path does not exist or is not a directory: {path}",
                "project_type": None
            }
        
        # Check for project type indicators
        project_type = None
        framework = None
        
        # Python indicators
        if (path_obj / "requirements.txt").exists() or (path_obj / "setup.py").exists() or (path_obj / "pyproject.toml").exists():
            project_type = "python"
            # Check for specific Python frameworks
            if (path_obj / "manage.py").exists():
                framework = "django"
            elif (path_obj / "app.py").exists() or (path_obj / "wsgi.py").exists() or any(f.name == 'flask' for f in (path_obj / "requirements.txt").open().readlines() if hasattr(f, 'name')) if (path_obj / "requirements.txt").exists() else False:
                framework = "flask"
            elif (path_obj / "fastapi").exists() or any(f.name == 'fastapi' for f in (path_obj / "requirements.txt").open().readlines() if hasattr(f, 'name')) if (path_obj / "requirements.txt").exists() else False:
                framework = "fastapi"
        
        # Node.js indicators
        elif (path_obj / "package.json").exists():
            project_type = "node"
            # Check for specific JS frameworks
            try:
                with open(path_obj / "package.json") as f:
                    package_data = json.load(f)
                    dependencies = {**package_data.get("dependencies", {}), **package_data.get("devDependencies", {})}
                    if "react" in dependencies:
                        framework = "react"
                    elif "vue" in dependencies:
                        framework = "vue"
                    elif "angular" in dependencies:
                        framework = "angular"
                    elif "next" in dependencies:
                        framework = "nextjs"
                    elif "express" in dependencies:
                        framework = "express"
            except (json.JSONDecodeError, IOError):
                pass
        
        # Go indicators
        elif (path_obj / "go.mod").exists():
            project_type = "go"
            # Check for go frameworks like gin, echo, etc.
            try:
                with open(path_obj / "go.mod") as f:
                    content = f.read()
                    if "github.com/gin-gonic/gin" in content:
                        framework = "gin"
                    elif "github.com/labstack/echo" in content:
                        framework = "echo"
            except IOError:
                pass
        
        # Rust indicators
        elif (path_obj / "Cargo.toml").exists():
            project_type = "rust"
            try:
                with open(path_obj / "Cargo.toml") as f:
                    content = f.read()
                    if "rocket" in content:
                        framework = "rocket"
                    elif "actix-web" in content:
                        framework = "actix"
            except IOError:
                pass
        
        # Java indicators
        elif (path_obj / "pom.xml").exists():
            project_type = "java"
            framework = "maven"
            # Check for Spring Framework
            try:
                with open(path_obj / "pom.xml") as f:
                    content = f.read()
                    if "org.springframework" in content:
                        framework = "spring"
            except IOError:
                pass
        elif (path_obj / "build.gradle").exists() or (path_obj / "build.gradle.kts").exists():
            project_type = "java"
            framework = "gradle"
            # Check for Spring Framework
            gradle_file = path_obj / "build.gradle" if (path_obj / "build.gradle").exists() else path_obj / "build.gradle.kts"
            try:
                with open(gradle_file) as f:
                    content = f.read()
                    if "org.springframework" in content:
                        framework = "spring"
            except IOError:
                pass
        
        # Ruby indicators
        elif (path_obj / "Gemfile").exists():
            project_type = "ruby"
            # Check for Rails
            try:
                with open(path_obj / "Gemfile") as f:
                    content = f.read()
                    if "rails" in content.lower():
                        framework = "rails"
            except IOError:
                pass
        
        # PHP indicators
        elif any(f.suffix == '.php' for f in path_obj.glob('**/*.php')):
            project_type = "php"
            # Check for Laravel or Symfony
            if (path_obj / "artisan").exists():
                framework = "laravel"
            elif (path_obj / "bin" / "console").exists():
                framework = "symfony"
            elif (path_obj / "composer.json").exists():
                try:
                    with open(path_obj / "composer.json") as f:
                        composer_data = json.load(f)
                        require = composer_data.get("require", {})
                        if "laravel/framework" in require:
                            framework = "laravel"
                        elif "symfony/symfony" in require:
                            framework = "symfony"
                except (json.JSONDecodeError, IOError):
                    pass
        
        # .NET indicators
        elif any(f.suffix == '.csproj' for f in path_obj.glob('**/*.csproj')) or any(f.suffix == '.fsproj' for f in path_obj.glob('**/*.fsproj')):
            project_type = "dotnet"
            # Check for ASP.NET Core
            for proj_file in path_obj.glob('**/*.csproj'):
                try:
                    with open(proj_file) as f:
                        content = f.read()
                        if "Microsoft.AspNetCore" in content:
                            framework = "aspnet"
                            break
                except IOError:
                    continue
        
        # C/C++ with CMake
        elif (path_obj / "CMakeLists.txt").exists():
            project_type = "cpp"
            framework = "cmake"
        
        if project_type:
            result = {
                "detected": True,
                "project_type": project_type,
                "project_path": str(path_obj)
            }
            if framework:
                result["framework"] = framework
            return result
        
        # Try from context
        # Get context manager from API
        context_manager = get_context_manager()
        context = context_manager.get_context_dict()
        if context.get("project_type"):
            return {
                "detected": True,
                "project_type": context["project_type"],
                "project_path": str(path_obj),
                "from_context": True
            }
        
        return {
            "detected": False,
            "error": "Could not detect project type",
            "project_type": None
        }
    
    async def generate_ci_configuration(
        self, 
        path: Union[str, Path],
        platform: str,
        project_type: Optional[str] = None,
        custom_settings: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Generate a CI/CD configuration file.
        
        Args:
            path: Path to the project
            platform: CI/CD platform to generate for
            project_type: Optional project type
            custom_settings: Optional custom settings
            
        Returns:
            Dictionary with the generation result
        """
        self._logger.info(f"Generating CI configuration for {platform}")
        
        path_obj = Path(path)
        
        # Check if platform is supported
        if platform not in self._supported_platforms:
            return {
                "success": False,
                "error": f"Unsupported CI/CD platform: {platform}",
                "platform": platform
            }
        
        # Detect project type if not provided
        if project_type is None:
            detection_result = await self.detect_project_type(path_obj)
            project_type = detection_result.get("project_type")
            
            if not project_type:
                return {
                    "success": False,
                    "error": f"Could not detect project type: {detection_result.get('error', 'Unknown error')}",
                    "platform": platform
                }
        
        # Generate configuration based on platform
        if platform == "github_actions":
            return await self._generate_github_actions(path_obj, project_type, custom_settings)
        elif platform == "gitlab_ci":
            return await self._generate_gitlab_ci(path_obj, project_type, custom_settings)
        elif platform == "jenkins":
            return await self._generate_jenkins(path_obj, project_type, custom_settings)
        elif platform == "travis":
            return await self._generate_travis(path_obj, project_type, custom_settings)
        elif platform == "circle_ci":
            return await self._generate_circle_ci(path_obj, project_type, custom_settings)
        elif platform == "azure_pipelines":
            return await self._generate_azure_pipelines(path_obj, project_type, custom_settings)
        elif platform == "bitbucket_pipelines":
            return await self._generate_bitbucket_pipelines(path_obj, project_type, custom_settings)
        
        return {
            "success": False,
            "error": f"Unsupported CI/CD platform: {platform}",
            "platform": platform
        }
    
    async def _generate_github_actions(
        self, 
        path: Path,
        project_type: str,
        custom_settings: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Generate GitHub Actions configuration.
        
        Args:
            path: Path to the project
            project_type: Project type
            custom_settings: Optional custom settings
            
        Returns:
            Dictionary with the generation result
        """
        self._logger.info(f"Generating GitHub Actions configuration for {project_type}")
        
        # Create .github/workflows directory
        workflows_dir = path / ".github" / "workflows"
        if not workflows_dir.exists():
            os.makedirs(workflows_dir, exist_ok=True)
        
        # Set default settings based on project type
        workflow: Dict[str, Any] = {} # Ensure workflow is initialized
        if project_type == "python":
            workflow = {
                "name": "Python CI",
                "on": {
                    "push": {
                        "branches": ["main", "master"]
                    },
                    "pull_request": {
                        "branches": ["main", "master"]
                    }
                },
                "jobs": {
                    "build": {
                        "runs-on": "ubuntu-latest",
                        "strategy": {
                            "matrix": {
                                "python-version": ["3.8", "3.9", "3.10"]
                            }
                        },
                        "steps": [
                            {
                                "name": "Checkout code",
                                "uses": "actions/checkout@v3"
                            },
                            {
                                "name": "Set up Python ${{ matrix.python-version }}",
                                "uses": "actions/setup-python@v4",
                                "with": {
                                    "python-version": "${{ matrix.python-version }}"
                                }
                            },
                            {
                                "name": "Install dependencies",
                                "run": "python -m pip install --upgrade pip\npip install -r requirements.txt\npip install pytest pytest-cov flake8"
                            },
                            {
                                "name": "Lint with flake8",
                                "run": "flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics"
                            },
                            {
                                "name": "Test with pytest",
                                "run": "pytest --cov=. --cov-report=xml"
                            },
                            {
                                "name": "Upload coverage to Codecov",
                                "uses": "codecov/codecov-action@v3",
                                "with": {
                                    "file": "./coverage.xml",
                                    "fail_ci_if_error": "false"
                                }
                            }
                        ]
                    }
                }
            }
        elif project_type == "node":
            workflow = {
                "name": "Node.js CI",
                "on": {
                    "push": {
                        "branches": ["main", "master"]
                    },
                    "pull_request": {
                        "branches": ["main", "master"]
                    }
                },
                "jobs": {
                    "build": {
                        "runs-on": "ubuntu-latest",
                        "strategy": {
                            "matrix": {
                                "node-version": ["14.x", "16.x", "18.x"]
                            }
                        },
                        "steps": [
                            {
                                "name": "Checkout code",
                                "uses": "actions/checkout@v3"
                            },
                            {
                                "name": "Use Node.js ${{ matrix.node-version }}",
                                "uses": "actions/setup-node@v3",
                                "with": {
                                    "node-version": "${{ matrix.node-version }}",
                                    "cache": "npm"
                                }
                            },
                            {
                                "name": "Install dependencies",
                                "run": "npm ci"
                            },
                            {
                                "name": "Run linting",
                                "run": "npm run lint --if-present"
                            },
                            {
                                "name": "Build",
                                "run": "npm run build --if-present"
                            },
                            {
                                "name": "Test",
                                "run": "npm test"
                            }
                        ]
                    }
                }
            }
        elif project_type == "go":
            workflow = {
                "name": "Go CI",
                "on": {
                    "push": {
                        "branches": ["main", "master"]
                    },
                    "pull_request": {
                        "branches": ["main", "master"]
                    }
                },
                "jobs": {
                    "build": {
                        "runs-on": "ubuntu-latest",
                        "steps": [
                            {
                                "name": "Checkout code",
                                "uses": "actions/checkout@v3"
                            },
                            {
                                "name": "Set up Go",
                                "uses": "actions/setup-go@v3",
                                "with": {
                                    "go-version": "1.18"
                                }
                            },
                            {
                                "name": "Build",
                                "run": "go build -v ./..."
                            },
                            {
                                "name": "Test",
                                "run": "go test -v ./..."
                            }
                        ]
                    }
                }
            }
        elif project_type == "rust":
            workflow = {
                "name": "Rust CI",
                "on": {
                    "push": {
                        "branches": ["main", "master"]
                    },
                    "pull_request": {
                        "branches": ["main", "master"]
                    }
                },
                "jobs": {
                    "build": {
                        "runs-on": "ubuntu-latest",
                        "steps": [
                            {
                                "name": "Checkout code",
                                "uses": "actions/checkout@v3"
                            },
                            {
                                "name": "Install Rust",
                                "uses": "actions-rs/toolchain@v1",
                                "with": {
                                    "profile": "minimal",
                                    "toolchain": "stable",
                                    "override": "true",
                                    "components": "rustfmt, clippy"
                                }
                            },
                            {
                                "name": "Check formatting",
                                "uses": "actions-rs/cargo@v1",
                                "with": {
                                    "command": "fmt",
                                    "args": "-- --check"
                                }
                            },
                            {
                                "name": "Clippy",
                                "uses": "actions-rs/cargo@v1",
                                "with": {
                                    "command": "clippy",
                                    "args": "-- -D warnings"
                                }
                            },
                            {
                                "name": "Build",
                                "uses": "actions-rs/cargo@v1",
                                "with": {
                                    "command": "build"
                                }
                            },
                            {
                                "name": "Test",
                                "uses": "actions-rs/cargo@v1",
                                "with": {
                                    "command": "test"
                                }
                            }
                        ]
                    }
                }
            }
        elif project_type == "java":
            workflow = {
                "name": "Java CI",
                "on": {
                    "push": {
                        "branches": ["main", "master"]
                    },
                    "pull_request": {
                        "branches": ["main", "master"]
                    }
                },
                "jobs": {
                    "build": {
                        "runs-on": "ubuntu-latest",
                        "strategy": {
                            "matrix": {
                                "java-version": ["11", "17"]
                            }
                        },
                        "steps": [
                            {
                                "name": "Checkout code",
                                "uses": "actions/checkout@v3"
                            },
                            {
                                "name": "Set up JDK ${{ matrix.java-version }}",
                                "uses": "actions/setup-java@v3",
                                "with": {
                                    "java-version": "${{ matrix.java-version }}",
                                    "distribution": "temurin",
                                    "cache": "maven"
                                }
                            },
                            {
                                "name": "Build with Maven",
                                "run": "mvn -B package --file pom.xml"
                            },
                            {
                                "name": "Test",
                                "run": "mvn test"
                            }
                        ]
                    }
                }
            }
            # Check if it's Gradle
            if (path / "build.gradle").exists() or (path / "build.gradle.kts").exists():
                workflow["jobs"]["build"]["steps"][2] = {
                    "name": "Build with Gradle",
                    "uses": "gradle/gradle-build-action@v2",
                    "with": {
                        "arguments": "build"
                    }
                }
                workflow["jobs"]["build"]["steps"][3] = {
                    "name": "Test",
                    "uses": "gradle/gradle-build-action@v2",
                    "with": {
                        "arguments": "test"
                    }
                }
        elif project_type == "ruby":
            workflow = {
                "name": "Ruby CI",
                "on": {
                    "push": {
                        "branches": ["main", "master"]
                    },
                    "pull_request": {
                        "branches": ["main", "master"]
                    }
                },
                "jobs": {
                    "build": {
                        "runs-on": "ubuntu-latest",
                        "strategy": {
                            "matrix": {
                                "ruby-version": ["2.7", "3.0", "3.1"]
                            }
                        },
                        "steps": [
                            {
                                "name": "Checkout code",
                                "uses": "actions/checkout@v3"
                            },
                            {
                                "name": "Set up Ruby ${{ matrix.ruby-version }}",
                                "uses": "ruby/setup-ruby@v1",
                                "with": {
                                    "ruby-version": "${{ matrix.ruby-version }}",
                                    "bundler-cache": "true"
                                }
                            },
                            {
                                "name": "Install dependencies",
                                "run": "bundle install"
                            },
                            {
                                "name": "Run tests",
                                "run": "bundle exec rake test"
                            }
                        ]
                    }
                }
            }
        elif project_type == "php":
            workflow = {
                "name": "PHP CI",
                "on": {
                    "push": {
                        "branches": ["main", "master"]
                    },
                    "pull_request": {
                        "branches": ["main", "master"]
                    }
                },
                "jobs": {
                    "build": {
                        "runs-on": "ubuntu-latest",
                        "strategy": {
                            "matrix": {
                                "php-version": ["7.4", "8.0", "8.1"]
                            }
                        },
                        "steps": [
                            {
                                "name": "Checkout code",
                                "uses": "actions/checkout@v3"
                            },
                            {
                                "name": "Set up PHP ${{ matrix.php-version }}",
                                "uses": "shivammathur/setup-php@v2",
                                "with": {
                                    "php-version": "${{ matrix.php-version }}",
                                    "extensions": "mbstring, xml, ctype, iconv, intl, pdo_sqlite",
                                    "coverage": "xdebug"
                                }
                            },
                            {
                                "name": "Install Composer dependencies",
                                "run": "composer install --prefer-dist --no-progress"
                            },
                            {
                                "name": "Run tests",
                                "run": "vendor/bin/phpunit"
                            }
                        ]
                    }
                }
            }
        elif project_type == "dotnet":
            workflow = {
                "name": ".NET CI",
                "on": {
                    "push": {
                        "branches": ["main", "master"]
                    },
                    "pull_request": {
                        "branches": ["main", "master"]
                    }
                },
                "jobs": {
                    "build": {
                        "runs-on": "ubuntu-latest",
                        "strategy": {
                            "matrix": {
                                "dotnet-version": ["6.0.x", "7.0.x"]
                            }
                        },
                        "steps": [
                            {
                                "name": "Checkout code",
                                "uses": "actions/checkout@v3"
                            },
                            {
                                "name": "Setup .NET ${{ matrix.dotnet-version }}",
                                "uses": "actions/setup-dotnet@v3",
                                "with": {
                                    "dotnet-version": "${{ matrix.dotnet-version }}"
                                }
                            },
                            {
                                "name": "Restore dependencies",
                                "run": "dotnet restore"
                            },
                            {
                                "name": "Build",
                                "run": "dotnet build --no-restore"
                            },
                            {
                                "name": "Test",
                                "run": "dotnet test --no-build --verbosity normal"
                            }
                        ]
                    }
                }
            }
        elif project_type == "cpp":
            workflow = {
                "name": "C/C++ CI",
                "on": {
                    "push": {
                        "branches": ["main", "master"]
                    },
                    "pull_request": {
                        "branches": ["main", "master"]
                    }
                },
                "jobs": {
                    "build": {
                        "runs-on": "ubuntu-latest",
                        "steps": [
                            {
                                "name": "Checkout code",
                                "uses": "actions/checkout@v3"
                            },
                            {
                                "name": "Configure CMake",
                                "run": "cmake -B ${{github.workspace}}/build -DCMAKE_BUILD_TYPE=Debug"
                            },
                            {
                                "name": "Build",
                                "run": "cmake --build ${{github.workspace}}/build"
                            },
                            {
                                "name": "Test",
                                "working-directory": "${{github.workspace}}/build",
                                "run": "ctest -C Debug"
                            }
                        ]
                    }
                }
            }
        else: # Default empty workflow if project_type is not recognized
             workflow = {
                 "name": f"{project_type} CI", 
                 "on": {
                     "push": {
                         "branches": ["main", "master"]
                     },
                     "pull_request": {
                         "branches": ["main", "master"]
                     }
                 }, 
                 "jobs": {
                     "build": {
                         "runs-on": "ubuntu-latest",
                         "steps": [
                             {
                                 "name": "Checkout code", 
                                 "uses": "actions/checkout@v3"
                             },
                             {
                                 "name": "Build",
                                 "run": "echo 'Add your build commands here'"
                             },
                             {
                                 "name": "Test",
                                 "run": "echo 'Add your test commands here'"
                             }
                         ]
                     }
                 }
             }
    
        # Update with custom settings using deep merge
        if custom_settings:
            workflow = deep_update(workflow, custom_settings)
    
        # Write the workflow file
        workflow_file = workflows_dir / f"{project_type}-ci.yml"
        try:
            with open(workflow_file, 'w') as f:
                yaml.dump(workflow, f, default_flow_style=False, sort_keys=False)
            
            return {
                "success": True,
                "platform": "github_actions",
                "project_type": project_type,
                "config_file": str(workflow_file)
            }
        except Exception as e:
            return {
                "success": False,
                "error": f"Failed to write GitHub Actions workflow: {str(e)}",
                "platform": "github_actions",
                "project_type": project_type
            }
    
    async def create_complete_pipeline(
        self,
        project_path: Union[str, Path],
        platform: str,
        pipeline_type: str = "full",  # "full", "build-only", "deploy-only"
        custom_settings: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Create a complete CI/CD pipeline for a project.
        
        Args:
            project_path: Path to the project
            platform: CI/CD platform to use
            pipeline_type: Type of pipeline to create
            custom_settings: Custom settings for the pipeline
            
        Returns:
            Dictionary with the creation result
        """
        project_path = Path(project_path)
        self._logger.info(f"Creating complete {pipeline_type} pipeline for {project_path} on {platform}")
        
        # Detect project type
        detection_result = await self.detect_project_type(project_path)
        if not detection_result.get("detected"):
            return {
                "success": False,
                "error": detection_result.get("error", "Could not detect project type"),
                "platform": platform
            }
        
        project_type = detection_result["project_type"]
        
        # Determine pipeline steps based on project type and pipeline type
        pipeline_steps = await self._determine_pipeline_steps(
            project_type, 
            platform, 
            pipeline_type
        )
        
        # Merge with custom settings
        if custom_settings:
            pipeline_steps = deep_update(pipeline_steps, custom_settings)
        
        # Generate configuration
        result = await self.generate_ci_configuration(
            path=project_path,
            platform=platform,
            project_type=project_type,
            custom_settings=pipeline_steps
        )
        
        if not result.get("success"):
            return result
        
        # Set up additional required files
        if pipeline_type == "full" or pipeline_type == "deploy-only":
            # Set up deployment configuration if needed
            deploy_result = await self._setup_deployment_config(
                project_path, 
                platform, 
                project_type,
                pipeline_steps
            )
            
            if deploy_result:
                result["deployment_config"] = deploy_result
        
        # Set up testing configurations if needed
        if pipeline_type == "full" or pipeline_type == "build-only":
            testing_result = await self._setup_testing_config(
                project_path,
                project_type,
                pipeline_steps
            )
            
            if testing_result:
                result["testing_config"] = testing_result
        
        return result
    
    async def _determine_pipeline_steps(
        self,
        project_type: str,
        platform: str,
        pipeline_type: str
    ) -> Dict[str, Any]:
        """
        Determine the steps for a CI/CD pipeline.
        
        Args:
            project_type: Type of project
            platform: CI/CD platform
            pipeline_type: Type of pipeline
            
        Returns:
            Dictionary with pipeline steps
        """
        self._logger.debug(f"Determining pipeline steps for {project_type} on {platform} ({pipeline_type})")
        
        # Base steps common to all pipelines
        pipeline_steps = {
            "build": True,
            "test": True,
            "lint": True,
            "security_scan": pipeline_type == "full",
            "package": pipeline_type != "build-only",
            "deploy": pipeline_type != "build-only",
            "notify": pipeline_type == "full"
        }
        
        # Add platform-specific settings
        if platform == "github_actions":
            # GitHub Actions specific settings
            pipeline_steps["triggers"] = {
                "push": ["main", "master"],
                "pull_request": ["main", "master"],
                "manual": pipeline_type != "build-only"
            }
            
            # Add deployment environment based on pipeline type
            if pipeline_type != "build-only":
                pipeline_steps["environments"] = ["staging"]
                if pipeline_type == "full":
                    pipeline_steps["environments"].append("production")
        
        elif platform == "gitlab_ci":
            # GitLab CI specific settings
            pipeline_steps["stages"] = ["build", "test", "package"]
            if pipeline_type != "build-only":
                pipeline_steps["stages"].extend(["deploy", "verify"])
            
            pipeline_steps["cache"] = True
            pipeline_steps["artifacts"] = True
        
        # Add project-type specific settings
        if project_type == "python":
            pipeline_steps["python_versions"] = ["3.8", "3.9", "3.10"]
            pipeline_steps["test_command"] = "pytest --cov"
            pipeline_steps["lint_command"] = "flake8"
        
        elif project_type == "node":
            pipeline_steps["node_versions"] = ["14", "16", "18"]
            pipeline_steps["test_command"] = "npm test"
            pipeline_steps["lint_command"] = "npm run lint"
        
        elif project_type == "go":
            pipeline_steps["go_versions"] = ["1.18", "1.19"]
            pipeline_steps["test_command"] = "go test ./..."
            pipeline_steps["lint_command"] = "golangci-lint run"
        
        elif project_type == "java":
            pipeline_steps["java_versions"] = ["11", "17"]
            pipeline_steps["test_command"] = "mvn test"
            pipeline_steps["lint_command"] = "mvn checkstyle:check"
        
        elif project_type == "rust":
            pipeline_steps["rust_versions"] = ["stable", "beta"]
            pipeline_steps["test_command"] = "cargo test"
            pipeline_steps["lint_command"] = "cargo clippy -- -D warnings"
            
        elif project_type == "ruby":
            pipeline_steps["ruby_versions"] = ["2.7", "3.0", "3.1"]
            pipeline_steps["test_command"] = "bundle exec rake test"
            pipeline_steps["lint_command"] = "bundle exec rubocop"
            
        elif project_type == "php":
            pipeline_steps["php_versions"] = ["7.4", "8.0", "8.1"]
            pipeline_steps["test_command"] = "vendor/bin/phpunit"
            pipeline_steps["lint_command"] = "vendor/bin/phpcs"
            
        elif project_type == "dotnet":
            pipeline_steps["dotnet_versions"] = ["6.0", "7.0"]
            pipeline_steps["test_command"] = "dotnet test"
            pipeline_steps["lint_command"] = "dotnet format --verify-no-changes"
            
        elif project_type == "cpp":
            pipeline_steps["compilers"] = ["gcc", "clang"]
            pipeline_steps["test_command"] = "ctest -V"
            pipeline_steps["lint_command"] = "cppcheck ."
            
        return pipeline_steps
    
    async def _setup_deployment_config(
        self,
        project_path: Path,
        platform: str,
        project_type: str,
        pipeline_steps: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Set up deployment configuration files.
        
        Args:
            project_path: Path to the project
            platform: CI/CD platform
            project_type: Type of project
            pipeline_steps: Pipeline steps configuration
            
        Returns:
            Dictionary with deployment configuration result
        """
        self._logger.info(f"Setting up deployment configuration for {project_type}")
        
        # Create deployment configuration based on project type
        if project_type == "python":
            # For Python, create a simple deployment script
            scripts_dir = project_path / "scripts"
            deploy_script = scripts_dir / "deploy.sh"
            
            # Create scripts directory if it doesn't exist
            os.makedirs(scripts_dir, exist_ok=True)
            
            # Write deployment script
            with open(deploy_script, "w") as f:
                f.write("""#!/bin/bash
set -e

# Deployment script for Python project
echo "Deploying Python application..."

# Install dependencies
pip install -r requirements.txt

# Check for common deployment frameworks
if [ -f "manage.py" ]; then
    echo "Django project detected"
    python manage.py migrate
    python manage.py collectstatic --noinput
elif [ -f "app.py" ] || [ -f "wsgi.py" ]; then
    echo "Flask/WSGI project detected"
else
    echo "Generic Python project"
fi

# Reload application (depends on hosting)
if [ -f "gunicorn.pid" ]; then
    echo "Reloading Gunicorn..."
    kill -HUP $(cat gunicorn.pid)
elif command -v systemctl &> /dev/null && systemctl list-units --type=service | grep -q "$(basename $(pwd))"; then
    echo "Restarting service..."
    systemctl restart $(basename $(pwd))
else
    echo "Starting application..."
    # Add your start command here
fi

echo "Deployment complete!"
""")
            
            # Make the script executable
            deploy_script.chmod(0o755)
            
            return {
                "success": True,
                "files_created": [str(deploy_script)],
                "message": "Created deployment script"
            }
        
        elif project_type == "node":
            # For Node.js, create a deployment configuration
            scripts_dir = project_path / "scripts"
            deploy_script = scripts_dir / "deploy.js"
            
            # Create scripts directory if it doesn't exist
            os.makedirs(scripts_dir, exist_ok=True)
            
            # Write deployment script
            with open(deploy_script, "w") as f:
                f.write("""// Deployment script for Node.js project
console.log('Deploying Node.js application...');

const { execSync } = require('child_process');
const fs = require('fs');
const path = require('path');

// Execute shell command and print output
function exec(command) {
    console.log(`> ${command}`);
    try {
        const output = execSync(command, { encoding: 'utf8' });
        if (output) console.log(output);
    } catch (error) {
        console.error(`Error: ${error.message}`);
        process.exit(1);
    }
}

// Install dependencies
exec('npm ci --production');

// Check for common frameworks
const packageJson = JSON.parse(fs.readFileSync('package.json', 'utf8'));
const dependencies = packageJson.dependencies || {};

if (dependencies.next) {
    console.log('Next.js project detected');
    exec('npm run build');
} else if (dependencies.react) {
    console.log('React project detected');
    exec('npm run build');
} else if (dependencies.vue) {
    console.log('Vue.js project detected');
    exec('npm run build');
} else if (dependencies.express) {
    console.log('Express.js project detected');
} else {
    console.log('Generic Node.js project');
}

// Restart application
try {
    if (fs.existsSync('process.pid')) {
        console.log('Reloading application...');
        const pid = fs.readFileSync('process.pid', 'utf8').trim();
        try {
            process.kill(pid, 'SIGUSR2');
            console.log(`Sent SIGUSR2 to process ${pid}`);
        } catch (err) {
            console.log(`Process ${pid} not found, starting fresh`);
            // Start application
            if (fs.existsSync('ecosystem.config.js')) {
                exec('npx pm2 reload ecosystem.config.js');
            } else {
                // Determine main file
                const mainFile = packageJson.main || 'index.js';
                exec(`npx pm2 start ${mainFile} --name ${path.basename(process.cwd())}`);
            }
        }
    } else if (fs.existsSync('ecosystem.config.js')) {
        console.log('Starting with PM2...');
        exec('npx pm2 reload ecosystem.config.js');
    } else {
        console.log('Starting application...');
        // Determine main file
        const mainFile = packageJson.main || 'index.js';
        exec(`npx pm2 start ${mainFile} --name ${path.basename(process.cwd())}`);
    }
} catch (error) {
    console.error(`Error managing application process: ${error.message}`);
}

console.log('Deployment complete!');
""")
            
            return {
                "success": True,
                "files_created": [str(deploy_script)],
                "message": "Created deployment script"
            }
        
        elif project_type == "go":
            # For Go, create a deployment script
            scripts_dir = project_path / "scripts"
            deploy_script = scripts_dir / "deploy.sh"
            
            # Create scripts directory if it doesn't exist
            os.makedirs(scripts_dir, exist_ok=True)
            
            # Write deployment script
            with open(deploy_script, "w") as f:
                f.write("""#!/bin/bash
set -e

# Deployment script for Go project
echo "Deploying Go application..."

# Build the application
go build -o bin/app

# Check if systemd service exists
SERVICE_NAME=$(basename $(pwd))
if systemctl list-units --type=service | grep -q "$SERVICE_NAME"; then
    echo "Restarting service $SERVICE_NAME..."
    sudo systemctl restart $SERVICE_NAME
else
    echo "Starting application..."
    # Create a systemd service file if needed
    if [ ! -f "/etc/systemd/system/$SERVICE_NAME.service" ]; then
        echo "Creating systemd service..."
        cat > /tmp/$SERVICE_NAME.service <<EOL
[Unit]
Description=$SERVICE_NAME
After=network.target

[Service]
Type=simple
User=$(whoami)
WorkingDirectory=$(pwd)
ExecStart=$(pwd)/bin/app
Restart=on-failure

[Install]
WantedBy=multi-user.target
EOL
        sudo mv /tmp/$SERVICE_NAME.service /etc/systemd/system/
        sudo systemctl daemon-reload
        sudo systemctl enable $SERVICE_NAME
        sudo systemctl start $SERVICE_NAME
    else
        # Start directly if no service exists and we can't create one
        nohup bin/app > logs/app.log 2>&1 &
        echo $! > app.pid
        echo "Application started with PID $(cat app.pid)"
    fi
fi

echo "Deployment complete!"
""")
            
            # Make the script executable
            deploy_script.chmod(0o755)
            
            return {
                "success": True,
                "files_created": [str(deploy_script)],
                "message": "Created deployment script"
            }
        
        elif project_type == "java":
            # For Java, create a deployment script
            scripts_dir = project_path / "scripts"
            deploy_script = scripts_dir / "deploy.sh"
            
            # Create scripts directory if it doesn't exist
            os.makedirs(scripts_dir, exist_ok=True)
            
            # Write deployment script
            with open(deploy_script, "w") as f:
                if (project_path / "pom.xml").exists():
                    # Maven project
                    f.write("""#!/bin/bash
set -e

# Deployment script for Java Maven project
echo "Deploying Java Maven application..."

# Build the application
mvn clean package

# Get the JAR file
JAR_FILE=$(find target -name "*.jar" | head -1)
if [ -z "$JAR_FILE" ]; then
    echo "No JAR file found in target directory."
    exit 1
fi

# Check if running as a service
SERVICE_NAME=$(basename $(pwd))
if systemctl list-units --type=service | grep -q "$SERVICE_NAME"; then
    echo "Restarting service $SERVICE_NAME..."
    sudo systemctl restart $SERVICE_NAME
else
    echo "Starting application..."
    # Check if there's a running instance
    if [ -f "app.pid" ]; then
        OLD_PID=$(cat app.pid)
        if ps -p $OLD_PID > /dev/null; then
            echo "Stopping previous instance (PID: $OLD_PID)..."
            kill $OLD_PID
            sleep 2
        fi
    fi
    
    # Start the application
    mkdir -p logs
    nohup java -jar $JAR_FILE > logs/app.log 2>&1 &
    PID=$!
    echo $PID > app.pid
    echo "Application started with PID $PID"
fi

echo "Deployment complete!"
""")
                else:
                    # Gradle project
                    f.write("""#!/bin/bash
set -e

# Deployment script for Java Gradle project
echo "Deploying Java Gradle application..."

# Build the application
./gradlew build

# Get the JAR file
JAR_FILE=$(find build/libs -name "*.jar" | head -1)
if [ -z "$JAR_FILE" ]; then
    echo "No JAR file found in build/libs directory."
    exit 1
fi

# Check if running as a service
SERVICE_NAME=$(basename $(pwd))
if systemctl list-units --type=service | grep -q "$SERVICE_NAME"; then
    echo "Restarting service $SERVICE_NAME..."
    sudo systemctl restart $SERVICE_NAME
else
    echo "Starting application..."
    # Check if there's a running instance
    if [ -f "app.pid" ]; then
        OLD_PID=$(cat app.pid)
        if ps -p $OLD_PID > /dev/null; then
            echo "Stopping previous instance (PID: $OLD_PID)..."
            kill $OLD_PID
            sleep 2
        fi
    fi
    
    # Start the application
    mkdir -p logs
    nohup java -jar $JAR_FILE > logs/app.log 2>&1 &
    PID=$!
    echo $PID > app.pid
    echo "Application started with PID $PID"
fi

echo "Deployment complete!"
""")
            
            # Make the script executable
            deploy_script.chmod(0o755)
            
            return {
                "success": True,
                "files_created": [str(deploy_script)],
                "message": "Created deployment script"
            }
        
        elif project_type == "rust":
            # For Rust, create a deployment script
            scripts_dir = project_path / "scripts"
            deploy_script = scripts_dir / "deploy.sh"
            
            # Create scripts directory if it doesn't exist
            os.makedirs(scripts_dir, exist_ok=True)
            
            # Write deployment script
            with open(deploy_script, "w") as f:
                f.write("""#!/bin/bash
set -e

# Deployment script for Rust project
echo "Deploying Rust application..."

# Build the application in release mode
cargo build --release

# Get the binary name from Cargo.toml
BINARY_NAME=$(grep -m 1 "name" Cargo.toml | cut -d '"' -f 2 | tr -d '[:space:]')
if [ -z "$BINARY_NAME" ]; then
    BINARY_NAME=$(basename $(pwd))
fi

# Check if systemd service exists
SERVICE_NAME=$BINARY_NAME
if systemctl list-units --type=service | grep -q "$SERVICE_NAME"; then
    echo "Restarting service $SERVICE_NAME..."
    sudo systemctl restart $SERVICE_NAME
else
    echo "Starting application..."
    # Create a systemd service file if needed
    if [ ! -f "/etc/systemd/system/$SERVICE_NAME.service" ]; then
        echo "Creating systemd service..."
        cat > /tmp/$SERVICE_NAME.service <<EOL
[Unit]
Description=$SERVICE_NAME
After=network.target

[Service]
Type=simple
User=$(whoami)
WorkingDirectory=$(pwd)
ExecStart=$(pwd)/target/release/$BINARY_NAME
Restart=on-failure

[Install]
WantedBy=multi-user.target
EOL
        sudo mv /tmp/$SERVICE_NAME.service /etc/systemd/system/
        sudo systemctl daemon-reload
        sudo systemctl enable $SERVICE_NAME
        sudo systemctl start $SERVICE_NAME
    else
        # Start directly if no service exists and we can't create one
        mkdir -p logs
        nohup target/release/$BINARY_NAME > logs/app.log 2>&1 &
        echo $! > app.pid
        echo "Application started with PID $(cat app.pid)"
    fi
fi

echo "Deployment complete!"
""")
            
            # Make the script executable
            deploy_script.chmod(0o755)
            
            return {
                "success": True,
                "files_created": [str(deploy_script)],
                "message": "Created deployment script"
            }
            
        elif project_type == "php":
            # For PHP, create a deployment script
            scripts_dir = project_path / "scripts"
            deploy_script = scripts_dir / "deploy.sh"
            
            # Create scripts directory if it doesn't exist
            os.makedirs(scripts_dir, exist_ok=True)
            
            # Write deployment script
            with open(deploy_script, "w") as f:
                f.write("""#!/bin/bash
set -e

# Deployment script for PHP project
echo "Deploying PHP application..."

# Install dependencies
composer install --no-dev --optimize-autoloader

# Check for Laravel
if [ -f "artisan" ]; then
    echo "Laravel project detected"
    php artisan migrate --force
    php artisan config:cache
    php artisan route:cache
    php artisan view:cache
fi

# Check for Symfony
if [ -f "bin/console" ]; then
    echo "Symfony project detected"
    php bin/console cache:clear --env=prod
    php bin/console doctrine:migrations:migrate --no-interaction
fi

# Reload PHP-FPM if available
if command -v systemctl &> /dev/null && systemctl list-units --type=service | grep -q "php.*-fpm"; then
    echo "Reloading PHP-FPM..."
    sudo systemctl reload php*-fpm.service
fi

echo "Deployment complete!"
""")
            
            # Make the script executable
            deploy_script.chmod(0o755)
            
            return {
                "success": True,
                "files_created": [str(deploy_script)],
                "message": "Created deployment script"
            }
            
        # Add more project types as needed
        
        return {
            "success": False,
            "message": f"No deployment configuration available for {project_type}"
        }
    
    async def _setup_testing_config(
        self,
        project_path: Path,
        project_type: str,
        pipeline_steps: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Set up testing configuration files.
        
        Args:
            project_path: Path to the project
            project_type: Type of project
            pipeline_steps: Pipeline steps configuration
            
        Returns:
            Dictionary with testing configuration result
        """
        self._logger.info(f"Setting up testing configuration for {project_type}")
        
        created_files = []
        
        if project_type == "python":
            # Check if pytest.ini exists, create if not
            pytest_ini = project_path / "pytest.ini"
            if not pytest_ini.exists():
                with open(pytest_ini, "w") as f:
                    f.write("""[pytest]
testpaths = tests
python_files = test_*.py
python_classes = Test*
python_functions = test_*
addopts = --verbose --cov=./ --cov-report=term-missing
""")
                created_files.append(str(pytest_ini))
            
            # Create a basic test directory and example test if not exists
            tests_dir = project_path / "tests"
            if not tests_dir.exists():
                os.makedirs(tests_dir, exist_ok=True)
                
                # Create __init__.py
                with open(tests_dir / "__init__.py", "w") as f:
                    f.write("# Test package initialization")
                created_files.append(str(tests_dir / "__init__.py"))
                
                # Create an example test
                with open(tests_dir / "test_example.py", "w") as f:
                    f.write("""import unittest

class TestExample(unittest.TestCase):
    def test_simple_assertion(self):
        self.assertEqual(1 + 1, 2)
        
    def test_truth_value(self):
        self.assertTrue(True)
""")
                created_files.append(str(tests_dir / "test_example.py"))
            
            return {
                "success": True,
                "files_created": created_files,
                "message": "Created testing configuration",
                "framework": "pytest"
            }
        
        elif project_type == "node":
            # Check if jest configuration exists in package.json
            package_json_path = project_path / "package.json"
            if package_json_path.exists():
                try:
                    import json
                    with open(package_json_path, "r") as f:
                        package_data = json.load(f)
                    
                    # Check if jest is configured
                    has_jest = False
                    if "jest" not in package_data and "scripts" in package_data:
                        # If not in scripts.test, add jest configuration
                        if "test" not in package_data["scripts"] or "jest" not in package_data["scripts"]["test"]:
                            package_data["scripts"]["test"] = "jest"
                            has_jest = True
                            
                            # Save the updated package.json
                            with open(package_json_path, "w") as f:
                                json.dump(package_data, f, indent=2)
                            
                    # Create jest.config.js if needed
                    jest_config = project_path / "jest.config.js"
                    if not jest_config.exists() and has_jest:
                        with open(jest_config, "w") as f:
                            f.write("""module.exports = {
  testEnvironment: 'node',
  coverageDirectory: 'coverage',
  collectCoverageFrom: [
    'src/**/*.js',
    '!src/index.js',
    '!**/node_modules/**',
  ],
  testMatch: ['**/__tests__/**/*.js', '**/?(*.)+(spec|test).js'],
};
""")
                        created_files.append(str(jest_config))
                    
                    # Create tests directory if needed
                    tests_dir = project_path / "__tests__"
                    if not tests_dir.exists() and has_jest:
                        os.makedirs(tests_dir, exist_ok=True)
                        
                        # Create an example test
                        with open(tests_dir / "example.test.js", "w") as f:
                            f.write("""describe('Example Test Suite', () => {
  test('adds 1 + 2 to equal 3', () => {
    expect(1 + 2).toBe(3);
  });
  
  test('true is truthy', () => {
    expect(true).toBeTruthy();
  });
});
""")
                        created_files.append(str(tests_dir / "example.test.js"))
                    
                    return {
                        "success": True,
                        "files_created": created_files,
                        "message": "Created testing configuration",
                        "framework": "jest"
                    }
                    
                except (json.JSONDecodeError, IOError) as e:
                    self._logger.error(f"Error reading or updating package.json: {str(e)}")
                    return {
                        "success": False,
                        "error": f"Failed to update package.json: {str(e)}"
                    }
                    
        elif project_type == "java":
            if (project_path / "pom.xml").exists():
                # Maven project, check for surefire plugin
                pom_path = project_path / "pom.xml"
                try:
                    with open(pom_path, "r") as f:
                        pom_content = f.read()
                        
                    if "maven-surefire-plugin" not in pom_content:
                        # Create a sample test if tests directory doesn't exist
                        test_dir = project_path / "src" / "test" / "java"
                        if not test_dir.exists():
                            os.makedirs(test_dir, exist_ok=True)
                            
                            # Create a simple test class
                            with open(test_dir / "ExampleTest.java", "w") as f:
                                f.write("""import org.junit.jupiter.api.Test;
import static org.junit.jupiter.api.Assertions.*;

class ExampleTest {
    @Test
    void simpleAssertion() {
        assertEquals(2, 1 + 1);
    }
    
    @Test
    void truthValue() {
        assertTrue(true);
    }
}
""")
                            created_files.append(str(test_dir / "ExampleTest.java"))
                            
                    return {
                        "success": True,
                        "files_created": created_files,
                        "message": "Created testing configuration",
                        "framework": "junit"
                    }
                except IOError as e:
                    self._logger.error(f"Error reading pom.xml: {str(e)}")
                    return {
                        "success": False,
                        "error": f"Failed to read pom.xml: {str(e)}"
                    }
            else:
                # Gradle project, check for test directory
                test_dir = project_path / "src" / "test" / "java"
                if not test_dir.exists():
                    os.makedirs(test_dir, exist_ok=True)
                    
                    # Create a simple test class
                    with open(test_dir / "ExampleTest.java", "w") as f:
                        f.write("""import org.junit.jupiter.api.Test;
import static org.junit.jupiter.api.Assertions.*;

class ExampleTest {
    @Test
    void simpleAssertion() {
        assertEquals(2, 1 + 1);
    }
    
    @Test
    void truthValue() {
        assertTrue(true);
    }
}
""")
                    created_files.append(str(test_dir / "ExampleTest.java"))
                    
                return {
                    "success": True,
                    "files_created": created_files,
                    "message": "Created testing configuration",
                    "framework": "junit"
                }
                
        elif project_type == "go":
            # Go tests are typically in the same package as the code
            # Create a simple test file if none exists
            main_go = None
            for file in project_path.glob("*.go"):
                if file.name == "main.go":
                    main_go = file
                    break
                    
            if main_go:
                test_file = project_path / (main_go.stem + "_test.go")
                if not test_file.exists():
                    with open(test_file, "w") as f:
                        f.write("""package main

import (
	"testing"
)

func TestExample(t *testing.T) {
	if 1+1 != 2 {
		t.Error("1+1 should equal 2")
	}
}

func TestTruthValue(t *testing.T) {
	if !true {
		t.Error("true should be true")
	}
}
""")
                    created_files.append(str(test_file))
                    
                return {
                    "success": True,
                    "files_created": created_files,
                    "message": "Created testing configuration",
                    "framework": "go-test"
                }
                
        elif project_type == "rust":
            # Check if tests directory exists in the src directory
            src_dir = project_path / "src"
            if src_dir.exists():
                # Create a tests directory if it doesn't exist
                tests_dir = src_dir / "tests"
                if not tests_dir.exists():
                    os.makedirs(tests_dir, exist_ok=True)
                    
                    # Create a simple test file
                    with open(tests_dir / "example_test.rs", "w") as f:
                        f.write("""#[cfg(test)]
mod tests {
    #[test]
    fn test_simple_assertion() {
        assert_eq!(2, 1 + 1);
    }
    
    #[test]
    fn test_truth_value() {
        assert!(true);
    }
}
""")
                    created_files.append(str(tests_dir / "example_test.rs"))
                    
                # Add test module to lib.rs if it exists
                lib_rs = src_dir / "lib.rs"
                if lib_rs.exists():
                    with open(lib_rs, "r") as f:
                        content = f.read()
                        
                    if "#[cfg(test)]" not in content and "mod tests" not in content:
                        with open(lib_rs, "a") as f:
                            f.write("""
#[cfg(test)]
mod tests {
    #[test]
    fn it_works() {
        assert_eq!(2, 1 + 1);
    }
}
""")
                
                return {
                    "success": True,
                    "files_created": created_files,
                    "message": "Created testing configuration",
                    "framework": "cargo-test"
                }
        
        # Add more project types as needed
        
        return {
            "success": False,
            "message": f"No testing configuration available for {project_type}"
        }
    
    def _merge_configs(self, base_config: Dict[str, Any], custom_config: Dict[str, Any]) -> Dict[str, Any]:
        """
        Merge base configuration with custom configuration recursively.
        
        Args:
            base_config: Base configuration
            custom_config: Custom configuration to merge in
            
        Returns:
            Merged configuration
        """
        return deep_update(base_config, custom_config)
        
    async def setup_ci_cd_pipeline(
        self,
        request: str,
        project_dir: Union[str, Path],
        repository_url: Optional[str] = None,
        platform: Optional[str] = None,
        deployment_targets: Optional[List[str]] = None,
        custom_config: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Set up a complete CI/CD pipeline based on a natural language request.
        
        Args:
            request: Natural language request
            project_dir: Project directory
            repository_url: Optional repository URL
            platform: Optional CI/CD platform override
            deployment_targets: Optional deployment targets override
            custom_config: Optional custom configuration
            
        Returns:
            Dictionary with the setup result
        """
        self._logger.info(f"Setting up CI/CD pipeline from request: {request}")
        
        # Analyze request to extract key information if not explicitly provided
        parsed_request = await self._parse_ci_cd_request(request)
        
        # Use provided values or fall back to parsed values
        repository_url = repository_url or parsed_request.get("repository_url")
        platform = platform or parsed_request.get("platform")
        deployment_targets = deployment_targets or parsed_request.get("deployment_targets")
        
        # If repository URL is not provided, try to infer from git config
        if not repository_url:
            try:
                git_output = await self._run_git_command(["remote", "get-url", "origin"], cwd=project_dir)
                if git_output:
                    repository_url = git_output.strip()
            except Exception as e:
                self._logger.warning(f"Could not determine repository URL from git: {str(e)}")
        
        # Determine repository provider
        repository_provider = "unknown"
        if repository_url:
            repository_provider = self.get_repository_provider_from_url(repository_url)
        
        # If platform is not specified, try to determine from repository provider
        if not platform:
            if repository_provider == "github":
                platform = "github_actions"
            elif repository_provider == "gitlab":
                platform = "gitlab_ci"
            elif repository_provider == "bitbucket":
                platform = "bitbucket_pipelines"
            elif repository_provider == "azure_devops":
                platform = "azure_pipelines"
            else:
                # Default to GitHub Actions
                platform = "github_actions"
        
        # Create repository info dictionary
        repository_info = {
            "url": repository_url,
            "provider": repository_provider
        }
        
        # Create the complete pipeline
        result = await self._create_complete_pipeline(
            project_dir=project_dir,
            repository_info=repository_info,
            platform=platform,
            deployment_targets=deployment_targets,
            custom_config=custom_config
        )
        
        # Add parsed request information to the result
        result["parsed_request"] = parsed_request
        
        return result
    
    async def _parse_ci_cd_request(self, request: str) -> Dict[str, Any]:
        """
        Parse a natural language CI/CD setup request to extract key information.
        
        Args:
            request: Natural language request
            
        Returns:
            Dictionary with extracted information
        """
        self._logger.info(f"Parsing CI/CD request: {request}")
        
        # Use AI to parse the request
        prompt = f"""
    Extract key information from this CI/CD setup request:
    "{request}"
    
    Return a JSON object with these fields:
    1. platform: The CI/CD platform name (github_actions, gitlab_ci, jenkins, etc.)
    2. repository_url: Repository URL if mentioned
    3. deployment_targets: List of deployment environments to set up
    4. testing_requirements: Any specific testing requirements
    5. build_requirements: Any specific build requirements
    6. security_requirements: Any security scanning requirements
    """
        
        try:
            # Import AI client from API layer
            from angela.api.ai import get_gemini_client, get_gemini_request_class
            
            # Get client and request class
            gemini_client = get_gemini_client()
            GeminiRequest = get_gemini_request_class()
            
            # Call AI service
            api_request = GeminiRequest(prompt=prompt, max_tokens=1000)
            response = await gemini_client.generate_text(api_request)
            
            # Parse the response
            import json
            import re
            
            # Try to find JSON in the response
            json_match = re.search(r'```(?:json)?\s*(.*?)\s*```', response.text, re.DOTALL)
            if json_match:
                json_str = json_match.group(1)
            else:
                # Assume the entire response is JSON
                json_str = response.text
            
            # Parse JSON
            parsed_info = json.loads(json_str)
            
            # Ensure expected keys exist
            expected_keys = ["platform", "repository_url", "deployment_targets", 
                            "testing_requirements", "build_requirements", "security_requirements"]
            for key in expected_keys:
                if key not in parsed_info:
                    parsed_info[key] = None
            
            return parsed_info
            
        except Exception as e:
            self._logger.error(f"Error parsing CI/CD request: {str(e)}")
            # Return minimal information on error
            return {
                "platform": None,
                "repository_url": None,
                "deployment_targets": None,
                "testing_requirements": None,
                "build_requirements": None,
                "security_requirements": None
            }
    
    async def _run_git_command(self, args: List[str], cwd: Union[str, Path] = ".") -> str:
        """
        Run a git command and return the output.
        
        Args:
            args: Git command arguments
            cwd: Working directory
            
        Returns:
            Command output as a string
        """
        try:
            # Get execution engine from API
            execution_engine = get_execution_engine()
            command = ["git"] + args
            command_str = " ".join(command)
            
            stdout, stderr, return_code = await execution_engine.execute_command(
                command=command_str,
                check_safety=True,
                working_dir=str(cwd)
            )
            
            if return_code != 0:
                raise RuntimeError(f"Git command failed: {stderr}")
            
            return stdout
        except Exception as e:
            self._logger.error(f"Error running git command: {str(e)}")
            raise
            
    async def _create_complete_pipeline(
        self, 
        project_dir: Union[str, Path], 
        repository_info: Dict[str, Any],
        platform: str,
        deployment_targets: Optional[List[str]] = None,
        custom_config: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Create a complete CI/CD pipeline for a project.
        
        Args:
            project_dir: Project directory
            repository_info: Repository information (URL, provider, etc.)
            platform: CI/CD platform (github_actions, gitlab_ci, etc.)
            deployment_targets: Optional list of deployment targets
            custom_config: Optional custom configuration
            
        Returns:
            Dictionary with the pipeline creation result
        """
        self._logger.info(f"Creating complete CI/CD pipeline on {platform}")
        
        # Detect project type if not provided
        detection_result = await self.detect_project_type(project_dir)
        project_type = detection_result.get("project_type")
        
        if not project_type:
            return {
                "success": False,
                "error": f"Could not detect project type: {detection_result.get('error', 'Unknown error')}",
                "platform": platform
            }
        
        # Determine pipeline steps
        pipeline_steps = await self._determine_pipeline_steps(
            project_type=project_type,
            platform=platform,
            pipeline_type="full"
        )
        
        # Set up testing configuration
        testing_config = await self._setup_testing_config(
            project_path=project_dir,
            project_type=project_type,
            pipeline_steps=pipeline_steps
        )
        
        # Set up deployment configuration if targets are specified
        deployment_config = None
        if deployment_targets:
            deployment_config = await self._setup_deployment_config(
                project_path=project_dir,
                platform=platform,
                project_type=project_type,
                pipeline_steps=pipeline_steps
            )
        
        # Generate the final pipeline configuration
        config = {
            "pipeline_steps": pipeline_steps
        }
        
        if testing_config and testing_config.get("success", False):
            config["testing_config"] = testing_config
        
        if deployment_config and deployment_config.get("success", False):
            config["deployment_config"] = deployment_config
        
        if custom_config:
            config = deep_update(config, custom_config)
        
        # Generate the actual CI/CD configuration file
        result = await self.generate_ci_configuration(
            path=project_dir,
            platform=platform,
            project_type=project_type,
            custom_settings=config
        )
        
        # Add pipeline metadata to the result
        result["pipeline_info"] = {
            "project_type": project_type,
            "platform": platform,
            "repository": repository_info,
            "testing": testing_config.get("framework") if testing_config and testing_config.get("success", False) else None,
            "deployment": deployment_targets if deployment_targets else []
        }
        
        return result


    async def _generate_gitlab_ci(
        self, 
        path: Path,
        project_type: str,
        custom_settings: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Generate GitLab CI configuration.
        
        Args:
            path: Path to the project
            project_type: Project type
            custom_settings: Optional custom settings
            
        Returns:
            Dictionary with the generation result
        """
        self._logger.info(f"Generating GitLab CI configuration for {project_type}")
        
        config: Dict[str, Any] = {} # Ensure config is initialized
        # Set default settings based on project type
        if project_type == "python":
            config = {
                "image": "python:3.9",
                "stages": ["test", "build", "deploy"],
                "before_script": [
                    "python -V",
                    "pip install -r requirements.txt"
                ],
                "test": {
                    "stage": "test",
                    "script": [
                        "pip install pytest pytest-cov",
                        "pytest --cov=. --cov-report=xml",
                    ],
                    "artifacts": {
                        "reports": {
                            "coverage_report": {
                                "coverage_format": "cobertura",
                                "path": "coverage.xml"
                            }
                        }
                    }
                },
                "lint": {
                    "stage": "test",
                    "script": [
                        "pip install flake8",
                        "flake8 ."
                    ]
                },
                "build": {
                    "stage": "build",
                    "script": [
                        "echo 'Building package'",
                        "pip install build",
                        "python -m build"
                    ],
                    "artifacts": {
                        "paths": ["dist/"]
                    }
                }
            }
        elif project_type == "node":
            config = {
                "image": "node:16",
                "stages": ["test", "build", "deploy"],
                "cache": {
                    "paths": ["node_modules/"]
                },
                "install_dependencies": {
                    "stage": "test",
                    "script": ["npm ci"]
                },
                "test": {
                    "stage": "test",
                    "script": ["npm test"]
                },
                "lint": {
                    "stage": "test",
                    "script": ["npm run lint"]
                },
                "build": {
                    "stage": "build",
                    "script": ["npm run build"],
                    "artifacts": {
                        "paths": ["dist/", "build/"]
                    }
                }
            }
        elif project_type == "go":
            config = {
                "image": "golang:1.18",
                "stages": ["test", "build", "deploy"],
                "before_script": [
                    "go version",
                    "go mod download"
                ],
                "test": {
                    "stage": "test",
                    "script": [
                        "go test -v ./..."
                    ]
                },
                "lint": {
                    "stage": "test",
                    "image": "golangci/golangci-lint:latest",
                    "script": [
                        "golangci-lint run"
                    ]
                },
                "build": {
                    "stage": "build",
                    "script": [
                        "go build -o bin/app"
                    ],
                    "artifacts": {
                        "paths": ["bin/"]
                    }
                }
            }
        elif project_type == "rust":
            config = {
                "image": "rust:latest",
                "stages": ["test", "build", "deploy"],
                "cache": {
                    "paths": ["target/"]
                },
                "test": {
                    "stage": "test",
                    "script": [
                        "cargo test"
                    ]
                },
                "lint": {
                    "stage": "test",
                    "script": [
                        "rustup component add clippy",
                        "cargo clippy -- -D warnings"
                    ]
                },
                "build": {
                    "stage": "build",
                    "script": [
                        "cargo build --release"
                    ],
                    "artifacts": {
                        "paths": ["target/release/"]
                    }
                }
            }
        elif project_type == "java":
            config = {
                "image": "maven:latest",
                "stages": ["test", "build", "deploy"],
                "cache": {
                    "paths": [".m2/repository"]
                },
                "test": {
                    "stage": "test",
                    "script": [
                        "mvn test"
                    ]
                },
                "build": {
                    "stage": "build",
                    "script": [
                        "mvn package"
                    ],
                    "artifacts": {
                        "paths": ["target/*.jar"]
                    }
                }
            }
            # Check if it's Gradle
            if (path / "build.gradle").exists() or (path / "build.gradle.kts").exists():
                config["image"] = "gradle:latest"
                config["cache"]["paths"] = [".gradle"]
                config["test"]["script"] = ["gradle test"]
                config["build"]["script"] = ["gradle build"]
                config["build"]["artifacts"]["paths"] = ["build/libs/*.jar"]
        elif project_type == "ruby":
            config = {
                "image": "ruby:latest",
                "stages": ["test", "build", "deploy"],
                "before_script": [
                    "ruby -v",
                    "bundle install"
                ],
                "test": {
                    "stage": "test",
                    "script": [
                        "bundle exec rake test"
                    ]
                },
                "lint": {
                    "stage": "test",
                    "script": [
                        "bundle exec rubocop"
                    ]
                },
                "build": {
                    "stage": "build",
                    "script": [
                        "bundle exec rake build"
                    ],
                    "artifacts": {
                        "paths": ["pkg/*.gem"]
                    }
                }
            }
        elif project_type == "php":
            config = {
                "image": "php:8.0",
                "stages": ["test", "build", "deploy"],
                "before_script": [
                    "php -v",
                    "composer install"
                ],
                "test": {
                    "stage": "test",
                    "script": [
                        "vendor/bin/phpunit"
                    ]
                },
                "lint": {
                    "stage": "test",
                    "script": [
                        "vendor/bin/phpcs"
                    ]
                },
                "build": {
                    "stage": "build",
                    "script": [
                        "echo 'Building PHP application'"
                    ]
                }
            }
        elif project_type == "dotnet":
            config = {
                "image": "mcr.microsoft.com/dotnet/sdk:6.0",
                "stages": ["test", "build", "deploy"],
                "before_script": [
                    "dotnet restore"
                ],
                "test": {
                    "stage": "test",
                    "script": [
                        "dotnet test"
                    ]
                },
                "build": {
                    "stage": "build",
                    "script": [
                        "dotnet build --no-restore",
                        "dotnet publish -c Release -o publish"
                    ],
                    "artifacts": {
                        "paths": ["publish/"]
                    }
                }
            }
        elif project_type == "cpp":
            config = {
                "image": "gcc:latest",
                "stages": ["test", "build", "deploy"],
                "before_script": [
                    "apt-get update && apt-get install -y cmake"
                ],
                "test": {
                    "stage": "test",
                    "script": [
                        "cmake -B build -DCMAKE_BUILD_TYPE=Debug",
                        "cmake --build build",
                        "cd build && ctest -V"
                    ]
                },
                "build": {
                    "stage": "build",
                    "script": [
                        "cmake -B build -DCMAKE_BUILD_TYPE=Release",
                        "cmake --build build"
                    ],
                    "artifacts": {
                        "paths": ["build/"]
                    }
                }
            }
        else: # Default empty config if project_type is not recognized
            config = {
                "image": "alpine", 
                "stages": ["build", "test"], 
                "build": {
                    "stage": "build",
                    "script": ["echo 'No build defined'"]
                },
                "test": {
                    "stage": "test",
                    "script": ["echo 'No tests defined'"]
                }
            }

        # Update with custom settings using deep merge
        if custom_settings:
            config = deep_update(config, custom_settings)
        
        # Write the config file
        config_file = path / ".gitlab-ci.yml"
        try:
            with open(config_file, 'w') as f:
                yaml.dump(config, f, default_flow_style=False, sort_keys=False)
            
            return {
                "success": True,
                "platform": "gitlab_ci",
                "project_type": project_type,
                "config_file": str(config_file)
            }
        except Exception as e:
            return {
                "success": False,
                "error": f"Failed to write GitLab CI config: {str(e)}",
                "platform": "gitlab_ci",
                "project_type": project_type
            }
    
    async def _generate_jenkins(
        self, 
        path: Path,
        project_type: str,
        custom_settings: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Generate Jenkins configuration (Jenkinsfile).
        
        Args:
            path: Path to the project
            project_type: Project type
            custom_settings: Optional custom settings
            
        Returns:
            Dictionary with the generation result
        """
        self._logger.info(f"Generating Jenkins configuration for {project_type}")
        
        # Set Jenkinsfile content based on project type
        content = ""
        
        if project_type == "python":
            content = """
pipeline {
    agent {
        docker {
            image 'python:3.9'
        }
    }
    stages {
        stage('Build') {
            steps {
                sh 'python -m pip install --upgrade pip'
                sh 'pip install -r requirements.txt'
            }
        }
        stage('Test') {
            steps {
                sh 'pip install pytest pytest-cov'
                sh 'pytest --cov=. --cov-report=xml'
            }
            post {
                always {
                    junit 'pytest-results.xml'
                    cobertura coberturaReportFile: 'coverage.xml'
                }
            }
        }
        stage('Lint') {
            steps {
                sh 'pip install flake8'
                sh 'flake8 .'
            }
        }
        stage('Deploy') {
            when {
                branch 'main'
            }
            steps {
                echo 'Deploying to production...'
                // Add deployment steps here
            }
        }
    }
}
"""
        elif project_type == "node":
            content = """
pipeline {
    agent {
        docker {
            image 'node:16'
        }
    }
    stages {
        stage('Build') {
            steps {
                sh 'npm ci'
                sh 'npm run build --if-present'
            }
        }
        stage('Test') {
            steps {
                sh 'npm test'
            }
        }
        stage('Lint') {
            steps {
                sh 'npm run lint --if-present'
            }
        }
        stage('Deploy') {
            when {
                branch 'main'
            }
            steps {
                echo 'Deploying to production...'
                // Add deployment steps here
            }
        }
    }
}
"""
        elif project_type == "go":
            content = """
pipeline {
    agent {
        docker {
            image 'golang:1.18'
        }
    }
    stages {
        stage('Build') {
            steps {
                sh 'go mod download'
                sh 'go build -o bin/app'
            }
        }
        stage('Test') {
            steps {
                sh 'go test -v ./...'
            }
        }
        stage('Deploy') {
            when {
                branch 'main'
            }
            steps {
                echo 'Deploying to production...'
                // Add deployment steps here
            }
        }
    }
}
"""
        elif project_type == "rust":
            content = """
pipeline {
    agent {
        docker {
            image 'rust:latest'
        }
    }
    stages {
        stage('Build') {
            steps {
                sh 'cargo build --release'
            }
        }
        stage('Test') {
            steps {
                sh 'cargo test'
            }
        }
        stage('Lint') {
            steps {
                sh 'rustup component add clippy'
                sh 'cargo clippy -- -D warnings'
            }
        }
        stage('Deploy') {
            when {
                branch 'main'
            }
            steps {
                echo 'Deploying to production...'
                // Add deployment steps here
            }
        }
    }
}
"""
        elif project_type == "java":
            if (path / "pom.xml").exists():
                content = """
pipeline {
    agent {
        docker {
            image 'maven:latest'
        }
    }
    stages {
        stage('Build') {
            steps {
                sh 'mvn clean package'
            }
        }
        stage('Test') {
            steps {
                sh 'mvn test'
            }
            post {
                always {
                    junit '**/target/surefire-reports/*.xml'
                }
            }
        }
        stage('Deploy') {
            when {
                branch 'main'
            }
            steps {
                echo 'Deploying to production...'
                // Add deployment steps here
            }
        }
    }
}
"""
            else:
                content = """
pipeline {
    agent {
        docker {
            image 'gradle:latest'
        }
    }
    stages {
        stage('Build') {
            steps {
                sh 'gradle build'
            }
        }
        stage('Test') {
            steps {
                sh 'gradle test'
            }
            post {
                always {
                    junit '**/build/test-results/**/*.xml'
                }
            }
        }
        stage('Deploy') {
            when {
                branch 'main'
            }
            steps {
                echo 'Deploying to production...'
                // Add deployment steps here
            }
        }
    }
}
"""
        elif project_type == "ruby":
            content = """
pipeline {
    agent {
        docker {
            image 'ruby:latest'
        }
    }
    stages {
        stage('Build') {
            steps {
                sh 'bundle install'
            }
        }
        stage('Test') {
            steps {
                sh 'bundle exec rake test'
            }
        }
        stage('Lint') {
            steps {
                sh 'bundle exec rubocop'
            }
        }
        stage('Deploy') {
            when {
                branch 'main'
            }
            steps {
                echo 'Deploying to production...'
                // Add deployment steps here
            }
        }
    }
}
"""
        elif project_type == "php":
            content = """
pipeline {
    agent {
        docker {
            image 'php:8.0-cli'
        }
    }
    stages {
        stage('Build') {
            steps {
                sh 'apt-get update && apt-get install -y git unzip'
                sh 'php -r "copy(\\'https://getcomposer.org/installer\\', \\'composer-setup.php\\');"'
                sh 'php composer-setup.php --install-dir=/usr/local/bin --filename=composer'
                sh 'composer install'
            }
        }
        stage('Test') {
            steps {
                sh 'vendor/bin/phpunit'
            }
        }
        stage('Deploy') {
            when {
                branch 'main'
            }
            steps {
                echo 'Deploying to production...'
                // Add deployment steps here
            }
        }
    }
}
"""
        elif project_type == "dotnet":
            content = """
pipeline {
    agent {
        docker {
            image 'mcr.microsoft.com/dotnet/sdk:6.0'
        }
    }
    stages {
        stage('Build') {
            steps {
                sh 'dotnet restore'
                sh 'dotnet build'
            }
        }
        stage('Test') {
            steps {
                sh 'dotnet test --logger:"trx;LogFileName=testresults.trx"'
            }
            post {
                always {
                    mstest testResultsFile: 'testresults.trx'
                }
            }
        }
        stage('Deploy') {
            when {
                branch 'main'
            }
            steps {
                echo 'Deploying to production...'
                // Add deployment steps here
            }
        }
    }
}
"""
        elif project_type == "cpp":
            content = """
pipeline {
    agent {
        docker {
            image 'gcc:latest'
        }
    }
    stages {
        stage('Build') {
            steps {
                sh 'apt-get update && apt-get install -y cmake'
                sh 'cmake -B build -DCMAKE_BUILD_TYPE=Release'
                sh 'cmake --build build'
            }
        }
        stage('Test') {
            steps {
                sh 'cd build && ctest -V'
            }
        }
        stage('Deploy') {
            when {
                branch 'main'
            }
            steps {
                echo 'Deploying to production...'
                // Add deployment steps here
            }
        }
    }
}
"""
        else:
            content = """
pipeline {
    agent any
    
    stages {
        stage('Build') {
            steps {
                echo 'Building...'
                // Add build steps here
            }
        }
        stage('Test') {
            steps {
                echo 'Testing...'
                // Add test steps here
            }
        }
        stage('Deploy') {
            when {
                branch 'main'
            }
            steps {
                echo 'Deploying...'
                // Add deployment steps here
            }
        }
    }
}
"""

        # Update with custom settings for Jenkins
        if custom_settings:
            # For Jenkins, we need to do template-based modification since it's a raw string
            content = self._apply_jenkins_customizations(content, custom_settings)
        
        # Write the Jenkinsfile
        jenkinsfile_path = path / "Jenkinsfile"
        try:
            with open(jenkinsfile_path, 'w') as f:
                f.write(content.strip())
            
            return {
                "success": True,
                "platform": "jenkins",
                "project_type": project_type,
                "config_file": str(jenkinsfile_path)
            }
        except Exception as e:
            return {
                "success": False,
                "error": f"Failed to write Jenkinsfile: {str(e)}",
                "platform": "jenkins",
                "project_type": project_type
            }
    
    def _apply_jenkins_customizations(self, content: str, custom_settings: Dict[str, Any]) -> str:
        """
        Apply customizations to a Jenkinsfile content.
        
        Args:
            content: Original Jenkinsfile content
            custom_settings: Custom settings to apply
            
        Returns:
            Modified Jenkinsfile content
        """
        modified_content = content
        
        # Handle agent customization
        if "agent" in custom_settings:
            agent_value = custom_settings["agent"]
            if isinstance(agent_value, str):
                if agent_value in ["any", "none"]:
                    # Replace the agent block with simple value
                    modified_content = re.sub(r'agent\s+\{[^}]*\}', f'agent {agent_value}', modified_content)
                else:
                    # Custom agent string (like a label)
                    modified_content = re.sub(r'agent\s+\{[^}]*\}', f'agent {{ label "{agent_value}" }}', modified_content)
            elif isinstance(agent_value, dict):
                # Create custom agent block
                agent_type = list(agent_value.keys())[0]
                agent_config = agent_value[agent_type]
                if agent_type == "docker":
                    if isinstance(agent_config, str):
                        # Simple docker image
                        agent_block = f'agent {{\n        docker {{\n            image \'{agent_config}\'\n        }}\n    }}'
                    else:
                        # Detailed docker configuration
                        agent_block = f'agent {{\n        docker {{\n'
                        for k, v in agent_config.items():
                            if isinstance(v, str):
                                agent_block += f'            {k} \'{v}\'\n'
                            else:
                                agent_block += f'            {k} {v}\n'
                        agent_block += '        }\n    }'
                    modified_content = re.sub(r'agent\s+\{[^}]*\}', agent_block, modified_content)
        
        # Handle stages customization
        if "stages" in custom_settings:
            # For each custom stage, find the corresponding stage in the original content
            for stage_name, stage_config in custom_settings["stages"].items():
                # Look for the stage in the content
                stage_pattern = rf'stage\([\'"]({stage_name}|{stage_name.title()})[\'"])\s*\{{[^{{}}]*}}'
                stage_match = re.search(stage_pattern, modified_content)
                
                if stage_match:
                    # If stage exists, modify it
                    original_stage = stage_match.group(0)
                    
                    # Create modified stage
                    if "steps" in stage_config:
                        # Replace steps
                        steps_block = "            steps {\n"
                        for step in stage_config["steps"]:
                            if isinstance(step, str):
                                steps_block += f'                {step}\n'
                            elif isinstance(step, dict):
                                step_type = list(step.keys())[0]
                                step_value = step[step_type]
                                if isinstance(step_value, str):
                                    steps_block += f'                {step_type} \'{step_value}\'\n'
                                else:
                                    steps_block += f'                {step_type} {json.dumps(step_value)}\n'
                        steps_block += "            }"
                        
                        # Replace steps in original stage
                        modified_stage = re.sub(r'steps\s*\{[^{}]*\}', steps_block, original_stage)
                        modified_content = modified_content.replace(original_stage, modified_stage)
                else:
                    # If stage doesn't exist, add it
                    new_stage = f"""
        stage('{stage_name}') {{
            steps {{
"""
                    if "steps" in stage_config:
                        for step in stage_config["steps"]:
                            if isinstance(step, str):
                                new_stage += f'                {step}\n'
                            elif isinstance(step, dict):
                                step_type = list(step.keys())[0]
                                step_value = step[step_type]
                                if isinstance(step_value, str):
                                    new_stage += f'                {step_type} \'{step_value}\'\n'
                                else:
                                    new_stage += f'                {step_type} {json.dumps(step_value)}\n'
                    new_stage += """            }
        }"""
                    
                    # Find the last stage and add the new stage after it
                    last_stage_match = re.search(r'(stage\([^\)]+\)\s*\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\})\s*\n\s*\}', modified_content)
                    if last_stage_match:
                        last_stage = last_stage_match.group(1)
                        modified_content = modified_content.replace(last_stage, last_stage + new_stage)
        
        # Handle additional pipeline options
        if "options" in custom_settings:
            options_block = "    options {\n"
            for option_name, option_value in custom_settings["options"].items():
                if isinstance(option_value, bool):
                    if option_value:
                        options_block += f"        {option_name}()\n"
                elif isinstance(option_value, (int, float)):
                    options_block += f"        {option_name}({option_value})\n"
                elif isinstance(option_value, str):
                    options_block += f"        {option_name}('{option_value}')\n"
                else:
                    options_block += f"        {option_name}({json.dumps(option_value)})\n"
            options_block += "    }\n"
            
            # Add options block after agent block
            agent_end = re.search(r'agent\s+(?:\{[^}]*\}|any|none)\s*', modified_content)
            if agent_end:
                insert_point = agent_end.end()
                modified_content = modified_content[:insert_point] + "\n" + options_block + modified_content[insert_point:]
        
        return modified_content
    
    async def _generate_travis(
        self, 
        path: Path,
        project_type: str,
        custom_settings: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Generate Travis CI configuration.
        
        Args:
            path: Path to the project
            project_type: Project type
            custom_settings: Optional custom settings
            
        Returns:
            Dictionary with the generation result
        """
        self._logger.info(f"Generating Travis CI configuration for {project_type}")
        
        config: Dict[str, Any] = {} # Ensure config is initialized
        # Set default settings based on project type
        if project_type == "python":
            config = {
                "language": "python",
                "python": ["3.8", "3.9", "3.10"],
                "install": [
                    "pip install -r requirements.txt",
                    "pip install pytest pytest-cov flake8"
                ],
                "script": [
                    "flake8 .",
                    "pytest --cov=."
                ],
                "after_success": [
                    "bash <(curl -s https://codecov.io/bash)"
                ]
            }
        elif project_type == "node":
            config = {
                "language": "node_js",
                "node_js": ["14", "16", "18"],
                "cache": "npm",
                "install": [
                    "npm ci"
                ],
                "script": [
                    "npm run lint --if-present",
                    "npm run build --if-present",
                    "npm test"
                ]
            }
        elif project_type == "go":
            config = {
                "language": "go",
                "go": ["1.17.x", "1.18.x"],
                "install": [
                    "go mod download"
                ],
                "script": [
                    "go build -v ./...",
                    "go test -v -race ./..."
                ]
            }
        elif project_type == "rust":
            config = {
                "language": "rust",
                "rust": ["stable", "beta"],
                "cache": "cargo",
                "before_script": [
                    "rustup component add clippy"
                ],
                "script": [
                    "cargo build --verbose",
                    "cargo test --verbose",
                    "cargo clippy -- -D warnings"
                ]
            }
        elif project_type == "java":
            if (path / "pom.xml").exists():
                config = {
                    "language": "java",
                    "jdk": ["openjdk11", "openjdk17"],
                    "script": [
                        "mvn clean verify"
                    ],
                    "cache": {
                        "directories": ["$HOME/.m2"]
                    }
                }
            else:
                config = {
                    "language": "java",
                    "jdk": ["openjdk11", "openjdk17"],
                    "before_cache": [
                        "rm -f  $HOME/.gradle/caches/modules-2/modules-2.lock",
                        "rm -fr $HOME/.gradle/caches/*/plugin-resolution/"
                    ],
                    "cache": {
                        "directories": [
                            "$HOME/.gradle/caches/",
                            "$HOME/.gradle/wrapper/"
                        ]
                    },
                    "script": [
                        "./gradlew build"
                    ]
                }
        elif project_type == "ruby":
            config = {
                "language": "ruby",
                "rvm": ["2.7", "3.0", "3.1"],
                "install": [
                    "bundle install"
                ],
                "script": [
                    "bundle exec rake test"
                ]
            }
        elif project_type == "php":
            config = {
                "language": "php",
                "php": ["7.4", "8.0", "8.1"],
                "install": [
                    "composer install"
                ],
                "script": [
                    "vendor/bin/phpunit"
                ]
            }
        elif project_type == "dotnet":
            config = {
                "language": "csharp",
                "mono": "none",
                "dotnet": ["6.0", "7.0"],
                "script": [
                    "dotnet restore",
                    "dotnet build",
                    "dotnet test"
                ]
            }
        elif project_type == "cpp":
            config = {
                "language": "cpp",
                "compiler": ["gcc", "clang"],
                "before_script": [
                    "mkdir -p build",
                    "cd build",
                    "cmake .."
                ],
                "script": [
                    "cmake --build .",
                    "ctest -V"
                ]
            }
        else: # Default empty config if project_type is not recognized
            config = {
                "language": "generic", 
                "script": ["echo 'No script defined'"]
            }
        
        # Update with custom settings using deep merge
        if custom_settings:
            config = deep_update(config, custom_settings)
        
        # Write the config file
        config_file = path / ".travis.yml"
        try:
            with open(config_file, 'w') as f:
                yaml.dump(config, f, default_flow_style=False, sort_keys=False)
            
            return {
                "success": True,
                "platform": "travis",
                "project_type": project_type,
                "config_file": str(config_file)
            }
        except Exception as e:
            return {
                "success": False,
                "error": f"Failed to write Travis CI config: {str(e)}",
                "platform": "travis",
                "project_type": project_type
            }
    
    async def _generate_circle_ci(
        self, 
        path: Path,
        project_type: str,
        custom_settings: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Generate CircleCI configuration.
        
        Args:
            path: Path to the project
            project_type: Project type
            custom_settings: Optional custom settings
            
        Returns:
            Dictionary with the generation result
        """
        self._logger.info(f"Generating CircleCI configuration for {project_type}")
        
        # Create .circleci directory
        circleci_dir = path / ".circleci"
        if not circleci_dir.exists():
            os.makedirs(circleci_dir, exist_ok=True)
        
        config: Dict[str, Any] = {} # Ensure config is initialized
        # Set default settings based on project type
        if project_type == "python":
            config = {
                "version": 2.1,
                "orbs": {
                    "python": "circleci/python@1.5"
                },
                "jobs": {
                    "build-and-test": {
                        "docker": [
                            {"image": "cimg/python:3.9"}
                        ],
                        "steps": [
                            "checkout",
                            {
                                "python/install-packages": {
                                    "pkg-manager": "pip",
                                    "packages": [
                                        "pytest",
                                        "pytest-cov"
                                    ]
                                }
                            },
                            {
                                "run": {
                                    "name": "Install dependencies",
                                    "command": "pip install -r requirements.txt"
                                }
                            },
                            {
                                "run": {
                                    "name": "Run tests",
                                    "command": "pytest --cov=. --cov-report=xml"
                                }
                            },
                            {
                                "store_artifacts": {
                                    "path": "coverage.xml"
                                }
                            }
                        ]
                    }
                },
                "workflows": {
                    "main": {
                        "jobs": [
                            "build-and-test"
                        ]
                    }
                }
            }
        elif project_type == "node":
            config = {
                "version": 2.1,
                "orbs": {
                    "node": "circleci/node@5.0.0"
                },
                "jobs": {
                    "build-and-test": {
                        "docker": [
                            {"image": "cimg/node:16.14"}
                        ],
                        "steps": [
                            "checkout",
                            {
                                "node/install-packages": {
                                    "pkg-manager": "npm"
                                }
                            },
                            {
                                "run": {
                                    "name": "Run tests",
                                    "command": "npm test"
                                }
                            },
                            {
                                "run": {
                                    "name": "Run lint",
                                    "command": "npm run lint --if-present"
                                }
                            },
                            {
                                "run": {
                                    "name": "Build",
                                    "command": "npm run build --if-present"
                                }
                            }
                        ]
                    }
                },
                "workflows": {
                    "main": {
                        "jobs": [
                            "build-and-test"
                        ]
                    }
                }
            }
        elif project_type == "go":
            config = {
                "version": 2.1,
                "jobs": {
                    "build-and-test": {
                        "docker": [
                            {"image": "cimg/go:1.18"}
                        ],
                        "steps": [
                            "checkout",
                            {
                                "run": {
                                    "name": "Download dependencies",
                                    "command": "go mod download"
                                }
                            },
                            {
                                "run": {
                                    "name": "Build",
                                    "command": "go build -v ./..."
                                }
                            },
                            {
                                "run": {
                                    "name": "Run tests",
                                    "command": "go test -v -race ./..."
                                }
                            }
                        ]
                    }
                },
                "workflows": {
                    "main": {
                        "jobs": [
                            "build-and-test"
                        ]
                    }
                }
            }
        elif project_type == "rust":
            config = {
                "version": 2.1,
                "jobs": {
                    "build-and-test": {
                        "docker": [
                            {"image": "cimg/rust:1.60"}
                        ],
                        "steps": [
                            "checkout",
                            {
                                "run": {
                                    "name": "Version information",
                                    "command": "rustc --version; cargo --version; rustup --version"
                                }
                            },
                            {
                                "run": {
                                    "name": "Build",
                                    "command": "cargo build --verbose"
                                }
                            },
                            {
                                "run": {
                                    "name": "Run tests",
                                    "command": "cargo test --verbose"
                                }
                            },
                            {
                                "run": {
                                    "name": "Lint",
                                    "command": "rustup component add clippy && cargo clippy -- -D warnings"
                                }
                            }
                        ]
                    }
                },
                "workflows": {
                    "main": {
                        "jobs": [
                            "build-and-test"
                        ]
                    }
                }
            }
        elif project_type == "java":
            if (path / "pom.xml").exists():
                config = {
                    "version": 2.1,
                    "jobs": {
                        "build-and-test": {
                            "docker": [
                                {"image": "cimg/openjdk:17.0"}
                            ],
                            "steps": [
                                "checkout",
                                {
                                    "run": {
                                        "name": "Build",
                                        "command": "mvn -B -DskipTests clean package"
                                    }
                                },
                                {
                                    "run": {
                                        "name": "Test",
                                        "command": "mvn test"
                                    }
                                },
                                {
                                    "store_test_results": {
                                        "path": "target/surefire-reports"
                                    }
                                }
                            ]
                        }
                    },
                    "workflows": {
                        "main": {
                            "jobs": [
                                "build-and-test"
                            ]
                        }
                    }
                }
            else:
                config = {
                    "version": 2.1,
                    "jobs": {
                        "build-and-test": {
                            "docker": [
                                {"image": "cimg/openjdk:17.0"}
                            ],
                            "steps": [
                                "checkout",
                                {
                                    "run": {
                                        "name": "Build",
                                        "command": "./gradlew build -x test"
                                    }
                                },
                                {
                                    "run": {
                                        "name": "Test",
                                        "command": "./gradlew test"
                                    }
                                },
                                {
                                    "store_test_results": {
                                        "path": "build/test-results/test"
                                    }
                                }
                            ]
                        }
                    },
                    "workflows": {
                        "main": {
                            "jobs": [
                                "build-and-test"
                            ]
                        }
                    }
                }
        elif project_type == "ruby":
            config = {
                "version": 2.1,
                "orbs": {
                    "ruby": "circleci/ruby@1.4"
                },
                "jobs": {
                    "build-and-test": {
                        "docker": [
                            {"image": "cimg/ruby:3.1-node"}
                        ],
                        "steps": [
                            "checkout",
                            {
                                "ruby/install-deps": {}
                            },
                            {
                                "run": {
                                    "name": "Run tests",
                                    "command": "bundle exec rake test"
                                }
                            }
                        ]
                    }
                },
                "workflows": {
                    "main": {
                        "jobs": [
                            "build-and-test"
                        ]
                    }
                }
            }
        elif project_type == "php":
            config = {
                "version": 2.1,
                "jobs": {
                    "build-and-test": {
                        "docker": [
                            {"image": "cimg/php:8.1"}
                        ],
                        "steps": [
                            "checkout",
                            {
                                "run": {
                                    "name": "Install dependencies",
                                    "command": "composer install"
                                }
                            },
                            {
                                "run": {
                                    "name": "Run tests",
                                    "command": "vendor/bin/phpunit"
                                }
                            }
                        ]
                    }
                },
                "workflows": {
                    "main": {
                        "jobs": [
                            "build-and-test"
                        ]
                    }
                }
            }
        elif project_type == "dotnet":
            config = {
                "version": 2.1,
                "jobs": {
                    "build-and-test": {
                        "docker": [
                            {"image": "mcr.microsoft.com/dotnet/sdk:6.0"}
                        ],
                        "steps": [
                            "checkout",
                            {
                                "run": {
                                    "name": "Restore",
                                    "command": "dotnet restore"
                                }
                            },
                            {
                                "run": {
                                    "name": "Build",
                                    "command": "dotnet build --no-restore"
                                }
                            },
                            {
                                "run": {
                                    "name": "Test",
                                    "command": "dotnet test --no-build --verbosity normal"
                                }
                            }
                        ]
                    }
                },
                "workflows": {
                    "main": {
                        "jobs": [
                            "build-and-test"
                        ]
                    }
                }
            }
        elif project_type == "cpp":
            config = {
                "version": 2.1,
                "jobs": {
                    "build-and-test": {
                        "docker": [
                            {"image": "gcc:latest"}
                        ],
                        "steps": [
                            "checkout",
                            {
                                "run": {
                                    "name": "Install dependencies",
                                    "command": "apt-get update && apt-get install -y cmake"
                                }
                            },
                            {
                                "run": {
                                    "name": "Configure",
                                    "command": "cmake -B build -DCMAKE_BUILD_TYPE=Release"
                                }
                            },
                            {
                                "run": {
                                    "name": "Build",
                                    "command": "cmake --build build"
                                }
                            },
                            {
                                "run": {
                                    "name": "Test",
                                    "command": "cd build && ctest -V"
                                }
                            }
                        ]
                    }
                },
                "workflows": {
                    "main": {
                        "jobs": [
                            "build-and-test"
                        ]
                    }
                }
            }
        else: # Default empty config if project_type is not recognized
            config = {
                "version": 2.1, 
                "jobs": {
                    "build": {
                        "docker": [
                            {"image": "cimg/base:stable"}
                        ],
                        "steps": [
                            "checkout",
                            {
                                "run": {
                                    "name": "Build and test",
                                    "command": "echo 'Add your build commands here'"
                                }
                            }
                        ]
                    }
                }, 
                "workflows": {
                    "main": {
                        "jobs": [
                            "build"
                        ]
                    }
                }
            }
        
        # Update with custom settings using deep merge
        if custom_settings:
            config = deep_update(config, custom_settings)
        
        # Write the config file
        config_file = circleci_dir / "config.yml"
        try:
            with open(config_file, 'w') as f:
                yaml.dump(config, f, default_flow_style=False, sort_keys=False)
            
            return {
                "success": True,
                "platform": "circle_ci",
                "project_type": project_type,
                "config_file": str(config_file)
            }
        except Exception as e:
            return {
                "success": False,
                "error": f"Failed to write CircleCI config: {str(e)}",
                "platform": "circle_ci",
                "project_type": project_type
            }
    
    async def _generate_azure_pipelines(
        self, 
        path: Path,
        project_type: str,
        custom_settings: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Generate Azure Pipelines configuration.
        
        Args:
            path: Path to the project
            project_type: Project type
            custom_settings: Optional custom settings
            
        Returns:
            Dictionary with the generation result
        """
        self._logger.info(f"Generating Azure Pipelines configuration for {project_type}")
        
        config: Dict[str, Any] = {
            "trigger": ["main", "master"],
            "pr": ["main", "master"]
        }
        
        # Set up pool
        config["pool"] = {"vmImage": "ubuntu-latest"}
        
        # Set up stages based on project type
        if project_type == "python":
            config["stages"] = [
                {
                    "stage": "Build",
                    "jobs": [
                        {
                            "job": "BuildAndTest",
                            "steps": [
                                {
                                    "task": "UsePythonVersion@0",
                                    "inputs": {
                                        "versionSpec": "3.9",
                                        "addToPath": "true"
                                    },
                                    "displayName": "Use Python 3.9"
                                },
                                {
                                    "script": "python -m pip install --upgrade pip",
                                    "displayName": "Install pip"
                                },
                                {
                                    "script": "pip install -r requirements.txt",
                                    "displayName": "Install dependencies"
                                },
                                {
                                    "script": "pip install pytest pytest-cov pytest-azurepipelines",
                                    "displayName": "Install testing tools"
                                },
                                {
                                    "script": "pytest --cov=.",
                                    "displayName": "Run tests with coverage"
                                }
                            ]
                        }
                    ]
                }
            ]
        elif project_type == "node":
            config["stages"] = [
                {
                    "stage": "Build",
                    "jobs": [
                        {
                            "job": "BuildAndTest",
                            "steps": [
                                {
                                    "task": "NodeTool@0",
                                    "inputs": {
                                        "versionSpec": "16.x"
                                    },
                                    "displayName": "Install Node.js"
                                },
                                {
                                    "script": "npm ci",
                                    "displayName": "Install dependencies"
                                },
                                {
                                    "script": "npm run lint --if-present",
                                    "displayName": "Lint"
                                },
                                {
                                    "script": "npm run build --if-present",
                                    "displayName": "Build"
                                },
                                {
                                    "script": "npm test",
                                    "displayName": "Test"
                                }
                            ]
                        }
                    ]
                }
            ]
        elif project_type == "dotnet":
            config["stages"] = [
                {
                    "stage": "Build",
                    "jobs": [
                        {
                            "job": "BuildAndTest",
                            "steps": [
                                {
                                    "task": "UseDotNet@2",
                                    "inputs": {
                                        "packageType": "sdk",
                                        "version": "6.0.x"
                                    },
                                    "displayName": "Use .NET 6.0"
                                },
                                {
                                    "task": "DotNetCoreCLI@2",
                                    "inputs": {
                                        "command": "restore"
                                    },
                                    "displayName": "Restore NuGet packages"
                                },
                                {
                                    "task": "DotNetCoreCLI@2",
                                    "inputs": {
                                        "command": "build",
                                        "arguments": "--configuration Release"
                                    },
                                    "displayName": "Build"
                                },
                                {
                                    "task": "DotNetCoreCLI@2",
                                    "inputs": {
                                        "command": "test",
                                        "arguments": "--configuration Release --collect:\"XPlat Code Coverage\""
                                    },
                                    "displayName": "Test"
                                },
                                {
                                    "task": "PublishCodeCoverageResults@1",
                                    "inputs": {
                                        "codeCoverageTool": "Cobertura",
                                        "summaryFileLocation": "$(Agent.TempDirectory)/**/coverage.cobertura.xml"
                                    },
                                    "displayName": "Publish code coverage"
                                }
                            ]
                        }
                    ]
                }
            ]
        elif project_type == "java":
            maven_config = [
                {
                    "stage": "Build",
                    "jobs": [
                        {
                            "job": "MavenBuildAndTest",
                            "steps": [
                                {
                                    "task": "JavaToolInstaller@0",
                                    "inputs": {
                                        "versionSpec": "11",
                                        "jdkArchitectureOption": "x64",
                                        "jdkSourceOption": "PreInstalled"
                                    },
                                    "displayName": "Set up JDK 11"
                                },
                                {
                                    "task": "Maven@3",
                                    "inputs": {
                                        "mavenPomFile": "pom.xml",
                                        "goals": "package",
                                        "options": "-B",
                                        "publishJUnitResults": "true",
                                        "testResultsFiles": "**/surefire-reports/TEST-*.xml"
                                    },
                                    "displayName": "Build with Maven"
                                }
                            ]
                        }
                    ]
                }
            ]
            
            gradle_config = [
                {
                    "stage": "Build",
                    "jobs": [
                        {
                            "job": "GradleBuildAndTest",
                            "steps": [
                                {
                                    "task": "JavaToolInstaller@0",
                                    "inputs": {
                                        "versionSpec": "11",
                                        "jdkArchitectureOption": "x64",
                                        "jdkSourceOption": "PreInstalled"
                                    },
                                    "displayName": "Set up JDK 11"
                                },
                                {
                                    "task": "Gradle@2",
                                    "inputs": {
                                        "gradleWrapperFile": "gradlew",
                                        "tasks": "build",
                                        "publishJUnitResults": "true",
                                        "testResultsFiles": "**/TEST-*.xml"
                                    },
                                    "displayName": "Build with Gradle"
                                }
                            ]
                        }
                    ]
                }
            ]
            
            # Check if it's Gradle or Maven
            if (path / "build.gradle").exists() or (path / "build.gradle.kts").exists():
                config["stages"] = gradle_config
            else:
                config["stages"] = maven_config
                
        elif project_type == "go":
            config["stages"] = [
                {
                    "stage": "Build",
                    "jobs": [
                        {
                            "job": "BuildAndTest",
                            "steps": [
                                {
                                    "task": "GoTool@0",
                                    "inputs": {
                                        "version": "1.18"
                                    },
                                    "displayName": "Set up Go"
                                },
                                {
                                    "script": "go mod download",
                                    "displayName": "Download dependencies"
                                },
                                {
                                    "script": "go build -v ./...",
                                    "displayName": "Build"
                                },
                                {
                                    "script": "go test -v -race ./...",
                                    "displayName": "Test"
                                }
                            ]
                        }
                    ]
                }
            ]
        elif project_type == "rust":
            config["stages"] = [
                {
                    "stage": "Build",
                    "jobs": [
                        {
                            "job": "BuildAndTest",
                            "steps": [
                                {
                                    "script": "curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y",
                                    "displayName": "Install Rust"
                                },
                                {
                                    "script": "source $HOME/.cargo/env && rustup component add clippy",
                                    "displayName": "Add Clippy"
                                },
                                {
                                    "script": "source $HOME/.cargo/env && cargo build --verbose",
                                    "displayName": "Build"
                                },
                                {
                                    "script": "source $HOME/.cargo/env && cargo test --verbose",
                                    "displayName": "Test"
                                },
                                {
                                    "script": "source $HOME/.cargo/env && cargo clippy -- -D warnings",
                                    "displayName": "Lint"
                                }
                            ]
                        }
                    ]
                }
            ]
        elif project_type == "php":
            config["stages"] = [
                {
                    "stage": "Build",
                    "jobs": [
                        {
                            "job": "BuildAndTest",
                            "steps": [
                                {
                                    "script": "sudo apt-get update && sudo apt-get install -y php-cli php-xml php-mbstring",
                                    "displayName": "Install PHP"
                                },
                                {
                                    "script": "php -r \"copy('https://getcomposer.org/installer', 'composer-setup.php');\"",
                                    "displayName": "Download Composer"
                                },
                                {
                                    "script": "php composer-setup.php --install-dir=/usr/local/bin --filename=composer",
                                    "displayName": "Install Composer"
                                },
                                {
                                    "script": "composer install",
                                    "displayName": "Install dependencies"
                                },
                                {
                                    "script": "vendor/bin/phpunit",
                                    "displayName": "Run tests"
                                }
                            ]
                        }
                    ]
                }
            ]
        elif project_type == "ruby":
            config["stages"] = [
                {
                    "stage": "Build",
                    "jobs": [
                        {
                            "job": "BuildAndTest",
                            "steps": [
                                {
                                    "task": "UseRubyVersion@0",
                                    "inputs": {
                                        "versionSpec": "3.1",
                                        "addToPath": "true"
                                    },
                                    "displayName": "Use Ruby 3.1"
                                },
                                {
                                    "script": "gem install bundler",
                                    "displayName": "Install bundler"
                                },
                                {
                                    "script": "bundle install",
                                    "displayName": "Install dependencies"
                                },
                                {
                                    "script": "bundle exec rake test",
                                    "displayName": "Run tests"
                                }
                            ]
                        }
                    ]
                }
            ]
        elif project_type == "cpp":
            config["stages"] = [
                {
                    "stage": "Build",
                    "jobs": [
                        {
                            "job": "BuildAndTest",
                            "steps": [
                                {
                                    "script": "sudo apt-get update && sudo apt-get install -y build-essential cmake",
                                    "displayName": "Install dependencies"
                                },
                                {
                                    "script": "cmake -B build -DCMAKE_BUILD_TYPE=Release",
                                    "displayName": "Configure CMake"
                                },
                                {
                                    "script": "cmake --build build",
                                    "displayName": "Build"
                                },
                                {
                                    "script": "cd build && ctest -V",
                                    "displayName": "Run tests"
                                }
                            ]
                        }
                    ]
                }
            ]
        else:
            config["stages"] = [
                {
                    "stage": "Build",
                    "jobs": [
                        {
                            "job": "DefaultBuild",
                            "steps": [
                                {
                                    "script": "echo 'Add build commands for your project type'",
                                    "displayName": "Build placeholder"
                                },
                                {
                                    "script": "echo 'Add test commands for your project type'",
                                    "displayName": "Test placeholder"
                                }
                            ]
                        }
                    ]
                }
            ]
        
        # Update with custom settings using deep merge
        if custom_settings:
            config = deep_update(config, custom_settings)
        
        # Write the config file
        config_file = path / "azure-pipelines.yml"
        try:
            with open(config_file, 'w') as f:
                yaml.dump(config, f, default_flow_style=False, sort_keys=False)
            
            return {
                "success": True,
                "platform": "azure_pipelines",
                "project_type": project_type,
                "config_file": str(config_file)
            }
        except Exception as e:
            return {
                "success": False,
                "error": f"Failed to write Azure Pipelines config: {str(e)}",
                "platform": "azure_pipelines",
                "project_type": project_type
            }
    
    async def _generate_bitbucket_pipelines(
        self, 
        path: Path,
        project_type: str,
        custom_settings: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Generate Bitbucket Pipelines configuration.
        
        Args:
            path: Path to the project
            project_type: Project type
            custom_settings: Optional custom settings
            
        Returns:
            Dictionary with the generation result
        """
        self._logger.info(f"Generating Bitbucket Pipelines configuration for {project_type}")
        
        config: Dict[str, Any] = {
            "image": "", # Will be set based on project type
            "pipelines": {
                "default": [],
                "branches": {
                    "main": [],
                    "master": []
                },
                "pull-requests": [],
                "tags": []
            }
        }
        
        # Set config based on project type
        if project_type == "python":
            config["image"] = "python:3.9"
            default_pipe = [
                {
                    "step": {
                        "name": "Build and test",
                        "caches": ["pip"],
                        "script": [
                            "pip install -r requirements.txt",
                            "pip install pytest pytest-cov",
                            "pytest --cov=."
                        ],
                        "after-script": [
                            "pip install codecov",
                            "codecov"
                        ]
                    }
                }
            ]
        elif project_type == "node":
            config["image"] = "node:16"
            default_pipe = [
                {
                    "step": {
                        "name": "Build and test",
                        "caches": ["node"],
                        "script": [
                            "npm ci",
                            "npm run lint --if-present",
                            "npm run build --if-present",
                            "npm test"
                        ]
                    }
                }
            ]
        elif project_type == "go":
            config["image"] = "golang:1.18"
            default_pipe = [
                {
                    "step": {
                        "name": "Build and test",
                        "script": [
                            "go mod download",
                            "go build -v ./...",
                            "go test -v -race ./..."
                        ]
                    }
                }
            ]
        elif project_type == "rust":
            config["image"] = "rust:latest"
            default_pipe = [
                {
                    "step": {
                        "name": "Build and test",
                        "caches": ["cargo"],
                        "script": [
                            "rustup component add clippy",
                            "cargo build --verbose",
                            "cargo test --verbose",
                            "cargo clippy -- -D warnings"
                        ]
                    }
                }
            ]
        elif project_type == "java":
            if (path / "pom.xml").exists():
                config["image"] = "maven:latest"
                default_pipe = [
                    {
                        "step": {
                            "name": "Build and test",
                            "caches": ["maven"],
                            "script": [
                                "mvn clean package"
                            ]
                        }
                    }
                ]
            else:
                config["image"] = "gradle:latest"
                default_pipe = [
                    {
                        "step": {
                            "name": "Build and test",
                            "caches": ["gradle"],
                            "script": [
                                "gradle build"
                            ]
                        }
                    }
                ]
        elif project_type == "ruby":
            config["image"] = "ruby:latest"
            default_pipe = [
                {
                    "step": {
                        "name": "Build and test",
                        "caches": ["bundler"],
                        "script": [
                            "bundle install",
                            "bundle exec rake test"
                        ]
                    }
                }
            ]
        elif project_type == "php":
            config["image"] = "php:8.0"
            default_pipe = [
                {
                    "step": {
                        "name": "Build and test",
                        "caches": ["composer"],
                        "script": [
                            "apt-get update && apt-get install -y git unzip",
                            "curl -sS https://getcomposer.org/installer | php -- --install-dir=/usr/local/bin --filename=composer",
                            "composer install",
                            "vendor/bin/phpunit"
                        ]
                    }
                }
            ]
        elif project_type == "dotnet":
            config["image"] = "mcr.microsoft.com/dotnet/sdk:6.0"
            default_pipe = [
                {
                    "step": {
                        "name": "Build and test",
                        "script": [
                            "dotnet restore",
                            "dotnet build",
                            "dotnet test"
                        ]
                    }
                }
            ]
        elif project_type == "cpp":
            config["image"] = "gcc:latest"
            default_pipe = [
                {
                    "step": {
                        "name": "Build and test",
                        "script": [
                            "apt-get update && apt-get install -y cmake",
                            "cmake -B build -DCMAKE_BUILD_TYPE=Release",
                            "cmake --build build",
                            "cd build && ctest -V"
                        ]
                    }
                }
            ]
        else:
            config["image"] = "alpine:latest"
            default_pipe = [
                {
                    "step": {
                        "name": "Build and test",
                        "script": [
                            "echo 'Add your build commands here'",
                            "echo 'Add your test commands here'"
                        ]
                    }
                }
            ]
        
        # Set up the pipeline steps
        config["pipelines"]["default"] = default_pipe
        config["pipelines"]["branches"]["main"] = default_pipe
        config["pipelines"]["branches"]["master"] = default_pipe
        config["pipelines"]["pull-requests"] = default_pipe
        
        # Add deployment step for tags
        deploy_pipe = list(default_pipe)
        deploy_pipe.append({
            "step": {
                "name": "Deploy on tag",
                "deployment": "production",
                "script": [
                    "echo 'Deploying to production...'"
                ]
            }
        })
        config["pipelines"]["tags"] = deploy_pipe
        
        # Update with custom settings using deep merge
        if custom_settings:
            config = deep_update(config, custom_settings)
        
        # Write the config file
        config_file = path / "bitbucket-pipelines.yml"
        try:
            with open(config_file, 'w') as f:
                yaml.dump(config, f, default_flow_style=False, sort_keys=False)
            
            return {
                "success": True,
                "platform": "bitbucket_pipelines",
                "project_type": project_type,
                "config_file": str(config_file)
            }
        except Exception as e:
            return {
                "success": False,
                "error": f"Failed to write Bitbucket Pipelines config: {str(e)}",
                "platform": "bitbucket_pipelines",
                "project_type": project_type
            }
    

    
    def get_repository_provider_from_url(self, url: str) -> str:
        """
        Determine the repository provider from a repository URL.
        
        Args:
            url: Repository URL
            
        Returns:
            Repository provider name
        """
        url = url.lower()
        
        if "github.com" in url:
            return "github"
        elif "gitlab.com" in url:
            return "gitlab"
        elif "bitbucket.org" in url:
            return "bitbucket"
        elif "dev.azure.com" in url or "visualstudio.com" in url:
            return "azure_devops"
        else:
            return "unknown"

# Global CI/CD integration instance
ci_cd_integration = CiCdIntegration()
</file>

<file path="components/toolchain/cross_tool_workflow_engine.py">
# angela/toolchain/cross_tool_workflow_engine.py
"""
Cross-Tool Workflow Engine for Angela CLI.

This module provides specialized workflow orchestration capabilities for
executing complex, multi-tool workflows across different CLI tools and services.
"""
import asyncio
import re
import os
import json
import shlex
import tempfile
from datetime import datetime
from typing import Dict, Any, List, Optional, Set, Union, Tuple
from pathlib import Path
from enum import Enum
import uuid
import textwrap 

from pydantic import BaseModel, Field

from angela.utils.logging import get_logger
# Updated imports using API layer
from angela.api.context import get_context_manager
from angela.core.registry import registry
from angela.api.ai import get_gemini_client, GeminiRequest
from angela.api.shell import get_terminal_formatter
from angela.api.execution import get_execution_engine, get_execution_hooks

logger = get_logger(__name__)


class CrossToolStep(BaseModel):
    """Model for a step in a cross-tool workflow."""
    id: str
    name: str
    description: str
    tool: str
    command: str
    transform_output: Optional[str] = None  # Code to transform output
    export_variables: Optional[List[str]] = None  # Variables to export
    required_variables: Optional[List[str]] = None  # Variables required by this step
    continue_on_failure: bool = False

class DataFlow(BaseModel):
    """Model for data flow between steps."""
    source_step: str
    target_step: str
    source_variable: str
    target_variable: str
    transformation: Optional[str] = None  # Transformation code

class CrossToolWorkflow(BaseModel):
    """Model for a cross-tool workflow."""
    id: str
    name: str
    description: str
    steps: Dict[str, CrossToolStep]
    dependencies: Dict[str, List[str]] = Field(default_factory=dict)  # Step ID -> List of dependent step IDs
    data_flow: List[DataFlow] = Field(default_factory=list)
    entry_points: List[str] = Field(default_factory=list)  # List of step IDs
    variables: Dict[str, Any] = Field(default_factory=dict)  # Initial variables
    metadata: Dict[str, Any] = Field(default_factory=dict)

class CrossToolWorkflowEngine:
    """
    Specialized workflow engine for orchestrating complex workflows
    across different CLI tools and services.
    """
    
    def __init__(self):
        """Initialize the cross-tool workflow engine."""
        self._logger = logger
        self._active_workflows = {}  # ID -> workflow state
        self._workflow_history = {}  # ID -> execution history
        
        # Initialize needed components
        self._enhanced_universal_cli = None
    
    def initialize(self):
        """Initialize the workflow engine."""
        # Get enhanced universal CLI using the registry
        self._enhanced_universal_cli = registry.get("enhanced_universal_cli")
        if not self._enhanced_universal_cli:
            try:
                # Updated to use API layer
                from angela.api.toolchain import get_enhanced_universal_cli
                self._enhanced_universal_cli = get_enhanced_universal_cli()
                registry.register("enhanced_universal_cli", self._enhanced_universal_cli)
            except ImportError:
                self._logger.error("Failed to import Enhanced Universal CLI")
                self._enhanced_universal_cli = None

    
    async def create_workflow(
        self,
        request: str,
        context: Dict[str, Any],
        tools: Optional[List[str]] = None,
        max_steps: int = 20
    ) -> CrossToolWorkflow:
        """
        Create a cross-tool workflow based on a natural language request.
        
        Args:
            request: Natural language request
            context: Context information
            tools: Optional list of tools to include
            max_steps: Maximum number of steps to generate
            
        Returns:
            A CrossToolWorkflow object
        """
        self._logger.info(f"Creating cross-tool workflow: {request}")
        
        # Initialize if needed
        if not self._enhanced_universal_cli:
            self.initialize()
        
        # If tools not provided, try to detect from the request
        if not tools:
            tools = await self._detect_required_tools(request)
            self._logger.info(f"Detected tools: {tools}")
        
        # Generate workflow
        try:
            workflow_data = await self._generate_workflow(request, context, tools, max_steps)
            
            # Create workflow object
            workflow_id = workflow_data.get("id", str(uuid.uuid4()))
            workflow_name = workflow_data.get("name", f"Workflow {workflow_id[:8]}")
            description = workflow_data.get("description", request)
            
            # Create step objects
            steps = {}
            steps_data = workflow_data.get("steps", {})
            for step_id, step_data in steps_data.items():
                steps[step_id] = CrossToolStep(
                    id=step_id,
                    name=step_data.get("name", f"Step {step_id}"),
                    description=step_data.get("description", ""),
                    tool=step_data.get("tool", ""),
                    command=step_data.get("command", ""),
                    transform_output=step_data.get("transform_output"),
                    export_variables=step_data.get("export_variables", []),
                    required_variables=step_data.get("required_variables", []),
                    continue_on_failure=step_data.get("continue_on_failure", False)
                )
            
            # Get dependencies
            dependencies = workflow_data.get("dependencies", {})
            
            # Get data flow
            data_flow = []
            data_flow_data = workflow_data.get("data_flow", [])
            for flow_data in data_flow_data:
                data_flow.append(DataFlow(
                    source_step=flow_data.get("source_step", ""),
                    target_step=flow_data.get("target_step", ""),
                    source_variable=flow_data.get("source_variable", ""),
                    target_variable=flow_data.get("target_variable", ""),
                    transformation=flow_data.get("transformation")
                ))
            
            # Get entry points
            entry_points = workflow_data.get("entry_points", [])
            if not entry_points and steps:
                # If no entry points provided, use first step
                entry_points = [next(iter(steps.keys()))]
            
            # Get initial variables
            variables = workflow_data.get("variables", {})
            
            # Create workflow object
            workflow = CrossToolWorkflow(
                id=workflow_id,
                name=workflow_name,
                description=description,
                steps=steps,
                dependencies=dependencies,
                data_flow=data_flow,
                entry_points=entry_points,
                variables=variables,
                metadata={
                    "created_at": datetime.now().isoformat(),
                    "request": request,
                    "tools": tools
                }
            )
            
            return workflow
        
        except Exception as e:
            self._logger.error(f"Error creating cross-tool workflow: {str(e)}")
            raise
    
    async def execute_workflow(
        self,
        workflow: CrossToolWorkflow,
        variables: Optional[Dict[str, Any]] = None,
        dry_run: bool = False
    ) -> Dict[str, Any]:
        """
        Execute a cross-tool workflow.
        
        Args:
            workflow: The workflow to execute
            variables: Optional initial variables
            dry_run: Whether to simulate execution without making changes
            
        Returns:
            Dictionary with execution results
        """
        self._logger.info(f"Executing cross-tool workflow: {workflow.name}")
        
        # Initialize execution state
        execution_state = {
            "workflow_id": workflow.id,
            "started_at": datetime.now().isoformat(),
            "dry_run": dry_run,
            "variables": workflow.variables.copy(),
            "completed_steps": set(),
            "failed_steps": set(),
            "results": {},
            "status": "running"
        }
        
        # Update with provided variables
        if variables:
            execution_state["variables"].update(variables)
        
        # Store the active workflow
        self._active_workflows[workflow.id] = execution_state
        
        try:
            # Determine initial steps to execute (entry points)
            steps_to_execute = self._get_initial_steps(workflow)
            
            # Track all steps
            all_steps = set(workflow.steps.keys())
            
            # Execute steps until no more steps can be executed
            while steps_to_execute:
                # Get next batch of steps to execute (based on dependencies)
                executable_steps = self._get_executable_steps(workflow, steps_to_execute, execution_state)
                
                if not executable_steps:
                    # Check if we're stuck
                    remaining_steps = all_steps - execution_state["completed_steps"] - execution_state["failed_steps"]
                    if remaining_steps:
                        self._logger.warning(f"Workflow execution is stuck. Remaining steps: {remaining_steps}")
                        execution_state["status"] = "stuck"
                    else:
                        self._logger.info("All workflow steps completed")
                        execution_state["status"] = "completed"
                    break
                
                # Execute steps
                for step_id in executable_steps:
                    step = workflow.steps[step_id]
                    
                    # Execute the step
                    self._logger.info(f"Executing step {step_id}: {step.name}")
                    result = await self._execute_step(step, workflow, execution_state)
                    
                    # Store the result
                    execution_state["results"][step_id] = result
                    
                    # Update step status
                    if result.get("success", False):
                        execution_state["completed_steps"].add(step_id)
                        
                        # Apply data flow from this step
                        await self._apply_data_flow(step_id, workflow, execution_state)
                    else:
                        execution_state["failed_steps"].add(step_id)
                        
                        # Check if this failure should stop the workflow
                        if not step.continue_on_failure:
                            self._logger.warning(f"Step {step_id} failed and is critical - stopping workflow")
                            execution_state["status"] = "failed"
                            break
                
                # If status is failed, stop execution
                if execution_state["status"] == "failed":
                    break
                
                # Update steps to execute - remove completed and failed steps
                steps_to_execute -= execution_state["completed_steps"]
                steps_to_execute -= execution_state["failed_steps"]
                
                # Add new executable steps based on dependencies
                new_steps = self._get_next_steps(workflow, execution_state)
                steps_to_execute.update(new_steps)
            
            # Calculate success based on status and critical steps
            critical_steps = [step_id for step_id, step in workflow.steps.items() 
                             if not step.continue_on_failure]
            
            failed_critical_steps = execution_state["failed_steps"].intersection(critical_steps)
            
            execution_state["success"] = (
                execution_state["status"] != "failed" and
                execution_state["status"] != "stuck" and
                len(failed_critical_steps) == 0
            )
            
            # Add end time
            execution_state["ended_at"] = datetime.now().isoformat()
            
            # Store execution history
            self._workflow_history[workflow.id] = {
                "workflow": workflow.dict(),
                "execution": {
                    "started_at": execution_state["started_at"],
                    "ended_at": execution_state["ended_at"],
                    "success": execution_state["success"],
                    "status": execution_state["status"],
                    "steps_completed": list(execution_state["completed_steps"]),
                    "steps_failed": list(execution_state["failed_steps"]),
                    "variables": execution_state["variables"]
                }
            }
            
            # Clear active workflow
            if workflow.id in self._active_workflows:
                del self._active_workflows[workflow.id]
            
            # Return execution results
            return {
                "workflow_id": execution_state["workflow_id"],
                "success": execution_state["success"],
                "status": execution_state["status"],
                "steps_total": len(all_steps),
                "steps_completed": len(execution_state["completed_steps"]),
                "steps_failed": len(execution_state["failed_steps"]),
                "started_at": execution_state["started_at"],
                "ended_at": execution_state["ended_at"],
                "variables": execution_state["variables"],
                "results": execution_state["results"]
            }
            
        except Exception as e:
            self._logger.error(f"Error executing workflow: {str(e)}")
            
            # Update execution state
            execution_state["status"] = "error"
            execution_state["error"] = str(e)
            execution_state["ended_at"] = datetime.now().isoformat()
            execution_state["success"] = False
            
            # Clear active workflow
            if workflow.id in self._active_workflows:
                del self._active_workflows[workflow.id]
            
            # Return error result
            return {
                "workflow_id": execution_state["workflow_id"],
                "success": False,
                "status": "error",
                "error": str(e),
                "started_at": execution_state["started_at"],
                "ended_at": execution_state["ended_at"]
            }
    
    def _get_initial_steps(self, workflow: CrossToolWorkflow) -> Set[str]:
        """
        Get initial steps to execute based on entry points.
        
        Args:
            workflow: The workflow
            
        Returns:
            Set of step IDs to execute initially
        """
        # Use entry points or first step if not specified
        if workflow.entry_points:
            return set(workflow.entry_points)
        elif workflow.steps:
            return {next(iter(workflow.steps.keys()))}
        else:
            return set()
    
    def _get_executable_steps(
        self,
        workflow: CrossToolWorkflow,
        steps_to_execute: Set[str],
        execution_state: Dict[str, Any]
    ) -> Set[str]:
        """
        Get steps that can be executed based on dependencies.
        
        Args:
            workflow: The workflow
            steps_to_execute: Steps being considered for execution
            execution_state: Current execution state
            
        Returns:
            Set of executable step IDs
        """
        executable_steps = set()
        
        for step_id in steps_to_execute:
            # Check if already completed or failed
            if (step_id in execution_state["completed_steps"] or 
                step_id in execution_state["failed_steps"]):
                continue
            
            # Check if step exists
            if step_id not in workflow.steps:
                continue
            
            # Check dependencies - step is executable if all dependencies are completed
            dependencies_satisfied = True
            
            # Get dependencies for this step
            for dep_step_id in workflow.dependencies.get(step_id, []):
                if dep_step_id not in execution_state["completed_steps"]:
                    dependencies_satisfied = False
                    break
            
            # Check required variables
            step = workflow.steps[step_id]
            for var_name in step.required_variables or []:
                if var_name not in execution_state["variables"]:
                    dependencies_satisfied = False
                    break
            
            if dependencies_satisfied:
                executable_steps.add(step_id)
        
        return executable_steps
    
    def _get_next_steps(
        self,
        workflow: CrossToolWorkflow,
        execution_state: Dict[str, Any]
    ) -> Set[str]:
        """
        Get next steps to execute based on dependencies.
        
        Args:
            workflow: The workflow
            execution_state: Current execution state
            
        Returns:
            Set of step IDs to execute next
        """
        next_steps = set()
        
        # For each step, check if its dependencies are satisfied
        for step_id in workflow.steps:
            # Skip completed or failed steps
            if (step_id in execution_state["completed_steps"] or 
                step_id in execution_state["failed_steps"]):
                continue
            
            # Check dependencies
            dependencies_satisfied = True
            for dep_step_id in workflow.dependencies.get(step_id, []):
                if dep_step_id not in execution_state["completed_steps"]:
                    dependencies_satisfied = False
                    break
            
            # Check required variables
            step = workflow.steps[step_id]
            for var_name in step.required_variables or []:
                if var_name not in execution_state["variables"]:
                    dependencies_satisfied = False
                    break
            
            if dependencies_satisfied:
                next_steps.add(step_id)
        
        return next_steps
    
    async def _execute_step(
        self,
        step: CrossToolStep,
        workflow: CrossToolWorkflow,
        execution_state: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Execute a single workflow step.
        
        Args:
            step: The step to execute
            workflow: The workflow
            execution_state: Current execution state
            
        Returns:
            Dictionary with step execution results
        """
        self._logger.debug(f"Executing step {step.id}: {step.name}")
        
        # Check if this is a dry run
        if execution_state["dry_run"]:
            return {
                "success": True,
                "dry_run": True,
                "message": f"[DRY RUN] Would execute: {step.tool} {step.command}"
            }
        
        # Parse the command, replacing variable references
        command = self._substitute_variables(step.command, execution_state["variables"])
        
        # Execute the command
        try:
            # Use enhanced universal CLI for command execution
            if self._enhanced_universal_cli:
                # Try to use the enhanced translation if available
                translation_result = await self._enhanced_universal_cli.translate_with_context(
                    request=command,
                    tool=step.tool
                )
                
                if translation_result.get("success", False) and "command" in translation_result:
                    command = translation_result["command"]
            
            # Get execution engine from API
            execution_engine = get_execution_engine()
            
            # Execute the command
            stdout, stderr, return_code = await execution_engine.execute_command(
                command=command,
                check_safety=True
            )
            
            # Process the output
            result = {
                "success": return_code == 0,
                "return_code": return_code,
                "stdout": stdout,
                "stderr": stderr,
                "command": command
            }
            
            # Apply output transformation if specified
            if step.transform_output:
                try:
                    transformed_output = await self._transform_step_output(
                        step.transform_output,
                        stdout,
                        stderr,
                        return_code
                    )
                    
                    if transformed_output is not None:
                        result["transformed_output"] = transformed_output
                except Exception as e:
                    self._logger.error(f"Error transforming output: {str(e)}")
            
            # Extract variables if specified
            if step.export_variables and (stdout or stderr):
                variables = self._extract_variables_from_output(stdout, step.export_variables)
                
                if variables:
                    result["variables"] = variables
                    # Update execution state variables
                    execution_state["variables"].update(variables)
                    
                    self._logger.debug(f"Extracted variables: {variables}")
            
            return result
            
        except Exception as e:
            self._logger.error(f"Error executing step {step.id}: {str(e)}")
            return {
                "success": False,
                "error": str(e)
            }
    
    async def _transform_step_output(
        self,
        transform_code: str,
        stdout: str,
        stderr: str,
        return_code: int
    ) -> Any:
        """
        Transform step output using the provided code.
        
        Args:
            transform_code: Python code for transformation
            stdout: Command standard output
            stderr: Command standard error
            return_code: Command return code
            
        Returns:
            Transformed output
        """
        # Create a sandbox for code execution
        sandbox = {
            "stdout": stdout,
            "stderr": stderr,
            "return_code": return_code,
            "result": None,
            "import_modules": ["json", "re"],
            "json": json,
            "re": re
        }
        
        # Prefix with safety wrapper
        safe_code = f"""
# Transformation code
import json
import re

def transform_output(stdout, stderr, return_code):
    result = None
    
    # User provided transformation code
{textwrap.indent(transform_code, '    ')}
    
    return result

# Execute transformation
result = transform_output(stdout, stderr, return_code)
"""
        
        # Execute in a temporary file
        with tempfile.NamedTemporaryFile(mode="w+", suffix=".py") as temp_file:
            temp_file.write(safe_code)
            temp_file.flush()
            
            # Run the transformation using subprocess
            try:
                import sys
                process = await asyncio.create_subprocess_exec(
                    sys.executable, temp_file.name,
                    stdout=asyncio.subprocess.PIPE,
                    stderr=asyncio.subprocess.PIPE,
                    env={"PYTHONPATH": os.pathsep.join(sys.path)}
                )
                
                proc_stdout, proc_stderr = await process.communicate()
                
                if process.returncode != 0:
                    self._logger.error(f"Error transforming output: {proc_stderr.decode()}")
                    return None
                
                # The result should be the last line of stdout
                result_line = proc_stdout.decode().strip().split("\n")[-1]
                
                try:
                    # Try to parse as JSON
                    transformed = json.loads(result_line)
                    return transformed
                except json.JSONDecodeError:
                    # Return as string
                    return result_line
                    
            except Exception as e:
                self._logger.error(f"Error executing transformation code: {str(e)}")
                return None
    
    def _extract_variables_from_output(
        self,
        output: str,
        variable_names: List[str]
    ) -> Dict[str, Any]:
        """
        Extract variables from command output.
        
        Args:
            output: Command output
            variable_names: List of variable names to extract
            
        Returns:
            Dictionary of extracted variables
        """
        variables = {}
        
        # Try to parse output as JSON first
        if output.strip().startswith('{') and output.strip().endswith('}'):
            try:
                json_data = json.loads(output)
                
                # Extract variables from JSON
                for var_name in variable_names:
                    if var_name in json_data:
                        variables[var_name] = json_data[var_name]
                
                return variables
            except json.JSONDecodeError:
                pass
        
        # Look for variable assignments in output
        for var_name in variable_names:
            # Look for patterns like "VAR=value" or "VAR: value"
            patterns = [
                rf'^{var_name}=(.+?)$',
                rf'^{var_name}:\s*(.+?)$',
                rf'export\s+{var_name}=(.+?)$'
            ]
            
            for pattern in patterns:
                match = re.search(pattern, output, re.MULTILINE)
                if match:
                    variables[var_name] = match.group(1).strip()
                    break
        
        return variables
    
    async def _apply_data_flow(
        self,
        source_step_id: str,
        workflow: CrossToolWorkflow,
        execution_state: Dict[str, Any]
    ) -> None:
        """
        Apply data flow from a completed step.
        
        Args:
            source_step_id: ID of the completed step
            workflow: The workflow
            execution_state: Current execution state
        """
        # Find data flow entries where this step is the source
        flows = [flow for flow in workflow.data_flow 
                if flow.source_step == source_step_id]
        
        if not flows:
            return
        
        for flow in flows:
            # Get the source variable from step results
            step_result = execution_state["results"].get(source_step_id, {})
            
            source_value = None
            
            # Check different places for the variable
            if flow.source_variable in step_result.get("variables", {}):
                source_value = step_result["variables"][flow.source_variable]
            elif flow.source_variable == "stdout":
                source_value = step_result.get("stdout", "")
            elif flow.source_variable == "stderr":
                source_value = step_result.get("stderr", "")
            elif flow.source_variable == "return_code":
                source_value = step_result.get("return_code", 0)
            elif "transformed_output" in step_result:
                # Check if transformed output is a dict
                transformed = step_result["transformed_output"]
                if isinstance(transformed, dict) and flow.source_variable in transformed:
                    source_value = transformed[flow.source_variable]
            
            if source_value is None:
                self._logger.warning(f"Source variable {flow.source_variable} not found in step {source_step_id}")
                continue
            
            # Apply transformation if specified
            if flow.transformation:
                try:
                    transformed_value = await self._transform_value(
                        flow.transformation,
                        source_value
                    )
                    
                    if transformed_value is not None:
                        source_value = transformed_value
                except Exception as e:
                    self._logger.error(f"Error transforming value: {str(e)}")
            
            # Set the target variable
            execution_state["variables"][flow.target_variable] = source_value
            
            self._logger.debug(f"Applied data flow: {flow.source_step}.{flow.source_variable} -> {flow.target_variable}")
    
    async def _transform_value(self, transform_code: str, value: Any) -> Any:
        """
        Transform a value using the provided code.
        
        Args:
            transform_code: Python code for transformation
            value: Value to transform
            
        Returns:
            Transformed value
        """
        # Create a sandbox for code execution
        sandbox = {
            "value": value,
            "result": None,
            "import_modules": ["json", "re"],
            "json": json,
            "re": re
        }
        
        # Prefix with safety wrapper
        safe_code = f"""
# Transformation code
import json
import re

def transform_value(value):
    result = None
    
    # User provided transformation code
{textwrap.indent(transform_code, '    ')}
    
    return result

# Execute transformation
result = transform_value(value)
"""
        
        # Execute in a temporary file
        with tempfile.NamedTemporaryFile(mode="w+", suffix=".py") as temp_file:
            temp_file.write(safe_code)
            temp_file.flush()
            
            # Run the transformation using subprocess
            try:
                import sys
                
                # Create a JSON serializable representation of the value
                value_json = json.dumps(value)
                
                process = await asyncio.create_subprocess_exec(
                    sys.executable, temp_file.name,
                    stdin=asyncio.subprocess.PIPE,
                    stdout=asyncio.subprocess.PIPE,
                    stderr=asyncio.subprocess.PIPE,
                    env={"PYTHONPATH": os.pathsep.join(sys.path)}
                )
                
                proc_stdout, proc_stderr = await process.communicate(value_json.encode())
                
                if process.returncode != 0:
                    self._logger.error(f"Error transforming value: {proc_stderr.decode()}")
                    return None
                
                # The result should be the last line of stdout
                result_line = proc_stdout.decode().strip().split("\n")[-1]
                
                try:
                    # Try to parse as JSON
                    transformed = json.loads(result_line)
                    return transformed
                except json.JSONDecodeError:
                    # Return as string
                    return result_line
                    
            except Exception as e:
                self._logger.error(f"Error executing transformation code: {str(e)}")
                return None
    
    def _substitute_variables(self, text: str, variables: Dict[str, Any]) -> str:
        """
        Substitute variable references in a string.
        
        Args:
            text: The string to substitute
            variables: Dictionary of variables
            
        Returns:
            String with variables substituted
        """
        if not text:
            return text
        
        result = text
        
        # Replace ${var} syntax
        for var_name, var_value in variables.items():
            placeholder = f"${{{var_name}}}"
            if placeholder in result:
                result = result.replace(placeholder, str(var_value))
        
        # Replace $var syntax (only for word boundaries)
        for var_name, var_value in variables.items():
            result = re.sub(r'\$' + var_name + r'\b', str(var_value), result)
        
        return result
    
    async def _detect_required_tools(self, request: str) -> List[str]:
        """
        Detect which tools are required for a workflow based on a request.
        
        Args:
            request: Natural language request
            
        Returns:
            List of detected tools
        """
        self._logger.debug(f"Detecting required tools for: {request}")
        
        # Use AI to detect required tools
        prompt = f"""
    Analyze this workflow request to determine which command-line tools would be needed:
    "{request}"
    
    Return a JSON array with the names of the required CLI tools (e.g., git, docker, aws, etc.)
    Sort them by importance (most important first).
    
    Format:
    ["tool1", "tool2", "tool3"]
    """
    
        try:
            # Get gemini client from API
            gemini_client = get_gemini_client()
            
            # Call AI service
            api_request = GeminiRequest(prompt=prompt, max_tokens=500)
            response = await gemini_client.generate_text(api_request)
            
            # Extract tools
            import json
            import re
            
            # Try to find JSON in the response
            json_match = re.search(r'```(?:json)?\s*(.*?)\s*```', response.text, re.DOTALL)
            if json_match:
                json_str = json_match.group(1)
            else:
                # Assume the entire response is JSON
                json_str = response.text
            
            # Parse JSON
            detected_tools = json.loads(json_str)
            
            # Validate tools
            if not isinstance(detected_tools, list):
                raise ValueError("Expected a list of tools")
            
            # Convert to strings and ensure proper format
            return [str(tool).strip().lower() for tool in detected_tools]
            
        except Exception as e:
            self._logger.error(f"Error detecting required tools: {str(e)}")
            
            # Try basic pattern matching as fallback
            common_tools = ["git", "docker", "npm", "pip", "aws", "gcloud", "kubectl"]
            detected = []
            
            for tool in common_tools:
                if tool in request.lower():
                    detected.append(tool)
            
            return detected or ["bash"]
    
    async def _generate_workflow(
        self,
        request: str,
        context: Dict[str, Any],
        tools: List[str],
        max_steps: int
    ) -> Dict[str, Any]:
        """
        Generate a workflow based on a request, context, and tools.
        
        Args:
            request: Natural language request
            context: Context information
            tools: List of tools to include
            max_steps: Maximum number of steps
            
        Returns:
            Dictionary with workflow definition
        """
        self._logger.info(f"Generating workflow for: {request}")
        
        # Create a detailed prompt for workflow generation
        project_info = self._extract_project_info(context)
        
        # Get information about the tools
        tool_info = await self._get_tool_info(tools)
        
        prompt = f"""
    You are an expert workflow designer. Create a detailed cross-tool workflow for this request:
    "{request}"
    
    Project information:
    {project_info}
    
    Tools to use: {', '.join(tools)}
    
    Tool information:
    {tool_info}
    
    Maximum steps: {max_steps}
    
    Create a complete workflow specification in JSON format with the following structure:
    {{
      "id": "unique_id",
      "name": "Workflow Name",
      "description": "Detailed description of what the workflow does",
      "steps": {{
        "step1": {{
          "name": "Step 1 Name",
          "description": "What this step does",
          "tool": "tool_name",
          "command": "command to execute",
          "transform_output": "optional code to transform output",
          "export_variables": ["variable1", "variable2"],
          "required_variables": ["dependency1", "dependency2"],
          "continue_on_failure": false
        }},
        // More steps...
      }},
      "dependencies": {{
        "step2": ["step1"],  // Step2 depends on step1
        "step3": ["step1", "step2"]  // Step3 depends on both step1 and step2
      }},
      "data_flow": [
        {{
          "source_step": "step1",
          "target_step": "step2",
          "source_variable": "variable1",
          "target_variable": "input1",
          "transformation": "optional transformation code"
        }}
        // More data flows...
      ],
      "entry_points": ["step1"],  // Steps to start execution with
      "variables": {{
        "initial_var1": "value1",
        "initial_var2": "value2"
      }}
    }}
    
    Ensure the workflow:
    1. Uses the correct syntax for each tool
    2. Includes proper command validation and error handling
    3. Correctly passes data between steps using the data_flow section
    4. Has meaningful step names and descriptions
    5. Uses absolute paths for file references whenever possible
    6. Has a logical sequence of steps with proper dependencies
    7. Uses as few steps as possible to accomplish the goal efficiently
    8. Makes proper use of variables for data that needs to be shared between steps
    """
    
        try:
            # Get gemini client from API
            gemini_client = get_gemini_client()
            
            # Call AI service
            api_request = GeminiRequest(prompt=prompt, max_tokens=4000)
            response = await gemini_client.generate_text(api_request)
            
            # Extract workflow JSON
            import json
            import re
            
            # Try to find JSON in the response
            json_match = re.search(r'```(?:json)?\s*(.*?)\s*```', response.text, re.DOTALL)
            if json_match:
                json_str = json_match.group(1)
            else:
                # Assume the entire response is JSON
                json_str = response.text
            
            # Parse JSON
            workflow_data = json.loads(json_str)
            
            # Ensure required fields are present
            if "id" not in workflow_data:
                workflow_data["id"] = str(uuid.uuid4())
            
            if "name" not in workflow_data or not workflow_data["name"]:
                workflow_data["name"] = f"Workflow for {request[:30]}..."
            
            if "description" not in workflow_data or not workflow_data["description"]:
                workflow_data["description"] = request
            
            if "steps" not in workflow_data or not workflow_data["steps"]:
                raise ValueError("Workflow must contain steps")
            
            if "dependencies" not in workflow_data:
                workflow_data["dependencies"] = {}
            
            if "data_flow" not in workflow_data:
                workflow_data["data_flow"] = []
            
            if "entry_points" not in workflow_data or not workflow_data["entry_points"]:
                # Use first step as entry point
                workflow_data["entry_points"] = [next(iter(workflow_data["steps"].keys()))]
            
            if "variables" not in workflow_data:
                workflow_data["variables"] = {}
            
            return workflow_data
            
        except Exception as e:
            self._logger.error(f"Error generating workflow: {str(e)}")
            raise
    
    def _extract_project_info(self, context: Dict[str, Any]) -> str:
        """
        Extract relevant project information from context.
        
        Args:
            context: Context information
            
        Returns:
            String with project information
        """
        info = []
        
        if "project_root" in context:
            info.append(f"Project root: {context['project_root']}")
        
        if "cwd" in context:
            info.append(f"Current directory: {context['cwd']}")
        
        if "project_type" in context:
            info.append(f"Project type: {context['project_type']}")
        
        # Add project state if available
        if "project_state" in context:
            state = context["project_state"]
            
            # Add Git information
            if "git_state" in state:
                git_state = state["git_state"]
                if git_state.get("is_git_repo", False):
                    info.append(f"Git repository: Yes")
                    if "current_branch" in git_state:
                        info.append(f"Current branch: {git_state['current_branch']}")
                    if git_state.get("has_changes", False):
                        info.append(f"Has uncommitted changes: Yes")
            
            # Add dependency information
            if "dependencies" in state:
                deps = state["dependencies"]
                if deps.get("package_manager"):
                    info.append(f"Package manager: {deps['package_manager']}")
        
        return "\n".join(info)
    
    async def _get_tool_info(self, tools: List[str]) -> str:
        """
        Get information about specified tools.
        
        Args:
            tools: List of tools
            
        Returns:
            String with tool information
        """
        info = []
        
        # Get execution engine from API
        execution_engine = get_execution_engine()
        
        for tool in tools:
            # Basic command for getting version information
            version_cmd = f"{tool} --version"
            
            try:
                stdout, stderr, return_code = await execution_engine.execute_command(
                    command=version_cmd,
                    check_safety=True
                )
                
                if return_code == 0 and stdout.strip():
                    info.append(f"{tool}: {stdout.strip()}")
                else:
                    # Try alternative version flag
                    version_cmd = f"{tool} -v"
                    stdout, stderr, return_code = await execution_engine.execute_command(
                        command=version_cmd,
                        check_safety=True
                    )
                    
                    if return_code == 0 and stdout.strip():
                        info.append(f"{tool}: {stdout.strip()}")
                    else:
                        info.append(f"{tool}: Available but version unknown")
            except Exception:
                # Just note that the tool is being used
                info.append(f"{tool}: Will be used in workflow")
        
        return "\n".join(info)
    
    async def get_workflow_status(self, workflow_id: str) -> Dict[str, Any]:
        """
        Get the status of a workflow execution.
        
        Args:
            workflow_id: ID of the workflow
            
        Returns:
            Dictionary with workflow status
        """
        # Check if workflow is active
        if workflow_id in self._active_workflows:
            execution_state = self._active_workflows[workflow_id]
            
            return {
                "workflow_id": workflow_id,
                "status": execution_state["status"],
                "active": True,
                "started_at": execution_state["started_at"],
                "steps_completed": len(execution_state["completed_steps"]),
                "steps_failed": len(execution_state["failed_steps"]),
                "current_variables": execution_state["variables"]
            }
        
        # Check workflow history
        if workflow_id in self._workflow_history:
            history = self._workflow_history[workflow_id]["execution"]
            
            return {
                "workflow_id": workflow_id,
                "status": history["status"],
                "active": False,
                "started_at": history["started_at"],
                "ended_at": history["ended_at"],
                "success": history.get("success", False),
                "steps_completed": len(history["steps_completed"]),
                "steps_failed": len(history["steps_failed"])
            }
        
        # Workflow not found
        return {
            "workflow_id": workflow_id,
            "status": "not_found",
            "active": False
        }
    
    async def update_workflow(
        self,
        workflow: CrossToolWorkflow,
        request: str,
        context: Dict[str, Any]
    ) -> CrossToolWorkflow:
        """
        Update an existing workflow based on a new request.
        
        Args:
            workflow: Existing workflow
            request: New request to incorporate
            context: Context information
            
        Returns:
            Updated workflow
        """
        self._logger.info(f"Updating workflow {workflow.name}: {request}")
        
        # Get existing tools
        existing_tools = set()
        for step in workflow.steps.values():
            if step.tool:
                existing_tools.add(step.tool)
        
        # Detect new tools required by the request
        new_tools = await self._detect_required_tools(request)
        
        # Combine tools
        tools = list(existing_tools) + [t for t in new_tools if t not in existing_tools]
        
        # Create an enhancing prompt
        prompt = f"""
    You are updating an existing workflow to incorporate new requirements.
    
    Original workflow:
    {json.dumps(workflow.dict(), indent=2)}
    
    New requirements:
    "{request}"
    
    Tools available: {', '.join(tools)}
    
    Update the workflow to incorporate the new requirements while preserving as much of the original workflow as possible.
    You should:
    1. Modify existing steps if they need to change
    2. Add new steps as needed
    3. Update dependencies and data flow
    4. Ensure the workflow remains coherent and efficient
    
    Return the complete updated workflow in the same JSON format as the original.
    """
    
        try:
            # Get gemini client from API
            gemini_client = get_gemini_client()
            
            # Call AI service
            api_request = GeminiRequest(prompt=prompt, max_tokens=4000)
            response = await gemini_client.generate_text(api_request)
            
            # Extract workflow JSON
            import json
            import re
            
            # Try to find JSON in the response
            json_match = re.search(r'```(?:json)?\s*(.*?)\s*```', response.text, re.DOTALL)
            if json_match:
                json_str = json_match.group(1)
            else:
                # Assume the entire response is JSON
                json_str = response.text
            
            # Parse JSON
            workflow_data = json.loads(json_str)
            
            # Create updated workflow
            updated_workflow = CrossToolWorkflow(**workflow_data)
            
            # Update metadata
            if "metadata" not in updated_workflow.dict() or not updated_workflow.metadata:
                updated_workflow.metadata = workflow.metadata.copy()
            
            updated_workflow.metadata["updated_at"] = datetime.now().isoformat()
            updated_workflow.metadata["update_request"] = request
            
            return updated_workflow
            
        except Exception as e:
            self._logger.error(f"Error updating workflow: {str(e)}")
            raise

# Create global instance
cross_tool_workflow_engine = CrossToolWorkflowEngine()

# Register it in the service registry
registry.register("cross_tool_workflow_engine", cross_tool_workflow_engine)

# Initialize on module import
cross_tool_workflow_engine.initialize()
</file>

<file path="components/toolchain/docker.py">
# angela/components/toolchain/docker.py
"""
Docker toolchain integration for Angela CLI.

This module provides functionality for interacting with Docker and Docker Compose,
including container management, image operations, and Dockerfile generation.
"""
import asyncio
import json
import os
import re
import shutil
import yaml
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple, Union, Set

from angela.utils.logging import get_logger
# Updated imports to use API layer
from angela.api.execution import get_execution_engine
from angela.api.context import get_context_manager
from angela.api.safety import get_command_risk_classifier

logger = get_logger(__name__)

# Constants for Docker file templates
DOCKERFILE_TEMPLATES = {
    "python": """FROM python:{python_version}-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

{expose_port}

CMD ["python", "{entry_point}"]
""",
    "node": """FROM node:{node_version}-alpine

WORKDIR /app

COPY package.json {package_lock} ./
RUN npm install {production_flag}

COPY . .

{expose_port}

CMD ["npm", "start"]
""",
    "golang": """FROM golang:{go_version}-alpine AS builder

WORKDIR /app

COPY go.mod go.sum ./
RUN go mod download

COPY . .
RUN go build -o main {main_file}

FROM alpine:latest

WORKDIR /app
COPY --from=builder /app/main .

{expose_port}

CMD ["./main"]
""",
    "java": """FROM maven:{maven_version}-jdk-{java_version} AS builder

WORKDIR /app
COPY pom.xml .
RUN mvn dependency:go-offline

COPY src ./src
RUN mvn package -DskipTests

FROM openjdk:{java_version}-jre-slim

WORKDIR /app
COPY --from=builder /app/target/{jar_file} app.jar

{expose_port}

CMD ["java", "-jar", "app.jar"]
""",
    "ruby": """FROM ruby:{ruby_version}-alpine

WORKDIR /app

COPY Gemfile Gemfile.lock ./
RUN bundle install --jobs 4 --retry 3

COPY . .

{expose_port}

CMD ["ruby", "{entry_point}"]
"""
}

DOCKER_COMPOSE_TEMPLATE = """version: '3'

services:
{services}
{networks}
{volumes}
"""

SERVICE_TEMPLATE = """  {service_name}:
    image: {image}
    build:
      context: {context}
      dockerfile: {dockerfile}
    {ports}
    {environment}
    {volumes}
    {depends_on}
    {networks}
"""

class DockerIntegration:
    """
    Integration with Docker and Docker Compose.
    
    Provides methods for interacting with Docker and Docker Compose,
    including command execution, file generation, and status checking.
    """
    
    def __init__(self):
        """Initialize Docker integration."""
        self._logger = logger
    
    async def is_docker_available(self) -> bool:
        """
        Check if Docker is available on the system.
        
        Returns:
            True if Docker is available, False otherwise
        """
        try:
            # Get execution engine from API
            execution_engine = get_execution_engine()
            
            stdout, stderr, exit_code = await execution_engine.execute_command(
                "docker --version",
                check_safety=True
            )
            return exit_code == 0
        except Exception as e:
            self._logger.error(f"Error checking Docker availability: {str(e)}")
            return False
    
    async def is_docker_compose_available(self) -> bool:
        """
        Check if Docker Compose is available on the system.
        
        Returns:
            True if Docker Compose is available, False otherwise
        """
        try:
            # Get execution engine from API
            execution_engine = get_execution_engine()
            
            # Try docker compose (v2) command first
            stdout, stderr, exit_code = await execution_engine.execute_command(
                "docker compose version",
                check_safety=True
            )
            if exit_code == 0:
                return True
            
            # Fall back to docker-compose (v1) command
            stdout, stderr, exit_code = await execution_engine.execute_command(
                "docker-compose --version",
                check_safety=True
            )
            return exit_code == 0
        except Exception as e:
            self._logger.error(f"Error checking Docker Compose availability: {str(e)}")
            return False
    
    async def get_docker_compose_command(self) -> str:
        """
        Get the appropriate Docker Compose command (v1 or v2).
        
        Returns:
            String with the appropriate command
        """
        try:
            # Get execution engine from API
            execution_engine = get_execution_engine()
            
            # Check if docker compose (v2) is available
            stdout, stderr, exit_code = await execution_engine.execute_command(
                "docker compose version",
                check_safety=True
            )
            if exit_code == 0:
                return "docker compose"
            
            # Fall back to docker-compose (v1)
            return "docker-compose"
        except Exception as e:
            self._logger.error(f"Error determining Docker Compose command: {str(e)}")
            return "docker compose"  # Default to v2 compose
    #
    # Container Management
    #
    
    async def list_containers(self, all_containers: bool = False) -> Dict[str, Any]:
        """
        List Docker containers.
        
        Args:
            all_containers: Whether to list all containers (including stopped)
            
        Returns:
            Dictionary with container list and status information
        """
        self._logger.info(f"Listing {'all' if all_containers else 'running'} Docker containers")
        
        try:
            # Get execution engine from API
            execution_engine = get_execution_engine()
            
            # Build command
            command = "docker ps --format json"
            if all_containers:
                command += " --all"
            
            # Execute command
            stdout, stderr, exit_code = await execution_engine.execute_command(
                command,
                check_safety=True
            )
            
            if exit_code != 0:
                return {
                    "success": False,
                    "error": f"Error listing containers: {stderr}",
                    "containers": []
                }
            
            # Parse JSON output
            containers = []
            
            # Check if output is in JSON format (newer Docker versions)
            if stdout.strip().startswith('{') or stdout.strip().startswith('['):
                # Parse JSON output for newer Docker versions
                for line in stdout.strip().split('\n'):
                    if line.strip():
                        try:
                            container = json.loads(line)
                            containers.append(container)
                        except json.JSONDecodeError as e:
                            self._logger.error(f"Error parsing container JSON: {str(e)}")
            else:
                # Parse tabular output
                lines = stdout.strip().split('\n')
                if len(lines) <= 1:  # Only header, no containers
                    return {
                        "success": True,
                        "containers": [],
                        "count": 0
                    }
                
                # Skip header
                lines = lines[1:]
                
                # Parse container lines
                for line in lines:
                    parts = re.split(r'\s{2,}', line.strip())
                    if len(parts) >= 7:
                        container = {
                            "id": parts[0],
                            "image": parts[1],
                            "command": parts[2],
                            "created": parts[3],
                            "status": parts[4],
                            "ports": parts[5] if len(parts) > 5 else "",
                            "names": parts[6] if len(parts) > 6 else ""
                        }
                        containers.append(container)
            
            return {
                "success": True,
                "containers": containers,
                "count": len(containers)
            }
        except Exception as e:
            self._logger.exception(f"Error listing containers: {str(e)}")
            return {
                "success": False,
                "error": f"Error listing containers: {str(e)}",
                "containers": []
            }
    
    async def get_container_details(self, container_id_or_name: str) -> Dict[str, Any]:
        """
        Get detailed information about a specific container.
        
        Args:
            container_id_or_name: Container ID or name
            
        Returns:
            Dictionary with container details
        """
        self._logger.info(f"Getting details for container: {container_id_or_name}")
        
        try:
            # Get execution engine from API
            execution_engine = get_execution_engine()
            
            # Execute command
            stdout, stderr, exit_code = await execution_engine.execute_command(
                f"docker inspect {container_id_or_name}",
                check_safety=True
            )
            
            if exit_code != 0:
                return {
                    "success": False,
                    "error": f"Error inspecting container: {stderr}"
                }
            
            # Parse container details
            try:
                details = json.loads(stdout)
                if not details or not isinstance(details, list):
                    return {
                        "success": False,
                        "error": "Invalid container details format"
                    }
                
                # Get first item (there should be only one)
                container_info = details[0]
                
                # Extract useful information
                result = {
                    "success": True,
                    "id": container_info.get("Id", ""),
                    "name": container_info.get("Name", "").lstrip('/'),
                    "image": container_info.get("Config", {}).get("Image", ""),
                    "state": container_info.get("State", {}),
                    "network_settings": container_info.get("NetworkSettings", {}),
                    "mounts": container_info.get("Mounts", []),
                    "config": container_info.get("Config", {}),
                    "created": container_info.get("Created", ""),
                    "full_details": container_info
                }
                
                return result
            except json.JSONDecodeError as e:
                return {
                    "success": False,
                    "error": f"Error parsing container details: {str(e)}"
                }
        except Exception as e:
            self._logger.exception(f"Error getting container details: {str(e)}")
            return {
                "success": False,
                "error": f"Error getting container details: {str(e)}"
            }
    
    async def start_container(self, container_id_or_name: str) -> Dict[str, Any]:
        """
        Start a Docker container.
        
        Args:
            container_id_or_name: Container ID or name
            
        Returns:
            Dictionary with operation status
        """
        self._logger.info(f"Starting container: {container_id_or_name}")
        
        try:
            # Get execution engine from API
            execution_engine = get_execution_engine()
            
            # Execute command
            stdout, stderr, exit_code = await execution_engine.execute_command(
                f"docker start {container_id_or_name}",
                check_safety=True
            )
            
            if exit_code != 0:
                return {
                    "success": False,
                    "error": f"Error starting container: {stderr}"
                }
            
            return {
                "success": True,
                "message": f"Container {container_id_or_name} started successfully",
                "output": stdout.strip()
            }
        except Exception as e:
            self._logger.exception(f"Error starting container: {str(e)}")
            return {
                "success": False,
                "error": f"Error starting container: {str(e)}"
            }
    
    async def stop_container(self, container_id_or_name: str, timeout: Optional[int] = None) -> Dict[str, Any]:
        """
        Stop a Docker container.
        
        Args:
            container_id_or_name: Container ID or name
            timeout: Optional timeout in seconds
            
        Returns:
            Dictionary with operation status
        """
        self._logger.info(f"Stopping container: {container_id_or_name}")
        
        try:
            # Get execution engine from API
            execution_engine = get_execution_engine()
            
            # Build command
            command = f"docker stop {container_id_or_name}"
            if timeout is not None:
                command += f" --time {timeout}"
            
            # Execute command
            stdout, stderr, exit_code = await execution_engine.execute_command(
                command,
                check_safety=True
            )
            
            if exit_code != 0:
                return {
                    "success": False,
                    "error": f"Error stopping container: {stderr}"
                }
            
            return {
                "success": True,
                "message": f"Container {container_id_or_name} stopped successfully",
                "output": stdout.strip()
            }
        except Exception as e:
            self._logger.exception(f"Error stopping container: {str(e)}")
            return {
                "success": False,
                "error": f"Error stopping container: {str(e)}"
            }
    
    async def restart_container(self, container_id_or_name: str, timeout: Optional[int] = None) -> Dict[str, Any]:
        """
        Restart a Docker container.
        
        Args:
            container_id_or_name: Container ID or name
            timeout: Optional timeout in seconds
            
        Returns:
            Dictionary with operation status
        """
        self._logger.info(f"Restarting container: {container_id_or_name}")
        
        try:
            # Get execution engine from API
            execution_engine = get_execution_engine()
            
            # Build command
            command = f"docker restart {container_id_or_name}"
            if timeout is not None:
                command += f" --time {timeout}"
            
            # Execute command
            stdout, stderr, exit_code = await execution_engine.execute_command(
                command,
                check_safety=True
            )
            
            if exit_code != 0:
                return {
                    "success": False,
                    "error": f"Error restarting container: {stderr}"
                }
            
            return {
                "success": True,
                "message": f"Container {container_id_or_name} restarted successfully",
                "output": stdout.strip()
            }
        except Exception as e:
            self._logger.exception(f"Error restarting container: {str(e)}")
            return {
                "success": False,
                "error": f"Error restarting container: {str(e)}"
            }
    
    async def remove_container(
        self, 
        container_id_or_name: str, 
        force: bool = False,
        remove_volumes: bool = False
    ) -> Dict[str, Any]:
        """
        Remove a Docker container.
        
        Args:
            container_id_or_name: Container ID or name
            force: Force removal of running container
            remove_volumes: Remove anonymous volumes
            
        Returns:
            Dictionary with operation status
        """
        self._logger.info(f"Removing container: {container_id_or_name}")
        
        try:
            # Get execution engine from API
            execution_engine = get_execution_engine()
            
            # Build command
            command = f"docker rm {container_id_or_name}"
            if force:
                command += " --force"
            if remove_volumes:
                command += " --volumes"
            
            # Execute command
            stdout, stderr, exit_code = await execution_engine.execute_command(
                command,
                check_safety=True
            )
            
            if exit_code != 0:
                return {
                    "success": False,
                    "error": f"Error removing container: {stderr}"
                }
            
            return {
                "success": True,
                "message": f"Container {container_id_or_name} removed successfully",
                "output": stdout.strip()
            }
        except Exception as e:
            self._logger.exception(f"Error removing container: {str(e)}")
            return {
                "success": False,
                "error": f"Error removing container: {str(e)}"
            }
    
    async def get_container_logs(
        self, 
        container_id_or_name: str, 
        tail: Optional[int] = None,
        follow: bool = False,
        timestamps: bool = False,
        since: Optional[str] = None,
        until: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Get logs from a Docker container.
        
        Args:
            container_id_or_name: Container ID or name
            tail: Number of lines to show from the end
            follow: Follow log output
            timestamps: Show timestamps
            since: Show logs since timestamp
            until: Show logs until timestamp
            
        Returns:
            Dictionary with container logs
        """
        self._logger.info(f"Getting logs for container: {container_id_or_name}")
        
        # Build command
        command = f"docker logs {container_id_or_name}"
        if tail is not None:
            command += f" --tail {tail}"
        if timestamps:
            command += " --timestamps"
        if since:
            command += f" --since {since}"
        if until:
            command += f" --until {until}"
        
        try:
            # Get execution engine from API
            execution_engine = get_execution_engine()
            
            if follow:
                # For follow mode, we need to stream the output
                # This is a simplified implementation - a more complex one would use
                # a proper streaming mechanism with callbacks
                command += " --follow"
                
                # Limit to 30 seconds maximum for safety
                process = await asyncio.create_subprocess_shell(
                    command,
                    stdout=asyncio.subprocess.PIPE,
                    stderr=asyncio.subprocess.PIPE
                )
                
                try:
                    # Wait for up to 30 seconds
                    stdout, stderr = await asyncio.wait_for(
                        process.communicate(), 
                        timeout=30
                    )
                    stdout_str = stdout.decode('utf-8', errors='replace')
                    stderr_str = stderr.decode('utf-8', errors='replace')
                    
                    return {
                        "success": process.returncode == 0,
                        "logs": stdout_str,
                        "error": stderr_str if process.returncode != 0 else None,
                        "followed": True,
                        "truncated": False
                    }
                except asyncio.TimeoutError:
                    # Kill the process after timeout
                    process.kill()
                    return {
                        "success": True,
                        "logs": "Log streaming timeout after 30 seconds",
                        "followed": True,
                        "truncated": True
                    }
            else:
                # For non-follow mode, just execute the command
                stdout, stderr, exit_code = await execution_engine.execute_command(
                    command,
                    check_safety=True
                )
                
                if exit_code != 0:
                    return {
                        "success": False,
                        "error": f"Error getting container logs: {stderr}"
                    }
                
                return {
                    "success": True,
                    "logs": stdout,
                    "followed": False
                }
        except Exception as e:
            self._logger.exception(f"Error getting container logs: {str(e)}")
            return {
                "success": False,
                "error": f"Error getting container logs: {str(e)}"
            }
    
    async def run_container(
        self,
        image: str,
        command: Optional[str] = None,
        name: Optional[str] = None,
        ports: Optional[List[str]] = None,
        volumes: Optional[List[str]] = None,
        environment: Optional[Dict[str, str]] = None,
        detach: bool = True,
        remove: bool = False,
        network: Optional[str] = None,
        interactive: bool = False
    ) -> Dict[str, Any]:
        """
        Run a Docker container.
        
        Args:
            image: Docker image to run
            command: Command to run in the container
            name: Name for the container
            ports: Port mappings (host:container)
            volumes: Volume mappings (host:container)
            environment: Environment variables
            detach: Run container in background
            remove: Remove container when it exits
            network: Connect to network
            interactive: Run container with interactive mode
            
        Returns:
            Dictionary with operation status
        """
        self._logger.info(f"Running container from image: {image}")
        
        # Build command
        docker_command = "docker run"
        
        if detach:
            docker_command += " --detach"
        if remove:
            docker_command += " --rm"
        if interactive:
            docker_command += " --interactive --tty"
        
        # Add name if provided
        if name:
            docker_command += f" --name {name}"
        
        # Add network if provided
        if network:
            docker_command += f" --network {network}"
        
        # Add port mappings
        if ports:
            for port_mapping in ports:
                docker_command += f" --publish {port_mapping}"
        
        # Add volume mappings
        if volumes:
            for volume_mapping in volumes:
                docker_command += f" --volume {volume_mapping}"
        
        # Add environment variables
        if environment:
            for key, value in environment.items():
                docker_command += f" --env {key}={value}"
        
        # Add image
        docker_command += f" {image}"
        
        # Add command if provided
        if command:
            docker_command += f" {command}"
        
        try:
            # Get execution engine from API
            execution_engine = get_execution_engine()
            
            # Execute command
            stdout, stderr, exit_code = await execution_engine.execute_command(
                docker_command,
                check_safety=True
            )
            
            if exit_code != 0:
                return {
                    "success": False,
                    "error": f"Error running container: {stderr}",
                    "command": docker_command
                }
            
            container_id = stdout.strip()
            
            return {
                "success": True,
                "message": f"Container started successfully{'in detached mode' if detach else ''}",
                "container_id": container_id,
                "command": docker_command
            }
        except Exception as e:
            self._logger.exception(f"Error running container: {str(e)}")
            return {
                "success": False,
                "error": f"Error running container: {str(e)}",
                "command": docker_command
            }
    
    async def exec_in_container(
        self,
        container_id_or_name: str,
        command: str,
        interactive: bool = False
    ) -> Dict[str, Any]:
        """
        Execute a command in a running container.
        
        Args:
            container_id_or_name: Container ID or name
            command: Command to execute
            interactive: Run in interactive mode
            
        Returns:
            Dictionary with execution result
        """
        self._logger.info(f"Executing command in container {container_id_or_name}: {command}")
        
        # Build docker exec command
        docker_command = f"docker exec"
        if interactive:
            docker_command += " --interactive --tty"
        
        docker_command += f" {container_id_or_name} {command}"
        
        try:
            # Get execution engine from API
            execution_engine = get_execution_engine()
            
            # Execute command
            stdout, stderr, exit_code = await execution_engine.execute_command(
                docker_command,
                check_safety=True
            )
            
            if exit_code != 0:
                return {
                    "success": False,
                    "error": f"Error executing command in container: {stderr}",
                    "command": docker_command
                }
            
            return {
                "success": True,
                "output": stdout,
                "command": docker_command
            }
        except Exception as e:
            self._logger.exception(f"Error executing command in container: {str(e)}")
            return {
                "success": False,
                "error": f"Error executing command in container: {str(e)}",
                "command": docker_command
            }
    #
    # Image Management
    #
    
    async def list_images(self, show_all: bool = False) -> Dict[str, Any]:
        """
        List Docker images.
        
        Args:
            show_all: Whether to show all images (including intermediate)
            
        Returns:
            Dictionary with image list
        """
        self._logger.info("Listing Docker images")
        
        try:
            # Get execution engine from API
            execution_engine = get_execution_engine()
            
            # Build command
            command = "docker images --format json"
            if show_all:
                command += " --all"
            
            # Execute command
            stdout, stderr, exit_code = await execution_engine.execute_command(
                command,
                check_safety=True
            )
            
            if exit_code != 0:
                return {
                    "success": False,
                    "error": f"Error listing images: {stderr}",
                    "images": []
                }
            
            # Parse image information
            images = []
            
            # Handle older Docker versions that don't support --format json
            if not stdout.strip().startswith('[') and not stdout.strip().startswith('{'):
                # Fall back to regular format parsing
                fallback_command = f"docker images{' --all' if show_all else ''}"
                stdout, stderr, exit_code = await execution_engine.execute_command(
                    fallback_command,
                    check_safety=True
                )
                
                if exit_code != 0:
                    return {
                        "success": False,
                        "error": f"Error listing images: {stderr}",
                        "images": []
                    }
                
                # Parse tabular output
                lines = stdout.strip().split('\n')
                if len(lines) <= 1:  # Only header, no images
                    return {
                        "success": True,
                        "images": [],
                        "count": 0
                    }
                
                # Skip header
                lines = lines[1:]
                
                # Parse image lines
                for line in lines:
                    parts = re.split(r'\s{2,}', line.strip())
                    if len(parts) >= 5:
                        image = {
                            "repository": parts[0],
                            "tag": parts[1],
                            "id": parts[2],
                            "created": parts[3],
                            "size": parts[4]
                        }
                        images.append(image)
            else:
                # Parse JSON output for newer Docker versions
                for line in stdout.strip().split('\n'):
                    if line.strip():
                        try:
                            image = json.loads(line)
                            images.append(image)
                        except json.JSONDecodeError as e:
                            self._logger.error(f"Error parsing image JSON: {str(e)}")
            
            return {
                "success": True,
                "images": images,
                "count": len(images)
            }
        except Exception as e:
            self._logger.exception(f"Error listing images: {str(e)}")
            return {
                "success": False,
                "error": f"Error listing images: {str(e)}",
                "images": []
            }
    
    async def build_image(
        self,
        context_path: Union[str, Path],
        tag: Optional[str] = None,
        dockerfile: Optional[str] = None,
        build_args: Optional[Dict[str, str]] = None,
        no_cache: bool = False
    ) -> Dict[str, Any]:
        """
        Build a Docker image.
        
        Args:
            context_path: Path to build context
            tag: Tag for the built image
            dockerfile: Path to Dockerfile (relative to context_path)
            build_args: Build arguments
            no_cache: Do not use cache when building
            
        Returns:
            Dictionary with build result
        """
        self._logger.info(f"Building Docker image from context: {context_path}")
        
        # Ensure context path exists
        context_path_obj = Path(context_path)
        if not context_path_obj.exists() or not context_path_obj.is_dir():
            return {
                "success": False,
                "error": f"Build context does not exist or is not a directory: {context_path}"
            }
        
        # Build command
        command = f"docker build {context_path_obj}"
        
        if tag:
            command += f" --tag {tag}"
        
        if dockerfile:
            command += f" --file {dockerfile}"
        
        if build_args:
            for key, value in build_args.items():
                command += f" --build-arg {key}={value}"
        
        if no_cache:
            command += " --no-cache"
        
        try:
            # Get execution engine from API
            execution_engine = get_execution_engine()
            
            # Execute command
            stdout, stderr, exit_code = await execution_engine.execute_command(
                command,
                check_safety=True
            )
            
            if exit_code != 0:
                return {
                    "success": False,
                    "error": f"Error building image: {stderr}",
                    "command": command
                }
            
            # Parse image ID from output
            image_id = None
            for line in stdout.splitlines():
                if line.strip().startswith("Successfully built "):
                    image_id = line.strip().split(" ")[-1]
                    break
            
            return {
                "success": True,
                "message": "Image built successfully",
                "image_id": image_id,
                "tag": tag,
                "output": stdout,
                "command": command
            }
        except Exception as e:
            self._logger.exception(f"Error building image: {str(e)}")
            return {
                "success": False,
                "error": f"Error building image: {str(e)}",
                "command": command
            }
    
    async def remove_image(
        self,
        image_id_or_name: str,
        force: bool = False
    ) -> Dict[str, Any]:
        """
        Remove a Docker image.
        
        Args:
            image_id_or_name: Image ID or name
            force: Force removal
            
        Returns:
            Dictionary with removal result
        """
        self._logger.info(f"Removing Docker image: {image_id_or_name}")
        
        # Build command
        command = f"docker rmi {image_id_or_name}"
        if force:
            command += " --force"
        
        try:
            # Execute command
            stdout, stderr, exit_code = await execution_engine.execute_command(
                command,
                check_safety=True
            )
            
            if exit_code != 0:
                return {
                    "success": False,
                    "error": f"Error removing image: {stderr}",
                    "command": command
                }
            
            return {
                "success": True,
                "message": f"Image {image_id_or_name} removed successfully",
                "output": stdout.strip(),
                "command": command
            }
        except Exception as e:
            self._logger.exception(f"Error removing image: {str(e)}")
            return {
                "success": False,
                "error": f"Error removing image: {str(e)}",
                "command": command
            }
    
    async def pull_image(self, image_name: str) -> Dict[str, Any]:
        """
        Pull a Docker image from a registry.
        
        Args:
            image_name: Image name to pull
            
        Returns:
            Dictionary with pull result
        """
        self._logger.info(f"Pulling Docker image: {image_name}")
        
        try:
            # Execute command
            stdout, stderr, exit_code = await execution_engine.execute_command(
                f"docker pull {image_name}",
                check_safety=True
            )
            
            if exit_code != 0:
                return {
                    "success": False,
                    "error": f"Error pulling image: {stderr}"
                }
            
            return {
                "success": True,
                "message": f"Image {image_name} pulled successfully",
                "output": stdout.strip()
            }
        except Exception as e:
            self._logger.exception(f"Error pulling image: {str(e)}")
            return {
                "success": False,
                "error": f"Error pulling image: {str(e)}"
            }
    
    #
    # Docker Compose
    #
    
    async def compose_up(
        self,
        compose_file: Optional[Union[str, Path]] = None,
        project_directory: Optional[Union[str, Path]] = None,
        detach: bool = True,
        build: bool = False,
        no_recreate: bool = False,
        force_recreate: bool = False,
        services: Optional[List[str]] = None
    ) -> Dict[str, Any]:
        """
        Start containers using Docker Compose.
        
        Args:
            compose_file: Path to docker-compose.yml (default: docker-compose.yml in project_directory)
            project_directory: Project directory (default: current directory)
            detach: Run in background
            build: Build images before starting
            no_recreate: Don't recreate containers
            force_recreate: Force recreate containers
            services: List of services to start (default: all)
            
        Returns:
            Dictionary with operation result
        """
        self._logger.info(f"Starting services with Docker Compose")
        
        # Get appropriate Docker Compose command
        compose_command = await self.get_docker_compose_command()
        
        # Determine project directory
        if project_directory is None:
            # Get context manager from API
            context_manager = get_context_manager()
            project_directory = context_manager.cwd
        else:
            project_directory = Path(project_directory)
        
        # Build command
        command = f"{compose_command}"
        
        if compose_file:
            command += f" -f {compose_file}"
        
        command += " up"
        
        if detach:
            command += " -d"
        
        if build:
            command += " --build"
        
        if no_recreate:
            command += " --no-recreate"
        
        if force_recreate:
            command += " --force-recreate"
        
        # Add services if specified
        if services:
            command += " " + " ".join(services)
        
        try:
            # Get execution engine from API
            execution_engine = get_execution_engine()
            
            # Execute command
            stdout, stderr, exit_code = await execution_engine.execute_command(
                command,
                check_safety=True,
                working_dir=str(project_directory)
            )
            
            if exit_code != 0:
                return {
                    "success": False,
                    "error": f"Error starting Docker Compose services: {stderr}",
                    "command": command,
                    "working_dir": str(project_directory)
                }
            
            return {
                "success": True,
                "message": "Docker Compose services started successfully",
                "output": stdout,
                "command": command,
                "working_dir": str(project_directory)
            }
        except Exception as e:
            self._logger.exception(f"Error starting Docker Compose services: {str(e)}")
            return {
                "success": False,
                "error": f"Error starting Docker Compose services: {str(e)}",
                "command": command,
                "working_dir": str(project_directory)
            }
    
    async def compose_down(
        self,
        compose_file: Optional[Union[str, Path]] = None,
        project_directory: Optional[Union[str, Path]] = None,
        remove_images: bool = False,
        remove_volumes: bool = False,
        remove_orphans: bool = False
    ) -> Dict[str, Any]:
        """
        Stop and remove containers, networks, volumes, and images created by compose up.
        
        Args:
            compose_file: Path to docker-compose.yml (default: docker-compose.yml in project_directory)
            project_directory: Project directory (default: current directory)
            remove_images: Remove images
            remove_volumes: Remove volumes
            remove_orphans: Remove containers for services not defined in the Compose file
            
        Returns:
            Dictionary with operation result
        """
        self._logger.info(f"Stopping services with Docker Compose")
        
        # Get appropriate Docker Compose command
        compose_command = await self.get_docker_compose_command()
        
        # Determine project directory
        if project_directory is None:
            # Get context manager from API
            context_manager = get_context_manager()
            project_directory = context_manager.cwd
        else:
            project_directory = Path(project_directory)
        
        # Build command
        command = f"{compose_command}"
        
        if compose_file:
            command += f" -f {compose_file}"
        
        command += " down"
        
        if remove_images:
            command += " --rmi all"
        
        if remove_volumes:
            command += " --volumes"
        
        if remove_orphans:
            command += " --remove-orphans"
        
        try:
            # Get execution engine from API
            execution_engine = get_execution_engine()
            
            # Execute command
            stdout, stderr, exit_code = await execution_engine.execute_command(
                command,
                check_safety=True,
                working_dir=str(project_directory)
            )
            
            if exit_code != 0:
                return {
                    "success": False,
                    "error": f"Error stopping Docker Compose services: {stderr}",
                    "command": command,
                    "working_dir": str(project_directory)
                }
            
            return {
                "success": True,
                "message": "Docker Compose services stopped successfully",
                "output": stdout,
                "command": command,
                "working_dir": str(project_directory)
            }
        except Exception as e:
            self._logger.exception(f"Error stopping Docker Compose services: {str(e)}")
            return {
                "success": False,
                "error": f"Error stopping Docker Compose services: {str(e)}",
                "command": command,
                "working_dir": str(project_directory)
            }
    
    async def compose_logs(
        self,
        compose_file: Optional[Union[str, Path]] = None,
        project_directory: Optional[Union[str, Path]] = None,
        services: Optional[List[str]] = None,
        follow: bool = False,
        tail: Optional[int] = None,
        timestamps: bool = False
    ) -> Dict[str, Any]:
        """
        View logs from Docker Compose services.
        
        Args:
            compose_file: Path to docker-compose.yml (default: docker-compose.yml in project_directory)
            project_directory: Project directory (default: current directory)
            services: List of services to show logs for (default: all)
            follow: Follow log output
            tail: Number of lines to show from the end
            timestamps: Show timestamps
            
        Returns:
            Dictionary with logs
        """
        self._logger.info(f"Getting logs from Docker Compose services")
        
        # Get appropriate Docker Compose command
        compose_command = await self.get_docker_compose_command()
        
        # Determine project directory
        if project_directory is None:
            # Get context manager from API
            context_manager = get_context_manager()
            project_directory = context_manager.cwd
        else:
            project_directory = Path(project_directory)
        
        # Build command
        command = f"{compose_command}"
        
        if compose_file:
            command += f" -f {compose_file}"
        
        command += " logs"
        
        if follow:
            command += " --follow"
        
        if tail is not None:
            command += f" --tail={tail}"
        
        if timestamps:
            command += " --timestamps"
        
        # Add services if specified
        if services:
            command += " " + " ".join(services)
        
        try:
            # Get execution engine from API
            execution_engine = get_execution_engine()
            
            if follow:
                # For follow mode, we need to stream the output
                # This is a simplified implementation
                
                # Limit to 30 seconds maximum for safety
                process = await asyncio.create_subprocess_shell(
                    command,
                    stdout=asyncio.subprocess.PIPE,
                    stderr=asyncio.subprocess.PIPE,
                    cwd=str(project_directory)
                )
                
                try:
                    # Wait for up to 30 seconds
                    stdout, stderr = await asyncio.wait_for(
                        process.communicate(), 
                        timeout=30
                    )
                    stdout_str = stdout.decode('utf-8', errors='replace')
                    stderr_str = stderr.decode('utf-8', errors='replace')
                    
                    return {
                        "success": process.returncode == 0,
                        "logs": stdout_str,
                        "error": stderr_str if process.returncode != 0 else None,
                        "followed": True,
                        "truncated": False,
                        "command": command,
                        "working_dir": str(project_directory)
                    }
                except asyncio.TimeoutError:
                    # Kill the process after timeout
                    process.kill()
                    return {
                        "success": True,
                        "logs": "Log streaming timeout after 30 seconds",
                        "followed": True,
                        "truncated": True,
                        "command": command,
                        "working_dir": str(project_directory)
                    }
            else:
                # Execute command
                stdout, stderr, exit_code = await execution_engine.execute_command(
                    command,
                    check_safety=True,
                    working_dir=str(project_directory)
                )
                
                if exit_code != 0:
                    return {
                        "success": False,
                        "error": f"Error getting Docker Compose logs: {stderr}",
                        "command": command,
                        "working_dir": str(project_directory)
                    }
                
                return {
                    "success": True,
                    "logs": stdout,
                    "command": command,
                    "working_dir": str(project_directory)
                }
        except Exception as e:
            self._logger.exception(f"Error getting Docker Compose logs: {str(e)}")
            return {
                "success": False,
                "error": f"Error getting Docker Compose logs: {str(e)}",
                "command": command,
                "working_dir": str(project_directory)
            }
    
    async def compose_ps(
        self,
        compose_file: Optional[Union[str, Path]] = None,
        project_directory: Optional[Union[str, Path]] = None,
        services: Optional[List[str]] = None,
        all_services: bool = False
    ) -> Dict[str, Any]:
        """
        List Docker Compose services.
        
        Args:
            compose_file: Path to docker-compose.yml (default: docker-compose.yml in project_directory)
            project_directory: Project directory (default: current directory)
            services: List of services to show (default: all)
            all_services: Show stopped services
            
        Returns:
            Dictionary with services list
        """
        self._logger.info(f"Listing Docker Compose services")
        
        # Get appropriate Docker Compose command
        compose_command = await self.get_docker_compose_command()
        
        # Determine project directory
        if project_directory is None:
            # Get context manager from API
            context_manager = get_context_manager()
            project_directory = context_manager.cwd
        else:
            project_directory = Path(project_directory)
        
        # Build command
        command = f"{compose_command}"
        
        if compose_file:
            command += f" -f {compose_file}"
        
        command += " ps"
        
        if all_services:
            command += " --all"
        
        # Add services if specified
        if services:
            command += " " + " ".join(services)
        
        try:
            # Get execution engine from API
            execution_engine = get_execution_engine()
            
            # Execute command
            stdout, stderr, exit_code = await execution_engine.execute_command(
                command,
                check_safety=True,
                working_dir=str(project_directory)
            )
            
            if exit_code != 0:
                return {
                    "success": False,
                    "error": f"Error listing Docker Compose services: {stderr}",
                    "command": command,
                    "working_dir": str(project_directory)
                }
            
            # Parse service information
            services_info = []
            
            # Skip empty output
            if not stdout.strip():
                return {
                    "success": True,
                    "services": [],
                    "count": 0,
                    "command": command,
                    "working_dir": str(project_directory)
                }
            
            # Parse table output (format varies between compose versions)
            lines = stdout.strip().split('\n')
            if len(lines) <= 1:  # Only header, no services
                return {
                    "success": True,
                    "services": [],
                    "count": 0,
                    "command": command,
                    "working_dir": str(project_directory)
                }
            
            # Try to parse as a table with header
            header = lines[0]
            data_lines = lines[1:]
            
            for line in data_lines:
                # Different compose versions have different formats
                # Try to extract at least name and state
                parts = re.split(r'\s{2,}', line.strip())
                if len(parts) >= 2:
                    service = {"name": parts[0]}
                    
                    # Detect format
                    if "Up" in line or "Exit" in line:
                        # Search for status like "Up 2 hours" or "Exit (1) 5 minutes ago"
                        status_match = re.search(r'(Up|Exit \(\d+\))[^,]*', line)
                        if status_match:
                            service["status"] = status_match.group(0)
                    
                    # Add all parts with labels if we can identify them
                    if len(header.split()) >= len(parts):
                        header_parts = re.split(r'\s{2,}', header.strip())
                        for i, value in enumerate(parts):
                            if i < len(header_parts):
                                key = header_parts[i].lower().replace(' ', '_')
                                service[key] = value
                    
                    services_info.append(service)
            
            return {
                "success": True,
                "services": services_info,
                "count": len(services_info),
                "raw_output": stdout,
                "command": command,
                "working_dir": str(project_directory)
            }
        except Exception as e:
            self._logger.exception(f"Error listing Docker Compose services: {str(e)}")
            return {
                "success": False,
                "error": f"Error listing Docker Compose services: {str(e)}",
                "command": command,
                "working_dir": str(project_directory)
            }
    
    #
    # Dockerfile Generation
    #
    
    async def detect_project_type(
        self, 
        project_directory: Union[str, Path]
    ) -> Dict[str, Any]:
        """
        Detect the type of project in a directory.
        
        Args:
            project_directory: Path to the project directory
            
        Returns:
            Dictionary with project type and details
        """
        self._logger.info(f"Detecting project type in: {project_directory}")
        
        project_dir = Path(project_directory)
        if not project_dir.exists() or not project_dir.is_dir():
            return {
                "success": False,
                "error": f"Project directory does not exist or is not a directory: {project_directory}"
            }
        
        # Look for common project markers
        markers = {
            "python": ["requirements.txt", "setup.py", "pyproject.toml", "Pipfile"],
            "node": ["package.json", "yarn.lock", "package-lock.json"],
            "golang": ["go.mod", "go.sum"],
            "java": ["pom.xml", "build.gradle", "build.gradle.kts"],
            "ruby": ["Gemfile", "Gemfile.lock"],
            "php": ["composer.json", "composer.lock"],
            "dotnet": ["*.csproj", "*.fsproj", "*.vbproj"]
        }
        
        # Check for each marker
        detected_types = {}
        
        for project_type, files in markers.items():
            for file_pattern in files:
                # Handle glob patterns
                if "*" in file_pattern:
                    matching_files = list(project_dir.glob(file_pattern))
                    if matching_files:
                        detected_types[project_type] = {
                            "marker_file": str(matching_files[0].relative_to(project_dir)),
                            "confidence": 0.9
                        }
                        break
                else:
                    file_path = project_dir / file_pattern
                    if file_path.exists():
                        detected_types[project_type] = {
                            "marker_file": file_pattern,
                            "confidence": 0.9
                        }
                        break
        
        # If nothing detected, try to infer from file extensions
        if not detected_types:
            extensions = {}
            
            # Count file extensions
            for file_path in project_dir.glob("**/*"):
                if file_path.is_file():
                    ext = file_path.suffix.lower()
                    if ext:
                        extensions[ext] = extensions.get(ext, 0) + 1
            
            # Map extensions to project types
            extension_types = {
                ".py": "python",
                ".js": "node",
                ".ts": "node",
                ".jsx": "node",
                ".tsx": "node",
                ".go": "golang",
                ".java": "java",
                ".rb": "ruby",
                ".php": "php",
                ".cs": "dotnet",
                ".fs": "dotnet",
                ".vb": "dotnet"
            }
            
            # Count by project type
            type_counts = {}
            for ext, count in extensions.items():
                if ext in extension_types:
                    project_type = extension_types[ext]
                    type_counts[project_type] = type_counts.get(project_type, 0) + count
            
            # Sort by count
            if type_counts:
                most_common = max(type_counts.items(), key=lambda x: x[1])
                detected_types[most_common[0]] = {
                    "inferred_from_extensions": True,
                    "file_count": most_common[1],
                    "confidence": 0.7
                }
        
        # Detect versions for the project
        version_info = {}
        for project_type in detected_types.keys():
            if project_type == "python":
                version_info["python_version"] = await self._detect_python_version(project_dir)
            elif project_type == "node":
                version_info["node_version"] = await self._detect_node_version(project_dir)
            elif project_type == "golang":
                version_info["go_version"] = await self._detect_go_version(project_dir)
            elif project_type == "java":
                java_info = await self._detect_java_version(project_dir)
                version_info.update(java_info)
            elif project_type == "ruby":
                version_info["ruby_version"] = await self._detect_ruby_version(project_dir)
        
        # Determine the most likely project type
        result = {
            "success": True,
            "detected_types": detected_types,
            "version_info": version_info
        }
        
        if detected_types:
            # Find the type with highest confidence
            best_type = max(detected_types.items(), key=lambda x: x[1]["confidence"])
            result["primary_type"] = best_type[0]
            result["details"] = best_type[1]
        else:
            result["primary_type"] = "unknown"
        
        return result
    
    async def _detect_python_version(self, project_dir: Path) -> str:
        """
        Detect Python version used in a project.
        
        Args:
            project_dir: Path to the project directory
            
        Returns:
            Python version string
        """
        # Check for explicit version in pyproject.toml
        pyproject_path = project_dir / "pyproject.toml"
        if pyproject_path.exists():
            try:
                with open(pyproject_path, 'r') as f:
                    content = f.read()
                    # Look for requires-python or python_requires
                    requires_match = re.search(r'(requires-python|python_requires)\s*=\s*["\']([^"\']+)["\']', content)
                    if requires_match:
                        version_req = requires_match.group(2)
                        # Extract a simple version number from the requirement
                        version_match = re.search(r'(\d+\.\d+)', version_req)
                        if version_match:
                            return version_match.group(1)
            except Exception as e:
                self._logger.error(f"Error reading pyproject.toml: {str(e)}")
        
        # Check .python-version file
        python_version_file = project_dir / ".python-version"
        if python_version_file.exists():
            try:
                with open(python_version_file, 'r') as f:
                    version = f.read().strip()
                    if version:
                        return version
            except Exception as e:
                self._logger.error(f"Error reading .python-version: {str(e)}")
        
        # Default to 3.10 if no specific version found
        return "3.10"
    
    async def _detect_node_version(self, project_dir: Path) -> str:
        """
        Detect Node.js version used in a project.
        
        Args:
            project_dir: Path to the project directory
            
        Returns:
            Node.js version string
        """
        # Check package.json for engines field
        package_json_path = project_dir / "package.json"
        if package_json_path.exists():
            try:
                with open(package_json_path, 'r') as f:
                    data = json.load(f)
                    if "engines" in data and "node" in data["engines"]:
                        # Extract a simple version number from the requirement
                        version_req = data["engines"]["node"]
                        version_match = re.search(r'(\d+\.\d+)', version_req)
                        if version_match:
                            return version_match.group(1)
            except Exception as e:
                self._logger.error(f"Error reading package.json: {str(e)}")
        
        # Check .nvmrc file
        nvmrc_file = project_dir / ".nvmrc"
        if nvmrc_file.exists():
            try:
                with open(nvmrc_file, 'r') as f:
                    version = f.read().strip()
                    if version:
                        # Clean up version string
                        version = version.lstrip('v')
                        version_match = re.search(r'(\d+\.\d+)', version)
                        if version_match:
                            return version_match.group(1)
            except Exception as e:
                self._logger.error(f"Error reading .nvmrc: {str(e)}")
        
        # Default to 18 if no specific version found
        return "18"
    
    async def _detect_go_version(self, project_dir: Path) -> str:
        """
        Detect Go version used in a project.
        
        Args:
            project_dir: Path to the project directory
            
        Returns:
            Go version string
        """
        # Check go.mod file
        go_mod_path = project_dir / "go.mod"
        if go_mod_path.exists():
            try:
                with open(go_mod_path, 'r') as f:
                    content = f.read()
                    # Look for go directive
                    go_match = re.search(r'go\s+(\d+\.\d+)', content)
                    if go_match:
                        return go_match.group(1)
            except Exception as e:
                self._logger.error(f"Error reading go.mod: {str(e)}")
        
        # Default to 1.19 if no specific version found
        return "1.19"
    
    async def _detect_java_version(self, project_dir: Path) -> Dict[str, str]:
        """
        Detect Java version and build tool used in a project.
        
        Args:
            project_dir: Path to the project directory
            
        Returns:
            Dictionary with java_version and build tool info
        """
        result = {
            "java_version": "17",  # Default
            "maven_version": "3.8"  # Default
        }
        
        # Check for Maven pom.xml
        pom_path = project_dir / "pom.xml"
        if pom_path.exists():
            result["build_tool"] = "maven"
            try:
                with open(pom_path, 'r') as f:
                    content = f.read()
                    # Look for Java version
                    java_match = re.search(r'<java.version>(\d+)</java.version>', content)
                    if java_match:
                        result["java_version"] = java_match.group(1)
                    
                    # Look for Maven compiler source
                    compiler_match = re.search(r'<maven.compiler.source>(\d+)</maven.compiler.source>', content)
                    if compiler_match:
                        result["java_version"] = compiler_match.group(1)
                    
                    # Try to find jar file name
                    artifact_match = re.search(r'<artifactId>([^<]+)</artifactId>', content)
                    if artifact_match:
                        result["jar_file"] = f"{artifact_match.group(1)}.jar"
            except Exception as e:
                self._logger.error(f"Error reading pom.xml: {str(e)}")
        
        # Check for Gradle build file
        gradle_path = project_dir / "build.gradle"
        if gradle_path.exists():
            result["build_tool"] = "gradle"
            try:
                with open(gradle_path, 'r') as f:
                    content = f.read()
                    # Look for Java version
                    java_match = re.search(r'sourceCompatibility\s*=\s*[\'"](\d+)[\'"]', content)
                    if java_match:
                        result["java_version"] = java_match.group(1)
            except Exception as e:
                self._logger.error(f"Error reading build.gradle: {str(e)}")
        
        return result
    
    async def _detect_ruby_version(self, project_dir: Path) -> str:
        """
        Detect Ruby version used in a project.
        
        Args:
            project_dir: Path to the project directory
            
        Returns:
            Ruby version string
        """
        # Check .ruby-version file
        ruby_version_file = project_dir / ".ruby-version"
        if ruby_version_file.exists():
            try:
                with open(ruby_version_file, 'r') as f:
                    version = f.read().strip()
                    if version:
                        return version
            except Exception as e:
                self._logger.error(f"Error reading .ruby-version: {str(e)}")
        
        # Check Gemfile
        gemfile_path = project_dir / "Gemfile"
        if gemfile_path.exists():
            try:
                with open(gemfile_path, 'r') as f:
                    content = f.read()
                    # Look for ruby directive
                    ruby_match = re.search(r'ruby\s+[\'"](\d+\.\d+\.\d+)[\'"]', content)
                    if ruby_match:
                        return ruby_match.group(1)
            except Exception as e:
                self._logger.error(f"Error reading Gemfile: {str(e)}")
        
        # Default to 3.1 if no specific version found
        return "3.1"
    
    async def detect_services(self, project_directory: Union[str, Path]) -> Dict[str, Any]:
        """
        Detect potential services in a project.
        
        Args:
            project_directory: Path to the project directory
            
        Returns:
            Dictionary with detected services
        """
        self._logger.info(f"Detecting services in project: {project_directory}")
        
        project_dir = Path(project_directory)
        if not project_dir.exists() or not project_dir.is_dir():
            return {
                "success": False,
                "error": f"Project directory does not exist or is not a directory: {project_directory}"
            }
        
        # Get project type
        project_type_info = await self.detect_project_type(project_dir)
        if not project_type_info["success"]:
            return project_type_info
        
        primary_type = project_type_info["primary_type"]
        version_info = project_type_info["version_info"]
        
        # Detect entry points based on project type
        entry_points = await self._detect_entry_points(project_dir, primary_type)
        
        # Detect ports used by the application
        ports = await self._detect_ports(project_dir, primary_type)
        
        # Detect dependencies that might indicate additional services
        dependencies = await self._detect_dependencies(project_dir, primary_type)
        
        # Detect databases
        databases = await self._detect_databases(project_dir, primary_type, dependencies)
        
        # Build services map
        services = {}
        
        # Main service for the project
        services["app"] = {
            "type": primary_type,
            "entry_point": entry_points.get("main"),
            "ports": ports.get("app", []),
            "build": {
                "context": ".",
                "dockerfile": "Dockerfile"
            }
        }
        
        # Add additional services based on dependencies
        for db_name, db_info in databases.items():
            services[db_name] = {
                "type": "database",
                "image": db_info["image"],
                "ports": db_info["ports"],
                "volumes": db_info["volumes"],
                "environment": db_info["environment"]
            }
        
        # Detect other services based on dockerfiles or compose files
        for docker_dir in project_dir.glob("**/[Dd]ocker"):
            if docker_dir.is_dir():
                for service_dir in docker_dir.glob("*"):
                    if service_dir.is_dir() and service_dir.name not in services:
                        dockerfile = service_dir / "Dockerfile"
                        if dockerfile.exists():
                            services[service_dir.name] = {
                                "type": "custom",
                                "build": {
                                    "context": str(service_dir.relative_to(project_dir)),
                                    "dockerfile": "Dockerfile"
                                }
                            }
        
        return {
            "success": True,
            "primary_type": primary_type,
            "version_info": version_info,
            "entry_points": entry_points,
            "ports": ports,
            "dependencies": dependencies,
            "databases": databases,
            "services": services
        }
    
    async def _detect_entry_points(
        self, 
        project_dir: Path, 
        project_type: str
    ) -> Dict[str, str]:
        """
        Detect entry points for a project.
        
        Args:
            project_dir: Path to the project directory
            project_type: Type of the project
            
        Returns:
            Dictionary with entry points
        """
        entry_points = {"main": None}
        
        if project_type == "python":
            # Check for common Python entry points
            candidates = [
                "app.py", "main.py", "run.py", "server.py", "api.py",
                "src/app.py", "src/main.py", "src/server.py"
            ]
            
            for candidate in candidates:
                candidate_path = project_dir / candidate
                if candidate_path.exists():
                    entry_points["main"] = candidate
                    break
            
            # Check for Flask or Django apps
            if (project_dir / "wsgi.py").exists():
                entry_points["main"] = "wsgi.py"
            elif (project_dir / "manage.py").exists():
                entry_points["main"] = "manage.py"
        
        elif project_type == "node":
            # Check package.json for main or scripts.start
            package_json_path = project_dir / "package.json"
            if package_json_path.exists():
                try:
                    with open(package_json_path, 'r') as f:
                        data = json.load(f)
                        if "main" in data:
                            entry_points["main"] = data["main"]
                        elif "scripts" in data and "start" in data["scripts"]:
                            # Use the start script
                            entry_points["main"] = "npm start"
                except Exception as e:
                    self._logger.error(f"Error reading package.json: {str(e)}")
            
            # Check common Node.js entry points
            if not entry_points["main"]:
                candidates = [
                    "index.js", "server.js", "app.js", "main.js",
                    "src/index.js", "src/server.js", "src/app.js"
                ]
                
                for candidate in candidates:
                    candidate_path = project_dir / candidate
                    if candidate_path.exists():
                        entry_points["main"] = candidate
                        break
        
        elif project_type == "golang":
            # Check for main.go
            candidates = [
                "main.go", "cmd/main.go", "cmd/app/main.go", "cmd/server/main.go"
            ]
            
            for candidate in candidates:
                candidate_path = project_dir / candidate
                if candidate_path.exists():
                    entry_points["main"] = candidate
                    break
        
        elif project_type == "ruby":
            # Check for common Ruby entry points
            candidates = [
                "app.rb", "main.rb", "server.rb", "config.ru"
            ]
            
            for candidate in candidates:
                candidate_path = project_dir / candidate
                if candidate_path.exists():
                    entry_points["main"] = candidate
                    break
        
        # If no entry point found, use a reasonable default
        if not entry_points["main"]:
            if project_type == "python":
                entry_points["main"] = "app.py"
            elif project_type == "node":
                entry_points["main"] = "index.js"
            elif project_type == "golang":
                entry_points["main"] = "main.go"
            elif project_type == "java":
                entry_points["main"] = "src/main/java/Main.java"
            elif project_type == "ruby":
                entry_points["main"] = "app.rb"
        
        return entry_points
    
    async def _detect_ports(
        self, 
        project_dir: Path, 
        project_type: str
    ) -> Dict[str, List[int]]:
        """
        Detect ports used by the application.
        
        Args:
            project_dir: Path to the project directory
            project_type: Type of the project
            
        Returns:
            Dictionary with ports information
        """
        ports = {
            "app": []
        }
        
        # Common default ports
        default_ports = {
            "http": 8080,
            "https": 8443,
            "django": 8000,
            "flask": 5000,
            "express": 3000,
            "react": 3000,
            "vue": 8080,
            "angular": 4200,
            "mongodb": 27017,
            "mysql": 3306,
            "postgresql": 5432,
            "redis": 6379
        }
        
        # Add default port based on project type
        if project_type == "python":
            # Look for common Python web frameworks
            try:
                # Read requirements.txt to check dependencies
                req_file = project_dir / "requirements.txt"
                if req_file.exists():
                    with open(req_file, 'r') as f:
                        requirements = f.read()
                        if "django" in requirements.lower():
                            ports["app"].append(default_ports["django"])
                        elif "flask" in requirements.lower():
                            ports["app"].append(default_ports["flask"])
                        elif "fastapi" in requirements.lower():
                            ports["app"].append(default_ports["http"])
            except Exception as e:
                self._logger.error(f"Error analyzing requirements: {str(e)}")
        
        elif project_type == "node":
            # Default to Express port
            ports["app"].append(default_ports["express"])
            
            # Check package.json for dependencies
            package_json_path = project_dir / "package.json"
            if package_json_path.exists():
                try:
                    with open(package_json_path, 'r') as f:
                        data = json.load(f)
                        dependencies = data.get("dependencies", {})
                        
                        if "react" in dependencies:
                            if "express" not in dependencies:
                                ports["app"] = [default_ports["react"]]
                        elif "vue" in dependencies:
                            ports["app"] = [default_ports["vue"]]
                        elif "angular" in dependencies:
                            ports["app"] = [default_ports["angular"]]
                except Exception as e:
                    self._logger.error(f"Error reading package.json: {str(e)}")
        
        # Look for port references in code files
        detected_ports = await self._scan_for_ports(project_dir)
        if detected_ports:
            # Add detected ports to app ports
            for port in detected_ports:
                if port not in ports["app"]:
                    ports["app"].append(port)
        
        # If no ports detected, use a sensible default
        if not ports["app"]:
            ports["app"].append(default_ports["http"])
        
        return ports
    
    async def _scan_for_ports(self, project_dir: Path) -> List[int]:
        """
        Scan project files for port specifications.
        
        Args:
            project_dir: Path to the project directory
            
        Returns:
            List of detected ports
        """
        detected_ports = set()
        
        # Pattern for common port assignments
        port_patterns = [
            r'(?:PORT|port)\s*=\s*(\d+)',
            r'\.listen\(\s*(\d+)',
            r'port\s*:\s*(\d+)',
            r'port=(\d+)',
            r'"port":\s*(\d+)',
            r"'port':\s*(\d+)",
            r'EXPOSE\s+(\d+)'
        ]
        
        # Limit scanning to common config and source files to avoid binary files
        # and reduce processing time
        file_patterns = [
            "*.py", "*.js", "*.ts", "*.jsx", "*.tsx", "*.go", "*.rb", "*.java",
            "*.yml", "*.yaml", "*.json", "*.env", "*.toml", "*.ini", "Dockerfile"
        ]
        
        for pattern in file_patterns:
            for file_path in project_dir.glob(f"**/{pattern}"):
                if file_path.is_file() and file_path.stat().st_size < 1000000:  # Skip large files
                    try:
                        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                            content = f.read()
                            for port_pattern in port_patterns:
                                matches = re.findall(port_pattern, content)
                                for match in matches:
                                    try:
                                        port = int(match)
                                        if 1 <= port <= 65535:  # Valid port range
                                            detected_ports.add(port)
                                    except ValueError:
                                        pass
                    except Exception as e:
                        # Skip files with encoding or access issues
                        pass
        
        return list(sorted(detected_ports))
    
    async def _detect_dependencies(
        self, 
        project_dir: Path, 
        project_type: str
    ) -> Dict[str, List[str]]:
        """
        Detect dependencies for a project.
        
        Args:
            project_dir: Path to the project directory
            project_type: Type of the project
            
        Returns:
            Dictionary with dependencies
        """
        dependencies = {
            "databases": [],
            "messaging": [],
            "cache": []
        }
        
        # Database keywords to look for
        database_keywords = {
            "mongodb": ["mongodb", "mongoose", "pymongo"],
            "mysql": ["mysql", "sequelize", "mysql-connector"],
            "postgresql": ["postgresql", "postgres", "pg", "psycopg2", "sqlalchemy"],
            "sqlite": ["sqlite", "sqlite3"],
            "redis": ["redis"],
            "elasticsearch": ["elasticsearch", "elastic"],
            "cassandra": ["cassandra"]
        }
        
        # Messaging systems
        messaging_keywords = {
            "rabbitmq": ["rabbitmq", "amqp"],
            "kafka": ["kafka"],
            "activemq": ["activemq"],
            "sqs": ["sqs", "aws-sdk"]
        }
        
        # Cache systems
        cache_keywords = {
            "redis": ["redis"],
            "memcached": ["memcached", "memcache"]
        }
        
        if project_type == "python":
            # Check requirements.txt
            req_file = project_dir / "requirements.txt"
            if req_file.exists():
                try:
                    with open(req_file, 'r') as f:
                        requirements = f.read().lower()
                        
                        # Check for database dependencies
                        for db, keywords in database_keywords.items():
                            if any(keyword in requirements for keyword in keywords):
                                dependencies["databases"].append(db)
                        
                        # Check for messaging dependencies
                        for msg, keywords in messaging_keywords.items():
                            if any(keyword in requirements for keyword in keywords):
                                dependencies["messaging"].append(msg)
                        
                        # Check for cache dependencies
                        for cache, keywords in cache_keywords.items():
                            if any(keyword in requirements for keyword in keywords):
                                dependencies["cache"].append(cache)
                except Exception as e:
                    self._logger.error(f"Error analyzing requirements: {str(e)}")
        
        elif project_type == "node":
            # Check package.json
            package_json_path = project_dir / "package.json"
            if package_json_path.exists():
                try:
                    with open(package_json_path, 'r') as f:
                        data = json.load(f)
                        deps = {**data.get("dependencies", {}), **data.get("devDependencies", {})}
                        deps_str = " ".join(deps.keys()).lower()
                        
                        # Check for database dependencies
                        for db, keywords in database_keywords.items():
                            if any(keyword in deps_str for keyword in keywords):
                                dependencies["databases"].append(db)
                        
                        # Check for messaging dependencies
                        for msg, keywords in messaging_keywords.items():
                            if any(keyword in deps_str for keyword in keywords):
                                dependencies["messaging"].append(msg)
                        
                        # Check for cache dependencies
                        for cache, keywords in cache_keywords.items():
                            if any(keyword in deps_str for keyword in keywords):
                                dependencies["cache"].append(cache)
                except Exception as e:
                    self._logger.error(f"Error reading package.json: {str(e)}")
        
        # Remove duplicates (e.g. redis can be both database and cache)
        for category in dependencies:
            dependencies[category] = list(set(dependencies[category]))
        
        return dependencies
    
    async def _detect_databases(
        self, 
        project_dir: Path, 
        project_type: str,
        dependencies: Dict[str, List[str]]
    ) -> Dict[str, Dict[str, Any]]:
        """
        Detect databases used by the project.
        
        Args:
            project_dir: Path to the project directory
            project_type: Type of the project
            dependencies: Detected dependencies
            
        Returns:
            Dictionary with database configurations
        """
        databases = {}
        
        # Map database names to configurations
        db_configs = {
            "mongodb": {
                "image": "mongo:6",
                "ports": ["27017:27017"],
                "volumes": ["mongodb_data:/data/db"],
                "environment": {
                    "MONGO_INITDB_ROOT_USERNAME": "root",
                    "MONGO_INITDB_ROOT_PASSWORD": "example"
                }
            },
            "mysql": {
                "image": "mysql:8",
                "ports": ["3306:3306"],
                "volumes": ["mysql_data:/var/lib/mysql"],
                "environment": {
                    "MYSQL_ROOT_PASSWORD": "example",
                    "MYSQL_DATABASE": "app"
                }
            },
            "postgresql": {
                "image": "postgres:14",
                "ports": ["5432:5432"],
                "volumes": ["postgres_data:/var/lib/postgresql/data"],
                "environment": {
                    "POSTGRES_PASSWORD": "example",
                    "POSTGRES_USER": "postgres",
                    "POSTGRES_DB": "app"
                }
            },
            "redis": {
                "image": "redis:7",
                "ports": ["6379:6379"],
                "volumes": ["redis_data:/data"],
                "command": "redis-server --appendonly yes"
            },
            "elasticsearch": {
                "image": "elasticsearch:8.6.0",
                "ports": ["9200:9200", "9300:9300"],
                "volumes": ["elasticsearch_data:/usr/share/elasticsearch/data"],
                "environment": {
                    "discovery.type": "single-node",
                    "ES_JAVA_OPTS": "-Xms512m -Xmx512m"
                }
            }
        }
        
        # Add detected databases
        for db in dependencies.get("databases", []):
            if db in db_configs:
                databases[db] = db_configs[db]
        
        # Add Redis if it's used as a cache
        if "redis" in dependencies.get("cache", []) and "redis" not in databases:
            databases["redis"] = db_configs["redis"]
        
        return databases
    
    async def generate_dockerfile(
        self,
        project_directory: Union[str, Path],
        output_file: Optional[Union[str, Path]] = None,
        overwrite: bool = False
    ) -> Dict[str, Any]:
        """
        Generate a Dockerfile for a project.
        
        Args:
            project_directory: Path to the project directory
            output_file: Path to output Dockerfile (default: <project_directory>/Dockerfile)
            overwrite: Whether to overwrite existing Dockerfile
            
        Returns:
            Dictionary with result information
        """
        self._logger.info(f"Generating Dockerfile for project: {project_directory}")
        
        project_dir = Path(project_directory)
        if not project_dir.exists() or not project_dir.is_dir():
            return {
                "success": False,
                "error": f"Project directory does not exist or is not a directory: {project_directory}"
            }
        
        # Determine output file
        if output_file is None:
            output_file = project_dir / "Dockerfile"
        else:
            output_file = Path(output_file)
        
        # Check if file already exists
        if output_file.exists() and not overwrite:
            return {
                "success": False,
                "error": f"Dockerfile already exists at {output_file}. Use overwrite=True to replace it."
            }
        
        # Detect project type
        project_info = await self.detect_project_type(project_dir)
        if not project_info["success"]:
            return project_info
        
        project_type = project_info["primary_type"]
        version_info = project_info["version_info"]
        
        # Detect entry points
        entry_points = await self._detect_entry_points(project_dir, project_type)
        
        # Detect ports
        ports_info = await self._detect_ports(project_dir, project_type)
        app_ports = ports_info.get("app", [8080])  # Default to 8080 if no ports detected
        
        # Generate Dockerfile content based on project type
        dockerfile_content = ""
        
        if project_type == "python":
            # Get Python version
            python_version = version_info.get("python_version", "3.10")
            
            # Format EXPOSE statement
            expose_port = ""
            if app_ports:
                expose_port = f"EXPOSE {app_ports[0]}"
            
            # Get entry point file
            entry_point = entry_points.get("main", "app.py")
            
            # Generate Dockerfile
            dockerfile_content = DOCKERFILE_TEMPLATES["python"].format(
                python_version=python_version,
                entry_point=entry_point,
                expose_port=expose_port
            )
        
        elif project_type == "node":
            # Get Node.js version
            node_version = version_info.get("node_version", "18")
            
            # Format EXPOSE statement
            expose_port = ""
            if app_ports:
                expose_port = f"EXPOSE {app_ports[0]}"
            
            # Check for package lock file
            package_lock = ""
            if (project_dir / "package-lock.json").exists():
                package_lock = "package-lock.json"
            elif (project_dir / "yarn.lock").exists():
                package_lock = "yarn.lock"
            
            # Determine if it's a production build
            production_flag = "--production"
            
            # Generate Dockerfile
            dockerfile_content = DOCKERFILE_TEMPLATES["node"].format(
                node_version=node_version,
                package_lock=package_lock,
                expose_port=expose_port,
                production_flag=production_flag
            )
        
        elif project_type == "golang":
            # Get Go version
            go_version = version_info.get("go_version", "1.19")
            
            # Format EXPOSE statement
            expose_port = ""
            if app_ports:
                expose_port = f"EXPOSE {app_ports[0]}"
            
            # Get main file
            main_file = entry_points.get("main", "main.go")
            
            # Generate Dockerfile
            dockerfile_content = DOCKERFILE_TEMPLATES["golang"].format(
                go_version=go_version,
                main_file=main_file,
                expose_port=expose_port
            )
        
        elif project_type == "java":
            # Get Java version
            java_version = version_info.get("java_version", "17")
            maven_version = version_info.get("maven_version", "3.8")
            jar_file = version_info.get("jar_file", "app.jar")
            
            # Format EXPOSE statement
            expose_port = ""
            if app_ports:
                expose_port = f"EXPOSE {app_ports[0]}"
            
            # Generate Dockerfile
            dockerfile_content = DOCKERFILE_TEMPLATES["java"].format(
                java_version=java_version,
                maven_version=maven_version,
                jar_file=jar_file,
                expose_port=expose_port
            )
        
        elif project_type == "ruby":
            # Get Ruby version
            ruby_version = version_info.get("ruby_version", "3.1")
            
            # Format EXPOSE statement
            expose_port = ""
            if app_ports:
                expose_port = f"EXPOSE {app_ports[0]}"
            
            # Get entry point file
            entry_point = entry_points.get("main", "app.rb")
            
            # Generate Dockerfile
            dockerfile_content = DOCKERFILE_TEMPLATES["ruby"].format(
                ruby_version=ruby_version,
                entry_point=entry_point,
                expose_port=expose_port
            )
        
        else:
            return {
                "success": False,
                "error": f"Unsupported project type: {project_type}",
                "suggestion": "Try specifying a different project type or creating a custom Dockerfile."
            }
        
        # Write Dockerfile
        try:
            with open(output_file, 'w') as f:
                f.write(dockerfile_content)
            
            return {
                "success": True,
                "message": f"Dockerfile generated successfully at {output_file}",
                "dockerfile_path": str(output_file),
                "project_type": project_type,
                "entry_point": entry_points.get("main"),
                "ports": app_ports,
                "content": dockerfile_content
            }
        except Exception as e:
            self._logger.exception(f"Error writing Dockerfile: {str(e)}")
            return {
                "success": False,
                "error": f"Error writing Dockerfile: {str(e)}"
            }
    
    async def generate_docker_compose(
        self,
        project_directory: Union[str, Path],
        output_file: Optional[Union[str, Path]] = None,
        overwrite: bool = False,
        include_databases: bool = True
    ) -> Dict[str, Any]:
        """
        Generate a docker-compose.yml file for a project.
        
        Args:
            project_directory: Path to the project directory
            output_file: Path to output file (default: <project_directory>/docker-compose.yml)
            overwrite: Whether to overwrite existing file
            include_databases: Whether to include detected database services
            
        Returns:
            Dictionary with result information
        """
        self._logger.info(f"Generating docker-compose.yml for project: {project_directory}")
        
        project_dir = Path(project_directory)
        if not project_dir.exists() or not project_dir.is_dir():
            return {
                "success": False,
                "error": f"Project directory does not exist or is not a directory: {project_directory}"
            }
        
        # Determine output file
        if output_file is None:
            output_file = project_dir / "docker-compose.yml"
        else:
            output_file = Path(output_file)
        
        # Check if file already exists
        if output_file.exists() and not overwrite:
            return {
                "success": False,
                "error": f"docker-compose.yml already exists at {output_file}. Use overwrite=True to replace it."
            }
        
        # Detect services
        services_info = await self.detect_services(project_dir)
        if not services_info["success"]:
            return services_info
        
        detected_services_map = services_info["services"] # Renamed to avoid conflict
        
        # Generate docker-compose.yml content
        compose_services_dict: Dict[str, Any] = {} # Use a dictionary to build services
        compose_networks_dict: Dict[str, Any] = {"app-network": {"driver": "bridge"}}
        compose_volumes_dict: Dict[str, Any] = {}
        
        # Check if Dockerfile exists for the main app
        app_dockerfile_exists = (project_dir / "Dockerfile").exists()
        
        # App service
        app_service_config = detected_services_map.get("app", {})
        app_service_name = "app" # Default app service name
        
        if app_service_config:
            app_entry: Dict[str, Any] = {}
            if app_dockerfile_exists:
                app_entry["build"] = {
                    "context": ".",
                    "dockerfile": "Dockerfile"
                }
                # If building, image name is usually not set here, or set to what it will be tagged as
            elif app_service_config.get("image"): # If no Dockerfile, but image specified
                app_entry["image"] = app_service_config["image"]

            if not app_entry.get("build") and not app_entry.get("image"):
                self._logger.warning("App service has neither Dockerfile nor explicit image. Skipping app service in compose.")
            else:
                app_ports_list = app_service_config.get("ports", [8080] if app_dockerfile_exists else [])
                if app_ports_list:
                    app_entry["ports"] = [f"{p}:{p}" for p in app_ports_list]
                
                app_entry["networks"] = ["app-network"]
                # Add other app_service_config like environment, volumes if defined
                if app_service_config.get("environment"):
                    app_entry["environment"] = app_service_config.get("environment")
                if app_service_config.get("volumes"):
                    app_entry["volumes"] = app_service_config.get("volumes")
                
                compose_services_dict[app_service_name] = app_entry

        # Generate database services
        depends_on_list_for_app = []
        if include_databases:
            databases = services_info.get("databases", {})
            for db_name, db_info in databases.items():
                db_entry: Dict[str, Any] = {"image": db_info["image"]}
                
                if db_info.get("ports"):
                    db_entry["ports"] = db_info["ports"] # Assuming they are already in "HOST:CONTAINER" format
                
                db_volume_definitions = db_info.get("volumes", [])
                if db_volume_definitions:
                    db_entry["volumes"] = db_volume_definitions
                    for vol_def in db_volume_definitions:
                        # Extract volume name if it's a named volume definition like "mydata:/data/db"
                        vol_name_match = re.match(r"([^:]+):", vol_def)
                        if vol_name_match:
                            compose_volumes_dict[vol_name_match.group(1)] = {} # Empty definition for named volume

                if db_info.get("environment"):
                    db_entry["environment"] = db_info["environment"]
                
                if db_info.get("command"): # For Redis example
                    db_entry["command"] = db_info["command"]

                db_entry["networks"] = ["app-network"]
                compose_services_dict[db_name] = db_entry
                depends_on_list_for_app.append(db_name)
        
        # Update app service with depends_on if needed
        if app_service_name in compose_services_dict and depends_on_list_for_app:
            compose_services_dict[app_service_name]["depends_on"] = depends_on_list_for_app
        
        # Final compose structure
        final_compose_structure: Dict[str, Any] = {"version": '3.8'} # Use a common recent version
        if compose_services_dict:
            final_compose_structure["services"] = compose_services_dict
        if compose_networks_dict:
            final_compose_structure["networks"] = compose_networks_dict
        if compose_volumes_dict:
            final_compose_structure["volumes"] = compose_volumes_dict
        
        # Write docker-compose.yml
        try:
            with open(output_file, 'w') as f:
                yaml.dump(final_compose_structure, f, sort_keys=False, default_flow_style=False)
            
            return {
                "success": True,
                "message": f"docker-compose.yml generated successfully at {output_file}",
                "compose_file_path": str(output_file),
                "services_included": list(compose_services_dict.keys()),
                "content": yaml.dump(final_compose_structure, sort_keys=False, default_flow_style=False)
            }
        except Exception as e:
            self._logger.exception(f"Error writing docker-compose.yml: {str(e)}")
            return {
                "success": False,
                "error": f"Error writing docker-compose.yml: {str(e)}"
            }

    async def generate_dockerignore(
        self,
        project_directory: Union[str, Path],
        output_file: Optional[Union[str, Path]] = None,
        overwrite: bool = False
    ) -> Dict[str, Any]:
        """
        Generate a .dockerignore file for a project.
        
        Args:
            project_directory: Path to the project directory
            output_file: Path to output file (default: <project_directory>/.dockerignore)
            overwrite: Whether to overwrite existing file
            
        Returns:
            Dictionary with result information
        """
        self._logger.info(f"Generating .dockerignore for project: {project_directory}")
        
        project_dir = Path(project_directory)
        if not project_dir.exists() or not project_dir.is_dir():
            return {
                "success": False,
                "error": f"Project directory does not exist or is not a directory: {project_directory}"
            }
        
        # Determine output file
        if output_file is None:
            output_file = project_dir / ".dockerignore"
        else:
            output_file = Path(output_file)
        
        # Check if file already exists
        if output_file.exists() and not overwrite:
            return {
                "success": False,
                "error": f".dockerignore already exists at {output_file}. Use overwrite=True to replace it."
            }
        
        # Detect project type
        project_info = await self.detect_project_type(project_dir)
        project_type = project_info["primary_type"]
        
        # Common files to ignore
        common_ignores = [
            "**/.git",
            "**/.gitignore",
            "**/.vscode",
            "**/.idea",
            "**/__pycache__",
            "**/node_modules",
            "**/dist",
            "**/build",
            "**/.env",
            "**/.DS_Store",
            "**/*.log",
            "**/*.swp",
            "**/*.swo",
            "Dockerfile",
            "docker-compose.yml",
            "README.md",
            ".dockerignore"
        ]
        
        # Type-specific ignores
        type_specific_ignores = {
            "python": [
                "**/*.pyc",
                "**/*.pyo",
                "**/*.pyd",
                "**/.Python",
                "**/env/",
                "**/venv/",
                "**/.venv/",
                "**/.pytest_cache/",
                "**/.coverage",
                "**/htmlcov/",
                "**/pytestdebug.log"
            ],
            "node": [
                "**/npm-debug.log",
                "**/yarn-debug.log",
                "**/yarn-error.log",
                "**/.pnpm-debug.log",
                "**/coverage/",
                "**/.next/",
                "**/out/",
                "**/docs/",
                "**/.eslintcache"
            ],
            "golang": [
                "**/vendor/",
                "**/*.test",
                "**/coverage.txt",
                "**/coverage.html"
            ],
            "java": [
                "**/target/",
                "**/.gradle/",
                "**/gradle-app.setting",
                "**/.gradletasknamecache",
                "**/bin/",
                "**/out/",
                "**/*.class",
                "**/*.jar"
            ],
            "ruby": [
                "**/.bundle/",
                "**/vendor/bundle",
                "**/lib/bundler/man/",
                "**/.rubocop-*",
                "**/*.gem",
                "**/coverage/"
            ]
        }
        
        # Combine common and type-specific ignores
        ignores = common_ignores.copy()
        if project_type in type_specific_ignores:
            ignores.extend(type_specific_ignores[project_type])
        
        # Sort and remove duplicates
        ignores = sorted(set(ignores))
        
        # Generate .dockerignore content
        content = "\n".join(ignores) + "\n"
        
        # Write .dockerignore
        try:
            with open(output_file, 'w') as f:
                f.write(content)
            
            return {
                "success": True,
                "message": f".dockerignore generated successfully at {output_file}",
                "path": str(output_file),
                "content": content
            }
        except Exception as e:
            self._logger.exception(f"Error writing .dockerignore: {str(e)}")
            return {
                "success": False,
                "error": f"Error writing .dockerignore: {str(e)}"
            }

    async def setup_docker_project(
        self,
        project_directory: Union[str, Path],
        generate_dockerfile: bool = True,
        generate_compose: bool = True,
        generate_dockerignore: bool = True,
        overwrite: bool = False,
        include_databases: bool = True,
        build_image: bool = False
    ) -> Dict[str, Any]:
        """
        Set up a complete Docker environment for a project.
        
        Args:
            project_directory: Path to the project directory
            generate_dockerfile: Whether to generate a Dockerfile
            generate_compose: Whether to generate a docker-compose.yml
            generate_dockerignore: Whether to generate a .dockerignore
            overwrite: Whether to overwrite existing files
            include_databases: Whether to include detected database services
            build_image: Whether to build the Docker image after setup
            
        Returns:
            Dictionary with setup results
        """
        self._logger.info(f"Setting up Docker environment for project: {project_directory}")
        
        project_dir = Path(project_directory)
        if not project_dir.exists() or not project_dir.is_dir():
            return {
                "success": False,
                "error": f"Project directory does not exist or is not a directory: {project_directory}"
            }
        
        results = {
            "success": True,
            "project_directory": str(project_dir),
            "files_generated": []
        }
        
        # Generate Dockerfile if requested
        if generate_dockerfile:
            dockerfile_result = await self.generate_dockerfile(
                project_dir,
                overwrite=overwrite
            )
            results["dockerfile"] = dockerfile_result
            
            if dockerfile_result["success"]:
                results["files_generated"].append(dockerfile_result["dockerfile_path"])
            else:
                # Non-fatal error if file exists and overwrite is False
                if "already exists" in dockerfile_result.get("error", ""):
                    results["dockerfile"]["skipped"] = True
                else:
                    # Fatal error for other issues
                    results["success"] = False
        
        # Generate docker-compose.yml if requested
        if generate_compose:
            compose_result = await self.generate_docker_compose(
                project_dir,
                overwrite=overwrite,
                include_databases=include_databases
            )
            results["docker_compose"] = compose_result
            
            if compose_result["success"]:
                results["files_generated"].append(compose_result["compose_file_path"])
            else:
                # Non-fatal error if file exists and overwrite is False
                if "already exists" in compose_result.get("error", ""):
                    results["docker_compose"]["skipped"] = True
                else:
                    # Fatal error for other issues
                    results["success"] = False
        
        # Generate .dockerignore if requested
        if generate_dockerignore:
            dockerignore_result = await self.generate_dockerignore(
                project_dir,
                overwrite=overwrite
            )
            results["dockerignore"] = dockerignore_result
            
            if dockerignore_result["success"]:
                results["files_generated"].append(dockerignore_result["path"])
            else:
                # Non-fatal error if file exists and overwrite is False
                if "already exists" in dockerignore_result.get("error", ""):
                    results["dockerignore"]["skipped"] = True
                else:
                    # Fatal error for other issues
                    results["success"] = False
        
        # Build Docker image if requested and Dockerfile was generated
        if build_image and results["success"] and results.get("dockerfile", {}).get("success", False):
            build_result = await self.build_image(
                context_path=project_dir,
                tag="app:latest"
            )
            results["build_image"] = build_result
            
            if not build_result["success"]:
                # Non-fatal error
                results["build_warnings"] = build_result.get("error", "Unknown build error")
        
        return results

# Global Docker integration instance
docker_integration = DockerIntegration()
</file>

<file path="components/toolchain/enhanced_universal_cli.py">
"""
Enhanced Universal CLI Integration for Angela CLI.

This module provides an extended interface to the Universal CLI Translator
with improved context awareness and tool chaining capabilities.
"""
import asyncio
import os
import re
import json
import shlex
from typing import Dict, Any, List, Optional, Set, Union, Tuple
from pathlib import Path

from angela.utils.logging import get_logger
from angela.core.registry import registry
# Updated imports to use API layer
from angela.api.context import get_context_manager
from angela.api.ai import get_gemini_client, GeminiRequest
from angela.api.shell import get_terminal_formatter
from angela.api.execution import get_execution_engine

logger = get_logger(__name__)

class EnhancedUniversalCLI:
    """
    Enhanced interface to the Universal CLI Translator with improved context
    awareness and tool chaining capabilities.
    """
    
    def __init__(self):
        """Initialize the enhanced universal CLI."""
        self._logger = logger
        self._command_history = {}  # Tool -> List of recent commands
        self._translator = None
    
    def initialize(self):
        """Initialize the translator."""
        self._translator = registry.get("universal_cli_translator")
        if not self._translator:
            try:
                # Updated to use API layer
                from angela.api.toolchain import get_universal_cli_translator
                self._translator = get_universal_cli_translator()
                
                # If the import worked but returned None, create a new instance
                if self._translator is None:
                    # Fall back to direct import if API doesn't return an instance
                    from angela.components.toolchain.universal_cli import UniversalCLITranslator
                    self._translator = UniversalCLITranslator()
                    self._logger.warning("Created new UniversalCLITranslator instance as fallback")
                    
                # Register the instance we found or created
                registry.register("universal_cli_translator", self._translator)
                self._logger.info("Registered universal_cli_translator from enhanced CLI")
            except ImportError:
                self._logger.error("Failed to import Universal CLI Translator - filename might be incorrect")
                return False
            except Exception as e:
                self._logger.error(f"Unexpected error initializing Universal CLI Translator: {e}")
                return False
        
        return self._translator is not None
    
    async def translate_with_context(
        self, 
        request: str, 
        tool: Optional[str] = None,
        context: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Translate a natural language request to a command with enhanced context awareness.
        
        Args:
            request: Natural language request
            tool: Optional specific tool to use
            context: Optional context information
            
        Returns:
            Dictionary with the translation result
        """
        self._logger.info(f"Translating request with context: {request}")
        
        if not self._translator:
            if not self.initialize():
                return {
                    "success": False,
                    "error": "Universal CLI Translator not available"
                }
        
        # Get context if not provided
        if context is None:
            # Get context manager from API
            context_manager = get_context_manager()
            context = context_manager.get_context_dict()
        
        # Extract tool from request if not specified
        if not tool:
            tool_match = re.search(r'\b(use|run|with|using)\s+(?:the\s+)?([a-zA-Z0-9_\-]+)', request, re.IGNORECASE)
            if tool_match:
                tool = tool_match.group(2).lower()
        
        # If tool is still not determined, analyze the request to guess
        if not tool:
            tool = await self._guess_tool_from_request(request)
        
        # Enhance the request with context awareness
        enhanced_request = request
        
        # Add context for specific tools
        if tool == "git":
            enhanced_request = await self._enhance_git_request(request, context)
        elif tool in ["docker", "docker-compose"]:
            enhanced_request = await self._enhance_docker_request(request, context)
        elif tool in ["aws", "gcloud", "az"]:
            enhanced_request = await self._enhance_cloud_request(request, context, tool)
        elif tool in ["npm", "yarn", "pip", "gem"]:
            enhanced_request = await self._enhance_package_request(request, context, tool)
        
        # Create an enhanced context for the translator
        enhanced_context = {
            **context,
            "tool": tool,
            "command_history": self._command_history.get(tool, [])
        }
        
        # Log the enhancement
        if enhanced_request != request:
            self._logger.debug(f"Enhanced request: {enhanced_request}")
        
        # Translate the request
        result = await self._translator.translate_request(enhanced_request, enhanced_context)
        
        # If successful, update command history
        if result.get("success", False) and "command" in result:
            if tool not in self._command_history:
                self._command_history[tool] = []
            
            # Add to history, limited to 10 commands
            self._command_history[tool].append(result["command"])
            if len(self._command_history[tool]) > 10:
                self._command_history[tool] = self._command_history[tool][-10:]
        
        return result
    
    async def _guess_tool_from_request(self, request: str) -> Optional[str]:
        """
        Guess which tool a request is likely for based on content analysis.
        
        Args:
            request: Natural language request
            
        Returns:
            Tool name or None if not determined
        """
        # Use AI to analyze the request
        prompt = f"""
Analyze this request to determine which command-line tool it's most likely related to:
"{request}"

Consider common CLI tools like:
- git (version control)
- docker or docker-compose (containers)
- npm, yarn, pip (package managers)
- aws, gcloud, az (cloud CLIs)
- kubectl (Kubernetes)
- terraform (Infrastructure as Code)
- ansible (configuration management)
- curl, wget (HTTP tools)

Return just the tool name in lowercase, nothing else.
"""

        try:
            # Get gemini client from API
            gemini_client = get_gemini_client()
            
            # Call AI service
            api_request = GeminiRequest(prompt=prompt, max_tokens=10)
            response = await gemini_client.generate_text(api_request)
            
            # Extract the tool name (should be a single word)
            tool = response.text.strip().lower()
            
            # Basic validation - tool should be a single word with allowed characters
            if re.match(r'^[a-z0-9_\-]+$', tool):
                self._logger.debug(f"Guessed tool from request: {tool}")
                return tool
            
            return None
            
        except Exception as e:
            self._logger.error(f"Error guessing tool from request: {str(e)}")
            return None
    
    async def _enhance_git_request(self, request: str, context: Dict[str, Any]) -> str:
        """
        Enhance a Git-related request with context information.
        
        Args:
            request: Original request
            context: Context information
            
        Returns:
            Enhanced request
        """
        # Get Git status if we have a project root
        project_root = context.get("project_root")
        if not project_root:
            return request
        
        # Check if "project_state" has Git information already
        if "project_state" in context and "git_state" in context["project_state"]:
            git_state = context["project_state"]["git_state"]
        else:
            # Get Git status
            try:
                # Get execution engine from API
                execution_engine = get_execution_engine()
                
                stdout, stderr, return_code = await execution_engine.execute_command(
                    command="git status --porcelain",
                    check_safety=True,
                    working_dir=project_root
                )
                
                if return_code == 0:
                    # Parse porcelain status
                    changes = []
                    for line in stdout.splitlines():
                        if line.strip():
                            changes.append(line.strip())
                    
                    git_state = {
                        "is_git_repo": True,
                        "has_changes": len(changes) > 0,
                        "changes": changes
                    }
                    
                    # Get branch name
                    stdout, stderr, return_code = await execution_engine.execute_command(
                        command="git rev-parse --abbrev-ref HEAD",
                        check_safety=True,
                        working_dir=project_root
                    )
                    
                    if return_code == 0:
                        git_state["current_branch"] = stdout.strip()
                else:
                    git_state = {"is_git_repo": False}
            except Exception as e:
                self._logger.debug(f"Error getting Git status: {str(e)}")
                git_state = {"is_git_repo": False}
        
        # Enhance the request with Git information
        if git_state.get("is_git_repo", False):
            enhanced_request = f"{request}\n\nContext: "
            
            if "current_branch" in git_state:
                enhanced_request += f"On branch {git_state['current_branch']}. "
            
            if git_state.get("has_changes", False):
                changes_count = len(git_state.get("changes", []))
                enhanced_request += f"Working tree has {changes_count} changes. "
            else:
                enhanced_request += f"Working tree clean. "
            
            return enhanced_request
        
        return request
    
    async def _enhance_docker_request(self, request: str, context: Dict[str, Any]) -> str:
        """
        Enhance a Docker-related request with context information.
        
        Args:
            request: Original request
            context: Context information
            
        Returns:
            Enhanced request
        """
        # Check for Docker Compose file in current directory
        cwd = context.get("cwd", os.getcwd())
        
        compose_files = [
            os.path.join(cwd, "docker-compose.yml"),
            os.path.join(cwd, "docker-compose.yaml"),
            os.path.join(cwd, "compose.yml"),
            os.path.join(cwd, "compose.yaml")
        ]
        
        has_compose = any(os.path.exists(f) for f in compose_files)
        
        # Check for Dockerfile in current directory
        has_dockerfile = os.path.exists(os.path.join(cwd, "Dockerfile"))
        
        # Get running containers
        try:
            # Updated to use API layer
            execution_engine = get_execution_engine()
            stdout, stderr, return_code = await execution_engine.execute_command(
                command="docker ps --format '{{.Names}}'",
                check_safety=True
            )
            
            if return_code == 0:
                containers = [c for c in stdout.strip().split('\n') if c]
            else:
                containers = []
        except Exception as e:
            self._logger.debug(f"Error getting Docker containers: {str(e)}")
            containers = []
        
        # Enhance the request with Docker information
        enhanced_request = f"{request}\n\nContext: "
        
        if has_compose:
            enhanced_request += f"Docker Compose file present in the current directory. "
        
        if has_dockerfile:
            enhanced_request += f"Dockerfile present in the current directory. "
        
        if containers:
            enhanced_request += f"Running containers: {', '.join(containers)}. "
        elif return_code == 0:  # Docker command worked but no containers
            enhanced_request += f"No running containers. "
        
        return enhanced_request
    
    async def _enhance_cloud_request(self, request: str, context: Dict[str, Any], tool: str) -> str:
        """
        Enhance a cloud CLI-related request with context information.
        
        Args:
            request: Original request
            context: Context information
            tool: The cloud CLI tool (aws, gcloud, az)
            
        Returns:
            Enhanced request
        """
        # Get cloud configuration
        config_info = ""
        
        try:
            # Get current profile/account/project
            if tool == "aws":
                # Updated to use API layer
                execution_engine = get_execution_engine()
                stdout, stderr, return_code = await execution_engine.execute_command(
                    command="aws configure get region",
                    check_safety=True
                )
                
                if return_code == 0:
                    region = stdout.strip()
                    config_info += f"AWS Region: {region}. "
                
                stdout, stderr, return_code = await execution_engine.execute_command(
                    command="aws configure get profile",
                    check_safety=True
                )
                
                if return_code == 0 and stdout.strip():
                    profile = stdout.strip()
                    config_info += f"AWS Profile: {profile}. "
            
            elif tool == "gcloud":
                # Updated to use API layer
                execution_engine = get_execution_engine()
                stdout, stderr, return_code = await execution_engine.execute_command(
                    command="gcloud config get-value project",
                    check_safety=True
                )
                
                if return_code == 0 and stdout.strip():
                    project = stdout.strip()
                    config_info += f"GCP Project: {project}. "
                
                stdout, stderr, return_code = await execution_engine.execute_command(
                    command="gcloud config get-value account",
                    check_safety=True
                )
                
                if return_code == 0 and stdout.strip():
                    account = stdout.strip()
                    config_info += f"GCP Account: {account}. "
            
            elif tool == "az":
                # Updated to use API layer
                execution_engine = get_execution_engine()
                stdout, stderr, return_code = await execution_engine.execute_command(
                    command="az account show --query name -o tsv",
                    check_safety=True
                )
                
                if return_code == 0 and stdout.strip():
                    account = stdout.strip()
                    config_info += f"Azure Subscription: {account}. "
        except Exception as e:
            self._logger.debug(f"Error getting cloud configuration: {str(e)}")
        
        # Only enhance if we have configuration information
        if config_info:
            return f"{request}\n\nContext: {config_info}"
        
        return request
    
    async def _enhance_package_request(self, request: str, context: Dict[str, Any], tool: str) -> str:
        """
        Enhance a package manager-related request with context information.
        
        Args:
            request: Original request
            context: Context information
            tool: The package manager (npm, yarn, pip, gem)
            
        Returns:
            Enhanced request
        """
        # Get project information
        project_root = context.get("project_root")
        if not project_root:
            return request
        
        # Check for package manager files
        package_files = {
            "npm": os.path.join(project_root, "package.json"),
            "yarn": os.path.join(project_root, "package.json"),
            "pip": [
                os.path.join(project_root, "requirements.txt"),
                os.path.join(project_root, "pyproject.toml"),
                os.path.join(project_root, "setup.py")
            ],
            "gem": os.path.join(project_root, "Gemfile")
        }
        
        # Check for the relevant package file
        if tool in ["npm", "yarn", "gem"]:
            file_path = package_files[tool]
            has_file = os.path.exists(file_path)
            
            if has_file:
                # Get basic package info
                try:
                    if tool in ["npm", "yarn"]:
                        with open(file_path, 'r') as f:
                            package_data = json.load(f)
                        
                        package_info = f"Project: {package_data.get('name', 'Unknown')}. "
                        package_info += f"Version: {package_data.get('version', 'Unknown')}. "
                        
                        deps_count = len(package_data.get("dependencies", {}))
                        dev_deps_count = len(package_data.get("devDependencies", {}))
                        
                        package_info += f"Dependencies: {deps_count}. "
                        package_info += f"Dev Dependencies: {dev_deps_count}. "
                        
                        return f"{request}\n\nContext: {package_info}"
                    
                    elif tool == "gem":
                        # Basic Gemfile info
                        with open(file_path, 'r') as f:
                            gemfile = f.read()
                        
                        # Count gem lines
                        gem_count = len(re.findall(r'^\s*gem\s+', gemfile, re.MULTILINE))
                        
                        package_info = f"Gemfile present with approximately {gem_count} gems. "
                        
                        return f"{request}\n\nContext: {package_info}"
                except Exception as e:
                    self._logger.debug(f"Error reading package file: {str(e)}")
        
        elif tool == "pip":
            # Check multiple possible Python package files
            files = package_files[tool]
            if not isinstance(files, list):
                files = [files]
            
            found_files = [f for f in files if os.path.exists(f)]
            
            if found_files:
                package_info = f"Python package files found: {', '.join([os.path.basename(f) for f in found_files])}. "
                
                # Try to count dependencies in requirements.txt if it exists
                req_txt = os.path.join(project_root, "requirements.txt")
                if os.path.exists(req_txt):
                    try:
                        with open(req_txt, 'r') as f:
                            req_content = f.read()
                        
                        # Count non-empty, non-comment lines
                        req_count = len([line for line in req_content.splitlines() 
                                        if line.strip() and not line.strip().startswith('#')])
                        
                        package_info += f"Requirements: approximately {req_count} packages. "
                    except Exception as e:
                        self._logger.debug(f"Error reading requirements.txt: {str(e)}")
                
                return f"{request}\n\nContext: {package_info}"
        
        return request
    
    async def get_supported_tools(self) -> List[str]:
        """
        Get a list of supported tools on the system.
        
        Returns:
            List of available CLI tools
        """
        if not self._translator:
            if not self.initialize():
                return []
        
        # Start with common tools to check
        common_tools = [
            "git", "docker", "docker-compose", "npm", "yarn", "pip", "gem",
            "aws", "gcloud", "az", "kubectl", "terraform", "ansible",
            "vagrant", "ssh", "scp", "rsync", "curl", "wget"
        ]
        
        available_tools = set()
        
        for tool in common_tools:
            try:
                suggestions = await self._translator.get_tool_suggestions(tool)
                if tool in suggestions:
                    available_tools.add(tool)
            except Exception as e:
                self._logger.debug(f"Error checking tool {tool}: {str(e)}")
        
        # Get additional tools from the universal CLI translator
        try:
            all_suggestions = await self._translator.get_tool_suggestions("")
            available_tools.update(all_suggestions)
        except Exception as e:
            self._logger.debug(f"Error getting additional tools: {str(e)}")
        
        return sorted(available_tools)
    
    async def get_tool_command_suggestions(self, tool: str, context: Dict[str, Any]) -> List[str]:
        """
        Get commonly used commands for a specific tool based on context.
        
        Args:
            tool: The tool name
            context: Context information
            
        Returns:
            List of suggested commands
        """
        self._logger.debug(f"Getting command suggestions for {tool}")
        
        # Use AI to generate contextual command suggestions
        project_type = context.get("project_type", "unknown")
        project_root = context.get("project_root", "unknown")
        
        prompt = f"""
    Suggest 5 commonly used commands for the CLI tool "{tool}" that would be most relevant for a {project_type} project.
    
    Current context:
    - Project type: {project_type}
    - Project directory: {project_root}
    
    For each command, provide:
    1. The exact command syntax (no explanations in the command itself)
    2. A one-line description of what the command does
    
    Format as JSON:
    {{
      "commands": [
        {{ "command": "command syntax", "description": "what it does" }},
        ...
      ]
    }}
    """
    
        try:
            # Get gemini client from API
            gemini_client = get_gemini_client()
            
            # Call AI service
            api_request = GeminiRequest(prompt=prompt, max_tokens=1000)
            response = await gemini_client.generate_text(api_request)
            
            # Extract JSON
            import json
            import re
            
            # Try to find JSON in the response
            json_match = re.search(r'```(?:json)?\s*(.*?)\s*```', response.text, re.DOTALL)
            if json_match:
                json_str = json_match.group(1)
            else:
                # Assume the entire response is JSON
                json_str = response.text
            
            # Parse JSON
            suggestions_data = json.loads(json_str)
            
            # Extract command suggestions
            commands = []
            for item in suggestions_data.get("commands", []):
                if "command" in item:
                    commands.append(item["command"])
            
            return commands
            
        except Exception as e:
            self._logger.error(f"Error getting command suggestions: {str(e)}")
            
            # Fallback: return common commands for some well-known tools
            common_commands = {
                "git": ["git status", "git add .", "git commit -m 'message'", "git push", "git pull"],
                "docker": ["docker ps", "docker images", "docker build -t name .", "docker run -p 8080:80 name", "docker logs container_name"],
                "npm": ["npm install", "npm start", "npm test", "npm run build", "npm update"],
                "pip": ["pip install -r requirements.txt", "pip list", "pip freeze > requirements.txt", "pip install package", "pip uninstall package"],
                "kubectl": ["kubectl get pods", "kubectl get services", "kubectl apply -f file.yaml", "kubectl describe pod name", "kubectl logs pod_name"]
            }
            
            return common_commands.get(tool, [])

enhanced_universal_cli = EnhancedUniversalCLI()

# Register it in the service registry
registry.register("enhanced_universal_cli", enhanced_universal_cli)

# Initialize on module import
enhanced_universal_cli.initialize()
</file>

<file path="components/toolchain/git.py">
# angela/components/toolchain/git.py

"""
Enhanced Git integration for Angela CLI.

This module provides advanced Git functionality for the code generation lifecycle,
such as automatic repository initialization, commit management, and feature branch creation.
"""
import os
import asyncio
import subprocess
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple, Union
import re

from angela.utils.logging import get_logger
# Updated import to use API layer
from angela.api.execution import get_execution_engine

logger = get_logger(__name__)

class GitIntegration:
    """
    Enhanced Git integration for the code generation lifecycle.
    """
    
    def __init__(self):
        """Initialize the Git integration."""
        self._logger = logger
    
    async def init_repository(
        self, 
        path: Union[str, Path], 
        initial_branch: str = "main",
        gitignore_template: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Initialize a Git repository.
        
        Args:
            path: Path to initialize the repository in
            initial_branch: Name of the initial branch
            gitignore_template: Optional template for .gitignore (e.g., 'python', 'node')
            
        Returns:
            Dictionary with the operation result
        """
        self._logger.info(f"Initializing Git repository in {path}")
        
        path_obj = Path(path)
        
        # Check if path exists
        if not path_obj.exists():
            return {
                "success": False,
                "error": f"Path does not exist: {path}",
                "command": None,
                "stdout": "",
                "stderr": f"Path does not exist: {path}"
            }
        
        # Check if already a Git repository
        if (path_obj / ".git").exists():
            return {
                "success": True,
                "message": "Repository already initialized",
                "command": None,
                "stdout": "Repository already initialized",
                "stderr": ""
            }
        
        # Initialize the repository
        init_command = f"git init -b {initial_branch}"
        
        # Get execution engine from API
        execution_engine = get_execution_engine()
        
        # Execute the command
        stdout, stderr, return_code = await execution_engine.execute_command(
            init_command,
            check_safety=True,
            working_dir=str(path_obj)
        )
        
        if return_code != 0:
            return {
                "success": False,
                "error": f"Failed to initialize repository: {stderr}",
                "command": init_command,
                "stdout": stdout,
                "stderr": stderr
            }
        
        # Create .gitignore if requested
        if gitignore_template:
            gitignore_result = await self._create_gitignore(path_obj, gitignore_template)
            if not gitignore_result["success"]:
                # Continue even if gitignore creation fails
                self._logger.warning(f"Failed to create .gitignore: {gitignore_result['error']}")
        
        return {
            "success": True,
            "message": "Repository initialized successfully",
            "command": init_command,
            "stdout": stdout,
            "stderr": stderr
        }
    
    async def stage_files(
        self, 
        path: Union[str, Path], 
        files: Optional[List[str]] = None
    ) -> Dict[str, Any]:
        """
        Stage files for a Git commit.
        
        Args:
            path: Path to the Git repository
            files: List of files to stage (all files if None)
            
        Returns:
            Dictionary with the operation result
        """
        self._logger.info(f"Staging files in {path}")
        
        path_obj = Path(path)
        
        # Check if path is a Git repository
        if not (path_obj / ".git").exists():
            return {
                "success": False,
                "error": f"Not a Git repository: {path}",
                "command": None,
                "stdout": "",
                "stderr": f"Not a Git repository: {path}"
            }
        
        # Build the git add command
        if files:
            # Quote file paths to handle spaces
            quoted_files = [f'"{f}"' for f in files]
            add_command = f"git add {' '.join(quoted_files)}"
        else:
            add_command = "git add ."
        
        # Get execution engine from API
        execution_engine = get_execution_engine()
        
        # Execute the command
        stdout, stderr, return_code = await execution_engine.execute_command(
            add_command,
            check_safety=True,
            working_dir=str(path_obj)
        )
        
        if return_code != 0:
            return {
                "success": False,
                "error": f"Failed to stage files: {stderr}",
                "command": add_command,
                "stdout": stdout,
                "stderr": stderr
            }
        
        return {
            "success": True,
            "message": "Files staged successfully",
            "command": add_command,
            "stdout": stdout,
            "stderr": stderr
        }
    
    async def commit_changes(
        self, 
        path: Union[str, Path], 
        message: str,
        files: Optional[List[str]] = None,
        auto_stage: bool = True
    ) -> Dict[str, Any]:
        """
        Commit changes to a Git repository.
        
        Args:
            path: Path to the Git repository
            message: Commit message
            files: Optional list of files to commit (all staged files if None)
            auto_stage: Whether to automatically stage files before committing
            
        Returns:
            Dictionary with the operation result
        """
        self._logger.info(f"Committing changes in {path}")
        
        path_obj = Path(path)
        
        # Check if path is a Git repository
        if not (path_obj / ".git").exists():
            return {
                "success": False,
                "error": f"Not a Git repository: {path}",
                "command": None,
                "stdout": "",
                "stderr": f"Not a Git repository: {path}"
            }
        
        # Stage files if requested
        if auto_stage:
            stage_result = await self.stage_files(path_obj, files)
            if not stage_result["success"]:
                return stage_result
        
        # Build the git commit command
        commit_command = f'git commit -m "{message}"'
        
        # Add specific files if provided and not auto-staging
        if files and not auto_stage:
            # Quote file paths to handle spaces
            quoted_files = [f'"{f}"' for f in files]
            commit_command += f" {' '.join(quoted_files)}"
        
        # Get execution engine from API
        execution_engine = get_execution_engine()
        
        # Execute the command
        stdout, stderr, return_code = await execution_engine.execute_command(
            commit_command,
            check_safety=True,
            working_dir=str(path_obj)
        )
        
        if return_code != 0:
            return {
                "success": False,
                "error": f"Failed to commit changes: {stderr}",
                "command": commit_command,
                "stdout": stdout,
                "stderr": stderr
            }
        
        return {
            "success": True,
            "message": "Changes committed successfully",
            "command": commit_command,
            "stdout": stdout,
            "stderr": stderr
        }
    
    async def create_branch(
        self, 
        path: Union[str, Path], 
        branch_name: str,
        checkout: bool = True,
        start_point: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Create a new Git branch.
        
        Args:
            path: Path to the Git repository
            branch_name: Name of the branch to create
            checkout: Whether to check out the new branch
            start_point: Optional starting point for the branch
            
        Returns:
            Dictionary with the operation result
        """
        self._logger.info(f"Creating branch {branch_name} in {path}")
        
        path_obj = Path(path)
        
        # Check if path is a Git repository
        if not (path_obj / ".git").exists():
            return {
                "success": False,
                "error": f"Not a Git repository: {path}",
                "command": None,
                "stdout": "",
                "stderr": f"Not a Git repository: {path}"
            }
        
        # Build the git branch command
        if checkout:
            branch_command = f"git checkout -b {branch_name}"
        else:
            branch_command = f"git branch {branch_name}"
        
        # Add start point if provided
        if start_point:
            branch_command += f" {start_point}"
        
        # Get execution engine from API
        execution_engine = get_execution_engine()
        
        # Execute the command
        stdout, stderr, return_code = await execution_engine.execute_command(
            branch_command,
            check_safety=True,
            working_dir=str(path_obj)
        )
        
        if return_code != 0:
            return {
                "success": False,
                "error": f"Failed to create branch: {stderr}",
                "command": branch_command,
                "stdout": stdout,
                "stderr": stderr
            }
        
        return {
            "success": True,
            "message": f"Branch {branch_name} created successfully",
            "command": branch_command,
            "stdout": stdout,
            "stderr": stderr
        }
    
    async def get_repository_status(
        self, 
        path: Union[str, Path]
    ) -> Dict[str, Any]:
        """
        Get the status of a Git repository.
        
        Args:
            path: Path to the Git repository
            
        Returns:
            Dictionary with the repository status
        """
        path_obj = Path(path)
        
        # Check if path is a Git repository
        if not (path_obj / ".git").exists():
            return {
                "is_repo": False,
                "error": f"Not a Git repository: {path}",
                "command": None,
                "stdout": "",
                "stderr": f"Not a Git repository: {path}"
            }
        
        # Get execution engine from API
        execution_engine = get_execution_engine()
        
        # Get current branch
        branch_command = "git branch --show-current"
        branch_stdout, branch_stderr, branch_code = await execution_engine.execute_command(
            branch_command,
            check_safety=True,
            working_dir=str(path_obj)
        )
        
        current_branch = branch_stdout.strip() if branch_code == 0 else "unknown"
        
        # Get status
        status_command = "git status --porcelain"
        status_stdout, status_stderr, status_code = await execution_engine.execute_command(
            status_command,
            check_safety=True,
            working_dir=str(path_obj)
        )
        
        if status_code != 0:
            return {
                "is_repo": True,
                "current_branch": current_branch,
                "error": f"Failed to get status: {status_stderr}",
                "command": status_command,
                "stdout": status_stdout,
                "stderr": status_stderr
            }
        
        # Parse status output
        status_lines = status_stdout.strip().split('\n') if status_stdout.strip() else []
        
        modified_files = []
        untracked_files = []
        staged_files = []
        
        for line in status_lines:
            if not line:
                continue
                
            status_code = line[:2]
            file_path = line[3:]
            
            if status_code.startswith('??'):
                untracked_files.append(file_path)
            elif status_code.startswith('M'):
                modified_files.append(file_path)
            elif status_code.startswith('A'):
                staged_files.append(file_path)
        
        return {
            "is_repo": True,
            "current_branch": current_branch,
            "modified_files": modified_files,
            "untracked_files": untracked_files,
            "staged_files": staged_files,
            "clean": len(status_lines) == 0,
            "command": status_command,
            "stdout": status_stdout,
            "stderr": status_stderr
        }
    
    async def _create_gitignore(
        self, 
        path: Union[str, Path], 
        template: str
    ) -> Dict[str, Any]:
        """
        Create a .gitignore file from a template.
        
        Args:
            path: Path to the Git repository
            template: Template to use (e.g., 'python', 'node')
            
        Returns:
            Dictionary with the operation result
        """
        self._logger.info(f"Creating .gitignore with template {template} in {path}")
        
        path_obj = Path(path)
        
        # Check if .gitignore already exists
        gitignore_path = path_obj / ".gitignore"
        if gitignore_path.exists():
            return {
                "success": True,
                "message": ".gitignore already exists",
                "path": str(gitignore_path),
                "modified": False
            }
        
        # Get template content
        if template == "python":
            content = """
# Byte-compiled / optimized / DLL files
__pycache__/
*.py[cod]
*$py.class

# Distribution / packaging
dist/
build/
*.egg-info/

# Virtual environments
venv/
env/
.env/
.venv/

# Unit test / coverage reports
htmlcov/
.tox/
.coverage
.coverage.*
.cache
coverage.xml
*.cover

# Local development settings
.env
.env.local

# IDE specific files
.idea/
.vscode/
*.swp
*.swo
"""
        elif template == "node":
            content = """
# Logs
logs
*.log
npm-debug.log*
yarn-debug.log*
yarn-error.log*

# Dependency directories
node_modules/
jspm_packages/

# Build output
dist/
build/

# Environment variables
.env
.env.local
.env.development
.env.test
.env.production

# IDE specific files
.idea/
.vscode/
*.swp
*.swo

# OS specific files
.DS_Store
Thumbs.db
"""
        else:
            # Generic gitignore
            content = """
# IDE specific files
.idea/
.vscode/
*.swp
*.swo

# OS specific files
.DS_Store
Thumbs.db

# Local development settings
.env
.env.local

# Logs
*.log
"""
        
        # Write the .gitignore file
        try:
            with open(gitignore_path, 'w') as f:
                f.write(content.strip())
            
            return {
                "success": True,
                "message": ".gitignore created successfully",
                "path": str(gitignore_path),
                "modified": True
            }
        except Exception as e:
            return {
                "success": False,
                "error": f"Failed to create .gitignore: {str(e)}",
                "path": str(gitignore_path),
                "modified": False
            }

# Global Git integration instance
git_integration = GitIntegration()
</file>

<file path="components/toolchain/package_managers.py">
# angela/components/toolchain/package_managers.py

"""
Package manager integration for Angela CLI.

This module provides functionality for interacting with package managers
to install dependencies required by generated code.
"""
import os
import json
import asyncio
import subprocess
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple, Union
import re

from angela.utils.logging import get_logger
# Updated imports to use API layer
from angela.api.execution import get_execution_engine
from angela.api.context import get_context_manager

logger = get_logger(__name__)

class PackageManagerIntegration:
    """
    Integration with package managers for dependency management.
    """
    
    def __init__(self):
        """Initialize the package manager integration."""
        self._logger = logger
        
        # Map of project types to package managers
        self._package_managers = {
            "python": ["pip", "pipenv", "poetry"],
            "node": ["npm", "yarn", "pnpm"],
            "ruby": ["gem", "bundler"],
            "php": ["composer"],
            "go": ["go"],
            "rust": ["cargo"],
            "java": ["maven", "gradle"]
        }
    
    async def detect_package_manager(
        self, 
        path: Union[str, Path],
        project_type: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Detect the package manager used in a project.
        
        Args:
            path: Path to the project
            project_type: Optional type of project
            
        Returns:
            Dictionary with the detected package manager info
        """
        self._logger.info(f"Detecting package manager in {path}")
        
        path_obj = Path(path)
        
        # Check if path exists
        if not path_obj.exists() or not path_obj.is_dir():
            return {
                "detected": False,
                "error": f"Path does not exist or is not a directory: {path}",
                "package_manager": None,
                "project_type": project_type
            }
        
        # Determine project type if not provided
        if project_type is None:
            # Try to detect from context
            # Get context manager from API
            context_manager = get_context_manager()
            context = context_manager.get_context_dict()
            
            if context.get("project_type"):
                project_type = context["project_type"]
            else:
                # Try to infer from files
                project_type = await self._infer_project_type(path_obj)
        
        # Files that indicate package managers
        package_manager_files = {
            "python": {
                "requirements.txt": "pip",
                "Pipfile": "pipenv",
                "pyproject.toml": "poetry"  # Could also be other tools
            },
            "node": {
                "package.json": "npm",  # Could also be yarn or pnpm
                "yarn.lock": "yarn",
                "pnpm-lock.yaml": "pnpm"
            },
            "ruby": {
                "Gemfile": "bundler"
            },
            "php": {
                "composer.json": "composer"
            },
            "go": {
                "go.mod": "go"
            },
            "rust": {
                "Cargo.toml": "cargo"
            },
            "java": {
                "pom.xml": "maven",
                "build.gradle": "gradle",
                "build.gradle.kts": "gradle"
            }
        }
        
        # Check for package manager files based on project type
        if project_type in package_manager_files:
            for file_name, manager in package_manager_files[project_type].items():
                if (path_obj / file_name).exists():
                    # For Python, check if poetry is actually used in pyproject.toml
                    if file_name == "pyproject.toml" and manager == "poetry":
                        # Check if [tool.poetry] section exists
                        try:
                            with open(path_obj / file_name, 'r') as f:
                                content = f.read()
                                if "[tool.poetry]" not in content:
                                    # Might be another tool, default to pip
                                    manager = "pip"
                        except Exception:
                            manager = "pip"
                    
                    # For Node.js, check if yarn or pnpm is used
                    if file_name == "package.json" and manager == "npm":
                        # If yarn.lock or pnpm-lock.yaml exists, use that instead
                        if (path_obj / "yarn.lock").exists():
                            manager = "yarn"
                        elif (path_obj / "pnpm-lock.yaml").exists():
                            manager = "pnpm"
                    
                    return {
                        "detected": True,
                        "package_manager": manager,
                        "project_type": project_type,
                        "indicator_file": file_name
                    }
        
        # If no specific package manager detected, use default for project type
        if project_type in self._package_managers:
            default_manager = self._package_managers[project_type][0]
            return {
                "detected": False,
                "package_manager": default_manager,
                "project_type": project_type,
                "indicator_file": None,
                "message": f"No package manager detected, defaulting to {default_manager}"
            }
        
        return {
            "detected": False,
            "error": f"Unable to detect package manager for project type: {project_type}",
            "package_manager": None,
            "project_type": project_type
        }
    
    async def install_dependencies(
        self, 
        path: Union[str, Path],
        dependencies: List[str],
        dev_dependencies: Optional[List[str]] = None,
        package_manager: Optional[str] = None,
        project_type: Optional[str] = None,
        update_dependency_file: bool = True,
        virtual_env: bool = False
    ) -> Dict[str, Any]:
        """
        Install dependencies using the appropriate package manager.
        
        Args:
            path: Path to the project
            dependencies: List of dependencies to install
            dev_dependencies: Optional list of development dependencies
            package_manager: Optional package manager to use
            project_type: Optional project type
            update_dependency_file: Whether to update dependency file
            virtual_env: Whether to use a virtual environment for Python
            
        Returns:
            Dictionary with the installation result
        """
        self._logger.info(f"Installing dependencies in {path}")
        
        path_obj = Path(path)
        
        # Check if path exists
        if not path_obj.exists() or not path_obj.is_dir():
            return {
                "success": False,
                "error": f"Path does not exist or is not a directory: {path}",
                "package_manager": package_manager,
                "project_type": project_type
            }
        
        # Detect package manager if not provided
        if package_manager is None or project_type is None:
            detection_result = await self.detect_package_manager(path_obj, project_type)
            package_manager = detection_result.get("package_manager")
            project_type = detection_result.get("project_type")
            
            if not package_manager:
                return {
                    "success": False,
                    "error": f"Unable to detect package manager: {detection_result.get('error', 'Unknown error')}",
                    "package_manager": None,
                    "project_type": project_type
                }
        
        # Install dependencies based on package manager
        if package_manager == "pip":
            return await self._install_pip_dependencies(
                path_obj, dependencies, dev_dependencies, update_dependency_file, virtual_env
            )
        elif package_manager == "npm":
            return await self._install_npm_dependencies(
                path_obj, dependencies, dev_dependencies, update_dependency_file
            )
        elif package_manager == "yarn":
            return await self._install_yarn_dependencies(
                path_obj, dependencies, dev_dependencies, update_dependency_file
            )
        elif package_manager == "poetry":
            return await self._install_poetry_dependencies(
                path_obj, dependencies, dev_dependencies, update_dependency_file
            )
        elif package_manager == "cargo":
            return await self._install_cargo_dependencies(
                path_obj, dependencies, dev_dependencies, update_dependency_file
            )
        # Add other package managers as needed
        
        return {
            "success": False,
            "error": f"Unsupported package manager: {package_manager}",
            "package_manager": package_manager,
            "project_type": project_type
        }
    
    async def _install_pip_dependencies(
        self, 
        path: Path,
        dependencies: List[str],
        dev_dependencies: Optional[List[str]] = None,
        update_dependency_file: bool = True,
        virtual_env: bool = False
    ) -> Dict[str, Any]:
        """
        Install Python dependencies using pip.
        
        Args:
            path: Path to the project
            dependencies: List of dependencies to install
            dev_dependencies: Optional list of development dependencies
            update_dependency_file: Whether to update requirements.txt
            virtual_env: Whether to use a virtual environment
            
        Returns:
            Dictionary with the installation result
        """
        self._logger.info(f"Installing Python dependencies with pip in {path}")
        
        results = {
            "success": True,
            "package_manager": "pip",
            "project_type": "python",
            "commands": [],
            "outputs": [],
            "errors": []
        }
        
        # Get execution engine from API
        execution_engine = get_execution_engine()
        
        # Create virtual environment if requested
        if virtual_env and not (path / "venv").exists():
            venv_command = "python -m venv venv"
            venv_stdout, venv_stderr, venv_code = await execution_engine.execute_command(
                venv_command,
                check_safety=True,
                working_dir=str(path)
            )
            
            results["commands"].append(venv_command)
            results["outputs"].append(venv_stdout)
            
            if venv_code != 0:
                results["success"] = False
                results["errors"].append(f"Failed to create virtual environment: {venv_stderr}")
                return results
        
        # Determine pip command
        pip_cmd = "venv/bin/pip" if virtual_env and (path / "venv").exists() else "pip"
        
        # Install dependencies
        if dependencies:
            deps_str = " ".join(dependencies)
            install_command = f"{pip_cmd} install {deps_str}"
            
            install_stdout, install_stderr, install_code = await execution_engine.execute_command(
                install_command,
                check_safety=True,
                working_dir=str(path)
            )
            
            results["commands"].append(install_command)
            results["outputs"].append(install_stdout)
            
            if install_code != 0:
                results["success"] = False
                results["errors"].append(f"Failed to install dependencies: {install_stderr}")
                return results
        
        
        # Install dev dependencies
        if dev_dependencies:
            dev_deps_str = " ".join(dev_dependencies)
            dev_install_command = f"{pip_cmd} install {dev_deps_str}"
            
            dev_stdout, dev_stderr, dev_code = await execution_engine.execute_command(
                dev_install_command,
                check_safety=True,
                working_dir=str(path)
            )
            
            results["commands"].append(dev_install_command)
            results["outputs"].append(dev_stdout)
            
            if dev_code != 0:
                results["success"] = False
                results["errors"].append(f"Failed to install dev dependencies: {dev_stderr}")
                return results
        
        # Update requirements.txt if requested
        if update_dependency_file:
            # Check if requirements.txt already exists
            req_file = path / "requirements.txt"
            existing_deps = []
            
            if req_file.exists():
                try:
                    with open(req_file, 'r') as f:
                        existing_deps = [line.strip() for line in f if line.strip() and not line.strip().startswith('#')]
                except Exception as e:
                    results["errors"].append(f"Failed to read requirements.txt: {str(e)}")
            
            # Combine existing and new dependencies
            all_deps = list(set(existing_deps + dependencies))
            
            # Write back to requirements.txt
            try:
                with open(req_file, 'w') as f:
                    for dep in sorted(all_deps):
                        f.write(f"{dep}\n")
                
                results["updated_files"] = [str(req_file)]
            except Exception as e:
                results["errors"].append(f"Failed to update requirements.txt: {str(e)}")
        
        return results
    
    async def _install_npm_dependencies(
        self, 
        path: Path,
        dependencies: List[str],
        dev_dependencies: Optional[List[str]] = None,
        update_dependency_file: bool = True
    ) -> Dict[str, Any]:
        """
        Install Node.js dependencies using npm.
        
        Args:
            path: Path to the project
            dependencies: List of dependencies to install
            dev_dependencies: Optional list of development dependencies
            update_dependency_file: Whether to update package.json
            
        Returns:
            Dictionary with the installation result
        """
        self._logger.info(f"Installing Node.js dependencies with npm in {path}")
        
        results = {
            "success": True,
            "package_manager": "npm",
            "project_type": "node",
            "commands": [],
            "outputs": [],
            "errors": []
        }
        
        # Get execution engine from API
        execution_engine = get_execution_engine()
        
        # Initialize npm project if package.json doesn't exist
        package_json = path / "package.json"
        if not package_json.exists() and update_dependency_file:
            init_command = "npm init -y"
            init_stdout, init_stderr, init_code = await execution_engine.execute_command(
                init_command,
                check_safety=True,
                working_dir=str(path)
            )
            
            results["commands"].append(init_command)
            results["outputs"].append(init_stdout)
            
            if init_code != 0:
                results["success"] = False
                results["errors"].append(f"Failed to initialize npm project: {init_stderr}")
                return results
        
        # Install dependencies
        if dependencies:
            deps_str = " ".join(dependencies)
            install_command = f"npm install --save {deps_str}"
            
            install_stdout, install_stderr, install_code = await execution_engine.execute_command(
                install_command,
                check_safety=True,
                working_dir=str(path)
            )
            
            results["commands"].append(install_command)
            results["outputs"].append(install_stdout)
            
            if install_code != 0:
                results["success"] = False
                results["errors"].append(f"Failed to install dependencies: {install_stderr}")
                return results
        
        # Install dev dependencies
        if dev_dependencies:
            dev_deps_str = " ".join(dev_dependencies)
            dev_install_command = f"npm install --save-dev {dev_deps_str}"
            
            dev_stdout, dev_stderr, dev_code = await execution_engine.execute_command(
                dev_install_command,
                check_safety=True,
                working_dir=str(path)
            )
            
            results["commands"].append(dev_install_command)
            results["outputs"].append(dev_stdout)
            
            if dev_code != 0:
                results["success"] = False
                results["errors"].append(f"Failed to install dev dependencies: {dev_stderr}")
                return results
        
        # Update package.json directly if using npm doesn't work
        if update_dependency_file and package_json.exists() and (dependencies or dev_dependencies):
            try:
                with open(package_json, 'r') as f:
                    package_data = json.load(f)
                
                # Make sure dependencies sections exist
                if dependencies and "dependencies" not in package_data:
                    package_data["dependencies"] = {}
                
                if dev_dependencies and "devDependencies" not in package_data:
                    package_data["devDependencies"] = {}
                
                # Update package.json
                with open(package_json, 'w') as f:
                    json.dump(package_data, f, indent=2)
                
                results["updated_files"] = [str(package_json)]
            except Exception as e:
                results["errors"].append(f"Failed to update package.json: {str(e)}")
        
        return results
    
    async def _install_yarn_dependencies(
        self, 
        path: Path,
        dependencies: List[str],
        dev_dependencies: Optional[List[str]] = None,
        update_dependency_file: bool = True # This param is not directly used by yarn add
                                            # as it always updates package.json and yarn.lock
    ) -> Dict[str, Any]:
        """
        Install Node.js dependencies using yarn.
        
        Args:
            path: Path to the project
            dependencies: List of dependencies to install
            dev_dependencies: Optional list of development dependencies
            update_dependency_file: Whether to update package.json (yarn add does this by default)
            
        Returns:
            Dictionary with the installation result
        """
        self._logger.info(f"Installing Node.js dependencies with yarn in {path}")
        
        results = {
            "success": True,
            "package_manager": "yarn",
            "project_type": "node",
            "commands": [],
            "outputs": [],
            "errors": []
        }
        
        # Initialize yarn project if package.json doesn't exist
        package_json = path / "package.json"
        if not package_json.exists(): # Yarn init creates package.json
            init_command = "yarn init -y"
            init_stdout, init_stderr, init_code = await execution_engine.execute_command(
                init_command,
                check_safety=True,
                working_dir=str(path)
            )
            
            results["commands"].append(init_command)
            results["outputs"].append(init_stdout)
            
            if init_code != 0:
                results["success"] = False
                results["errors"].append(f"Failed to initialize yarn project: {init_stderr}")
                return results
        
        # Install dependencies
        if dependencies:
            deps_str = " ".join(dependencies)
            install_command = f"yarn add {deps_str}"
            
            install_stdout, install_stderr, install_code = await execution_engine.execute_command(
                install_command,
                check_safety=True,
                working_dir=str(path)
            )
            
            results["commands"].append(install_command)
            results["outputs"].append(install_stdout)
            
            if install_code != 0:
                results["success"] = False # Fixed: Added = False
                results["errors"].append(f"Failed to install dependencies: {install_stderr}") # Fixed: Added error reporting
                return results # Fixed: Added early return
        
        # Install dev dependencies
        if dev_dependencies:
            dev_deps_str = " ".join(dev_dependencies)
            dev_install_command = f"yarn add --dev {dev_deps_str}" # or yarn add -D
            
            dev_stdout, dev_stderr, dev_code = await execution_engine.execute_command(
                dev_install_command,
                check_safety=True,
                working_dir=str(path)
            )
            
            results["commands"].append(dev_install_command)
            results["outputs"].append(dev_stdout)
            
            if dev_code != 0:
                results["success"] = False
                results["errors"].append(f"Failed to install dev dependencies: {dev_stderr}")
                return results
        
        if update_dependency_file and package_json.exists():
             results["updated_files"] = [str(package_json), str(path / "yarn.lock")]

        return results
    
    async def _install_poetry_dependencies(
        self, 
        path: Path,
        dependencies: List[str],
        dev_dependencies: Optional[List[str]] = None,
        update_dependency_file: bool = True
    ) -> Dict[str, Any]:
        """
        Install Python dependencies using Poetry.
        
        Args:
            path: Path to the project
            dependencies: List of dependencies to install
            dev_dependencies: Optional list of development dependencies
            update_dependency_file: Whether to update pyproject.toml
            
        Returns:
            Dictionary with the installation result
        """
        self._logger.info(f"Installing Python dependencies with Poetry in {path}")
        
        results = {
            "success": True,
            "package_manager": "poetry",
            "project_type": "python",
            "commands": [],
            "outputs": [],
            "errors": []
        }
        
        # Initialize poetry project if pyproject.toml doesn't exist
        pyproject_toml = path / "pyproject.toml"
        if not pyproject_toml.exists() and update_dependency_file:
            init_command = "poetry init --no-interaction"
            init_stdout, init_stderr, init_code = await execution_engine.execute_command(
                init_command,
                check_safety=True,
                working_dir=str(path)
            )
            
            results["commands"].append(init_command)
            results["outputs"].append(init_stdout)
            
            if init_code != 0:
                results["success"] = False
                results["errors"].append(f"Failed to initialize Poetry project: {init_stderr}")
                return results
        
        # Install dependencies
        if dependencies:
            for dep in dependencies:
                install_command = f"poetry add {dep}"
                
                install_stdout, install_stderr, install_code = await execution_engine.execute_command(
                    install_command,
                    check_safety=True,
                    working_dir=str(path)
                )
                
                results["commands"].append(install_command)
                results["outputs"].append(install_stdout)
                
                if install_code != 0:
                    results["success"] = False
                    results["errors"].append(f"Failed to install dependency {dep}: {install_stderr}")
                    return results
        
        # Install dev dependencies
        if dev_dependencies:
            for dev_dep in dev_dependencies:
                dev_install_command = f"poetry add --dev {dev_dep}"
                
                dev_stdout, dev_stderr, dev_code = await execution_engine.execute_command(
                    dev_install_command,
                    check_safety=True,
                    working_dir=str(path)
                )
                
                results["commands"].append(dev_install_command)
                results["outputs"].append(dev_stdout)
                
                if dev_code != 0:
                    results["success"] = False
                    results["errors"].append(f"Failed to install dev dependency {dev_dep}: {dev_stderr}")
                    return results
        
        return results
    
    async def _install_cargo_dependencies(
        self, 
        path: Path,
        dependencies: List[str],
        dev_dependencies: Optional[List[str]] = None,
        update_dependency_file: bool = True
    ) -> Dict[str, Any]:
        """
        Install Rust dependencies using Cargo.
        
        Args:
            path: Path to the project
            dependencies: List of dependencies to install
            dev_dependencies: Optional list of development dependencies
            update_dependency_file: Whether to update Cargo.toml
            
        Returns:
            Dictionary with the installation result
        """
        self._logger.info(f"Installing Rust dependencies with Cargo in {path}")
        
        results = {
            "success": True,
            "package_manager": "cargo",
            "project_type": "rust",
            "commands": [],
            "outputs": [],
            "errors": []
        }
        
        # Check if this is a Cargo project
        cargo_toml = path / "Cargo.toml"
        if not cargo_toml.exists():
            if update_dependency_file:
                # Initialize a new Cargo project
                project_name = path.name.replace("-", "_").lower()
                init_command = f"cargo init --name {project_name}"
                
                init_stdout, init_stderr, init_code = await execution_engine.execute_command(
                    init_command,
                    check_safety=True,
                    working_dir=str(path)
                )
                
                results["commands"].append(init_command)
                results["outputs"].append(init_stdout)
                
                if init_code != 0:
                    results["success"] = False
                    results["errors"].append(f"Failed to initialize Cargo project: {init_stderr}")
                    return results
            else:
                results["success"] = False
                results["errors"].append("Not a Cargo project and update_dependency_file is False")
                return results
        
        # Add dependencies to Cargo.toml
        if (dependencies or dev_dependencies) and update_dependency_file:
            try:
                with open(cargo_toml, 'r') as f:
                    cargo_content = f.read()
                
                # Add [dependencies] section if it doesn't exist
                if dependencies and "[dependencies]" not in cargo_content:
                    cargo_content += "\n[dependencies]\n"
                
                # Add dependencies
                if dependencies:
                    for dep in dependencies:
                        # Check if dependency is already in the file
                        if dep not in cargo_content:
                            # Parse dependency name and version (if provided)
                            if "=" in dep:
                                dep_name, dep_version = dep.split("=", 1)
                                cargo_content += f'{dep_name.strip()} = {dep_version.strip()}\n'
                            else:
                                cargo_content += f'{dep.strip()} = "*"\n'
                
                # Add [dev-dependencies] section if it doesn't exist
                if dev_dependencies and "[dev-dependencies]" not in cargo_content:
                    cargo_content += "\n[dev-dependencies]\n"
                
                # Add dev dependencies
                if dev_dependencies:
                    for dep in dev_dependencies:
                        # Check if dependency is already in the file
                        if dep not in cargo_content:
                            # Parse dependency name and version (if provided)
                            if "=" in dep:
                                dep_name, dep_version = dep.split("=", 1)
                                cargo_content += f'{dep_name.strip()} = {dep_version.strip()}\n'
                            else:
                                cargo_content += f'{dep.strip()} = "*"\n'
                
                # Write back to Cargo.toml
                with open(cargo_toml, 'w') as f:
                    f.write(cargo_content)
                
                results["updated_files"] = [str(cargo_toml)]
            except Exception as e:
                results["errors"].append(f"Failed to update Cargo.toml: {str(e)}")
        
        # Run cargo build to install dependencies
        build_command = "cargo build"
        build_stdout, build_stderr, build_code = await execution_engine.execute_command(
            build_command,
            check_safety=True,
            working_dir=str(path)
        )
        
        results["commands"].append(build_command)
        results["outputs"].append(build_stdout)
        
        if build_code != 0:
            results["success"] = False
            results["errors"].append(f"Failed to build project: {build_stderr}")
            return results
        
        return results
    
    async def _infer_project_type(self, path: Path) -> Optional[str]:
        """
        Infer the project type from the files in the directory.
        
        Args:
            path: Path to the project
            
        Returns:
            Inferred project type, or None if unable to infer
        """
        # Check for key files that indicate project type
        if (path / "requirements.txt").exists() or (path / "setup.py").exists() or (path / "pyproject.toml").exists():
            return "python"
        elif (path / "package.json").exists():
            return "node"
        elif (path / "Gemfile").exists() or (path / "Gemfile.lock").exists():
            return "ruby"
        elif (path / "composer.json").exists():
            return "php"
        elif (path / "go.mod").exists():
            return "go"
        elif (path / "Cargo.toml").exists():
            return "rust"
        elif (path / "pom.xml").exists() or (path / "build.gradle").exists() or (path / "build.gradle.kts").exists():
            return "java"
        
        # Count file extensions to guess project type
        extensions = {}
        for file_path in path.glob("**/*"):
            if file_path.is_file():
                ext = file_path.suffix.lower()
                if ext:
                    extensions[ext] = extensions.get(ext, 0) + 1
        
        # Determine project type based on most common extension
        if extensions:
            py_exts = extensions.get(".py", 0)
            js_exts = extensions.get(".js", 0) + extensions.get(".jsx", 0) + extensions.get(".ts", 0) + extensions.get(".tsx", 0)
            rb_exts = extensions.get(".rb", 0)
            php_exts = extensions.get(".php", 0)
            go_exts = extensions.get(".go", 0)
            rs_exts = extensions.get(".rs", 0)
            java_exts = extensions.get(".java", 0)
            
            max_ext = max([
                ("python", py_exts),
                ("node", js_exts),
                ("ruby", rb_exts),
                ("php", php_exts),
                ("go", go_exts),
                ("rust", rs_exts),
                ("java", java_exts)
            ], key=lambda x: x[1])
            
            if max_ext[1] > 0:
                return max_ext[0]
        
        return None

# Global package manager integration instance
package_manager_integration = PackageManagerIntegration()
</file>

<file path="components/toolchain/test_frameworks.py">
# angela/components/toolchain/test_frameworks.py
"""
Test framework integration for Angela CLI.

This module provides functionality for integrating with test frameworks
and generating test files for generated code.
"""
import os
import re
import asyncio
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple, Union
import importlib
import inspect

from angela.utils.logging import get_logger

from angela.api.generation import get_code_file_class
from angela.api.context import get_context_manager


logger = get_logger(__name__)
CodeFile = get_code_file_class()

class TestFrameworkIntegration:
    """
    Integration with test frameworks for automated testing.
    """
    
    def __init__(self):
        """Initialize the test framework integration."""
        self._logger = logger
        
        # Map of project types to test frameworks
        self._test_frameworks = {
            "python": ["pytest", "unittest"],
            "node": ["jest", "mocha"],
            "ruby": ["rspec", "minitest"],
            "go": ["go_test"],
            "rust": ["cargo_test"],
            "java": ["junit", "testng"]
        }
    
    async def detect_test_framework(
        self, 
        path: Union[str, Path],
        project_type: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Detect the test framework used in a project.
        
        Args:
            path: Path to the project
            project_type: Optional type of project
            
        Returns:
            Dictionary with the detected test framework info
        """
        self._logger.info(f"Detecting test framework in {path}")
        
        path_obj = Path(path)
        
        # Check if path exists
        if not path_obj.exists() or not path_obj.is_dir():
            return {
                "detected": False,
                "error": f"Path does not exist or is not a directory: {path}",
                "test_framework": None,
                "project_type": project_type
            }
        
        # Determine project type if not provided
        if project_type is None:
            # Try to detect from context
            # Get context manager from API
            context_manager = get_context_manager()
            context = context_manager.get_context_dict()
            
            if context.get("project_type"):
                project_type = context["project_type"]
            else:
                # Try to infer from files
                if (path_obj / "requirements.txt").exists() or (path_obj / "setup.py").exists():
                    project_type = "python"
                elif (path_obj / "package.json").exists():
                    project_type = "node"
                elif (path_obj / "Gemfile").exists():
                    project_type = "ruby"
                elif (path_obj / "go.mod").exists():
                    project_type = "go"
                elif (path_obj / "Cargo.toml").exists():
                    project_type = "rust"
                elif (path_obj / "pom.xml").exists() or (path_obj / "build.gradle").exists():
                    project_type = "java"
        
        # Files that indicate test frameworks
        test_framework_files = {
            "python": {
                "pytest.ini": "pytest",
                "conftest.py": "pytest",
                "test_*.py": "pytest",  # Pattern for pytest files
                "*_test.py": "unittest"  # Pattern for unittest files
            },
            "node": {
                "jest.config.js": "jest",
                "jest.config.ts": "jest",
                "package.json": "jest",  # Need to check content for jest dependency
                "mocha.opts": "mocha",
                ".mocharc.js": "mocha",
                ".mocharc.json": "mocha"
            },
            "ruby": {
                ".rspec": "rspec",
                "spec_helper.rb": "rspec",
                "test_helper.rb": "minitest"
            },
            "go": {
                "*_test.go": "go_test"  # Pattern for Go test files
            },
            "rust": {
                "**/tests/*.rs": "cargo_test"  # Pattern for Rust test files
            },
            "java": {
                "pom.xml": "junit",  # Need to check content for junit dependency
                "build.gradle": "junit"  # Need to check content for junit dependency
            }
        }
        
        # Check for test framework files based on project type
        if project_type in test_framework_files:
            for file_pattern, framework in test_framework_files[project_type].items():
                # Check for exact file matches
                if not file_pattern.startswith("*"):
                    if (path_obj / file_pattern).exists():
                        # For package.json, check if jest is a dependency
                        if file_pattern == "package.json" and framework == "jest":
                            try:
                                import json
                                with open(path_obj / file_pattern, 'r') as f:
                                    package_data = json.load(f)
                                
                                # Check for jest in dependencies or devDependencies
                                deps = package_data.get("dependencies", {})
                                dev_deps = package_data.get("devDependencies", {})
                                
                                if "jest" in deps or "jest" in dev_deps:
                                    return {
                                        "detected": True,
                                        "test_framework": framework,
                                        "project_type": project_type,
                                        "indicator_file": file_pattern
                                    }
                                # If jest not found, check for mocha
                                elif "mocha" in deps or "mocha" in dev_deps:
                                    return {
                                        "detected": True,
                                        "test_framework": "mocha",
                                        "project_type": project_type,
                                        "indicator_file": file_pattern
                                    }
                            except Exception:
                                pass
                        else:
                            return {
                                "detected": True,
                                "test_framework": framework,
                                "project_type": project_type,
                                "indicator_file": file_pattern
                            }
                else:
                    # Check for pattern matches
                    pattern = file_pattern.replace("*", "**")
                    matches = list(path_obj.glob(pattern))
                    if matches:
                        return {
                            "detected": True,
                            "test_framework": framework,
                            "project_type": project_type,
                            "indicator_file": str(matches[0].relative_to(path_obj))
                        }
        
        # If no specific test framework detected, use default for project type
        if project_type in self._test_frameworks:
            default_framework = self._test_frameworks[project_type][0]
            return {
                "detected": False,
                "test_framework": default_framework,
                "project_type": project_type,
                "indicator_file": None,
                "message": f"No test framework detected, defaulting to {default_framework}"
            }
        
        return {
            "detected": False,
            "error": f"Unable to detect test framework for project type: {project_type}",
            "test_framework": None,
            "project_type": project_type
        }
    
    async def generate_test_files(
        self, 
        src_files: List[CodeFile],
        test_framework: Optional[str] = None,
        project_type: Optional[str] = None,
        test_dir: Optional[str] = None,
        root_dir: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Generate test files for source files.
        
        Args:
            src_files: List of source files to generate tests for
            test_framework: Optional test framework to use
            project_type: Optional project type
            test_dir: Optional directory for test files
            root_dir: Optional root directory of the project
            
        Returns:
            Dictionary with the test generation result
        """
        self._logger.info(f"Generating test files for {len(src_files)} source files")
        
        # Get project root
        if root_dir is None:
            # Get context manager from API
            context_manager = get_context_manager()
            context = context_manager.get_context_dict()
            root_dir = context.get("project_root", os.getcwd())
        
        root_path = Path(root_dir)
        
        # Detect project type if not provided
        if project_type is None:
            # Try to detect from context
            # Get context manager from API
            context_manager = get_context_manager()
            context = context_manager.get_context_dict()
            
            if context.get("project_type"):
                project_type = context["project_type"]
            else:
                if (root_path / "requirements.txt").exists() or (root_path / "setup.py").exists():
                    project_type = "python"
                elif (root_path / "package.json").exists():
                    project_type = "node"
                elif (root_path / "Gemfile").exists():
                    project_type = "ruby"
                elif (root_path / "go.mod").exists():
                    project_type = "go"
                elif (root_path / "Cargo.toml").exists():
                    project_type = "rust"
                elif (root_path / "pom.xml").exists() or (root_path / "build.gradle").exists():
                    project_type = "java"
        
        # Detect test framework if not provided
        if test_framework is None:
            detection_result = await self.detect_test_framework(root_path, project_type)
            test_framework = detection_result.get("test_framework")
            
            if not test_framework:
                # Use default for project type
                if project_type in self._test_frameworks:
                    test_framework = self._test_frameworks[project_type][0]
                else:
                    return {
                        "success": False,
                        "error": "Unable to determine test framework",
                        "test_framework": None,
                        "project_type": project_type
                    }
        
        # Determine test directory
        if test_dir is None:
            if project_type == "python":
                if (root_path / "tests").exists():
                    test_dir = "tests"
                else:
                    test_dir = "test"
            elif project_type == "node":
                if (root_path / "tests").exists():
                    test_dir = "tests"
                elif (root_path / "__tests__").exists():
                    test_dir = "__tests__"
                else:
                    test_dir = "test"
            elif project_type == "ruby":
                if (root_path / "spec").exists():
                    test_dir = "spec"
                else:
                    test_dir = "test"
            elif project_type == "go":
                test_dir = "."  # Go tests are usually in the same directory
            elif project_type == "rust":
                test_dir = "tests"
            elif project_type == "java":
                test_dir = "src/test/java"
            else:
                test_dir = "tests"
        
        test_path = root_path / test_dir
        
        # Create test directory if it doesn't exist
        if not test_path.exists():
            os.makedirs(test_path, exist_ok=True)
        
        # Generate test files based on framework
        if test_framework == "pytest":
            return await self._generate_pytest_files(src_files, test_path, root_path)
        elif test_framework == "unittest":
            return await self._generate_unittest_files(src_files, test_path, root_path)
        elif test_framework == "jest":
            return await self._generate_jest_files(src_files, test_path, root_path)
        elif test_framework == "mocha":
            return await self._generate_mocha_files(src_files, test_path, root_path)
        elif test_framework == "go_test":
            return await self._generate_go_test_files(src_files, test_path, root_path)
        # Add other test frameworks as needed
        
        return {
            "success": False,
            "error": f"Unsupported test framework: {test_framework}",
            "test_framework": test_framework,
            "project_type": project_type
        }
    
    async def _generate_pytest_files(
        self, 
        src_files: List[CodeFile],
        test_path: Path,
        root_path: Path
    ) -> Dict[str, Any]:
        """
        Generate pytest test files.
        
        Args:
            src_files: List of source files to generate tests for
            test_path: Path to the test directory
            root_path: Path to the project root
            
        Returns:
            Dictionary with the test generation result
        """
        self._logger.info(f"Generating pytest files in {test_path}")
        
        generated_files = []
        errors = []
        
        # Create conftest.py if it doesn't exist
        conftest_path = test_path / "conftest.py"
        if not conftest_path.exists():
            try:
                with open(conftest_path, 'w') as f:
                    f.write("""
# pytest fixtures and configuration

import pytest

# Define fixtures that can be used by multiple tests
@pytest.fixture
def sample_fixture():
    \"\"\"Example fixture.\"\"\"
    return "sample_value"
""".strip())
                
                generated_files.append(str(conftest_path))
            except Exception as e:
                errors.append(f"Failed to create conftest.py: {str(e)}")
        
        # Generate test file for each source file
        for src_file in src_files:
            # Skip if not a Python file
            if not src_file.path.endswith('.py'):
                continue
            
            # Determine test file path
            src_rel_path = src_file.path
            if src_rel_path.startswith('/'):
                # Convert absolute path to relative
                try:
                    src_rel_path = str(Path(src_rel_path).relative_to(root_path))
                except ValueError:
                    # Not under the project root, use the filename
                    src_rel_path = os.path.basename(src_rel_path)
            
            # Create test filename
            test_filename = f"test_{os.path.basename(src_rel_path)}"
            
            # Create module structure
            module_parts = os.path.dirname(src_rel_path).split('/')
            test_file_dir = test_path
            
            for part in module_parts:
                if part and part != '.':
                    test_file_dir = test_file_dir / part
                    os.makedirs(test_file_dir, exist_ok=True)
                    
                    # Create __init__.py if it doesn't exist
                    init_path = test_file_dir / "__init__.py"
                    if not init_path.exists():
                        try:
                            with open(init_path, 'w') as f:
                                f.write("# Test package initialization")
                            
                            generated_files.append(str(init_path))
                        except Exception as e:
                            errors.append(f"Failed to create {init_path}: {str(e)}")
            
            test_file_path = test_file_dir / test_filename
            
            # Generate test content
            test_content = await self._generate_python_test_content(src_file, "pytest")
            
            # Write the test file
            try:
                with open(test_file_path, 'w') as f:
                    f.write(test_content)
                
                generated_files.append(str(test_file_path))
            except Exception as e:
                errors.append(f"Failed to create {test_file_path}: {str(e)}")
        
        return {
            "success": len(errors) == 0,
            "test_framework": "pytest",
            "project_type": "python",
            "generated_files": generated_files,
            "errors": errors,
            "file_count": len(generated_files)
        }
    
    async def _generate_unittest_files(
        self, 
        src_files: List[CodeFile],
        test_path: Path,
        root_path: Path
    ) -> Dict[str, Any]:
        """
        Generate unittest test files.
        
        Args:
            src_files: List of source files to generate tests for
            test_path: Path to the test directory
            root_path: Path to the project root
            
        Returns:
            Dictionary with the test generation result
        """
        self._logger.info(f"Generating unittest files in {test_path}")
        
        generated_files = []
        errors = []
        
        # Generate test file for each source file
        for src_file in src_files:
            # Skip if not a Python file
            if not src_file.path.endswith('.py'):
                continue
            
            # Determine test file path
            src_rel_path = src_file.path
            if src_rel_path.startswith('/'):
                # Convert absolute path to relative
                try:
                    src_rel_path = str(Path(src_rel_path).relative_to(root_path))
                except ValueError:
                    # Not under the project root, use the filename
                    src_rel_path = os.path.basename(src_rel_path)
            
            # Create test filename (unittest style)
            test_filename = f"{os.path.splitext(os.path.basename(src_rel_path))[0]}_test.py"
            
            # Create module structure
            module_parts = os.path.dirname(src_rel_path).split('/')
            test_file_dir = test_path
            
            for part in module_parts:
                if part and part != '.':
                    test_file_dir = test_file_dir / part
                    os.makedirs(test_file_dir, exist_ok=True)
                    
                    # Create __init__.py if it doesn't exist
                    init_path = test_file_dir / "__init__.py"
                    if not init_path.exists():
                        try:
                            with open(init_path, 'w') as f:
                                f.write("# Test package initialization")
                            
                            generated_files.append(str(init_path))
                        except Exception as e:
                            errors.append(f"Failed to create {init_path}: {str(e)}")
            
            test_file_path = test_file_dir / test_filename
            
            # Generate test content
            test_content = await self._generate_python_test_content(src_file, "unittest")
            
            # Write the test file
            try:
                with open(test_file_path, 'w') as f:
                    f.write(test_content)
                
                generated_files.append(str(test_file_path))
            except Exception as e:
                errors.append(f"Failed to create {test_file_path}: {str(e)}")
        
        return {
            "success": len(errors) == 0,
            "test_framework": "unittest",
            "project_type": "python",
            "generated_files": generated_files,
            "errors": errors,
            "file_count": len(generated_files)
        }
    
    async def _generate_python_test_content(
        self, 
        src_file: CodeFile, 
        framework: str
    ) -> str:
        """
        Generate Python test content for a source file.
        
        Args:
            src_file: Source file to generate test for
            framework: Test framework to use ('pytest' or 'unittest')
            
        Returns:
            Test file content
        """
        # Convert relative path to module import path
        module_path = src_file.path.replace('/', '.').replace('\\', '.')
        if module_path.endswith('.py'):
            module_path = module_path[:-3]
        
        # Check for 'src/' prefix and remove it for imports
        if module_path.startswith('src.'):
            module_path = module_path[4:]
        
        # Analyze source code to extract classes and functions
        classes = []
        functions = []
        
        # Use regex to extract classes and functions
        class_pattern = r'class\s+(\w+)'
        function_pattern = r'def\s+(\w+)\s*\('
        
        for match in re.finditer(class_pattern, src_file.content):
            class_name = match.group(1)
            if not class_name.startswith('_') and class_name != 'Test':
                classes.append(class_name)
        
        for match in re.finditer(function_pattern, src_file.content):
            function_name = match.group(1)
            if not function_name.startswith('_') and function_name not in ['setup', 'teardown']:
                functions.append(function_name)
        
        # Generate test content based on framework
        if framework == "pytest":
            content = f"""
# Test file for {src_file.path}
import pytest
from {module_path} import *

"""
            
            # Add class tests
            for class_name in classes:
                content += f"""
class Test{class_name}:
    def setup_method(self):
        \"\"\"Set up test fixtures.\"\"\"
        self.instance = {class_name}()
    
    def teardown_method(self):
        \"\"\"Tear down test fixtures.\"\"\"
        pass
    
    def test_{class_name.lower()}_initialization(self):
        \"\"\"Test {class_name} initialization.\"\"\"
        assert self.instance is not None
    
    # Add more tests for class methods
"""
            
            # Add function tests
            for function_name in functions:
                content += f"""
def test_{function_name}():
    \"\"\"Test {function_name} function.\"\"\"
    # TODO: Add test implementation
    assert {function_name} is not None
"""
        
        elif framework == "unittest":
            content = f"""
# Test file for {src_file.path}
import unittest
from {module_path} import *

"""
            
            # Add class tests
            for class_name in classes:
                content += f"""
class {class_name}Test(unittest.TestCase):
    def setUp(self):
        \"\"\"Set up test fixtures.\"\"\"
        self.instance = {class_name}()
    
    def tearDown(self):
        \"\"\"Tear down test fixtures.\"\"\"
        pass
    
    def test_{class_name.lower()}_initialization(self):
        \"\"\"Test {class_name} initialization.\"\"\"
        self.assertIsNotNone(self.instance)
    
    # Add more tests for class methods
"""
            
            # Add function tests
            for function_name in functions:
                content += f"""
class {function_name.capitalize()}Test(unittest.TestCase):
    def test_{function_name}(self):
        \"\"\"Test {function_name} function.\"\"\"
        # TODO: Add test implementation
        self.assertIsNotNone({function_name})
"""
            
            # Add main block
            content += """

if __name__ == '__main__':
    unittest.main()
"""
        
        return content.strip()
    
    async def _generate_jest_files(
        self, 
        src_files: List[CodeFile],
        test_path: Path,
        root_path: Path
    ) -> Dict[str, Any]:
        """
        Generate Jest test files.
        
        Args:
            src_files: List of source files to generate tests for
            test_path: Path to the test directory
            root_path: Path to the project root
            
        Returns:
            Dictionary with the test generation result
        """
        self._logger.info(f"Generating Jest files in {test_path}")
        
        generated_files = []
        errors = []
        
        # Create jest.config.js if it doesn't exist
        jest_config_path = root_path / "jest.config.js"
        if not jest_config_path.exists():
            try:
                with open(jest_config_path, 'w') as f:
                    f.write("""
module.exports = {
  testEnvironment: 'node',
  testMatch: ['**/__tests__/**/*.js?(x)', '**/?(*.)+(spec|test).js?(x)'],
  collectCoverage: true,
  coverageDirectory: 'coverage',
  collectCoverageFrom: [
    'src/**/*.{js,jsx}',
    '!**/node_modules/**',
    '!**/vendor/**'
  ]
};
""".strip())
                
                generated_files.append(str(jest_config_path))
            except Exception as e:
                errors.append(f"Failed to create jest.config.js: {str(e)}")
        
        # Generate test file for each source file
        for src_file in src_files:
            # Skip if not a JavaScript file
            if not src_file.path.endswith(('.js', '.jsx', '.ts', '.tsx')):
                continue
            
            # Determine test file path
            src_rel_path = src_file.path
            if src_rel_path.startswith('/'):
                # Convert absolute path to relative
                try:
                    src_rel_path = str(Path(src_rel_path).relative_to(root_path))
                except ValueError:
                    # Not under the project root, use the filename
                    src_rel_path = os.path.basename(src_rel_path)
            
            # Create test filename (Jest style)
            file_basename = os.path.basename(src_rel_path)
            base, ext = os.path.splitext(file_basename)
            test_filename = f"{base}.test{ext}"
            
            # Create directory structure
            test_file_dir = test_path
            os.makedirs(test_file_dir, exist_ok=True)
            
            test_file_path = test_file_dir / test_filename
            
            # Generate test content
            test_content = await self._generate_js_test_content(src_file, "jest", src_rel_path)
            
            # Write the test file
            try:
                with open(test_file_path, 'w') as f:
                    f.write(test_content)
                
                generated_files.append(str(test_file_path))
            except Exception as e:
                errors.append(f"Failed to create {test_file_path}: {str(e)}")
        
        return {
            "success": len(errors) == 0,
            "test_framework": "jest",
            "project_type": "node",
            "generated_files": generated_files,
            "errors": errors,
            "file_count": len(generated_files)
        }
    
    async def _generate_js_test_content(
        self, 
        src_file: CodeFile, 
        framework: str,
        rel_path: str
    ) -> str:
        """
        Generate JavaScript test content for a source file.
        
        Args:
            src_file: Source file to generate test for
            framework: Test framework to use ('jest' or 'mocha')
            rel_path: Relative path to the source file
            
        Returns:
            Test file content
        """
        # Create import path
        import_path = os.path.splitext(rel_path)[0]  # Remove extension
        
        # Handle path format
        if import_path.startswith('./') or import_path.startswith('../'):
            pass  # Keep as-is
        else:
            # Make it relative
            import_path = f"../{import_path}"
        
        # Get module type (ESM or CommonJS)
        is_esm = "export " in src_file.content or "import " in src_file.content
        
        # Analyze source code to extract exports
        exports = []
        
        # Match named exports and default exports
        export_patterns = [
            r'export\s+const\s+(\w+)',
            r'export\s+function\s+(\w+)',
            r'export\s+class\s+(\w+)',
            r'export\s+default\s+(?:function\s+)?(\w+)',
            r'module\.exports\s*=\s*{([^}]+)}',
            r'exports\.(\w+)\s*='
        ]
        
        has_default_export = "export default" in src_file.content or "module.exports = " in src_file.content
        
        for pattern in export_patterns:
            for match in re.finditer(pattern, src_file.content):
                if pattern == r'module\.exports\s*=\s*{([^}]+)}':
                    # Extract multiple exports from module.exports = {...}
                    exports_str = match.group(1)
                    for export_name in re.findall(r'(\w+)\s*:', exports_str):
                        exports.append(export_name)
                else:
                    export_name = match.group(1)
                    exports.append(export_name)
        
        # Generate test content based on framework
        if framework == "jest":
            if is_esm:
                content = f"""
// Tests for {rel_path}
import {{ {', '.join(exports)} }} from '{import_path}';
"""
                if has_default_export:
                    content += f"""
import defaultExport from '{import_path}';
"""
            else:
                content = f"""
// Tests for {rel_path}
const {{ {', '.join(exports)} }} = require('{import_path}');
"""
            
            content += """

describe('Module functionality', () => {
"""
            
            # Add tests for each export
            for export_name in exports:
                content += f"""
  describe('{export_name}', () => {{
    test('should be defined', () => {{
      expect({export_name}).toBeDefined();
    }});
    
    // Add more specific tests here
  }});
"""
            
            if has_default_export:
                content += """
  describe('default export', () => {
    test('should be defined', () => {
      expect(defaultExport).toBeDefined();
    });
    
    // Add more specific tests here
  });
"""
            
            # Close the describe block
            content += "});\n"
        
        elif framework == "mocha":
            if is_esm:
                content = f"""
// Tests for {rel_path}
import {{ {', '.join(exports)} }} from '{import_path}';
import assert from 'assert';
"""
                if has_default_export:
                    content += f"""
import defaultExport from '{import_path}';
"""
            else:
                content = f"""
// Tests for {rel_path}
const {{ {', '.join(exports)} }} = require('{import_path}');
const assert = require('assert');
"""
            
            content += """

describe('Module functionality', function() {
"""
            
            # Add tests for each export
            for export_name in exports:
                content += f"""
  describe('{export_name}', function() {{
    it('should be defined', function() {{
      assert({export_name} !== undefined);
    }});
    
    // Add more specific tests here
  }});
"""
            
            if has_default_export:
                content += """
  describe('default export', function() {
    it('should be defined', function() {
      assert(defaultExport !== undefined);
    });
    
    // Add more specific tests here
  });
"""
            
            # Close the describe block
            content += "});\n"
        
        return content.strip()
    
    async def _generate_mocha_files(
        self, 
        src_files: List[CodeFile],
        test_path: Path,
        root_path: Path
    ) -> Dict[str, Any]:
        """
        Generate Mocha test files.
        
        Args:
            src_files: List of source files to generate tests for
            test_path: Path to the test directory
            root_path: Path to the project root
            
        Returns:
            Dictionary with the test generation result
        """
        self._logger.info(f"Generating Mocha files in {test_path}")
        
        generated_files = []
        errors = []
        
        # Create .mocharc.js if it doesn't exist
        mocha_config_path = root_path / ".mocharc.js"
        if not mocha_config_path.exists():
            try:
                with open(mocha_config_path, 'w') as f:
                    f.write("""
module.exports = {
  require: '@babel/register',
  reporter: 'spec',
  timeout: 5000,
  recursive: true,
  'watch-files': ['test/**/*.js', 'src/**/*.js']
};
""".strip())
                
                generated_files.append(str(mocha_config_path))
            except Exception as e:
                errors.append(f"Failed to create .mocharc.js: {str(e)}")
        
        # Generate test file for each source file
        for src_file in src_files:
            # Skip if not a JavaScript file
            if not src_file.path.endswith(('.js', '.jsx', '.ts', '.tsx')):
                continue
            
            # Determine test file path
            src_rel_path = src_file.path
            if src_rel_path.startswith('/'):
                # Convert absolute path to relative
                try:
                    src_rel_path = str(Path(src_rel_path).relative_to(root_path))
                except ValueError:
                    # Not under the project root, use the filename
                    src_rel_path = os.path.basename(src_rel_path)
            
            # Create test filename (Mocha style)
            file_basename = os.path.basename(src_rel_path)
            base, ext = os.path.splitext(file_basename)
            test_filename = f"{base}.spec{ext}"
            
            # Create directory structure
            test_file_dir = test_path
            os.makedirs(test_file_dir, exist_ok=True)
            
            test_file_path = test_file_dir / test_filename
            
            # Generate test content
            test_content = await self._generate_js_test_content(src_file, "mocha", src_rel_path)
            
            # Write the test file
            try:
                with open(test_file_path, 'w') as f:
                    f.write(test_content)
                
                generated_files.append(str(test_file_path))
            except Exception as e:
                errors.append(f"Failed to create {test_file_path}: {str(e)}")
        
        return {
            "success": len(errors) == 0,
            "test_framework": "mocha",
            "project_type": "node",
            "generated_files": generated_files,
            "errors": errors,
            "file_count": len(generated_files)
        }
    
    async def _generate_go_test_files(
        self, 
        src_files: List[CodeFile],
        test_path: Path,
        root_path: Path
    ) -> Dict[str, Any]:
        """
        Generate Go test files.
        
        Args:
            src_files: List of source files to generate tests for
            test_path: Path to the test directory
            root_path: Path to the project root
            
        Returns:
            Dictionary with the test generation result
        """
        self._logger.info(f"Generating Go test files in {test_path}")
        
        generated_files = []
        errors = []
        
        # In Go, test files are typically in the same directory as the source files
        # with a _test.go suffix
        
        # Generate test file for each source file
        for src_file in src_files:
            # Skip if not a Go file
            if not src_file.path.endswith('.go'):
                continue
            
            # Extract package information from source file
            package_match = re.search(r'package\s+(\w+)', src_file.content)
            if not package_match:
                errors.append(f"Could not determine package for {src_file.path}")
                continue
            
            package_name = package_match.group(1)
            
            # Determine test file path
            src_rel_path = src_file.path
            if src_rel_path.startswith('/'):
                # Convert absolute path to relative
                try:
                    src_rel_path = str(Path(src_rel_path).relative_to(root_path))
                except ValueError:
                    # Not under the project root, use the filename
                    src_rel_path = os.path.basename(src_rel_path)
            
            # Create test filename (Go style)
            dir_name = os.path.dirname(src_rel_path)
            file_basename = os.path.basename(src_rel_path)
            base, _ = os.path.splitext(file_basename)
            test_filename = f"{base}_test.go"
            
            # Go tests are in the same directory as the source
            if dir_name:
                test_file_dir = root_path / dir_name
            else:
                test_file_dir = root_path
            
            os.makedirs(test_file_dir, exist_ok=True)
            
            test_file_path = test_file_dir / test_filename
            
            # Analyze source code to extract functions and types
            function_pattern = r'func\s+(\w+)'
            type_pattern = r'type\s+(\w+)\s+'
            
            functions = []
            types = []
            
            for match in re.finditer(function_pattern, src_file.content):
                function_name = match.group(1)
                if not function_name.startswith('Test') and function_name[0].isupper():  # Exported functions
                    functions.append(function_name)
            
            for match in re.finditer(type_pattern, src_file.content):
                type_name = match.group(1)
                if type_name[0].isupper():  # Exported types
                    types.append(type_name)
            
            # Generate test content
            content = f"""
package {package_name}

import (
    "testing"
)

"""
            
            # Add function tests
            for function_name in functions:
                content += f"""
func Test{function_name}(t *testing.T) {{
    // TODO: Implement test
    t.Run("{function_name} basic test", func(t *testing.T) {{
        // t.Fatal("Not implemented")
    }})
}}
"""
            
            # Add type tests
            for type_name in types:
                content += f"""
func Test{type_name}(t *testing.T) {{
    // TODO: Implement test
    t.Run("{type_name} initialization", func(t *testing.T) {{
        // t.Fatal("Not implemented")
    }})
}}
"""
            
            # Add benchmark examples if there are functions
            if functions:
                content += f"""
func Benchmark{functions[0]}(b *testing.B) {{
    for i := 0; i < b.N; i++ {{
        // TODO: Call the function to benchmark
    }}
}}
"""
            
            # Write the test file
            try:
                with open(test_file_path, 'w') as f:
                    f.write(content.strip())
                
                generated_files.append(str(test_file_path))
            except Exception as e:
                errors.append(f"Failed to create {test_file_path}: {str(e)}")
        
        return {
            "success": len(errors) == 0,
            "test_framework": "go_test",
            "project_type": "go",
            "generated_files": generated_files,
            "errors": errors,
            "file_count": len(generated_files)
        }

test_framework_integration = TestFrameworkIntegration()
</file>

<file path="components/toolchain/universal_cli.py">
# angela/components/toolchain/universal_cli.py

"""
Universal CLI Translator for Angela CLI.

This module provides the ability to translate natural language requests
into commands for arbitrary CLI tools by analyzing their help documentation
and applying knowledge about CLI conventions.
"""
import asyncio
import re
import shlex
import subprocess
import os
from typing import Dict, Any, List, Optional, Tuple, Set, Union
from pathlib import Path

from pydantic import BaseModel, Field

# Updated imports to use API layer
from angela.api.ai import get_gemini_client, GeminiRequest
from angela.api.context import get_context_manager
from angela.utils.logging import get_logger
from angela.api.safety import get_command_validator
from angela.core.registry import registry

logger = get_logger(__name__)

class CommandParameter(BaseModel):
    """Model for a command parameter."""
    name: str
    description: Optional[str] = None
    required: bool = False
    type: str = "string"  # string, number, boolean, etc.
    short_flag: Optional[str] = None  # e.g. -f
    long_flag: Optional[str] = None  # e.g. --file
    default: Optional[Any] = None
    values: Optional[List[str]] = None  # possible values for enum-like parameters

class CommandOption(BaseModel):
    """Model for a command option."""
    name: str
    description: Optional[str] = None
    short_flag: Optional[str] = None  # e.g. -v
    long_flag: Optional[str] = None  # e.g. --verbose
    takes_value: bool = False

class CommandDefinition(BaseModel):
    """Model for a CLI command definition."""
    tool: str  # The CLI tool name (e.g., "git", "docker")
    command: str  # The command name (e.g., "commit", "run")
    description: Optional[str] = None
    usage: Optional[str] = None
    parameters: List[CommandParameter] = Field(default_factory=list)
    options: List[CommandOption] = Field(default_factory=list)
    subcommands: Dict[str, Dict[str, Any]] = Field(default_factory=dict)
    examples: List[str] = Field(default_factory=list)

class UniversalCLITranslator:
    """
    Translates natural language requests into commands for arbitrary CLI tools.
    
    This class analyzes help documentation from CLI tools to understand their
    command structure and parameters, then matches natural language requests
    to appropriate commands and generates valid command strings.
    """
    
    def __init__(self):
        """Initialize the translator."""
        self._logger = logger
        self._command_cache: Dict[str, CommandDefinition] = {}
        self._analysis_cache: Dict[str, Dict[str, Any]] = {}
        self._recently_used_tools: List[str] = []
    
    async def translate_request(
        self, 
        request: str, 
        context: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Translate a natural language request into a command for a CLI tool.
        
        Args:
            request: The natural language request
            context: Context information
            
        Returns:
            Dictionary with the translation result
        """
        self._logger.info(f"Translating request: {request}")
        
        # Analyze the request to determine the likely tool and command
        analysis = await self._analyze_request(request, context)
        
        if not analysis or not analysis.get("tool"):
            return {
                "success": False,
                "error": "Could not determine which CLI tool to use",
                "request": request
            }
        
        tool = analysis["tool"]
        command = analysis.get("command", "")
        
        # Add to recently used tools
        self._update_recently_used_tools(tool)
        
        # Get the command definition for the tool
        command_def = await self._get_command_definition(tool, command)
        
        if not command_def:
            return {
                "success": False,
                "error": f"Could not get command definition for {tool} {command}",
                "request": request,
                "tool": tool,
                "command": command
            }
        
        # Generate a command string using the definition and the request
        cmd_result = await self._generate_command_string(request, command_def, analysis)
        
        if not cmd_result.get("success"):
            return {
                "success": False,
                "error": cmd_result.get("error", "Failed to generate command string"),
                "request": request,
                "tool": tool,
                "command": command
            }
        
        # Validate the generated command for safety
        command_str = cmd_result["command"]
        # Get command validator from API
        command_validator = get_command_validator()
        is_safe, error_message = command_validator(command_str)
        
        if not is_safe:
            return {
                "success": False,
                "error": f"Generated command fails safety validation: {error_message}",
                "request": request,
                "tool": tool,
                "command": command,
                "generated_command": command_str
            }
        
        # Return the successful result
        return {
            "success": True,
            "command": command_str,
            "tool": tool,
            "subcommand": command,
            "explanation": cmd_result.get("explanation", ""),
            "request": request
        }
    
    def _update_recently_used_tools(self, tool: str) -> None:
        """
        Update the list of recently used tools.
        
        Args:
            tool: The tool name to add/move to front
        """
        # Remove the tool if already in the list
        if tool in self._recently_used_tools:
            self._recently_used_tools.remove(tool)
        
        # Add to the front of the list
        self._recently_used_tools.insert(0, tool)
        
        # Keep list at a reasonable size
        self._recently_used_tools = self._recently_used_tools[:10]
    
    async def _analyze_request(
        self, 
        request: str, 
        context: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Analyze a request to determine the likely CLI tool and command.
        
        Args:
            request: The natural language request
            context: Context information
            
        Returns:
            Dictionary with analysis results
        """
        # Check if we've already analyzed this request
        cache_key = request.strip().lower()
        if cache_key in self._analysis_cache:
            self._logger.debug(f"Using cached analysis for request: {request}")
            return self._analysis_cache[cache_key]
        
        # Prepare the context information for the prompt
        recently_used = ", ".join(self._recently_used_tools) if self._recently_used_tools else "None"
        
        # Use AI to analyze the request
        prompt = f"""
You are an expert command-line tools analyst. Your task is to analyze a natural language request and determine:
1. Which CLI tool the user likely wants to use
2. Which command or subcommand for that tool
3. What parameters and options might be needed

User's request: "{request}"

Context information:
- Current directory: {context.get('cwd', 'Unknown')}
- Recently used tools: {recently_used}

Return a JSON object with:
- tool: The name of the CLI tool (e.g., "git", "docker", "aws")
- command: The specific command or subcommand (e.g., "commit", "run", "s3 cp")
- parameters: List of likely parameters needed, each with a name and value
- options: List of likely options needed, each with a name (e.g., "verbose")

Only include the most likely tool. If you're unsure, make your best guess.
"""
        
        self._logger.debug("Sending analysis request to AI service")
        
        # Get gemini client from API
        gemini_client = get_gemini_client()
        
        # Call AI service
        api_request = GeminiRequest(prompt=prompt, max_tokens=1000)
        response = await gemini_client.generate_text(api_request)
        
        try:
            # Extract JSON from the response
            import json
            
            # Try to find JSON in the response
            json_match = re.search(r'```(?:json)?\s*(.*?)\s*```', response.text, re.DOTALL)
            if json_match:
                json_str = json_match.group(1)
            else:
                # Assume the entire response is JSON
                json_str = response.text
            
            # Parse the JSON
            analysis = json.loads(json_str)
            
            # Cache the result
            self._analysis_cache[cache_key] = analysis
            
            self._logger.debug(f"Analysis found tool: {analysis.get('tool')}, command: {analysis.get('command')}")
            
            return analysis
            
        except Exception as e:
            self._logger.error(f"Error parsing analysis response: {str(e)}")
            return {"error": str(e)}
    
    async def _get_command_definition(
        self, 
        tool: str, 
        command: Optional[str] = None
    ) -> Optional[CommandDefinition]:
        """
        Get or create a command definition for a CLI tool.
        
        Args:
            tool: The CLI tool name (e.g., "git", "docker")
            command: Optional specific command to get
            
        Returns:
            CommandDefinition object or None if not found
        """
        # Check if we already have this command cached
        cache_key = f"{tool}:{command}" if command else tool
        if cache_key in self._command_cache:
            self._logger.debug(f"Using cached command definition for {cache_key}")
            return self._command_cache[cache_key]
        
        # Check if the tool is available
        if not await self._is_tool_available(tool):
            self._logger.warning(f"Tool {tool} not available in the system")
            return None
        
        # Generate the help command based on the tool and command
        help_cmd = await self._generate_help_command(tool, command)
        
        # Run the help command to get documentation
        help_text = await self._get_help_text(help_cmd)
        if not help_text:
            self._logger.warning(f"Could not get help text for {tool} {command}")
            return None
        
        # Parse the help text to create a command definition
        command_def = await self._parse_help_text(tool, command, help_text)
        
        if command_def:
            # Cache the result
            self._command_cache[cache_key] = command_def
            self._logger.debug(f"Cached command definition for {cache_key}")
        
        return command_def
    
    async def _is_tool_available(self, tool: str) -> bool:
        """
        Check if a CLI tool is available in the system.
        
        Args:
            tool: The CLI tool name
            
        Returns:
            True if the tool is available, False otherwise
        """
        try:
            self._logger.debug(f"Checking if tool is available: {tool}")
            
            # Try to run the command with --version or --help
            process = await asyncio.create_subprocess_exec(
                tool, "--version",
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            await process.communicate()
            
            if process.returncode == 0:
                self._logger.debug(f"Tool {tool} is available (--version check)")
                return True
            
            # Try with --help if --version failed
            process = await asyncio.create_subprocess_exec(
                tool, "--help",
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            await process.communicate()
            
            availability = process.returncode == 0
            self._logger.debug(f"Tool {tool} {'is' if availability else 'is not'} available (--help check)")
            return availability
            
        except Exception as e:
            self._logger.error(f"Error checking tool availability for {tool}: {str(e)}")
            return False
    
    async def _generate_help_command(
        self, 
        tool: str, 
        command: Optional[str] = None
    ) -> str:
        """
        Generate a help command for a CLI tool.
        
        Args:
            tool: The CLI tool name
            command: Optional specific command
            
        Returns:
            Help command string
        """
        if command:
            # Try with both --help and help syntax
            return f"{tool} {command} --help"
        else:
            return f"{tool} --help"
    
    async def _get_help_text(self, help_cmd: str) -> Optional[str]:
        """
        Get help text by running a help command.
        
        Args:
            help_cmd: The help command to run
            
        Returns:
            Help text string or None if failed
        """
        try:
            self._logger.debug(f"Running help command: {help_cmd}")
            
            # Split the command into args
            args = shlex.split(help_cmd)
            
            # Run the command
            process = await asyncio.create_subprocess_exec(
                *args,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            
            stdout, stderr = await process.communicate()
            
            # Combine stdout and stderr as some tools print help to stderr
            help_text = stdout.decode('utf-8', errors='replace')
            if not help_text and stderr:
                help_text = stderr.decode('utf-8', errors='replace')
            
            if help_text:
                self._logger.debug(f"Got help text of length {len(help_text)}")
            else:
                self._logger.warning(f"No help text received for command: {help_cmd}")
            
            return help_text if help_text else None
            
        except Exception as e:
            self._logger.error(f"Error running help command '{help_cmd}': {str(e)}")
            return None
    
    async def _parse_help_text(
        self, 
        tool: str, 
        command: Optional[str], 
        help_text: str
    ) -> Optional[CommandDefinition]:
        """
        Parse help text to create a command definition.
        
        Args:
            tool: The CLI tool name
            command: Optional specific command
            help_text: Help text to parse
            
        Returns:
            CommandDefinition object or None if parsing failed
        """
        # This is a complex task that varies by tool, so we'll use AI to help
        self._logger.debug(f"Parsing help text for {tool} {command or ''}")
        
        # Truncate help text if too long for prompt
        max_help_text_len = 4000
        truncated_help = help_text
        if len(help_text) > max_help_text_len:
            truncated_help = help_text[:max_help_text_len] + "... [truncated]"
            self._logger.debug(f"Truncated help text from {len(help_text)} to {max_help_text_len} characters")
        
        prompt = f"""
You are an expert in command-line interfaces. Parse this help text for {tool} {command or ''} and extract:
1. Command description
2. Usage syntax
3. Parameters (required inputs)
4. Options (flags)
5. Examples

Help text:
```
{truncated_help}
```

Return a structured JSON object with:
- tool: "{tool}"
- command: "{command or ''}"
- description: Command description
- usage: Usage syntax
- parameters: List of parameters with name, description, required status, type
- options: List of options with name, description, short_flag, long_flag
- examples: List of example commands

Focus on accuracy. Skip any sections you can't confidently parse.
"""

        gemini_client = get_gemini_client()        
        # Call AI service
        api_request = GeminiRequest(prompt=prompt, max_tokens=2000)
        response = await gemini_client.generate_text(api_request)
        
        try:
            # Extract JSON from the response
            import json
            
            # Try to find JSON in the response
            json_match = re.search(r'```(?:json)?\s*(.*?)\s*```', response.text, re.DOTALL)
            if json_match:
                json_str = json_match.group(1)
            else:
                # Assume the entire response is JSON
                json_str = response.text
            
            # Parse the JSON
            cmd_data = json.loads(json_str)
            
            self._logger.debug(f"Successfully parsed help text into command definition")
            
            # Create CommandParameter objects
            parameters = []
            for param_data in cmd_data.get("parameters", []):
                param = CommandParameter(
                    name=param_data["name"],
                    description=param_data.get("description"),
                    required=param_data.get("required", False),
                    type=param_data.get("type", "string")
                )
                parameters.append(param)
            
            # Create CommandOption objects
            options = []
            for opt_data in cmd_data.get("options", []):
                opt = CommandOption(
                    name=opt_data["name"],
                    description=opt_data.get("description"),
                    short_flag=opt_data.get("short_flag"),
                    long_flag=opt_data.get("long_flag"),
                    takes_value=opt_data.get("takes_value", False)
                )
                options.append(opt)
            
            # Create the CommandDefinition
            cmd_def = CommandDefinition(
                tool=tool,
                command=command or "",
                description=cmd_data.get("description"),
                usage=cmd_data.get("usage"),
                parameters=parameters,
                options=options,
                examples=cmd_data.get("examples", [])
            )
            
            return cmd_def
            
        except Exception as e:
            self._logger.error(f"Error parsing help text for {tool} {command}: {str(e)}")
            return None
    
    async def _generate_command_string(
        self, 
        request: str, 
        command_def: CommandDefinition, 
        analysis: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Generate a command string based on the request and command definition.
        
        Args:
            request: The natural language request
            command_def: The command definition
            analysis: The request analysis results
            
        Returns:
            Dictionary with the command string and explanation
        """
        self._logger.debug(f"Generating command string for request: {request}")
        
        # Use AI to generate the command string
        prompt = f"""
You are an expert CLI command generator. Your task is to generate a valid command based on user request and command definition.

User request: "{request}"

Command definition:
- Tool: {command_def.tool}
- Command: {command_def.command}
- Description: {command_def.description or 'N/A'}
- Usage: {command_def.usage or 'N/A'}

Parameters:
{self._format_parameters(command_def.parameters)}

Options:
{self._format_options(command_def.options)}

Examples:
{self._format_examples(command_def.examples)}

Generate a complete, valid command that fulfills the user's request.
Along with the command, provide a brief explanation of what it does and why you chose specific parameters/options.

Return your response as:
COMMAND: <the exact command string>
EXPLANATION: <explanation of the command>

Be precise and accurate. Include necessary quotes for paths or arguments that need them.
"""
        
        # Get gemini client from API
        gemini_client = get_gemini_client()
        
        # Call AI service
        api_request = GeminiRequest(prompt=prompt, max_tokens=1000)
        response = await gemini_client.generate_text(api_request)
        
        try:
            # Extract the command and explanation from the response
            command_match = re.search(r'COMMAND:\s*(.+?)(?:\n|$)', response.text)
            explanation_match = re.search(r'EXPLANATION:\s*(.+(?:\n.+)*)', response.text)
            
            if not command_match:
                self._logger.error("Could not extract command from AI response")
                return {
                    "success": False,
                    "error": "Could not extract command from AI response"
                }
            
            command = command_match.group(1).strip()
            explanation = explanation_match.group(1).strip() if explanation_match else ""
            
            self._logger.debug(f"Generated command: {command}")
            
            return {
                "success": True,
                "command": command,
                "explanation": explanation
            }
            
        except Exception as e:
            self._logger.error(f"Error extracting command from response: {str(e)}")
            return {
                "success": False,
                "error": str(e)
            }
    
    def _format_parameters(self, parameters: List[CommandParameter]) -> str:
        """Format parameters for the prompt."""
        if not parameters:
            return "None"
        
        result = []
        for param in parameters:
            required = "Required" if param.required else "Optional"
            result.append(f"- {param.name}: {param.description or 'No description'} ({required}, Type: {param.type})")
        
        return "\n".join(result)
    
    def _format_options(self, options: List[CommandOption]) -> str:
        """Format options for the prompt."""
        if not options:
            return "None"
        
        result = []
        for opt in options:
            flags = []
            if opt.short_flag:
                flags.append(opt.short_flag)
            if opt.long_flag:
                flags.append(opt.long_flag)
            
            flags_str = ", ".join(flags) if flags else "No flag"
            result.append(f"- {opt.name}: {opt.description or 'No description'} (Flags: {flags_str})")
        
        return "\n".join(result)
    
    def _format_examples(self, examples: List[str]) -> str:
        """Format examples for the prompt."""
        if not examples:
            return "None"
        
        result = []
        for i, example in enumerate(examples, 1):
            result.append(f"{i}. {example}")
        
        return "\n".join(result)
    
    async def get_tool_suggestions(self, partial_tool: str = "") -> List[str]:
        """
        Get suggestions for available tools.
        
        Args:
            partial_tool: Optional partial tool name to filter by
            
        Returns:
            List of tool suggestions
        """
        # Check common locations for executables
        path_dirs = os.environ.get("PATH", "").split(os.pathsep)
        
        tools = set()
        for path_dir in path_dirs:
            if not os.path.exists(path_dir):
                continue
                
            for entry in os.listdir(path_dir):
                entry_path = os.path.join(path_dir, entry)
                if os.path.isfile(entry_path) and os.access(entry_path, os.X_OK):
                    if not partial_tool or entry.startswith(partial_tool):
                        tools.add(entry)
        
        # Prioritize recently used tools
        result = []
        for tool in self._recently_used_tools:
            if tool in tools and (not partial_tool or tool.startswith(partial_tool)):
                result.append(tool)
                tools.remove(tool)
        
        # Add remaining tools
        result.extend(sorted(tools))
        
        return result


# Global instance
universal_cli_translator = UniversalCLITranslator()
</file>

<file path="components/utils/__init__.py">
# angela/utils/__init__.py
"""
Utility functions for Angela CLI.

This package provides common utilities like logging, configuration management,
and helper functions used throughout the application.
"""

from .logging import setup_logging, get_logger



__all__ = ['setup_logging', 'get_logger']
</file>

<file path="components/utils/enhanced_logging.py">
# angela/utils/enhanced_logging.py
import json
import inspect
import logging
import traceback
import sys
from datetime import datetime
from typing import Dict, Any, Optional, Union

class EnhancedLogger:
    """Enhanced logger with context tracking and structured output."""
    
    def __init__(self, name: str):
        self._logger = logging.getLogger(name)
        self._context: Dict[str, Any] = {}
    
    def add_context(self, key: str, value: Any) -> None:
        """Add context information for subsequent log messages."""
        self._context[key] = value
    
    def remove_context(self, key: str) -> None:
        """Remove context information."""
        if key in self._context:
            del self._context[key]
    
    def clear_context(self) -> None:
        """Clear all context information."""
        self._context.clear()
    
    def with_context(self, **context) -> 'EnhancedLogger':
        """Create a new logger with added context."""
        new_logger = EnhancedLogger(self._logger.name)
        new_logger._context = {**self._context, **context}
        return new_logger
    
    def _format_message(self, msg: str, extra: Optional[Dict[str, Any]] = None) -> str:
        """Format message with context information."""
        # Get caller info
        frame = inspect.currentframe().f_back.f_back
        func_name = frame.f_code.co_name
        filename = frame.f_code.co_filename.split('/')[-1]
        lineno = frame.f_lineno
        
        # Combine contexts
        context = {**self._context}
        if extra:
            context.update(extra)
        
        # Create structured log
        log_data = {
            "timestamp": datetime.now().isoformat(),
            "message": msg,
            "context": context,
            "caller": f"{filename}:{func_name}:{lineno}"
        }
        
        return json.dumps(log_data)
    
    def debug(self, msg: str, *args, **kwargs) -> None:
        """Log a debug message with context."""
        extra = kwargs.pop("extra", {})
        self._logger.debug(self._format_message(msg, extra), *args, **kwargs)
    
    def info(self, msg: str, *args, **kwargs) -> None:
        """Log an info message with context."""
        extra = kwargs.pop("extra", {})
        self._logger.info(self._format_message(msg, extra), *args, **kwargs)
    
    def warning(self, msg: str, *args, **kwargs) -> None:
        """Log a warning message with context."""
        extra = kwargs.pop("extra", {})
        self._logger.warning(self._format_message(msg, extra), *args, **kwargs)
    
    def error(self, msg: str, *args, **kwargs) -> None:
        """Log an error message with context."""
        extra = kwargs.pop("extra", {})
        self._logger.error(self._format_message(msg, extra), *args, **kwargs)
    
    def critical(self, msg: str, *args, **kwargs) -> None:
        """Log a critical message with context."""
        extra = kwargs.pop("extra", {})
        self._logger.critical(self._format_message(msg, extra), *args, **kwargs)
    
    def exception(self, msg: str, exc_info: Union[bool, BaseException] = True, 
                 *args, **kwargs) -> None:
        """Log an exception with context."""
        extra = kwargs.pop("extra", {})
        # Add exception info to context
        if exc_info:
            if isinstance(exc_info, BaseException):
                exc_type = type(exc_info).__name__
                exc_message = str(exc_info)
            else:
                exc_info = sys.exc_info()
                exc_type = exc_info[0].__name__ if exc_info[0] else "Unknown"
                exc_message = str(exc_info[1]) if exc_info[1] else ""
            
            exception_context = {
                "exception_type": exc_type,
                "exception_message": exc_message,
                "traceback": traceback.format_exc()
            }
            if not extra:
                extra = {}
            extra["exception"] = exception_context
            
        self._logger.exception(self._format_message(msg, extra), *args, **kwargs)
    
    def log(self, level: int, msg: str, *args, **kwargs) -> None:
        """Log a message with the specified level."""
        extra = kwargs.pop("extra", {})
        self._logger.log(level, self._format_message(msg, extra), *args, **kwargs)

    @property
    def name(self) -> str:
        """Get the logger name."""
        return self._logger.name
    
    @property
    def level(self) -> int:
        """Get the logger level."""
        return self._logger.level
    
    @level.setter
    def level(self, level: int) -> None:
        """Set the logger level."""
        self._logger.setLevel(level)
</file>

<file path="components/utils/logging.py">
# angela/utils/logging.py
"""
Logging configuration for Angela CLI.
"""
import sys
import logging
from pathlib import Path

from loguru import logger
from angela.constants import LOG_DIR, LOG_FORMAT, LOG_ROTATION, LOG_RETENTION
from angela.components.utils.enhanced_logging import EnhancedLogger

# Dictionary to store enhanced logger instances
_enhanced_loggers = {}

def setup_logging(debug: bool = False) -> None:
    """
    Configure the application logging.
    
    Args:
        debug: Whether to enable debug logging.
    """
    # Ensure log directory exists
    LOG_DIR.mkdir(parents=True, exist_ok=True)
    
    # Remove default handlers
    logger.remove()
    
    # Add console handler with appropriate level
    log_level = "DEBUG" if debug else "INFO"
    logger.add(
        sys.stderr,
        format=LOG_FORMAT,
        level=log_level,
        diagnose=debug,  # Include variable values in traceback if debug is True
    )
    
    # Add file handler
    log_file = LOG_DIR / "angela.log"
    logger.add(
        log_file,
        format=LOG_FORMAT,
        level="INFO",
        rotation=LOG_ROTATION,
        retention=LOG_RETENTION,
        compression="zip",
    )
    
    # Add structured JSON log file
    json_log_file = LOG_DIR / "angela_structured.log"
    logger.add(
        json_log_file,
        serialize=True,  # Output as JSON
        level="INFO",
        rotation=LOG_ROTATION,
        retention=LOG_RETENTION,
        compression="zip",
    )
    
    logger.debug(f"Logging initialized. Log files: {log_file}, {json_log_file}")


def get_logger(name: str = "angela") -> EnhancedLogger:
    """
    Get a logger instance with the given name.
    
    Args:
        name: The name for the logger.
        
    Returns:
        An enhanced logger instance.
    """
    # Check if we already have an enhanced logger for this name
    if name in _enhanced_loggers:
        return _enhanced_loggers[name]
    
    # Create a new enhanced logger
    enhanced_logger = EnhancedLogger(name)
    _enhanced_loggers[name] = enhanced_logger
    
    return enhanced_logger
</file>

<file path="components/workflows/__init__.py">
# angela/workflows/__init__.py
"""
Workflow management for Angela CLI.

This package handles creating, managing, and executing user-defined
workflows - reusable sequences of commands that can be invoked by name.
"""

# Export the main components that other modules will need
from .manager import workflow_manager
from .sharing import workflow_sharing_manager

__all__ = ['workflow_manager', 'workflow_sharing_manager']
</file>

<file path="components/workflows/manager.py">
# angela/workflows/manager.py
"""
Workflow management for Angela CLI.

This module handles user-defined workflows - reusable sequences
of commands that can be invoked by name.
"""
import os
import json
import shlex
from pathlib import Path
from typing import Dict, Any, List, Tuple, Optional
from datetime import datetime
from dataclasses import dataclass, field, asdict

from pydantic import BaseModel, Field

from angela.config import config_manager  
from angela.utils.logging import get_logger
from angela.api.ai import get_gemini_client, get_gemini_request_class
from angela.api.intent import get_task_planner

logger = get_logger(__name__)



# File for storing workflows
WORKFLOWS_FILE = config_manager.CONFIG_DIR / "workflows.json"

class WorkflowStep(BaseModel):
    """Model for a step in a workflow."""
    command: str = Field(..., description="The command to execute")
    explanation: str = Field(..., description="Explanation of what the command does")
    optional: bool = Field(False, description="Whether this step is optional")
    requires_confirmation: bool = Field(False, description="Whether this step requires explicit confirmation")


class Workflow(BaseModel):
    """Model for a user-defined workflow."""
    name: str = Field(..., description="Unique name for the workflow")
    description: str = Field(..., description="Human-readable description")
    steps: List[WorkflowStep] = Field(..., description="Steps in the workflow")
    variables: Dict[str, str] = Field(default_factory=dict, description="Variable placeholders")
    created: datetime = Field(default_factory=datetime.now, description="When the workflow was created")
    modified: datetime = Field(default_factory=datetime.now, description="When the workflow was last modified")
    tags: List[str] = Field(default_factory=list, description="Tags for categorizing workflows")
    author: Optional[str] = Field(None, description="Author of the workflow")


class WorkflowManager:
    """
    Manager for user-defined workflows.
    
    This class handles:
    1. Defining new workflows from natural language descriptions
    2. Storing and retrieving workflows
    3. Executing workflows with parameter substitution
    4. Listing and searching available workflows
    """
    
    def __init__(self):
        """Initialize the workflow manager."""
        self._workflows: Dict[str, Workflow] = {}
        self._workflow_file = WORKFLOWS_FILE
        self._logger = logger
        self._load_workflows()
    
    def _load_workflows(self) -> None:
        """Load workflows from the storage file."""
        try:
            if self._workflow_file.exists():
                with open(self._workflow_file, "r") as f:
                    data = json.load(f)
                    
                for workflow_data in data:
                    try:
                        # Handle datetime serialization
                        if "created" in workflow_data:
                            workflow_data["created"] = datetime.fromisoformat(workflow_data["created"])
                        if "modified" in workflow_data:
                            workflow_data["modified"] = datetime.fromisoformat(workflow_data["modified"])
                            
                        workflow = Workflow(**workflow_data)
                        self._workflows[workflow.name] = workflow
                    except Exception as e:
                        self._logger.error(f"Error loading workflow: {str(e)}")
                
                self._logger.info(f"Loaded {len(self._workflows)} workflows")
            else:
                self._logger.info("No workflows file found, starting with empty workflows")
                self._save_workflows()  # Create the file
        except Exception as e:
            self._logger.error(f"Error loading workflows: {str(e)}")
    
    def _save_workflows(self) -> None:
        """Save workflows to the storage file."""
        try:
            # Ensure the directory exists
            self._workflow_file.parent.mkdir(parents=True, exist_ok=True)
            
            # Convert workflows to serializable dict
            data = []
            for workflow in self._workflows.values():
                workflow_dict = workflow.dict()
                # Handle datetime serialization
                workflow_dict["created"] = workflow_dict["created"].isoformat()
                workflow_dict["modified"] = workflow_dict["modified"].isoformat()
                data.append(workflow_dict)
            
            # Write to file
            with open(self._workflow_file, "w") as f:
                json.dump(data, f, indent=2)
                
            self._logger.info(f"Saved {len(self._workflows)} workflows")
        except Exception as e:
            self._logger.error(f"Error saving workflows: {str(e)}")
    
    async def define_workflow(
        self, 
        name: str, 
        description: str, 
        steps: List[Dict[str, Any]],
        variables: Optional[Dict[str, str]] = None,
        tags: Optional[List[str]] = None,
        author: Optional[str] = None
    ) -> Workflow:
        """
        Define a new workflow or update an existing one.
        
        Args:
            name: Unique name for the workflow
            description: Human-readable description
            steps: List of step dictionaries with commands and explanations
            variables: Optional variable placeholders
            tags: Optional tags for categorization
            author: Optional author name
            
        Returns:
            The created or updated Workflow
        """
        # Convert steps to WorkflowStep objects
        workflow_steps = []
        for step_data in steps:
            workflow_step = WorkflowStep(
                command=step_data["command"],
                explanation=step_data.get("explanation", ""),
                optional=step_data.get("optional", False),
                requires_confirmation=step_data.get("requires_confirmation", False)
            )
            workflow_steps.append(workflow_step)
        
        # Check if workflow already exists
        if name in self._workflows:
            # Update existing workflow
            workflow = self._workflows[name]
            workflow.description = description
            workflow.steps = workflow_steps
            workflow.variables = variables or {}
            workflow.modified = datetime.now()
            if tags:
                workflow.tags = tags
            if author:
                workflow.author = author
                
            self._logger.info(f"Updated workflow: {name}")
        else:
            # Create new workflow
            workflow = Workflow(
                name=name,
                description=description,
                steps=workflow_steps,
                variables=variables or {},
                tags=tags or [],
                author=author
            )
            self._workflows[name] = workflow
            self._logger.info(f"Created new workflow: {name}")
        
        # Save updated workflows
        self._save_workflows()
        
        return workflow
    
    async def define_workflow_from_natural_language(
        self, 
        name: str, 
        description: str, 
        natural_language: str,
        context: Dict[str, Any]
    ) -> Workflow:
        """
        Define a workflow from a natural language description.
        
        Args:
            name: Unique name for the workflow
            description: Human-readable description
            natural_language: Natural language description of the workflow steps
            context: Context information
            
        Returns:
            The created Workflow
        """
        # Get task_planner through API
        task_planner = get_task_planner()
        
        self._logger.info(f"Creating workflow from natural language: {name}")
        
        # Generate a plan using the task planner
        try:
            plan = await task_planner.plan_task(natural_language, context)
            
            # Convert plan steps to workflow steps
            steps = []
            for plan_step in plan.steps:
                step = {
                    "command": plan_step.command,
                    "explanation": plan_step.explanation,
                    "optional": False,
                    "requires_confirmation": plan_step.estimated_risk >= 3  # High or Critical risk
                }
                steps.append(step)
                
            # Identify potential variables
            variables = await self._identify_variables(steps, natural_language)
            
            # Create the workflow
            workflow = await self.define_workflow(
                name=name,
                description=description,
                steps=steps,
                variables=variables,
                tags=["user-defined"]
            )
            
            return workflow
            
        except Exception as e:
            self._logger.exception(f"Error creating workflow from natural language: {str(e)}")
            # Create a placeholder workflow
            placeholder_workflow = await self.define_workflow(
                name=name,
                description=description,
                steps=[{
                    "command": f"echo 'Error creating workflow: {str(e)}'",
                    "explanation": "This is a placeholder for a workflow that could not be created",
                    "optional": False,
                    "requires_confirmation": False
                }],
                tags=["error", "placeholder"]
            )
            return placeholder_workflow
    
    async def _identify_variables(
        self, 
        steps: List[Dict[str, Any]], 
        natural_language: str
    ) -> Dict[str, str]:
        """
        Identify potential variables in workflow steps.
        
        Args:
            steps: The workflow steps
            natural_language: Original natural language description
            
        Returns:
            Dictionary of variable names and descriptions
        """
        # Extract all commands
        commands = [step["command"] for step in steps]
        
        # Build prompt for variable identification
        prompt = f"""
    Identify potential variables in the following workflow commands:
    
    Commands:
    {json.dumps(commands, indent=2)}
    
    Original description:
    {natural_language}
    
    Identify parameters or values that might change each time the workflow is run.
    For each variable, provide:
    1. A variable name (use format like $NAME or {{NAME}})
    2. A description of what the variable represents
    
    Format your response as JSON:
    {{
      "variables": {{
        "$VARIABLE1": "Description of variable 1",
        "$VARIABLE2": "Description of variable 2",
        ...
      }}
    }}
    """
        
        # Get AI components through API
        gemini_client = get_gemini_client()
        GeminiRequest = get_gemini_request_class()
        
        # Call AI service
        api_request = GeminiRequest(prompt=prompt, max_tokens=2000)
        response = await gemini_client.generate_text(api_request)
        
        # Parse the response
        try:
            # Extract JSON from the response
            import re
            json_match = re.search(r'```(?:json)?\s*(.*?)\s*```', response.text, re.DOTALL)
            if json_match:
                json_str = json_match.group(1)
            else:
                # Fallback to entire response
                json_str = response.text
                
            # Parse JSON
            result = json.loads(json_str)
            variables = result.get("variables", {})
            
            self._logger.info(f"Identified {len(variables)} variables")
            return variables
            
        except Exception as e:
            self._logger.error(f"Error identifying variables: {str(e)}")
            return {}
    
    def get_workflow(self, name: str) -> Optional[Workflow]:
        """
        Get a workflow by name.
        
        Args:
            name: Name of the workflow to retrieve
            
        Returns:
            The Workflow if found, None otherwise
        """
        return self._workflows.get(name)
    
    def list_workflows(self, tag: Optional[str] = None) -> List[Workflow]:
        """
        List all workflows, optionally filtered by tag.
        
        Args:
            tag: Optional tag to filter workflows
            
        Returns:
            List of matching Workflows
        """
        if tag:
            return [w for w in self._workflows.values() if tag in w.tags]
        else:
            return list(self._workflows.values())
    
    def search_workflows(self, query: str) -> List[Workflow]:
        """
        Search for workflows by name or description.
        
        Args:
            query: Search query
            
        Returns:
            List of matching Workflows
        """
        query_lower = query.lower()
        results = []
        
        for workflow in self._workflows.values():
            # Check name, description, and tags
            if (query_lower in workflow.name.lower() or 
                query_lower in workflow.description.lower() or
                any(query_lower in tag.lower() for tag in workflow.tags)):
                results.append(workflow)
                
        return results
    
    def delete_workflow(self, name: str) -> bool:
        """
        Delete a workflow by name.
        
        Args:
            name: Name of the workflow to delete
            
        Returns:
            True if deleted, False if not found
        """
        if name in self._workflows:
            del self._workflows[name]
            self._save_workflows()
            self._logger.info(f"Deleted workflow: {name}")
            return True
        
        return False
    
    async def execute_workflow(
        self, 
        workflow_name: str, 
        variables: Dict[str, Any],
        context: Dict[str, Any],
        dry_run: bool = False
    ) -> Dict[str, Any]:
        """
        Execute a workflow with variable substitution.
        
        Args:
            workflow_name: Name of the workflow to execute
            variables: Variable values for substitution
            context: Context information
            dry_run: Whether to simulate execution without making changes
            
        Returns:
            Dictionary with execution results
        """
        workflow = self.get_workflow(workflow_name)
        if not workflow:
            return {
                "success": False,
                "error": f"Workflow not found: {workflow_name}"
            }
        
        # Get task_planner through API
        task_planner = get_task_planner()
        
        # Import task plan models through API
        from angela.api.intent import get_task_plan_classes
        TaskPlan, PlanStep = get_task_plan_classes()
        
        # Convert workflow to a task plan
        plan_steps = []
        for i, step in enumerate(workflow.steps):
            # Apply variable substitution
            command = self._substitute_variables(step.command, variables)
            
            plan_step = PlanStep(
                command=command,
                explanation=step.explanation,
                dependencies=[i-1] if i > 0 else [],  # Simple linear dependencies
                estimated_risk=3 if step.requires_confirmation else 1  # Default risk levels
            )
            plan_steps.append(plan_step)
            
        plan = TaskPlan(
            goal=f"Execute workflow: {workflow.name}",
            steps=plan_steps,
            context=context
        )
        
        # Execute the plan
        results = await task_planner.execute_plan(plan, dry_run=dry_run)
        
        return {
            "workflow": workflow.name,
            "description": workflow.description,
            "steps": len(workflow.steps),
            "results": results,
            "success": all(result.get("success", False) for result in results),
            "dry_run": dry_run
        }
    
    def _substitute_variables(self, command: str, variables: Dict[str, Any]) -> str:
        """
        Substitute variables in a command.
        
        Args:
            command: The command template
            variables: Variable values for substitution
            
        Returns:
            Command with variables substituted
        """
        result = command
        
        # Handle ${VAR} and $VAR syntax
        for var_name, var_value in variables.items():
            # Remove leading $ if present
            clean_name = var_name[1:] if var_name.startswith('$') else var_name
            
            # Substitute ${VAR} syntax
            result = result.replace(f"${{{clean_name}}}", str(var_value))
            
            # Substitute $VAR syntax
            result = result.replace(f"${clean_name}", str(var_value))
        
        return result


# Global workflow manager instance
workflow_manager = WorkflowManager()
</file>

<file path="components/workflows/sharing.py">
# angela/workflows/sharing.py

import os
import sys
import json
import tempfile
import shutil
import zipfile
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple, Union
from datetime import datetime
import uuid
import hashlib

from pydantic import BaseModel, Field

from angela.config import config_manager  # Changed from angela.api.config import get_config_manager
from angela.utils.logging import get_logger
from angela.api.workflows import get_workflow_manager, get_workflow_model_classes

logger = get_logger(__name__)


Workflow, _ = get_workflow_model_classes()

# Constants
WORKFLOW_EXPORT_DIR = config_manager.CONFIG_DIR / "exported_workflows"
WORKFLOW_IMPORT_DIR = config_manager.CONFIG_DIR / "imported_workflows"

class WorkflowExportMetadata(BaseModel):
    """Metadata for an exported workflow package."""
    id: str = Field(..., description="Unique identifier for this workflow package")
    name: str = Field(..., description="Name of the workflow")
    version: str = Field("1.0.0", description="Version of the workflow")
    description: str = Field(..., description="Description of the workflow")
    author: Optional[str] = Field(None, description="Author of the workflow")
    created: str = Field(..., description="Creation timestamp")
    exported: str = Field(..., description="Export timestamp")
    checksum: str = Field(..., description="SHA-256 checksum of the workflow data")
    tags: List[str] = Field(default_factory=list, description="Tags for the workflow")
    dependencies: Dict[str, str] = Field(default_factory=dict, description="External dependencies")

class WorkflowSharingManager:
    """Manager for workflow sharing, importing, and exporting."""
    
    def __init__(self, workflow_manager=None):
        """
        Initialize the workflow sharing manager.
        
        Args:
            workflow_manager: Optional workflow manager instance (for testing)
        """
        # Get workflow_manager through API if not provided
        self._workflow_manager = workflow_manager or get_workflow_manager()
        self._logger = logger
        
        # Ensure directories exist
        WORKFLOW_EXPORT_DIR.mkdir(parents=True, exist_ok=True)
        WORKFLOW_IMPORT_DIR.mkdir(parents=True, exist_ok=True)
    
    async def export_workflow(
        self, 
        workflow_name: str,
        output_path: Optional[Path] = None,
        include_dependencies: bool = True
    ) -> Dict[str, Any]:
        """
        Export a workflow to a shareable package.
        
        Args:
            workflow_name: Name of the workflow to export
            output_path: Optional custom output path
            include_dependencies: Whether to include external dependencies
            
        Returns:
            Dictionary with export results
        """
        # Get the workflow
        workflow = self._workflow_manager.get_workflow(workflow_name)
        if not workflow:
            return {
                "success": False,
                "error": f"Workflow not found: {workflow_name}"
            }
        
        try:
            # Create a temporary directory for packaging
            with tempfile.TemporaryDirectory() as temp_dir:
                temp_path = Path(temp_dir)
                
                # Convert workflow to serializable dict
                workflow_dict = workflow.dict()
                # Handle datetime serialization
                workflow_dict["created"] = workflow_dict["created"].isoformat()
                workflow_dict["modified"] = workflow_dict["modified"].isoformat()
                
                # Create workflow data file
                workflow_data_path = temp_path / "workflow.json"
                with open(workflow_data_path, "w") as f:
                    json.dump(workflow_dict, f, indent=2)
                
                # Generate checksum
                checksum = self._generate_checksum(workflow_data_path)
                
                # Create metadata
                metadata = WorkflowExportMetadata(
                    id=str(uuid.uuid4()),
                    name=workflow.name,
                    description=workflow.description,
                    author=workflow.author,
                    created=workflow.created.isoformat(),
                    exported=datetime.now().isoformat(),
                    checksum=checksum,
                    tags=workflow.tags
                )
                
                # Detect dependencies if requested
                if include_dependencies:
                    dependencies = await self._detect_dependencies(workflow)
                    metadata.dependencies = dependencies
                
                # Write metadata
                metadata_path = temp_path / "metadata.json"
                with open(metadata_path, "w") as f:
                    json.dump(metadata.dict(), f, indent=2)
                
                # Create README with information
                readme_path = temp_path / "README.md"
                with open(readme_path, "w") as f:
                    f.write(f"# {workflow.name}\n\n")
                    f.write(f"{workflow.description}\n\n")
                    if workflow.author:
                        f.write(f"Author: {workflow.author}\n\n")
                    f.write(f"Created: {workflow.created.isoformat()}\n")
                    f.write(f"Exported: {datetime.now().isoformat()}\n\n")
                    f.write("## Steps\n\n")
                    for i, step in enumerate(workflow.steps, 1):
                        f.write(f"### Step {i}: {step.command}\n")
                        f.write(f"{step.explanation}\n\n")
                
                # Determine output path
                if not output_path:
                    safe_name = workflow.name.replace(" ", "_").lower()
                    output_path = WORKFLOW_EXPORT_DIR / f"{safe_name}.angela-workflow"
                
                # Create zip archive
                with zipfile.ZipFile(output_path, "w") as zip_file:
                    for file_path in [workflow_data_path, metadata_path, readme_path]:
                        zip_file.write(file_path, arcname=file_path.name)
                
                return {
                    "success": True,
                    "workflow": workflow.name,
                    "output_path": str(output_path),
                    "metadata": metadata.dict()
                }
                
        except Exception as e:
            self._logger.exception(f"Error exporting workflow {workflow_name}: {str(e)}")
            return {
                "success": False,
                "error": f"Export failed: {str(e)}"
            }
    
    async def import_workflow(
        self, 
        workflow_path: Union[str, Path],
        rename: Optional[str] = None,
        replace_existing: bool = False
    ) -> Dict[str, Any]:
        """
        Import a workflow from a package.
        
        Args:
            workflow_path: Path to the workflow package
            rename: Optional new name for the workflow
            replace_existing: Whether to replace existing workflow with same name
            
        Returns:
            Dictionary with import results
        """
        path_obj = Path(workflow_path)
        
        # Check if file exists
        if not path_obj.exists():
            return {
                "success": False,
                "error": f"File not found: {path_obj}"
            }
        
        try:
            # Create a temporary directory for extraction
            with tempfile.TemporaryDirectory() as temp_dir:
                temp_path = Path(temp_dir)
                
                # Extract the zip archive
                with zipfile.ZipFile(path_obj, "r") as zip_file:
                    zip_file.extractall(temp_path)
                
                # Check metadata
                metadata_path = temp_path / "metadata.json"
                if not metadata_path.exists():
                    return {
                        "success": False,
                        "error": "Invalid workflow package: missing metadata.json"
                    }
                
                # Load metadata
                with open(metadata_path, "r") as f:
                    metadata = WorkflowExportMetadata(**json.load(f))
                
                # Check workflow data
                workflow_data_path = temp_path / "workflow.json"
                if not workflow_data_path.exists():
                    return {
                        "success": False,
                        "error": "Invalid workflow package: missing workflow.json"
                    }
                
                # Verify checksum
                computed_checksum = self._generate_checksum(workflow_data_path)
                if computed_checksum != metadata.checksum:
                    return {
                        "success": False,
                        "error": "Checksum verification failed. The workflow package may be corrupted."
                    }
                
                # Load workflow data
                with open(workflow_data_path, "r") as f:
                    workflow_data = json.load(f)
                
                # Apply rename if provided
                if rename:
                    workflow_data["name"] = rename
                
                # Check if workflow already exists
                existing_workflow = self._workflow_manager.get_workflow(workflow_data["name"])
                if existing_workflow and not replace_existing:
                    return {
                        "success": False,
                        "error": f"Workflow '{workflow_data['name']}' already exists. Use replace_existing=True to replace it."
                    }
                
                # Import the workflow
                workflow = await self._workflow_manager.define_workflow_from_data(
                    workflow_data,
                    source=f"Imported from {path_obj.name}"
                )
                
                return {
                    "success": True,
                    "workflow": workflow.name,
                    "metadata": metadata.dict()
                }
                
        except Exception as e:
            self._logger.exception(f"Error importing workflow from {workflow_path}: {str(e)}")
            return {
                "success": False,
                "error": f"Import failed: {str(e)}"
            }
    
    def _generate_checksum(self, file_path: Path) -> str:
        """
        Generate SHA-256 checksum of a file.
        
        Args:
            file_path: Path to the file
            
        Returns:
            Hex digest of the checksum
        """
        sha256 = hashlib.sha256()
        with open(file_path, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                sha256.update(chunk)
        return sha256.hexdigest()
    
    async def _detect_dependencies(self, workflow: Workflow) -> Dict[str, str]:
        """
        Detect external dependencies of a workflow.
        
        Args:
            workflow: The workflow to analyze
            
        Returns:
            Dictionary of dependencies
        """
        dependencies = {}
        
        # Check for tool dependencies in commands
        for step in workflow.steps:
            command = step.command.split()[0] if step.command else ""
            
            # Common tools to check
            if command in ["python", "python3"]:
                # Check Python version
                result = await self._run_command("python --version")
                if result["success"]:
                    dependencies["python"] = result["stdout"].strip().replace("Python ", "")
            elif command in ["node", "npm"]:
                # Check Node.js/npm version
                result = await self._run_command("node --version")
                if result["success"]:
                    dependencies["node"] = result["stdout"].strip().replace("v", "")
            elif command == "docker":
                # Check Docker version
                result = await self._run_command("docker --version")
                if result["success"]:
                    dependencies["docker"] = result["stdout"].strip()
            # Add more tool checks as needed
        
        return dependencies
    
    async def _run_command(self, command: str) -> Dict[str, Any]:
        """
        Run a shell command and return its output.
        
        Args:
            command: The command to run
            
        Returns:
            Dictionary with command results
        """
        import asyncio
        
        try:
            process = await asyncio.create_subprocess_shell(
                command,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            
            stdout, stderr = await process.communicate()
            
            return {
                "command": command,
                "stdout": stdout.decode('utf-8', errors='replace').strip(),
                "stderr": stderr.decode('utf-8', errors='replace').strip(),
                "return_code": process.returncode,
                "success": process.returncode == 0
            }
        except Exception as e:
            self._logger.error(f"Error running command '{command}': {str(e)}")
            return {
                "command": command,
                "stdout": "",
                "stderr": str(e),
                "return_code": -1,
                "success": False
            }

# Global workflow sharing manager instance
workflow_sharing_manager = WorkflowSharingManager(get_workflow_manager)
</file>

<file path="core/__init__.py">
# angela/core/__init__.py
"""
Core infrastructure for Angela CLI.

This module provides the foundation for all Angela components.
"""
from angela.core.registry import registry, singleton_service

__all__ = ['registry', 'singleton_service']
</file>

<file path="core/events.py">
# angela/core/events.py
from typing import Dict, Any, Callable, List
import asyncio
from angela.utils.logging import get_logger

class EventBus:
    """Central event bus for system-wide communication."""
    
    def __init__(self):
        self._handlers: Dict[str, List[Callable]] = {}
        self._logger = get_logger(__name__)
    
    def subscribe(self, event_type: str, handler: Callable) -> None:
        """Subscribe to an event type."""
        if event_type not in self._handlers:
            self._handlers[event_type] = []
        self._handlers[event_type].append(handler)
        self._logger.debug(f"Handler subscribed to {event_type}")
    
    def unsubscribe(self, event_type: str, handler: Callable) -> None:
        """Unsubscribe from an event type."""
        if event_type in self._handlers and handler in self._handlers[event_type]:
            self._handlers[event_type].remove(handler)
            self._logger.debug(f"Handler unsubscribed from {event_type}")
    
    async def publish(self, event_type: str, data: Dict[str, Any]) -> None:
        """Publish an event to all subscribers."""
        self._logger.debug(f"Publishing event: {event_type}")
        
        if event_type not in self._handlers:
            return
            
        # Call all handlers asynchronously
        tasks = []
        for handler in self._handlers[event_type]:
            if asyncio.iscoroutinefunction(handler):
                tasks.append(asyncio.create_task(handler(event_type, data)))
            else:
                handler(event_type, data)
        
        # Wait for all async handlers to complete
        if tasks:
            await asyncio.gather(*tasks, return_exceptions=True)

# Global event bus instance
event_bus = EventBus()
</file>

<file path="core/registry.py">
# angela/core/registry.py
"""
Enhanced service registry with improved error handling and initialization.

This registry provides centralized access to all components in the Angela CLI.
It supports lazy initialization, dependency injection, and proper error handling.
"""
from typing import Dict, Any, Type, Optional, Callable, TypeVar, List, Set
import logging
import threading
from functools import wraps

# Type variable for generic methods
T = TypeVar('T')

class ServiceRegistry:
    """
    Service registry with improved error handling and initialization support.
    
    This registry implements:
    - Lazy initialization via get_or_create
    - Factory functions for complex initialization
    - Instance tracking for debugging
    - Thread safety for concurrent access
    """
    
    _instance = None
    _lock = threading.RLock()
    
    @classmethod
    def get_instance(cls) -> 'ServiceRegistry':
        """Get the singleton instance of the registry."""
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = ServiceRegistry()
        return cls._instance
    
    def __init__(self):
        """Initialize the registry."""
        self._services: Dict[str, Any] = {}
        self._factories: Dict[str, Callable[[], Any]] = {}
        self._service_types: Dict[str, Type] = {}
        self._initialization_order: List[str] = []
        self._logger = logging.getLogger(__name__)
    
    def register(self, name: str, service: Any) -> Any:
        """
        Register a service with the registry.
        
        Args:
            name: Unique identifier for the service
            service: The service instance to register
            
        Returns:
            The registered service (for method chaining)
        """
        with self._lock:
            self._services[name] = service
            self._service_types[name] = type(service)
            if name not in self._initialization_order:
                self._initialization_order.append(name)
            
            self._logger.debug(f"Registered service: {name} ({type(service).__name__})")
            return service
    
    def register_factory(self, name: str, factory: Callable[[], Any]) -> None:
        """
        Register a factory function for lazy initialization.
        
        Args:
            name: Unique identifier for the service
            factory: Callable that returns the service instance
        """
        with self._lock:
            self._factories[name] = factory
            self._logger.debug(f"Registered factory for: {name}")
    
    def get(self, name: str) -> Optional[Any]:
        """
        Get a service from the registry.
        
        Args:
            name: The name of the service to retrieve
            
        Returns:
            The service instance or None if not found
        """
        # Return existing service if available
        if name in self._services:
            return self._services[name]
        
        # Try to create service using factory if available
        if name in self._factories:
            try:
                with self._lock:
                    # Check again in case another thread created it
                    if name in self._services:
                        return self._services[name]
                    
                    self._logger.debug(f"Creating service via factory: {name}")
                    service = self._factories[name]()
                    return self.register(name, service)
            except Exception as e:
                self._logger.error(f"Error creating service '{name}' via factory: {e}", exc_info=True)
                return None
        
        return None
    
    def get_or_create(self, name: str, cls: Type[T], factory: Optional[Callable[[], T]] = None, *args, **kwargs) -> T:
        """
        Get a service or create it if it doesn't exist.
        
        Args:
            name: Service name
            cls: Class for type checking and default instantiation
            factory: Optional factory function to create the instance.
                     If provided, cls is used for type checking only.
            *args, **kwargs: Arguments to pass to the class constructor if factory is None.
            
        Returns:
            The existing or newly created service
            
        Raises:
            Exception: If service creation fails
        """
        service = self.get(name) 
        if service is not None:
            if not isinstance(service, cls): 
                self._logger.warning(
                    f"Type mismatch for service '{name}': expected {cls.__name__}, "
                    f"got {type(service).__name__}. Service was likely created by a different factory/method."
                )
            return service
        
        try:
            with self._lock:
                if name in self._services:
                    return self._services[name]
                
                self._logger.debug(f"Creating service: {name} (Type hint: {cls.__name__})")
                if factory:
                    self._logger.debug(f"Using provided factory for {name}")
                    service_instance = factory()
                else:
                    self._logger.debug(f"Using direct instantiation for {name} with class {cls.__name__}")
                    service_instance = cls(*args, **kwargs)
                
                if not isinstance(service_instance, cls):
                    self._logger.error(
                        f"Factory for '{name}' produced an instance of type {type(service_instance).__name__}, "
                        f"but type {cls.__name__} was expected for type checking."
                    )
                    raise TypeError(f"Factory for {name} did not produce an instance of {cls.__name__}")

                return self.register(name, service_instance)
        except Exception as e:
            self._logger.error(f"Error creating service '{name}': {e}", exc_info=True)
            raise
    
    def clear(self) -> None:
        """Clear all registered services."""
        with self._lock:
            self._logger.debug("Clearing service registry")
            self._services.clear()
            self._factories.clear()
            self._service_types.clear()
            self._initialization_order.clear()
    
    def partial_clear(self, prefix: str) -> None:
        """
        Clear services with names starting with the given prefix.
        
        Args:
            prefix: Prefix for service names to clear
        """
        with self._lock:
            to_remove = [
                name for name in self._services
                if name.startswith(prefix)
            ]
            
            for name in to_remove:
                self._services.pop(name, None)
                self._factories.pop(name, None)
                self._service_types.pop(name, None)
                
                if name in self._initialization_order:
                    self._initialization_order.remove(name)
            
            self._logger.debug(f"Cleared {len(to_remove)} services with prefix '{prefix}'")
    
    def list_services(self) -> Dict[str, Type]:
        """
        Get a dictionary of all registered services and their types.
        
        Returns:
            Dictionary of service names and their types
        """
        with self._lock:
            return self._service_types.copy()
    
    def get_initialization_order(self) -> List[str]:
        """
        Get the order in which services were initialized.
        
        Returns:
            List of service names in initialization order
        """
        with self._lock:
            return self._initialization_order.copy()


    def get_safe(self, name: str, fallback_import_path: Optional[str] = None) -> Optional[Any]:
        """
        Get a service with fallback to direct import if needed.
        
        Args:
            name: Service name to retrieve
            fallback_import_path: Optional import path to try if service not found
            
        Returns:
            Service instance or None
        """
        # Try registry first
        service = self.get(name)
        if service is not None:
            return service
            
        # Try fallback import if provided
        if fallback_import_path:
            try:
                module_path, attr_name = fallback_import_path.rsplit('.', 1)
                module = __import__(module_path, fromlist=[attr_name])
                return getattr(module, attr_name)
            except (ImportError, AttributeError) as e:
                self._logger.error(f"Fallback import failed for {fallback_import_path}: {e}")
        
        return None
        

# Create global registry instance
registry = ServiceRegistry.get_instance()


# Optional: Decorator for singleton services
def singleton_service(service_name: Optional[str] = None):
    """
    Decorator to mark a class as a singleton service.
    
    When a class is decorated with @singleton_service, it will be
    automatically registered in the global registry the first time
    it's instantiated.
    
    Args:
        service_name: Optional name for the service. If not provided,
                     the class name will be used.
    """
    def decorator(cls: Type[T]) -> Type[T]:
        orig_init = cls.__init__
        
        @wraps(orig_init)
        def __init__(self, *args, **kwargs):
            # Store the original init arguments
            self._init_args = args
            self._init_kwargs = kwargs
            
            # Call the original __init__
            orig_init(self, *args, **kwargs)
            
            # Register the instance
            name = service_name or cls.__name__.lower()
            registry.register(name, self)
        
        # Replace the __init__ method
        cls.__init__ = __init__
        
        # Add a get_instance classmethod
        @classmethod
        def get_instance(cls, *args, **kwargs):
            name = service_name or cls.__name__.lower()
            return registry.get_or_create(name, cls, *args, **kwargs)
        
        cls.get_instance = get_instance
        
        return cls
    
    return decorator
</file>

<file path="utils/async_utils.py">
# angela/utils/async_utils.py
"""
Utility functions for handling async operations in synchronous contexts.

This module provides helper functions to safely run asynchronous code
from synchronous contexts without causing runtime errors.
"""
import asyncio
import functools
import threading
from typing import Any, Callable, Coroutine, TypeVar, cast, Optional

from angela.utils.logging import get_logger

logger = get_logger(__name__)

T = TypeVar('T')

def run_async(coroutine: Coroutine[Any, Any, T]) -> T:
    """
    Run an async coroutine from a synchronous context.
    
    This function creates a new event loop, runs the coroutine to completion,
    and then closes the loop. It's safe to use when there is no active
    event loop in the current thread.
    
    Args:
        coroutine: The coroutine (async function) to run
        
    Returns:
        The result of the coroutine
        
    Example:
        ```
        # Instead of this (which will fail if no event loop):
        result = asyncio.create_task(some_async_function())
        
        # Use this:
        result = run_async(some_async_function())
        ```
    """
    try:
        # Check if there's already a running loop
        try:
            loop = asyncio.get_running_loop()
            logger.warning(
                "Called run_async from a thread that already has a running event loop. "
                "This might lead to unexpected behavior."
            )
            raise RuntimeError(
                "run_async called from an async context. "
                "Use 'await coroutine' directly instead."
            )
        except RuntimeError:
            pass
        
        # Create a new event loop
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            return loop.run_until_complete(coroutine)
        finally:
            # Always close the loop to clean up resources
            loop.close()
            # Reset the event loop to None to avoid affecting other code
            asyncio.set_event_loop(None)
    except Exception as e:
        logger.error(f"Error running async coroutine: {str(e)}")
        raise  # Re-raise the exception after logging

def to_sync(async_func: Callable[..., Coroutine[Any, Any, T]]) -> Callable[..., T]:
    """
    Convert an async function to a synchronous function.
    
    This decorator wraps an async function so it can be called
    like a regular synchronous function.
    
    Args:
        async_func: The async function to convert
        
    Returns:
        A synchronous version of the function
        
    Example:
        ```
        @to_sync
        async def fetch_data(url):
            # async code here
            return result
            
        # Now you can call it synchronously:
        result = fetch_data("https://example.com")
        ```
    """
    @functools.wraps(async_func)
    def wrapper(*args: Any, **kwargs: Any) -> T:
        coroutine = async_func(*args, **kwargs)
        return run_async(coroutine)
    return wrapper

def run_async_background(
    coroutine: Coroutine[Any, Any, T], 
    callback: Optional[Callable[[T], None]] = None,
    error_callback: Optional[Callable[[Exception], None]] = None
) -> threading.Thread:
    """
    Run an async coroutine in a background thread with optional callbacks.
    
    This is useful for fire-and-forget operations where you don't need
    to wait for the result.
    
    Args:
        coroutine: The coroutine to run
        callback: Optional function to call with the result when done
        error_callback: Optional function to call if an error occurs
        
    Returns:
        The background thread object (already started)
        
    Example:
        ```
        def on_complete(result):
            print(f"Operation completed with result: {result}")
            
        def on_error(error):
            print(f"Operation failed: {error}")
            
        thread = run_async_background(
            long_running_operation(), 
            callback=on_complete,
            error_callback=on_error
        )
        # Continue with other work while operation runs in background
        ```
    """
    def _run_in_thread() -> None:
        try:
            result = run_async(coroutine)
            if callback:
                callback(result)
        except Exception as e:
            logger.error(f"Error in background async task: {str(e)}")
            if error_callback:
                error_callback(e)
                
    thread = threading.Thread(target=_run_in_thread)
    thread.daemon = True  # Make the thread exit when the main program exits
    thread.start()
    return thread
</file>

<file path="utils/logging.py">
# angela/utils/logging.py
from angela.components.utils.logging import setup_logging, get_logger

__all__ = ['setup_logging', 'get_logger']
</file>

<file path="__init__.py">
# angela/__init__.py
"""
Angela CLI: AI-powered command-line assistant integrated into your terminal shell.
"""

__version__ = '0.1.0'

# Import key functions needed at the top level
from angela.api.cli import app
from angela.core.registry import registry


def init_application():
    """Initialize all application components."""

    from angela.components.safety import register_safety_functions
    register_safety_functions()   
    
    # Import and register core components
    from angela.api.cli import get_app
    from angela.api.execution import get_execution_engine, get_adaptive_engine
    from angela.api.safety import get_command_validator
    from angela.api.context import get_context_manager, get_semantic_context_manager
    from angela.orchestrator import orchestrator
    
    # Register critical components
    registry.register("app", get_app())
    registry.register("execution_engine", get_execution_engine())
    registry.register("adaptive_engine", get_adaptive_engine())
    registry.register("orchestrator", orchestrator)
    
    # Initialize toolchain components
    from angela.api.toolchain import get_universal_cli_translator, get_enhanced_universal_cli
    from angela.api.toolchain import get_cross_tool_workflow_engine, get_ci_cd_integration
    
    # Register toolchain components
    registry.register("universal_cli_translator", get_universal_cli_translator())
    registry.register("enhanced_universal_cli", get_enhanced_universal_cli())
    registry.register("cross_tool_workflow_engine", get_cross_tool_workflow_engine())
    registry.register("ci_cd_integration", get_ci_cd_integration())
    
    # Initialize monitoring components
    from angela.api.monitoring import get_proactive_assistant
    proactive_assistant = get_proactive_assistant()
    registry.register("proactive_assistant", proactive_assistant)
    
    # Start proactive assistant
    try:
        proactive_assistant.start()
    except Exception as e:
        from angela.utils.logging import get_logger
        logger = get_logger(__name__)
        logger.error(f"Failed to start proactive assistant: {str(e)}")
    
    # Initialize context and semantic context
    context_manager = get_context_manager()
    semantic_context_manager = get_semantic_context_manager()
    
    # Initialize project inference
    from angela.api.context import initialize_project_inference
    initialize_project_inference()
    
    # Log initialization completion
    from angela.utils.logging import get_logger
    logger = get_logger(__name__)
    logger.info("Application initialization completed")
</file>

<file path="__main__.py">
# angela/__main__.py 
"""
Entry point for Angela CLI.
"""
from angela.components.cli import app
from angela import init_application

if __name__ == "__main__":
    # Initialize all application components
    init_application()
    
    # CLI application
    app()
</file>

<file path="config.py">
# angela/config.py
"""
Configuration management for Angela CLI.
Uses TOML format for configuration files.
"""
import os
from pathlib import Path
from typing import Dict, Any, Optional
import sys
from angela.utils.logging import get_logger


# --- TOML Library Handling ---

# Reader (tomllib for >= 3.11, tomli for < 3.11)
if sys.version_info >= (3, 11):
    import tomllib
    _TOML_LOAD_AVAILABLE = True
    _TOML_READ_ERROR_TYPE = tomllib.TOMLDecodeError
else:
    try:
        import tomli as tomllib # Alias tomli as tomllib
        _TOML_LOAD_AVAILABLE = True
        _TOML_READ_ERROR_TYPE = tomllib.TOMLDecodeError
    except ImportError:
        tomllib = None
        _TOML_LOAD_AVAILABLE = False
        _TOML_READ_ERROR_TYPE = Exception # Fallback

# Writer (tomli-w)
try:
    import tomli_w
    _TOML_WRITE_AVAILABLE = True
except ImportError:
    tomli_w = None
    _TOML_WRITE_AVAILABLE = False

# --- Pydantic and Environment Handling ---
from pydantic import BaseModel, Field
from dotenv import load_dotenv

from angela.constants import CONFIG_DIR, CONFIG_FILE
CONFIG_DIR = Path.home() / ".angela"

# --- Configuration Models ---

class ApiConfig(BaseModel):
    """API configuration settings."""
    gemini_api_key: Optional[str] = Field(None, description="Google Gemini API Key")


class UserConfig(BaseModel):
    """User-specific configuration settings."""
    default_project_root: Optional[Path] = Field(None, description="Default project root directory")
    confirm_all_actions: bool = Field(False, description="Whether to confirm all actions regardless of risk level")


class AppConfig(BaseModel):
    """Application configuration settings."""
    api: ApiConfig = Field(default_factory=ApiConfig, description="API configuration")
    user: UserConfig = Field(default_factory=UserConfig, description="User configuration")
    debug: bool = Field(False, description="Enable debug mode")


# --- Configuration Manager ---

class ConfigManager:
    """Manages the configuration for the Angela CLI application using TOML."""

    def __init__(self):
        """Initializes the ConfigManager with default settings."""
        self._config: AppConfig = AppConfig()
        self.CONFIG_DIR = CONFIG_DIR
        self._load_environment()
        self._ensure_config_dir()
        self._logger = get_logger(__name__)
        # Note: Loading from file happens via the global instance later

    def _load_environment(self) -> None:
        """Loads API keys from environment variables and .env file."""
        load_dotenv() # Load .env file if present
        gemini_api_key = os.getenv("GEMINI_API_KEY")
        if gemini_api_key:
            self._config.api.gemini_api_key = gemini_api_key
            # Add other environment variable loadings here if needed

    def _ensure_config_dir(self) -> None:
        """Ensures the application's configuration directory exists."""
        try:
            CONFIG_DIR.mkdir(parents=True, exist_ok=True)
        except OSError as e:
            print(f"Error creating configuration directory {CONFIG_DIR}: {e}")
            # Depending on severity, might want to raise or exit here

    # Update in angela/config.py
    
    def load_config(self) -> None:
        """Loads configuration from the TOML config file."""
        if not CONFIG_FILE.exists():
            # Only log at debug level, not print to console
            self._logger.debug(f"Configuration file not found at '{CONFIG_FILE}'. Saving default configuration.")
            self.save_config() # Save default TOML config
            return
        
        if not _TOML_LOAD_AVAILABLE:
            self._logger.error(f"Warning: Cannot load TOML config file '{CONFIG_FILE}'.")
            if sys.version_info < (3, 11):
                self._logger.error("       Reason: 'tomli' package is not installed for this Python version.")
                self._logger.error("       To fix, ensure 'tomli; python_version < \"3.11\"' is in your dependencies.")
            else:
                 self._logger.error("       Reason: Could not import the built-in 'tomllib' module.") # Should be unlikely
            self._logger.error("       Using default configuration and environment variables.")
            return
        
        try:
            # Changed: only log at debug level, not print to console
            self._logger.debug(f"Loading configuration from: {CONFIG_FILE}")
            with open(CONFIG_FILE, "rb") as f: # TOML requires binary read mode
                config_data = tomllib.load(f)
        
            # Update configuration with loaded data, using Pydantic validation
            if "api" in config_data and isinstance(config_data["api"], dict):
                self._config.api = ApiConfig(**config_data["api"])
        
            if "user" in config_data and isinstance(config_data["user"], dict):
                 # Pydantic will handle Path conversion from string during validation
                 self._config.user = UserConfig(**config_data["user"])
        
            if "debug" in config_data:
                # Explicitly check type for robustness
                if isinstance(config_data["debug"], bool):
                     self._config.debug = config_data["debug"]
                else:
                     self._logger.warning(f"Invalid type for 'debug' in {CONFIG_FILE}. Expected boolean, got {type(config_data['debug'])}. Ignoring.")
        
        except _TOML_READ_ERROR_TYPE as e:
             self._logger.error(f"Error decoding TOML configuration file ({CONFIG_FILE}): {e}")
             self._logger.error("       Please check the file syntax. Using default configuration and environment variables.")
             self._logger.error("       Resetting configuration to default.")
             self._config = AppConfig()
             self._load_environment()
        except FileNotFoundError as e:
            self._logger.error(f"Configuration file not found: {e}")
            self._logger.error("       Using default configuration and environment variables.")
            self._config = AppConfig()
            self._load_environment()
        except PermissionError as e:
            self._logger.error(f"Permission error accessing configuration file: {e}")
            self._logger.error("       Using default configuration and environment variables.")
            self._config = AppConfig()
            self._load_environment()
        except IOError as e:
            self._logger.error(f"I/O error accessing configuration file: {e}")
            self._logger.error("       Using default configuration and environment variables.")
            self._config = AppConfig()
            self._load_environment()


    def save_config(self) -> None:
        """Saves the current configuration to the config file (as TOML)."""
        if not _TOML_WRITE_AVAILABLE:
             print(f"Error: Cannot save TOML config. 'tomli-w' package not installed.")
             print(f"       To fix, add 'tomli-w' to your dependencies and reinstall.")
             print(f"       Skipping save to {CONFIG_FILE}.")
             return

        try:
            # Convert Pydantic model to dict.
            # Need to handle Path object manually for TOML serialization.
            config_dict = self._config.model_dump()
            if config_dict.get("user", {}).get("default_project_root"):
               # Convert Path to string if it exists
               config_dict["user"]["default_project_root"] = str(config_dict["user"]["default_project_root"])

            # Ensure parent directory exists (usually handled by _ensure_config_dir)
            CONFIG_FILE.parent.mkdir(parents=True, exist_ok=True)

            # Write to file using tomli_w in binary write mode
            with open(CONFIG_FILE, "wb") as f:
                tomli_w.dump(config_dict, f)
            print(f"Configuration saved successfully to {CONFIG_FILE}") # Confirmation message

        except Exception as e:
            print(f"Error saving TOML configuration to {CONFIG_FILE}: {e}")
            # Consider more specific error handling if needed


    @property
    def config(self) -> AppConfig:
        """Provides read-only access to the current application configuration."""
        return self._config


# --- Global Instance ---

# Create a single, globally accessible instance of the ConfigManager
config_manager = ConfigManager()

# Load the configuration from file immediately when this module is imported.
# This makes the loaded config available to other modules that import config_manager.
config_manager.load_config()
</file>

<file path="constants.py">
# angela/constants.py
"""
Constants for the Angela CLI application.
"""
from pathlib import Path
import os

# Application information
APP_NAME = "angela-cli"
APP_VERSION = "0.1.0"
APP_DESCRIPTION = "AI-powered command-line assistant integrated into your terminal shell"

# Paths
BASE_DIR = Path(__file__).parent.parent.absolute()
CONFIG_DIR = Path(os.path.expanduser("~/.config/angela"))
CONFIG_FILE = CONFIG_DIR / "config.toml"
LOG_DIR = CONFIG_DIR / "logs"
HISTORY_FILE = CONFIG_DIR / "history.json"

# Shell integration
SHELL_INVOKE_COMMAND = "angela"
BASH_INTEGRATION_PATH = BASE_DIR / "shell" / "angela.bash"
ZSH_INTEGRATION_PATH = BASE_DIR / "shell" / "angela.zsh"

# Project markers for detection
PROJECT_MARKERS = [
    ".git",               # Git repository
    "package.json",       # Node.js project
    "requirements.txt",   # Python project
    "Cargo.toml",         # Rust project
    "pom.xml",            # Maven project
    "build.gradle",       # Gradle project
    "Dockerfile",         # Docker project
    "docker-compose.yml", # Docker Compose project
    "CMakeLists.txt",     # CMake project
    "Makefile",           # Make project
]

# Logging
LOG_FORMAT = "{time:YYYY-MM-DD HH:mm:ss} | {level} | {message}"
LOG_ROTATION = "100 MB"
LOG_RETENTION = "10 days"

# API
GEMINI_MODEL = "gemini-2.5-pro-preview-03-25"
GEMINI_MAX_TOKENS = 4000
GEMINI_TEMPERATURE = 0.2
REQUEST_TIMEOUT = 45  # seconds

# Safety
RISK_LEVELS = {
    "SAFE": 0,            # Reading operations, info commands
    "LOW": 1,             # Directory creation, simple file operations
    "MEDIUM": 2,          # File content changes, non-critical configurations
    "HIGH": 3,            # System configuration, package installation
    "CRITICAL": 4,        # Destructive operations, security-sensitive changes
}

# Default confirmation requirements by risk level
DEFAULT_CONFIRMATION_REQUIREMENTS = {
    0: False,  # SAFE: No confirmation needed
    1: False,  # LOW: No confirmation needed
    2: True,   # MEDIUM: Confirmation needed
    3: True,   # HIGH: Confirmation needed
    4: True,   # CRITICAL: Confirmation needed with warning
}
</file>

<file path="orchestrator.py">
# angela/orchestrator.py
"""
Main orchestration service for Angela CLI.

This module coordinates all the components of Angela CLI, from receiving
user requests to executing commands with safety checks.
"""
import asyncio
import re
import shlex
from typing import Dict, Any, Optional, List, Tuple, Union
from pathlib import Path
from enum import Enum
import uuid
from rich.console import Console
from datetime import datetime
from rich.panel import Panel

# Use API imports to avoid circular dependencies
from angela.api.ai import get_gemini_client, get_gemini_request_class, get_parse_ai_response_func
from angela.api.ai import get_build_prompt_func, get_error_analyzer, get_content_analyzer, get_intent_analyzer
from angela.api.ai import get_confidence_scorer, get_command_suggestion_class
from angela.api.context import get_context_manager, get_session_manager, get_history_manager, get_file_resolver
from angela.api.context import get_file_activity_tracker, get_activity_type, get_context_enhancer
from angela.api.execution import get_execution_engine, get_adaptive_engine, get_rollback_manager, get_execution_hooks
from angela.api.intent import get_task_planner, get_plan_model_classes, get_enhanced_task_planner
from angela.api.workflows import get_workflow_manager
from angela.api.shell import get_terminal_formatter, get_output_type_enum
from angela.api.shell import display_advanced_plan, display_execution_results
from angela.api.monitoring import get_background_monitor, get_network_monitor
from angela.api.safety import get_command_risk_classifier, get_adaptive_confirmation
from angela.api.toolchain import get_docker_integration

# Get the core models and classes
from angela.utils.logging import get_logger

# Import execution modules via api to avoid circular imports
from angela.api.execution import get_execution_engine, get_adaptive_engine
execution_engine = get_execution_engine()
adaptive_engine = get_adaptive_engine()


# Import other required components
GeminiRequest = get_gemini_request_class()
parse_ai_response = get_parse_ai_response_func()
build_prompt = get_build_prompt_func()
context_manager = get_context_manager()
session_manager = get_session_manager()
history_manager = get_history_manager()
error_analyzer = get_error_analyzer()
intent_analyzer = get_intent_analyzer() 
confidence_scorer = get_confidence_scorer()
task_planner = get_task_planner()
workflow_manager = get_workflow_manager()
file_resolver = get_file_resolver()
file_activity_tracker = get_file_activity_tracker()
ActivityType = get_activity_type()
terminal_formatter = get_terminal_formatter()
OutputType = get_output_type_enum()
rollback_manager = get_rollback_manager()
execution_hooks = get_execution_hooks()
background_monitor = get_background_monitor()
network_monitor = get_network_monitor()
content_analyzer = get_content_analyzer()
context_enhancer = get_context_enhancer()
gemini_client = get_gemini_client()
enhanced_task_planner = get_enhanced_task_planner()
CommandSuggestion = get_command_suggestion_class()

# Get classification and confirmation from API
from angela.api.safety import (
    classify_command_risk,  # Imports the helper function from api/safety.py
    analyze_command_impact, # Imports the new helper function from api/safety.py
    get_adaptive_confirmation # Imports the getter for the adaptive_confirmation instance
)

adaptive_confirmation_handler = get_adaptive_confirmation()

# Get Docker integration
docker_integration = get_docker_integration()

# Get necessary plan model classes
plan_models = get_plan_model_classes()
PlanStep = plan_models[0]
TaskPlan = plan_models[1]
PlanStepType = plan_models[2]
AdvancedPlanStep = plan_models[3] 
AdvancedTaskPlan = plan_models[4]

logger = get_logger(__name__)

console = Console()


class RequestType(Enum):
    """Types of requests that can be handled by the orchestrator."""
    COMMAND = "command"                # Single command suggestion
    MULTI_STEP = "multi_step"          # Multi-step operation
    FILE_CONTENT = "file_content"      # File content analysis/manipulation
    WORKFLOW_DEFINITION = "workflow"   # Define a new workflow
    WORKFLOW_EXECUTION = "run_workflow" # Execute a workflow
    CLARIFICATION = "clarification"    # Request for clarification
    CODE_GENERATION = "code_generation" # Generate a complete project
    FEATURE_ADDITION = "feature_addition" # Add feature to existing project
    TOOLCHAIN_OPERATION = "toolchain_operation" # DevOps operations (CI/CD, etc.)
    CODE_REFINEMENT = "code_refinement" # Refine/improve existing code
    CODE_ARCHITECTURE = "code_architecture" # Analyze or enhance architecture
    UNKNOWN = "unknown"                # Unknown request type
    UNIVERSAL_CLI = "universal_cli"  # Request to use the Universal CLI Translator
    COMPLEX_WORKFLOW = "complex_workflow"  # Complex workflow involving multiple tools
    CI_CD_PIPELINE = "ci_cd_pipeline"  # CI/CD pipeline setup and execution
    PROACTIVE_SUGGESTION = "proactive_suggestion"    
        
class Orchestrator:
    """Main orchestration service for Angela CLI."""
    
    def __init__(self):
        """Initialize the orchestrator."""
        self._logger = logger
        self._background_tasks = set()
        self._background_monitor = background_monitor
        self._network_monitor = network_monitor
        
        # Register the monitoring insight callback
        self._background_monitor.register_insight_callback(self._handle_monitoring_insight)
        
    async def process_request(
        self, 
        request: str, 
        execute: bool = True,
        dry_run: bool = False
    ) -> Dict[str, Any]:
        '''
        Process a request from the user with enhanced context.
        
        Args:
            request: The user request
            execute: Whether to execute commands
            dry_run: Whether to simulate execution without making changes
            
        Returns:
            Dictionary with processing results
        '''
        
        # Initialize dependencies we'll need (getting from registry avoids circular imports)
        error_recovery_manager = self._get_error_recovery_manager()
            
        # Refresh context to ensure we have the latest information
        context_manager.refresh_context()
        context = context_manager.get_context_dict()
        
        # Add session context for continuity across requests
        session_context = session_manager.get_context()
        context["session"] = session_context
        
        # Enhance context with project information, dependencies, and recent activity
        try:
            # Get context enhancer from registry
            from angela.core.registry import registry
            context_enhancer = registry.get("context_enhancer")
            
            if context_enhancer:
                # If available in registry, use it
                context = await context_enhancer.enrich_context(context)
            else:
                # If not in registry, try direct import as fallback
                self._logger.warning("context_enhancer not found in registry, attempting direct import")
                try:
                    # Use API layer to access context_enhancer
                    from angela.api.context import get_context_enhancer
                    context_enhancer = get_context_enhancer()
                    if context_enhancer:
                        # Register for future use
                        registry.register("context_enhancer", context_enhancer)
                        context = await context_enhancer.enrich_context(context)
                    else:
                        self._logger.warning("context_enhancer is None after direct import, attempting to create instance")
                        try:
                            # Import via API layer
                            from angela.api.context import get_context_enhancer_class
                            temp_enhancer = get_context_enhancer_class()()
                            registry.register("context_enhancer", temp_enhancer)
                            context = await temp_enhancer.enrich_context(context)
                            self._logger.info("Successfully created and used temporary context_enhancer")
                        except Exception as create_error:
                            self._logger.error(f"Failed to create context_enhancer instance: {create_error}")
                            self._logger.warning("Continuing with basic context")
                except ImportError as e:
                    self._logger.error(f"Failed to import context_enhancer directly: {e}")
                    self._logger.warning("Continuing with basic context")
        except Exception as e:
            self._logger.error(f"Error during context enhancement: {str(e)}")
            self._logger.warning("Continuing with unenriched context")
        
        self._logger.info(f"Processing request: {request}")
        self._logger.debug(f"Context contains {len(context)} keys")
        
        # Perform quick intent analysis to determine if we should extract file references
        request_intent = self._analyze_quick_intent(request)
        
        # Only extract file references for certain intents
        if request_intent in ["read", "modify", "analyze", "unknown"]:
            # Extract and resolve file references
            file_references = await file_resolver.extract_references(request, context)
            if file_references:
                # Add resolved file references to context
                context["resolved_files"] = [
                    {"reference": ref, "path": str(path) if path else None}
                    for ref, path in file_references
                ]
                self._logger.debug(f"Resolved {len(file_references)} file references")
        else:
            self._logger.debug(f"Skipping file resolution for {request_intent} intent")
        
        try:
            # Analyze the request to determine its type
            request_type = await self._determine_request_type(request, context)
            self._logger.info(f"Determined request type: {request_type.value}")
            
   
            # Process the request based on its type
            if request_type == RequestType.COMMAND:
                # Handle single command request
                return await self._process_command_request(request, context, execute, dry_run)
                
            elif request_type == RequestType.MULTI_STEP:
                # Handle multi-step operation
                return await self._process_multi_step_request(request, context, execute, dry_run)
                
            elif request_type == RequestType.FILE_CONTENT:
                # Handle file content analysis/manipulation
                return await self._process_file_content_request(request, context, execute, dry_run)
                
            elif request_type == RequestType.WORKFLOW_DEFINITION:
                # Handle workflow definition
                return await self._process_workflow_definition(request, context)
                
            elif request_type == RequestType.WORKFLOW_EXECUTION:
                # Handle workflow execution
                return await self._process_workflow_execution(request, context, execute, dry_run)
                
            elif request_type == RequestType.CLARIFICATION:
                # Handle request for clarification
                return await self._process_clarification_request(request, context)
                
            elif request_type == RequestType.CODE_GENERATION:
                return await self._process_code_generation_request(request, context, execute, dry_run)
                
            elif request_type == RequestType.FEATURE_ADDITION:
                return await self._process_feature_addition_request(request, context, execute, dry_run)
                
            elif request_type == RequestType.TOOLCHAIN_OPERATION:
                return await self._process_toolchain_operation(request, context, execute, dry_run)
                
            elif request_type == RequestType.CODE_REFINEMENT:
                return await self._process_code_refinement_request(request, context, execute, dry_run)
                
            elif request_type == RequestType.CODE_ARCHITECTURE:
                return await self._process_code_architecture_request(request, context, execute, dry_run)
                
            elif request_type == RequestType.UNIVERSAL_CLI:
                return await self._process_universal_cli_request(request, context, execute, dry_run)
                
            elif request_type == RequestType.COMPLEX_WORKFLOW:
                return await self._process_complex_workflow_request(request, context, execute, dry_run)
                
            elif request_type == RequestType.CI_CD_PIPELINE:
                return await self._process_ci_cd_pipeline_request(request, context, execute, dry_run)
                
            elif request_type == RequestType.PROACTIVE_SUGGESTION:
                return await self._process_proactive_suggestion(request, context)
                
            else:
                # Handle unknown request type
                return await self._process_unknown_request(request, context)
            
        except Exception as e:
            self._logger.exception(f"Error processing request: {str(e)}")
            # Fallback behavior
            return {
                "request": request,
                "response": f"An error occurred while processing your request: {str(e)}",
                "error": str(e),
                "context": context,
            }
            

    
    async def _determine_request_type(
            self, 
            request: str, 
            context: Dict[str, Any]
        ) -> RequestType:
            """
            Determine the type of request.
            
            Args:
                request: The user request
                context: Context information
                
            Returns:
                RequestType enum value
            """
            # Check for keywords and patterns indicating different request types
            
            # Workflow definition patterns
            workflow_def_patterns = [
                r'\b(?:define|create|make|add)\s+(?:a\s+)?(?:new\s+)?workflow\b',
                r'\bworkflow\s+(?:called|named)\b',
                r'\bsave\s+(?:this|these)\s+(?:as\s+(?:a\s+)?)?workflow\b',
            ]
            
            # Workflow execution patterns
            workflow_exec_patterns = [
                r'\brun\s+(?:the\s+)?workflow\b',
                r'\bexecute\s+(?:the\s+)?workflow\b',
                r'\bstart\s+(?:the\s+)?workflow\b',
            ]
            
            # File content patterns
            file_content_patterns = [
                r'\b(?:analyze|understand|summarize|examine)\s+(?:the\s+)?(?:content|code|text)\b',
                r'\b(?:modify|change|update|edit|refactor)\s+(?:the\s+)?(?:content|code|text|file)\b',
                r'\bfind\s+(?:in|inside|within)\s+(?:the\s+)?file\b',
            ]
            
            # Multi-step operation patterns
            multi_step_patterns = [
                r'\b(?:multiple steps|sequence|series|several|many)\b',
                r'\band then\b',
                r'\bafter that\b',
                r'\bone by one\b',
                r'\bstep by step\b',
                r'\bautomatically\b',
            ]
     
            # Docker operation patterns
            docker_patterns = [
                r'\bdocker\b',
                r'\bcontainer\b',
                r'\bdockerfile\b',
                r'\bdocker-compose\b',
                r'\bdocker\s+compose\b',
                r'\bimage\b.+\b(?:build|run|pull|push)\b',
                r'\b(?:build|run|pull|push)\b.+\bimage\b',
                r'\b(?:start|stop|restart|remove)\b.+\bcontainer\b',
                r'\bcontainer\b.+\b(?:start|stop|restart|remove)\b',
                r'\bgenerate\b.+\b(?:dockerfile|docker-compose)\b',
                r'\bsetup\s+docker\b',
                r'\bdocker\s+(?:ps|logs|images|rmi|exec)\b',
            ]
    
            code_generation_patterns = [
                r'\bcreate\s+(?:a\s+)?(?:new\s+)?(?:project|app|website|application)\b',
                r'\bgenerate\s+(?:a\s+)?(?:new\s+)?(?:project|app|website|application)\b',
                r'\bmake\s+(?:a\s+)?(?:new\s+)?(?:project|app|website|application)\b',
                r'\bbuild\s+(?:a\s+)?(?:whole|complete|full|entire)\b',
            ]
            
            # Feature addition patterns
            feature_addition_patterns = [
                r'\badd\s+(?:a\s+)?(?:new\s+)?feature\b',
                r'\bimplement\s+(?:a\s+)?(?:new\s+)?feature\b',
                r'\bcreate\s+(?:a\s+)?(?:new\s+)?feature\b',
                r'\bextend\s+(?:the\s+)?(?:project|app|code|application)\b',
            ]
            
            # Toolchain operation patterns
            toolchain_patterns = [
                r'\bsetup\s+(?:ci|cd|ci/cd|cicd|continuous integration|deployment)\b',
                r'\bconfigure\s+(?:ci|cd|ci/cd|cicd|continuous integration|deployment|git)\b',
                r'\bgenerate\s+(?:ci|cd|jenkins|gitlab|github)\b',
                r'\binstall\s+dependencies\b',
                r'\binitialize\s+(?:git|repo|repository)\b',
            ]
            
            # Code refinement patterns
            refinement_patterns = [
                r'\brefine\s+(?:the\s+)?code\b',
                r'\bimprove\s+(?:the\s+)?code\b',
                r'\boptimize\s+(?:the\s+)?code\b',
                r'\brefactor\s+(?:the\s+)?code\b',
                r'\bupdate\s+(?:the\s+)?code\b',
                r'\benhance\s+(?:the\s+)?code\b',
            ]
            
            # Architecture patterns
            architecture_patterns = [
                r'\banalyze\s+(?:the\s+)?(?:architecture|structure)\b',
                r'\bimprove\s+(?:the\s+)?(?:architecture|structure)\b',
                r'\bredesign\s+(?:the\s+)?(?:architecture|structure)\b',
                r'\bproject\s+structure\b',
            ]
    
            # CI/CD patterns
            ci_cd_patterns = [
                r'\bset\s*up\s+(?:a\s+)?(?:ci|cd|ci/cd|cicd|continuous integration|deployment)(?:\s+pipeline)?\b',
                r'\bcreate\s+(?:a\s+)?(?:ci|cd|ci/cd|cicd|continuous integration)(?:\s+pipeline)?\b',
                r'\bci/cd\s+(?:pipeline|setup|configuration)\b',
                r'\bpipeline\s+(?:setup|configuration|for)\b',
                r'\bgithub\s+actions\b',
                r'\bgitlab\s+ci\b',
                r'\bjenkins(?:file)?\b',
                r'\btravis\s+ci\b',
                r'\bcircle\s+ci\b',
                r'\b(?:automate|automation)\s+(?:build|test|deploy)\b',
            ]
            
            # First check for CI/CD patterns since they're more specific
            for pattern in ci_cd_patterns:
                if re.search(pattern, request, re.IGNORECASE):
                    return RequestType.CI_CD_PIPELINE
            
            # Then check for Universal CLI patterns
            universal_cli_patterns = [
                r'\buse\s+(?:the\s+)?(.+?)\s+(?:cli|command|tool)\b',
                r'\brun\s+(?:a\s+)?(.+?)\s+command\b',
                r'\b(?:execute|with)\s+(?:the\s+)?(.+?)\s+tool\b',
            ]
            
            for pattern in universal_cli_patterns:
                match = re.search(pattern, request, re.IGNORECASE)
                if match:
                    tool = match.group(1).strip().lower()
                    if tool not in ["angela", "workflow"]:  # Exclude Angela's own commands
                        return RequestType.UNIVERSAL_CLI
            
            # Check for complex workflow patterns
            complex_workflow_patterns = [
                r'\bcomplex\s+workflow\b',
                r'\bcomplete\s+(?:ci/cd|cicd|pipeline)\b',
                r'\bautomated\s+(?:build|test|deploy)\b',
                r'\bend-to-end\s+workflow\b',
                r'\bchain\s+of\s+commands\b',
                r'\bmulti-step\s+operation\s+across\b',
                r'\bpipeline\s+using\b',
                r'\bseries\s+of\s+tools\b',
            ]
            
            for pattern in complex_workflow_patterns:
                if re.search(pattern, request, re.IGNORECASE):
                    return RequestType.COMPLEX_WORKFLOW
            
            # Check for common CLI tools explicitly mentioned
            common_tools = ["git", "docker", "aws", "kubectl", "terraform", "npm", "pip", "yarn"]
            tool_words = request.lower().split()
            for tool in common_tools:
                if tool in tool_words:
                    # Make sure it's a standalone word, not part of another word
                    # Check the positions where the tool appears
                    positions = [i for i, word in enumerate(tool_words) if word == tool]
                    for pos in positions:
                        # Check if it's a command (usually preceded by use, run, with, etc.)
                        if pos > 0 and tool_words[pos-1] in ["use", "run", "with", "using", "execute"]:
                            return RequestType.UNIVERSAL_CLI
                    
                    # If tool is the first word in the request, it's likely a direct usage
                    if tool_words[0] == tool:
                        return RequestType.UNIVERSAL_CLI
            
            # Also check for complexity indicators combined with multiple tool mentions
            tool_mentions = sum(1 for tool in ["git", "docker", "aws", "kubernetes", "npm", "pip"] 
                               if tool in request.lower())
            has_complex_indicators = any(indicator in request.lower() for indicator in 
                                       ["pipeline", "sequence", "then", "after", "followed"])
        
            if tool_mentions >= 2 and has_complex_indicators:
                return RequestType.COMPLEX_WORKFLOW
    
            # Check for code generation first (highest priority)
            for pattern in code_generation_patterns:
                if re.search(pattern, request, re.IGNORECASE):
                    return RequestType.CODE_GENERATION
            
            # Check for feature addition
            for pattern in feature_addition_patterns:
                if re.search(pattern, request, re.IGNORECASE):
                    return RequestType.FEATURE_ADDITION
            
            # Check for toolchain operations
            for pattern in toolchain_patterns:
                if re.search(pattern, request, re.IGNORECASE):
                    return RequestType.TOOLCHAIN_OPERATION
            
            # Check for code refinement
            for pattern in refinement_patterns:
                if re.search(pattern, request, re.IGNORECASE):
                    return RequestType.CODE_REFINEMENT
            
            # Check for architecture analysis
            for pattern in architecture_patterns:
                if re.search(pattern, request, re.IGNORECASE):
                    return RequestType.CODE_ARCHITECTURE
     
            # Check for workflow definition
            for pattern in workflow_def_patterns:
                if re.search(pattern, request, re.IGNORECASE):
                    return RequestType.WORKFLOW_DEFINITION
            
            # Check for workflow execution
            for pattern in workflow_exec_patterns:
                if re.search(pattern, request, re.IGNORECASE):
                    return RequestType.WORKFLOW_EXECUTION
    
            # Check for Docker operations first
            for pattern in docker_patterns:
                if re.search(pattern, request, re.IGNORECASE):
                    return RequestType.TOOLCHAIN_OPERATION
    
            # Check for code generation (high priority)
            for pattern in code_generation_patterns:
                if re.search(pattern, request, re.IGNORECASE):
                    return RequestType.CODE_GENERATION
            
            # Check for feature addition
            for pattern in feature_addition_patterns:
                if re.search(pattern, request, re.IGNORECASE):
                    return RequestType.FEATURE_ADDITION
            
            # Check for toolchain operations
            for pattern in toolchain_patterns:
                if re.search(pattern, request, re.IGNORECASE):
                    return RequestType.TOOLCHAIN_OPERATION
            
            # Check for code refinement
            for pattern in refinement_patterns:
                if re.search(pattern, request, re.IGNORECASE):
                    return RequestType.CODE_REFINEMENT
            
            # Check for architecture analysis
            for pattern in architecture_patterns:
                if re.search(pattern, request, re.IGNORECASE):
                    return RequestType.CODE_ARCHITECTURE
    
            for pattern in complex_workflow_patterns:
                if re.search(pattern, request, re.IGNORECASE):
                    return RequestType.COMPLEX_WORKFLOW
                    
            # Check for file content analysis/manipulation
            file_mentions = re.search(r'\b(?:file|code|script|document)\b', request, re.IGNORECASE)
            if file_mentions:
                for pattern in file_content_patterns:
                    if re.search(pattern, request, re.IGNORECASE):
                        return RequestType.FILE_CONTENT
            
            # Check for multi-step operation
            complexity_indicators = sum(bool(re.search(pattern, request, re.IGNORECASE)) for pattern in multi_step_patterns)
            if complexity_indicators >= 1 or len(request.split()) > 15:
                # Complex requests or longer instructions often imply multi-step operations
                return RequestType.MULTI_STEP
            
            # Default to single command
            return RequestType.COMMAND


    def _analyze_quick_intent(self, request: str) -> str:
        """
        Perform a quick analysis of request intent to determine if file resolution is needed.
        
        Args:
            request: The user request
            
        Returns:
            String indicating the high-level intent: "create", "read", "modify", "analyze", "unknown"
        """
        request_lower = request.lower()
        
        # Check for file creation keywords
        creation_keywords = [
            "create", "generate", "make", "new file", "save as", 
            "write a new", "write new", "save it as", "output to"
        ]
        for keyword in creation_keywords:
            if keyword in request_lower:
                return "create"
        
        # Check for file reading keywords
        read_keywords = [
            "read", "open", "show", "display", "view", "cat", 
            "print", "output", "list", "contents of"
        ]
        for keyword in read_keywords:
            if keyword in request_lower:
                return "read"
        
        # Check for file modification keywords
        modify_keywords = [
            "edit", "modify", "update", "change", "replace", "delete",
            "remove", "rename", "move", "copy"
        ]
        for keyword in modify_keywords:
            if keyword in request_lower:
                return "modify"
        
        # Check for analysis keywords
        analyze_keywords = [
            "analyze", "examine", "check", "inspect", "review",
            "summarize", "understand", "evaluate"
        ]
        for keyword in analyze_keywords:
            if keyword in request_lower:
                return "analyze"
        
        # Default case
        return "unknown"

    async def _process_code_generation_request(
        self, 
        request: str, 
        context: Dict[str, Any], 
        execute: bool, 
        dry_run: bool
    ) -> Dict[str, Any]:
        """
        Process a code generation request.
        
        Args:
            request: The user request
            context: Context information
            execute: Whether to execute the generation
            dry_run: Whether to simulate without making changes
            
        Returns:
            Dictionary with processing results
        """
        self._logger.info(f"Processing code generation request: {request}")
        
        # Import here to avoid circular imports - use API layer
        from angela.api.generation import get_code_generation_engine
        code_generation_engine = get_code_generation_engine()
        
        # Project details extraction
        project_details = await self._extract_project_details(request)
        
        # Get output directory (default to current dir)
        output_dir = project_details.get("output_dir", context.get("cwd", "."))
        
        # Get project type if specified
        project_type = project_details.get("project_type")
        
        # Create result structure
        result = {
            "request": request,
            "type": "code_generation",
            "context": context,
            "project_details": project_details
        }
        
        # Skip execution if not requested
        if not execute and not dry_run:
            return result
        
        try:
            # Generate the project using the generation engine
            with console.status(f"[bold green]Generating project based on: {request}[/bold green]"):
                project_plan = await code_generation_engine.generate_project(
                    description=request,  # Use full request as description
                    output_dir=output_dir,
                    project_type=project_type,
                    context=context
                )
            
            # Add project plan to result
            result["project_plan"] = {
                "name": project_plan.name,
                "description": project_plan.description,
                "project_type": project_plan.project_type,
                "file_count": len(project_plan.files),
                "structure_explanation": project_plan.structure_explanation
            }
            
            # Create files if not in dry run mode
            if not dry_run:
                with console.status("[bold green]Creating project files...[/bold green]"):
                    creation_result = await code_generation_engine.create_project_files(project_plan)
                    result["creation_result"] = creation_result
                    result["success"] = creation_result.get("success", False)
            else:
                result["dry_run"] = True
                result["success"] = True
                
            # Add Git initialization if appropriate
            if not dry_run and project_details.get("git_init", True):
                # Use API to get git_integration
                from angela.api.toolchain import get_git_integration
                git_integration = get_git_integration()
                
                with console.status("[bold green]Initializing Git repository...[/bold green]"):
                    git_result = await git_integration.init_repository(
                        path=output_dir,
                        initial_branch="main",
                        gitignore_template=project_plan.project_type
                    )
                    result["git_result"] = git_result
            
            return result
        except Exception as e:
            self._logger.exception(f"Error in code generation: {str(e)}")
            result["error"] = str(e)
            result["success"] = False
            return result

    async def _process_feature_addition_request(
        self, 
        request: str, 
        context: Dict[str, Any], 
        execute: bool, 
        dry_run: bool
    ) -> Dict[str, Any]:
        """
        Process a feature addition request.
        
        Args:
            request: The user request
            context: Context information
            execute: Whether to execute the feature addition
            dry_run: Whether to simulate without making changes
            
        Returns:
            Dictionary with processing results
        """
        self._logger.info(f"Processing feature addition request: {request}")
        
        # Import here to avoid circular imports - use API layer
        from angela.api.generation import get_code_generation_engine
        code_generation_engine = get_code_generation_engine()
        
        # Extract feature details
        feature_details = await self._extract_feature_details(request, context)
        
        # Get project directory (default to current dir)
        project_dir = feature_details.get("project_dir", context.get("cwd", "."))
        
        # Create result structure
        result = {
            "request": request,
            "type": "feature_addition",
            "context": context,
            "feature_details": feature_details
        }
        
        # Skip execution if not requested
        if not execute and not dry_run:
            return result
        
        # Add the feature to the project
        with console.status(f"[bold green]Adding feature to project: {request}[/bold green]"):
            addition_result = await code_generation_engine.add_feature_to_project(
                description=feature_details.get("description", request),
                project_dir=project_dir,
                context=context
            )
        
        result["addition_result"] = addition_result
        
        # Create branch if specified and not in dry run mode
        if not dry_run and feature_details.get("branch"):
            # Use API layer to get git_integration
            from angela.api.toolchain import get_git_integration
            git_integration = get_git_integration()
            
            branch_name = feature_details.get("branch")
            with console.status(f"[bold green]Creating branch: {branch_name}[/bold green]"):
                branch_result = await git_integration.create_branch(
                    path=project_dir,
                    branch_name=branch_name,
                    checkout=True
                )
                result["branch_result"] = branch_result
        
        return result



    async def _process_toolchain_operation(
        self, 
        request: str, 
        context: Dict[str, Any], 
        execute: bool, 
        dry_run: bool
    ) -> Dict[str, Any]:
        """
        Process a toolchain operation request.
        
        Args:
            request: The user request
            context: Context information
            execute: Whether to execute the operation
            dry_run: Whether to simulate without making changes
            
        Returns:
            Dictionary with processing results
        """
        self._logger.info(f"Processing toolchain operation request: {request}")
        
        # Extract operation details
        operation_details = await self._extract_toolchain_operation(request, context)
        
        # Get operation type
        operation_type = operation_details.get("operation_type", "unknown")
        
        # Create result structure
        result = {
            "request": request,
            "type": "toolchain_operation",
            "context": context,
            "operation_details": operation_details,
            "operation_type": operation_type
        }
        
        # Skip execution if not requested
        if not execute and not dry_run:
            return result
        
        # Process based on operation type
        if operation_type == "docker":
            result.update(await self._process_docker_operation(request, operation_details, context, dry_run))
        elif operation_type == "ci_cd":
            result.update(await self._process_ci_cd_operation(operation_details, context, dry_run))
        elif operation_type == "package_management":
            result.update(await self._process_package_operation(operation_details, context, dry_run))
        elif operation_type == "git":
            result.update(await self._process_git_operation(operation_details, context, dry_run))
        elif operation_type == "testing":
            result.update(await self._process_testing_operation(operation_details, context, dry_run))
        else:
            result["error"] = f"Unknown toolchain operation type: {operation_type}"
        
        return result
        
    async def _process_docker_operation(
        self,
        request: str,
        operation_details: Dict[str, Any],
        context: Dict[str, Any],
        dry_run: bool
    ) -> Dict[str, Any]:
        """
        Process a Docker operation request.
        
        Args:
            request: The user request
            operation_details: Details about the operation
            context: Context information
            dry_run: Whether to simulate without making changes
            
        Returns:
            Dictionary with processing results
        """
        self._logger.info(f"Processing Docker operation: {request}")
        
        # Get docker_integration from registry
        from angela.core.registry import registry
        docker_integration = registry.get("docker_integration")
        if not docker_integration:
            return {
                "success": False,
                "error": "Docker integration not available in the system."
            }
            
        # Check Docker availability
        docker_available = await docker_integration.is_docker_available()
        if not docker_available:
            return {
                "success": False,
                "error": "Docker is not available on this system. Please install Docker to use this feature."
            }
            
        # Determine specific Docker operation
        docker_action = operation_details.get("docker_action", "")
        
        # Handle different Docker operations
        if "setup" in request.lower() or "generate" in request.lower() or "dockerfile" in request.lower() or "docker-compose" in request.lower():
            # Generate Docker configuration files
            project_dir = operation_details.get("project_dir", context.get("cwd", "."))
            
            result = await docker_integration.setup_docker_project(
                project_directory=project_dir,
                generate_dockerfile="dockerfile" in request.lower() or "setup" in request.lower(),
                generate_compose="compose" in request.lower() or "setup" in request.lower(),
                generate_dockerignore="dockerignore" in request.lower() or "setup" in request.lower(),
                overwrite=False,  # Default to safe operation
                include_databases="database" in request.lower() or "db" in request.lower(),
                build_image="build" in request.lower() and dry_run is False
            )
            
            # Format the result
            formatted_result = {
                "success": result.get("success", False),
                "message": "Docker setup completed",
                "files_generated": result.get("files_generated", []),
                "docker_details": result
            }
            
            return formatted_result
            
        elif "build" in request.lower() or "image" in request.lower():
            # Build Docker image
            project_dir = operation_details.get("project_dir", context.get("cwd", "."))
            image_tag = operation_details.get("image_tag", "app:latest")
            
            result = await docker_integration.build_image(
                context_path=project_dir,
                tag=image_tag,
                no_cache="no cache" in request.lower()
            )
            
            return {
                "success": result.get("success", False),
                "message": f"Docker image build {'completed' if result.get('success', False) else 'failed'}",
                "image_details": result
            }
            
        elif "run" in request.lower() or "start" in request.lower() or "launch" in request.lower():
            # Run Docker container
            image = operation_details.get("image", "")
            if not image:
                # Try to extract image from request
                image_match = re.search(r'(?:run|start|launch)\s+(?:container\s+)?(?:from\s+)?(\S+)(?:\s+image)?', request, re.IGNORECASE)
                if image_match:
                    image = image_match.group(1)
                else:
                    image = "app:latest"  # Default
            
            # Extract ports if mentioned
            ports = []
            ports_match = re.search(r'port[s]?\s+(\d+(?::\d+)?(?:,\s*\d+(?::\d+)?)*)', request, re.IGNORECASE)
            if ports_match:
                ports_str = ports_match.group(1)
                ports = [p.strip() for p in ports_str.split(',')]
            
            # Run container
            result = await docker_integration.run_container(
                image=image,
                ports=ports,
                detach=True,
                remove="remove" in request.lower() or "rm" in request.lower()
            )
            
            return {
                "success": result.get("success", False),
                "message": f"Docker container {'started' if result.get('success', False) else 'failed to start'}",
                "container_details": result
            }
            
        elif "compose" in request.lower() or "up" in request.lower():
            # Docker Compose operation
            project_dir = operation_details.get("project_dir", context.get("cwd", "."))
            
            # Check Docker Compose availability
            compose_available = await docker_integration.is_docker_compose_available()
            if not compose_available:
                return {
                    "success": False,
                    "error": "Docker Compose is not available on this system. Please install Docker Compose to use this feature."
                }
            
            # Determine if it's compose up or down
            if "down" in request.lower() or "stop" in request.lower():
                result = await docker_integration.compose_down(
                    project_directory=project_dir,
                    remove_volumes="volumes" in request.lower(),
                    remove_images="images" in request.lower() or "rmi" in request.lower()
                )
            else:
                # Default to compose up
                result = await docker_integration.compose_up(
                    project_directory=project_dir,
                    detach=True,
                    build="build" in request.lower()
                )
            
            return {
                "success": result.get("success", False),
                "message": f"Docker Compose operation {'completed' if result.get('success', False) else 'failed'}",
                "compose_details": result
            }
            
        else:
            # Generate and execute appropriate Docker command
            suggestion = await self._get_ai_suggestion(request, context)
            
            if not suggestion.command or not suggestion.command.startswith("docker"):
                return {
                    "success": False,
                    "error": "Unable to generate appropriate Docker command for this request.",
                    "suggestion": suggestion
                }
                
            # Execute the command using the engine
            if dry_run:
                return {
                    "success": True,
                    "dry_run": True,
                    "command": suggestion.command,
                    "explanation": suggestion.explanation
                }
                
            stdout, stderr, exit_code = await execution_engine.execute_command(
                suggestion.command,
                check_safety=True
            )
            
            return {
                "success": exit_code == 0,
                "command": suggestion.command,
                "stdout": stdout,
                "stderr": stderr,
                "return_code": exit_code,
                "explanation": suggestion.explanation
            }
    
    async def _extract_toolchain_operation(
        self, 
        request: str,
        context: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Extract toolchain operation details from a request.
        
        Args:
            request: The user request
            context: Context information
            
        Returns:
            Dictionary with operation details
        """
        # First check if this is a Docker request
        for pattern in [r'\bdocker\b', r'\bcontainer\b', r'\bdockerfile\b', r'\bdocker-compose\b']:
            if re.search(pattern, request, re.IGNORECASE):
                docker_details = await self._extract_docker_operation_details(request, context)
                if docker_details:
                    return docker_details
        
        # Use AI to extract operation details for other toolchain operations
        prompt = f"""
    Extract toolchain operation details from this request:
    "{request}"
    
    Return a JSON object with:
    1. operation_type: One of "ci_cd", "package_management", "git", "testing", "docker"
    2. platform: For CI/CD, the platform (e.g., "github_actions", "gitlab_ci")
    3. project_dir: The project directory (default to ".")
    4. dependencies: For package management, list of dependencies
    5. test_framework: For testing, the test framework
    6. docker_action: For Docker, the specific action (e.g., "build", "run", "compose")
    7. image: For Docker run, the image name
    
    Format:
    {{
      "operation_type": "type",
      "platform": "platform",
      "project_dir": "directory",
      "dependencies": ["dep1", "dep2"],
      "test_framework": "framework"
    }}
    
    Only include keys relevant to the operation type.
    """

        try:
            # Call AI service
            api_request = GeminiRequest(prompt=prompt, max_tokens=1000)
            response = await gemini_client.generate_text(api_request)
            
            # Parse the response
            import json
            import re
            
            # Try to find JSON in the response
            json_match = re.search(r'```(?:json)?\s*(.*?)\s*```', response.text, re.DOTALL)
            if json_match:
                json_str = json_match.group(1)
            else:
                # Assume the entire response is JSON
                json_str = response.text
            
            # Parse JSON
            details = json.loads(json_str)
            
            # Default project directory to context's project_root if available
            if "project_dir" not in details and context.get("project_root"):
                details["project_dir"] = context.get("project_root")
            
            return details
            
        except Exception as e:
            self._logger.error(f"Error extracting toolchain operation details: {str(e)}")
            # Return minimal details on failure
            return {
                "operation_type": "unknown",
                "project_dir": context.get("project_root", ".")
            }
            
    async def _extract_docker_operation_details(
        self,
        request: str,
        context: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Extract Docker operation details from a request.
        
        Args:
            request: The user request
            context: Context information
            
        Returns:
            Dictionary with Docker operation details
        """
        details = {
            "operation_type": "docker",
            "project_dir": context.get("project_root", context.get("cwd", "."))
        }
        
        # Extract Docker action based on keywords
        if re.search(r'\b(build|create)\b.+\b(image|dockerfile)\b', request, re.IGNORECASE) or re.search(r'\bimage.+\b(build|create)\b', request, re.IGNORECASE):
            details["docker_action"] = "build"
            
            # Try to extract image tag if present
            tag_match = re.search(r'tag\s+(\S+)', request, re.IGNORECASE)
            if tag_match:
                details["image_tag"] = tag_match.group(1)
            else:
                details["image_tag"] = "app:latest"
                
        elif re.search(r'\b(run|start|launch)\b.+\b(container|image)\b', request, re.IGNORECASE) or re.search(r'\b(container|image).+\b(run|start|launch)\b', request, re.IGNORECASE):
            details["docker_action"] = "run"
            
            # Try to extract image if present
            image_match = re.search(r'(image|container)\s+(\S+)', request, re.IGNORECASE)
            if image_match:
                details["image"] = image_match.group(2)
            else:
                # Look for any word that might be an image name
                words = request.split()
                for i, word in enumerate(words):
                    if word.lower() in ["image", "from"] and i < len(words) - 1:
                        details["image"] = words[i+1].strip(",:;")
                        break
            
            # Extract ports if present
            port_match = re.search(r'port\s+(\d+)', request, re.IGNORECASE)
            if port_match:
                port = port_match.group(1)
                details["ports"] = [f"{port}:{port}"]
                
        elif re.search(r'\b(setup|generate|create)\b.+\b(docker|dockerfile|compose)\b', request, re.IGNORECASE):
            details["docker_action"] = "setup"
            
            # Determine which files to generate
            details["generate_dockerfile"] = "dockerfile" in request.lower()
            details["generate_compose"] = "compose" in request.lower()
            details["generate_dockerignore"] = "ignore" in request.lower()
            
            # If not specified, generate all
            if not any([details["generate_dockerfile"], details["generate_compose"], details["generate_dockerignore"]]):
                details["generate_dockerfile"] = True
                details["generate_compose"] = True
                details["generate_dockerignore"] = True
                
        elif re.search(r'\b(compose|docker-compose)\b.+\b(up|start|run)\b', request, re.IGNORECASE):
            details["docker_action"] = "compose_up"
            
            # Extract services if mentioned
            services_match = re.search(r'service[s]?\s+(\w+(?:,\s*\w+)*)', request, re.IGNORECASE)
            if services_match:
                services_str = services_match.group(1)
                details["services"] = [s.strip() for s in services_str.split(',')]
                
        elif re.search(r'\b(compose|docker-compose)\b.+\b(down|stop)\b', request, re.IGNORECASE):
            details["docker_action"] = "compose_down"
            
            # Check for additional options
            details["remove_volumes"] = "volume" in request.lower()
            details["remove_images"] = "image" in request.lower() or "rmi" in request.lower()
            
        elif re.search(r'\b(stop|kill)\b.+\b(container)\b', request, re.IGNORECASE):
            details["docker_action"] = "stop"
            
            # Try to extract container ID or name
            container_match = re.search(r'(container|id|name)\s+(\S+)', request, re.IGNORECASE)
            if container_match:
                details["container"] = container_match.group(2)
                
        elif re.search(r'\b(rm|remove)\b.+\b(container)\b', request, re.IGNORECASE):
            details["docker_action"] = "rm"
            
            # Try to extract container ID or name
            container_match = re.search(r'(container|id|name)\s+(\S+)', request, re.IGNORECASE)
            if container_match:
                details["container"] = container_match.group(2)
                
            # Check for force flag
            details["force"] = "force" in request.lower()
            
        elif re.search(r'\b(ps|list)\b.+\b(container)', request, re.IGNORECASE):
            details["docker_action"] = "ps"
            
            # Check for all flag
            details["all"] = "all" in request.lower()
            
        elif re.search(r'\b(logs|log)\b', request, re.IGNORECASE):
            details["docker_action"] = "logs"
            
            # Try to extract container ID or name
            container_match = re.search(r'(container|id|name)\s+(\S+)', request, re.IGNORECASE)
            if container_match:
                details["container"] = container_match.group(2)
                
            # Check for follow flag
            details["follow"] = "follow" in request.lower() or "tail" in request.lower()
            
        elif re.search(r'\b(exec|execute|run)\b.+\b(command|in)\b', request, re.IGNORECASE):
            details["docker_action"] = "exec"
            
            # Try to extract container ID or name
            container_match = re.search(r'(container|id|name)\s+(\S+)', request, re.IGNORECASE)
            if container_match:
                details["container"] = container_match.group(2)
                
            # Try to extract command
            command_match = re.search(r'command\s+(.+?)$', request, re.IGNORECASE)
            if command_match:
                details["command"] = command_match.group(1)
                
        else:
            # Default to general docker action
            details["docker_action"] = "general"
            
        return details



    
    async def _process_ci_cd_operation(
        self, 
        operation_details: Dict[str, Any],
        context: Dict[str, Any],
        dry_run: bool
    ) -> Dict[str, Any]:
        """
        Process a CI/CD operation request.
        
        Args:
            operation_details: Details about the operation
            context: Context information
            dry_run: Whether to simulate without making changes
            
        Returns:
            Dictionary with processing results
        """
        # Import here to avoid circular imports - use API layer
        from angela.api.toolchain import get_ci_cd_integration
        ci_cd_integration = get_ci_cd_integration()
        
        self._logger.info(f"Processing CI/CD operation: {operation_details.get('platform', 'unknown')}")
        
        # Get details
        platform = operation_details.get("platform", "github_actions")
        project_dir = operation_details.get("project_dir", context.get("cwd", "."))
        
        # Execute CI/CD operation
        if dry_run:
            return {
                "success": True,
                "dry_run": True,
                "message": f"Would configure CI/CD for {platform} in {project_dir}"
            }
        
        try:
            result = await ci_cd_integration.generate_ci_configuration(
                path=project_dir,
                platform=platform
            )
            
            return {
                "success": result.get("success", False),
                "message": result.get("message", "CI/CD configuration completed"),
                "ci_cd_details": result
            }
        except Exception as e:
            self._logger.exception(f"Error processing CI/CD operation: {str(e)}")
            return {
                "success": False,
                "error": f"Error processing CI/CD operation: {str(e)}"
            }
    
    async def _process_package_operation(
        self,
        operation_details: Dict[str, Any],
        context: Dict[str, Any],
        dry_run: bool
    ) -> Dict[str, Any]:
        """
        Process a package management operation request.
        
        Args:
            operation_details: Details about the operation
            context: Context information
            dry_run: Whether to simulate without making changes
            
        Returns:
            Dictionary with processing results
        """
        # Import here to avoid circular imports - use API layer
        from angela.api.toolchain import get_package_manager_integration
        package_manager_integration = get_package_manager_integration()
        
        self._logger.info(f"Processing package operation")
        
        # Get details
        project_dir = operation_details.get("project_dir", context.get("cwd", "."))
        dependencies = operation_details.get("dependencies", [])
        dev_dependencies = operation_details.get("dev_dependencies", [])
        
        if not dependencies and not dev_dependencies:
            return {
                "success": False,
                "error": "No dependencies specified for package operation"
            }
        
        # Execute package operation
        if dry_run:
            return {
                "success": True,
                "dry_run": True,
                "message": f"Would install {len(dependencies)} dependencies and {len(dev_dependencies)} dev dependencies in {project_dir}"
            }
        
        try:
            result = await package_manager_integration.install_dependencies(
                path=project_dir,
                dependencies=dependencies,
                dev_dependencies=dev_dependencies
            )
            
            return {
                "success": result.get("success", False),
                "message": result.get("message", "Dependencies installed successfully"),
                "package_details": result
            }
        except Exception as e:
            self._logger.exception(f"Error processing package operation: {str(e)}")
            return {
                "success": False,
                "error": f"Error processing package operation: {str(e)}"
            }
    
    async def _process_git_operation(
        self,
        operation_details: Dict[str, Any],
        context: Dict[str, Any],
        dry_run: bool
    ) -> Dict[str, Any]:
        """
        Process a Git operation request.
        
        Args:
            operation_details: Details about the operation
            context: Context information
            dry_run: Whether to simulate without making changes
            
        Returns:
            Dictionary with processing results
        """
        # Import here to avoid circular imports - use API layer
        from angela.api.toolchain import get_git_integration
        git_integration = get_git_integration()
        
        self._logger.info(f"Processing Git operation: {operation_details.get('git_action', 'unknown')}")
        
        # Get details
        git_action = operation_details.get("git_action", "")
        project_dir = operation_details.get("project_dir", context.get("cwd", "."))
        
        # Handle different Git operations based on git_action
        if git_action == "init":
            return await self._process_git_init(operation_details, project_dir, dry_run)
        elif git_action == "commit":
            return await self._process_git_commit(operation_details, project_dir, dry_run)
        elif git_action == "branch":
            return await self._process_git_branch(operation_details, project_dir, dry_run)
        elif git_action == "status":
            return await self._process_git_status(operation_details, project_dir, dry_run)
        else:
            # Generate and execute appropriate Git command using AI
            suggestion = await self._get_ai_suggestion(
                operation_details.get("request", f"git {git_action}"),
                context
            )
            
            if dry_run:
                return {
                    "success": True,
                    "dry_run": True,
                    "command": suggestion.command,
                    "explanation": suggestion.explanation
                }
            
            # Execute the command
            stdout, stderr, exit_code = await execution_engine.execute_command(
                suggestion.command,
                check_safety=True,
                working_dir=project_dir
            )
            
            return {
                "success": exit_code == 0,
                "command": suggestion.command,
                "stdout": stdout,
                "stderr": stderr,
                "return_code": exit_code,
                "explanation": suggestion.explanation
            }
    
    async def _process_git_init(self, operation_details, project_dir, dry_run):
        """Process git init operation."""
        # Use API layer to get git_integration
        from angela.api.toolchain import get_git_integration
        git_integration = get_git_integration()
        
        if dry_run:
            return {
                "success": True,
                "dry_run": True,
                "message": f"Would initialize Git repository in {project_dir}"
            }
        
        # Get initialization parameters
        branch = operation_details.get("branch", "main")
        gitignore = operation_details.get("gitignore", True)
        
        # Initialize repository
        result = await git_integration.init_repository(
            path=project_dir,
            initial_branch=branch,
            gitignore_template=operation_details.get("gitignore_template")
        )
        
        return {
            "success": result.get("success", False),
            "message": result.get("message", "Git repository initialized"),
            "git_details": result
        }
    
    async def _process_git_commit(self, operation_details, project_dir, dry_run):
        """Process git commit operation."""
        # Use API layer to get git_integration
        from angela.api.toolchain import get_git_integration
        git_integration = get_git_integration()
        
        if dry_run:
            return {
                "success": True,
                "dry_run": True,
                "message": f"Would commit changes in {project_dir}"
            }
        
        # Get commit parameters
        message = operation_details.get("message", "Update via Angela CLI")
        add_all = operation_details.get("add_all", True)
        
        # Stage files if requested
        if add_all:
            await git_integration.stage_files(path=project_dir, files=["."])
        
        # Commit changes
        result = await git_integration.commit_changes(
            path=project_dir,
            message=message
        )
        
        return {
            "success": result.get("success", False),
            "message": result.get("message", "Changes committed successfully"),
            "git_details": result
        }
    
    async def _process_git_branch(self, operation_details, project_dir, dry_run):
        """Process git branch operation."""
        # Use API layer to get git_integration
        from angela.api.toolchain import get_git_integration
        git_integration = get_git_integration()
        
        if dry_run:
            return {
                "success": True,
                "dry_run": True,
                "message": f"Would create/switch branch in {project_dir}"
            }
        
        # Get branch parameters
        branch_name = operation_details.get("branch_name", "")
        if not branch_name:
            return {
                "success": False,
                "error": "Branch name not specified"
            }
        
        checkout = operation_details.get("checkout", True)
        
        # Create branch
        result = await git_integration.create_branch(
            path=project_dir,
            branch_name=branch_name,
            checkout=checkout
        )
        
        return {
            "success": result.get("success", False),
            "message": result.get("message", f"Branch {branch_name} created"),
            "git_details": result
        }
    
    async def _process_git_status(self, operation_details, project_dir, dry_run):
        """Process git status operation."""
        # Use API layer to get git_integration
        from angela.api.toolchain import get_git_integration
        git_integration = get_git_integration()
        
        # Get repository status
        result = await git_integration.get_repository_status(path=project_dir)
        
        return {
            "success": result.get("success", False),
            "message": "Git status retrieved",
            "status": result.get("status", {}),
            "git_details": result
        }
    
    async def _process_testing_operation(
        self,
        operation_details: Dict[str, Any],
        context: Dict[str, Any],
        dry_run: bool
    ) -> Dict[str, Any]:
        """
        Process a testing operation request.
        
        Args:
            operation_details: Details about the operation
            context: Context information
            dry_run: Whether to simulate without making changes
            
        Returns:
            Dictionary with processing results
        """
        self._logger.info(f"Processing testing operation")
        
        # Get details
        project_dir = operation_details.get("project_dir", context.get("cwd", "."))
        test_framework = operation_details.get("test_framework", "")
        test_path = operation_details.get("test_path", "")
        
        # Execute testing operation
        if dry_run:
            return {
                "success": True,
                "dry_run": True,
                "message": f"Would run tests using {test_framework} in {project_dir}"
            }
        
        try:
            # Determine test command based on framework
            command = ""
            if test_framework == "pytest":
                command = f"pytest {test_path}" if test_path else "pytest"
            elif test_framework == "jest":
                command = f"npx jest {test_path}" if test_path else "npx jest"
            elif test_framework == "go":
                command = f"go test {test_path}" if test_path else "go test ./..."
            elif test_framework == "maven":
                command = "mvn test"
            elif test_framework == "gradle":
                command = "./gradlew test"
            else:
                # Default to using AI to generate appropriate test command
                suggestion = await self._get_ai_suggestion(
                    f"run tests for {test_framework} in {project_dir}",
                    context
                )
                command = suggestion.command
            
            # Execute test command
            stdout, stderr, exit_code = await execution_engine.execute_command(
                command,
                check_safety=True,
                working_dir=project_dir
            )
            
            return {
                "success": exit_code == 0,
                "command": command,
                "stdout": stdout,
                "stderr": stderr,
                "return_code": exit_code,
                "message": "Tests completed successfully" if exit_code == 0 else "Tests failed"
            }
        except Exception as e:
            self._logger.exception(f"Error processing testing operation: {str(e)}")
            return {
                "success": False,
                "error": f"Error processing testing operation: {str(e)}"
            }
            
    async def _extract_feature_details(
        self, 
        request: str,
        context: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Extract feature details from a feature addition request.
        
        Args:
            request: The user request
            context: Context information
            
        Returns:
            Dictionary with feature details
        """
        # Use AI to extract feature details
        prompt = f"""
    Extract feature details from this feature addition request:
    "{request}"
    
    Return a JSON object with:
    1. description: A clear description of the feature to add
    2. project_dir: The project directory (default to ".")
    3. branch: A branch name if specified
    
    Format:
    {{
      "description": "feature description",
      "project_dir": "directory",
      "branch": "branch-name"
    }}
    
    Only include keys where you have clear information from the request.
    """
        
        try:
            # Call AI service
            api_request = GeminiRequest(prompt=prompt, max_tokens=1000)
            response = await gemini_client.generate_text(api_request)
            
            # Parse the response
            import json
            import re
            
            # Try to find JSON in the response
            json_match = re.search(r'```(?:json)?\s*(.*?)\s*```', response.text, re.DOTALL)
            if json_match:
                json_str = json_match.group(1)
            else:
                # Assume the entire response is JSON
                json_str = response.text
            
            # Parse JSON
            details = json.loads(json_str)
            
            # Default project directory to context's project_root if available
            if "project_dir" not in details and context.get("project_root"):
                details["project_dir"] = context.get("project_root")
            
            return details
            
        except Exception as e:
            self._logger.error(f"Error extracting feature details: {str(e)}")
            # Return minimal details on failure
            return {
                "project_dir": context.get("project_root", "."),
                "description": request
            }


    
    async def _extract_project_details(
        self, 
        request: str
    ) -> Dict[str, Any]:
        """
        Extract project details from a code generation request.
        
        Args:
            request: The user request
            
        Returns:
            Dictionary with project details
        """
        # Use AI to extract project details
        prompt = f"""
    Extract project details from this code generation request:
    "{request}"
    
    Return a JSON object with:
    1. project_type: The type of project (e.g., "python", "node", "java", etc.)
    2. framework: Any specific framework mentioned (e.g., "django", "react", "spring")
    3. output_dir: Where the project should be created (default to ".")
    4. git_init: Whether to initialize a Git repo (default to true)
    5. description: A clear description of what the project should do/be
    
    Format:
    {{
      "project_type": "type",
      "framework": "framework",
      "output_dir": "directory",
      "git_init": true/false,
      "description": "description"
    }}
    
    Only include keys where you have clear information from the request.
    If something is ambiguous, omit the key rather than guessing.
    """
        
        try:
            # Call AI service
            api_request = GeminiRequest(prompt=prompt, max_tokens=1000)
            response = await gemini_client.generate_text(api_request)
            
            # Parse the response
            import json
            import re
            
            # Try to find JSON in the response
            json_match = re.search(r'```(?:json)?\s*(.*?)\s*```', response.text, re.DOTALL)
            if json_match:
                json_str = json_match.group(1)
            else:
                # Assume the entire response is JSON
                json_str = response.text
            
            # Parse JSON
            details = json.loads(json_str)
            return details
            
        except Exception as e:
            self._logger.error(f"Error extracting project details: {str(e)}")
            # Return minimal details on failure
            return {
                "output_dir": ".",
                "git_init": True,
                "description": request
            }


    

    async def _process_command_request(
        self, 
        request: str, 
        context: Dict[str, Any], 
        execute: bool, 
        dry_run: bool
    ) -> Dict[str, Any]:
        """
        Process a single command request with enhanced flow and visualizations.
        
        Args:
            request: The user request
            context: Context information
            execute: Whether to execute the command
            dry_run: Whether to simulate execution without making changes
            
        Returns:
            Dictionary with processing results
        """
        # Import components via API
        from angela.api.shell import get_terminal_formatter, get_output_type_enum
        from angela.api.safety import get_adaptive_confirmation_handler
        
        terminal_formatter = get_terminal_formatter()
        OutputType = get_output_type_enum()
        adaptive_confirmation_handler = get_adaptive_confirmation_handler()
        
        # Start a loading timer for initial processing
        loading_task = asyncio.create_task(
            terminal_formatter.display_loading_timer("Angela's decrypting the payload....")
        )
        
        try:
            # Analyze intent with enhanced NLU
            intent_result = intent_analyzer.analyze_intent(request)
            
            # Check if we've seen a similar request before
            similar_command = history_manager.search_similar_command(request)
            
            # Get command suggestion from AI
            suggestion = await self._get_ai_suggestion(
                request, 
                context, 
                similar_command, 
                intent_result
            )
            
            # Score confidence in the suggestion
            confidence = confidence_scorer.score_command_confidence(
                request=request,
                command=suggestion.command,
                context=context
            )
            
            # Cancel the loading display
            loading_task.cancel()
            try:
                await loading_task
            except asyncio.CancelledError:
                pass
                
            # Analyze command risk and impact
            risk_level, risk_reason = classify_command_risk(suggestion.command)
            impact = analyze_command_impact(suggestion.command)
            
            # Generate preview of what the command will do
            from angela.api.safety import get_command_preview_generator
            preview_generator = get_command_preview_generator()
            preview = await preview_generator.generate_preview(suggestion.command)
            
            # Store results in a dictionary
            result = {
                "request": request,
                "suggestion": suggestion,
                "confidence": confidence,
                "intent": intent_result.intent_type if hasattr(intent_result, 'intent_type') else "unknown",
                "context": context,
                "type": "command",
                "risk_level": risk_level,
                "risk_reason": risk_reason,
                "impact": impact,
                "preview": preview
            }
            
            # If confidence is low, offer clarification
            if confidence < 0.6 and not dry_run and not context.get("session", {}).get("skip_clarification"):
                # Display command with low confidence warning
                terminal_formatter.print_command(suggestion.command, title="Suggested Command")
                
                # Display confidence score
                confidence_color = "red"  # Low confidence
                confidence_stars = int(confidence * 5)
                confidence_display = "★" * confidence_stars + "☆" * (5 - confidence_stars)
                
                console.print(Panel(
                    f"[bold]Confidence Score:[/bold] [{confidence_color}]{confidence:.2f}[/{confidence_color}] {confidence_display}\n"
                    "[dim](I'm not very confident this is what you meant)[/dim]",
                    title="Low Confidence Warning",
                    border_style=confidence_color,
                    expand=False
                ))
                
                # Ask for confirmation using inline prompt
                prompt_text = "Proceed with this command anyway?"
                should_proceed = await terminal_formatter.display_inline_confirmation(prompt_text)
                
                if not should_proceed:
                    result["cancelled"] = True
                    return {
                        "request": request,
                        "response": "Command cancelled due to low confidence.",
                        "context": context,
                    }
            
            # Execute the command if requested
            if execute or dry_run:
                self._logger.info(f"{'Dry run' if dry_run else 'Executing'} suggested command: {suggestion.command}")
                
                # Get confirmation using adaptive confirmation system
                confirmed = await adaptive_confirmation_handler(
                    command=suggestion.command,
                    risk_level=risk_level,
                    risk_reason=risk_reason,
                    impact=impact,
                    preview=preview,
                    explanation=suggestion.explanation,
                    natural_request=request,
                    dry_run=dry_run,
                    confidence_score=confidence
                )
                
                if not confirmed and not dry_run:
                    self._logger.info(f"Command execution cancelled by user: {suggestion.command}")
                    result["cancelled"] = True
                    return result
                
                # Execute with timer and philosophical quotes
                stdout, stderr, return_code, execution_time = await terminal_formatter.display_execution_timer(
                    suggestion.command,
                    with_philosophy=True
                )
                
                # Store execution results
                execution_result = {
                    "command": suggestion.command,
                    "success": return_code == 0,
                    "stdout": stdout,
                    "stderr": stderr,
                    "return_code": return_code,
                    "execution_time": execution_time,
                    "dry_run": dry_run
                }
                
                result["execution"] = execution_result
                
                # Display the execution results using the terminal formatter
                await terminal_formatter.display_command_summary(
                    command=suggestion.command,
                    success=return_code == 0,
                    stdout=stdout,
                    stderr=stderr,
                    return_code=return_code,
                    execution_time=execution_time
                )
                
                # If execution failed, analyze error and suggest fixes
                if not return_code == 0 and stderr:
                    error_analysis = error_analyzer.analyze_error(suggestion.command, stderr)
                    fix_suggestions = error_analyzer.generate_fix_suggestions(suggestion.command, stderr)
                    
                    result["error_analysis"] = error_analysis
                    result["fix_suggestions"] = fix_suggestions
                    
                    # Display error analysis
                    terminal_formatter.print_error_analysis(error_analysis)
                
                # Add to history
                history_manager.add_command(
                    command=suggestion.command,
                    natural_request=request,
                    success=return_code == 0,
                    output=stdout,
                    error=stderr,
                    risk_level=risk_level
                )
                
                # For multi-step operations, offer to learn from successful executions
                if return_code == 0 and risk_level > 0:
                    from angela.api.safety import offer_command_learning
                    await offer_command_learning(suggestion.command)
            
            return result
        
        except asyncio.CancelledError:
            # Handle the case where the loading task gets cancelled
            return {
                "request": request,
                "context": context,
                "error": "Operation was cancelled",
                "success": False
            }
            
        except Exception as e:
            # Cancel the loading display if it's still running
            if loading_task and not loading_task.done():
                loading_task.cancel()
                try:
                    await loading_task
                except asyncio.CancelledError:
                    pass  # Expected
                    
            self._logger.exception(f"Error processing command request: {str(e)}")
            return {
                "request": request,
                "type": "command",
                "context": context,
                "error": str(e),
                "success": False
            }
            

    async def _process_multi_step_request(
        self, 
        request: str, 
        context: Dict[str, Any], 
        execute: bool, 
        dry_run: bool
    ) -> Dict[str, Any]:
        """Process a multi-step operation request with enhanced visualization and transactions."""
        self._logger.info(f"Processing multi-step request: {request}")
        
        # Import components via API
        from angela.api.shell import get_terminal_formatter
        from angela.api.execution import get_rollback_manager
        
        terminal_formatter = get_terminal_formatter()
        rollback_manager = get_rollback_manager()
        
        # Start a loading timer for initial processing
        loading_task = asyncio.create_task(
            terminal_formatter.display_loading_timer("Planning multi-step operation...")
        )
        
        # Start a transaction for this multi-step operation
        transaction_id = None
        if not dry_run:
            transaction_id = await rollback_manager.start_transaction(f"Multi-step plan: {request[:50]}...")
        
        try:
            # Determine if we should use advanced planning
            complexity = await task_planner._determine_complexity(request)
            
            # Cancel the loading display
            loading_task.cancel()
            
            if complexity == "advanced":
                # Use the advanced planner for complex tasks
                plan = await task_planner.plan_task(request, context, complexity)
                
                # Record the plan in the transaction
                if transaction_id:
                    await self._record_plan_in_transaction(
                        plan_id=plan.id,
                        goal=plan.goal,
                        plan_data=plan.dict(),
                        transaction_id=transaction_id
                    )
                
                # Create result with the plan
                if isinstance(plan, AdvancedTaskPlan):
                    # Advanced plan with branching
                    result = {
                        "request": request,
                        "type": "advanced_multi_step",
                        "context": context,
                        "plan": {
                            "id": plan.id,
                            "goal": plan.goal,
                            "description": plan.description,
                            "steps": [
                                {
                                    "id": step_id,
                                    "type": step.type,
                                    "description": step.description,
                                    "command": getattr(step, "command", None),
                                    "dependencies": step.dependencies,
                                    "risk": step.estimated_risk
                                }
                                for step_id, step in plan.steps.items()
                            ],
                            "entry_points": plan.entry_points,
                            "step_count": len(plan.steps)
                        }
                    }
                    
                    # Execute the plan if requested
                    if execute or dry_run:
                        # Display the plan with rich formatting
                        await terminal_formatter.display_advanced_plan(plan)
                        
                        # Get confirmation using inline confirmation
                        prompt_text = f"Execute this {len(plan.steps)}-step plan?"
                        confirmed = await terminal_formatter.display_inline_confirmation(prompt_text)
                        
                        if confirmed or dry_run:
                            # Start progress tracking loading timer
                            execution_timer = asyncio.create_task(
                                terminal_formatter.display_loading_timer("Executing multi-step plan...")
                            )
                            
                            try:
                                # Execute the plan with transaction support
                                execution_results = await task_planner.execute_plan(
                                    plan, 
                                    dry_run=dry_run,
                                    transaction_id=transaction_id
                                )
                                
                                # Cancel the timer
                                execution_timer.cancel()
                                
                                # Display execution results
                                await terminal_formatter.display_execution_results(plan, execution_results)
                                
                                result["execution_results"] = execution_results
                                result["success"] = execution_results.get("success", False)
                                
                                # Update transaction status
                                if transaction_id:
                                    status = "completed" if result["success"] else "failed"
                                    await rollback_manager.end_transaction(transaction_id, status)
                            finally:
                                # Ensure timer is cancelled even on error
                                if not execution_timer.done():
                                    execution_timer.cancel()
                        else:
                            result["cancelled"] = True
                            result["success"] = False
                            
                            # End the transaction as cancelled
                            if transaction_id:
                                await rollback_manager.end_transaction(transaction_id, "cancelled")
                    else:
                        # Basic plan (fallback)
                        result = await self._handle_basic_plan_execution(plan, request, context, execute, dry_run, transaction_id)
                else:
                    # Use the basic planner for simple tasks
                    plan = await task_planner.plan_task(request, context)
                    result = await self._handle_basic_plan_execution(plan, request, context, execute, dry_run, transaction_id)
                
                return result
            
        except asyncio.CancelledError:
            # Handle the case where a task gets cancelled
            if transaction_id:
                await rollback_manager.end_transaction(transaction_id, "cancelled")
                
            return {
                "request": request,
                "context": context,
                "error": "Operation was cancelled",
                "success": False
            }
            
        except Exception as e:
            # Handle any exceptions and end the transaction
            if transaction_id:
                await rollback_manager.end_transaction(transaction_id, "failed")
            
            # Cancel the loading timer if still running
            if loading_task and not loading_task.done():
                loading_task.cancel()
                
            self._logger.exception(f"Error processing multi-step request: {str(e)}")
            raise
        
    async def _process_basic_multi_step(
        self,
        plan: TaskPlan,
        request: str,
        context: Dict[str, Any],
        execute: bool,
        dry_run: bool,
        transaction_id: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Process a basic multi-step plan with transaction support.
        
        Args:
            plan: The task plan
            request: The user request
            context: Context information
            execute: Whether to execute the commands
            dry_run: Whether to simulate execution without making changes
            transaction_id: Transaction ID for tracking operations
            
        Returns:
            Dictionary with processing results
        """
        # Record the plan in the transaction if not already done
        if transaction_id and not dry_run:
            await rollback_manager.record_plan_execution(
                plan_id=getattr(plan, "id", str(uuid.uuid4())),
                goal=plan.goal,
                plan_data=plan.dict(),
                transaction_id=transaction_id
            )
        
        # Create result with the plan
        result = {
            "request": request,
            "type": "multi_step",
            "context": context,
            "plan": {
                "goal": plan.goal,
                "steps": [
                    {
                        "command": step.command,
                        "explanation": step.explanation,
                        "dependencies": step.dependencies,
                        "risk": step.estimated_risk
                    }
                    for step in plan.steps
                ],
                "step_count": len(plan.steps)
            }
        }
        
        # Execute the plan if requested
        if execute or dry_run:
            # Display the plan with rich formatting
            await self._display_plan(plan)
            
            # Get confirmation for plan execution
            confirmed = await self._confirm_plan_execution(plan, dry_run)
            
            if confirmed or dry_run:
                # Execute the plan with transaction support
                execution_results = await task_planner.execute_plan(
                    plan, 
                    dry_run=dry_run,
                    transaction_id=transaction_id
                )
                result["execution_results"] = execution_results
                result["success"] = all(r.get("success", False) for r in execution_results)
                
                # Update transaction status
                if transaction_id:
                    status = "completed" if result["success"] else "failed"
                    await rollback_manager.end_transaction(transaction_id, status)
                
                # Handle errors with recovery
                if not result["success"] and not dry_run:
                    recovered_results = await self._handle_execution_errors(plan, execution_results)
                    result["recovery_attempted"] = True
                    result["recovered_results"] = recovered_results
                    # Update success status if recovery was successful
                    if all(r.get("success", False) for r in recovered_results):
                        result["success"] = True
            else:
                result["cancelled"] = True
                result["success"] = False
                
                # End the transaction as cancelled
                if transaction_id:
                    await rollback_manager.end_transaction(transaction_id, "cancelled")
        
        return result



    async def _handle_advanced_plan_execution(
        self,
        plan: AdvancedTaskPlan,
        request: str,
        context: Dict[str, Any],
        execute: bool,
        dry_run: bool,
        transaction_id: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Handle execution of an advanced plan.
        
        Args:
            plan: The advanced plan to execute
            request: The original request
            context: Context information
            execute: Whether to execute commands
            dry_run: Whether to simulate execution
            transaction_id: Transaction ID for rollback
            
        Returns:
            Dictionary with results
        """
        # Create result structure with the plan
        result = {
            "request": request,
            "type": "advanced_multi_step",
            "context": context,
            "plan": {
                "id": plan.id,
                "goal": plan.goal,
                "description": plan.description,
                "steps": [
                    {
                        "id": step_id,
                        "type": step.type,
                        "description": step.description,
                        "command": getattr(step, "command", None),
                        "dependencies": step.dependencies,
                        "risk": step.estimated_risk
                    }
                    for step_id, step in plan.steps.items()
                ],
                "entry_points": plan.entry_points,
                "step_count": len(plan.steps)
            }
        }
        
        # Display the plan for the user
        await self._display_advanced_plan(plan)
        
        # Execute the plan if requested
        if execute or dry_run:
            # Get confirmation for plan execution
            confirmed = await self._confirm_advanced_plan_execution(plan, dry_run)
            
            if confirmed or dry_run:
                # Execute the plan with initial variables from context if needed
                initial_variables = self._extract_initial_variables(context)
                
                execution_results = await enhanced_task_planner.execute_plan(
                    plan, 
                    dry_run=dry_run,
                    transaction_id=transaction_id,
                    initial_variables=initial_variables
                )
                
                result["execution_results"] = execution_results
                result["success"] = execution_results.get("success", False)
                
                # Update transaction status
                if transaction_id:
                    rollback_manager = self._get_rollback_manager()
                    if rollback_manager:
                        status = "completed" if result["success"] else "failed"
                        await rollback_manager.end_transaction(transaction_id, status)
                        
                # For failed steps, show error information
                if not result["success"]:
                    failed_step = execution_results.get("failed_step")
                    if failed_step and failed_step in execution_results.get("results", {}):
                        step_result = execution_results["results"][failed_step]
                        error_msg = step_result.get("error", "Unknown error")
                        self._logger.error(f"Step {failed_step} failed: {error_msg}")
                        
                        # Format error for display
                        await terminal_formatter.display_step_error(
                            failed_step,
                            error_msg,
                            step_result.get("type", "unknown"),
                            step_result.get("description", "")
                        )
            else:
                result["cancelled"] = True
                result["success"] = False
                
                # End the transaction as cancelled
                if transaction_id:
                    rollback_manager = self._get_rollback_manager()
                    if rollback_manager:
                        await rollback_manager.end_transaction(transaction_id, "cancelled")
        
        return result
    
    async def _handle_basic_plan_execution(
        self,
        plan: TaskPlan,
        request: str,
        context: Dict[str, Any],
        execute: bool,
        dry_run: bool,
        transaction_id: Optional[str] = None
    ) -> Dict[str, Any]:
        """Handle execution of a basic plan with enhanced visualization."""
        # Import terminal formatter
        from angela.api.shell import get_terminal_formatter
        terminal_formatter = get_terminal_formatter()
        
        # Create result with the plan
        result = {
            "request": request,
            "type": "multi_step",
            "context": context,
            "plan": {
                "goal": plan.goal,
                "steps": [
                    {
                        "command": step.command,
                        "explanation": step.explanation,
                        "dependencies": step.dependencies,
                        "risk": step.estimated_risk
                    }
                    for step in plan.steps
                ],
                "step_count": len(plan.steps)
            }
        }
        
        # Execute the plan if requested
        if execute or dry_run:
            # Display the plan
            await terminal_formatter.display_task_plan(plan)
            
            # Get confirmation for plan execution using inline confirmation
            prompt_text = f"Execute this {len(plan.steps)}-step plan?"
            confirmed = await terminal_formatter.display_inline_confirmation(prompt_text)
            
            if confirmed or dry_run:
                # Start a loading timer for execution
                execution_timer = asyncio.create_task(
                    terminal_formatter.display_loading_timer("Executing multi-step plan...")
                )
                
                try:
                    # Execute the plan with transaction support
                    execution_results = await task_planner.execute_plan(
                        plan, 
                        dry_run=dry_run,
                        transaction_id=transaction_id
                    )
                    
                    # Cancel the timer
                    execution_timer.cancel()
                    
                    # Display multi-step execution results
                    await terminal_formatter.display_multi_step_execution(plan, execution_results)
                    
                    result["execution_results"] = execution_results
                    result["success"] = all(r.get("success", False) for r in execution_results)
                    
                    # Update transaction status
                    if transaction_id:
                        status = "completed" if result["success"] else "failed"
                        await rollback_manager.end_transaction(transaction_id, status)
                    
                    # Handle errors with recovery if needed
                    if not result["success"] and not dry_run:
                        recovered_results = await self._handle_execution_errors(plan, execution_results)
                        result["recovery_attempted"] = True
                        result["recovered_results"] = recovered_results
                        # Update success status if recovery was successful
                        if all(r.get("success", False) for r in recovered_results):
                            result["success"] = True
                finally:
                    # Ensure timer is cancelled even on error
                    if not execution_timer.done():
                        execution_timer.cancel()
            else:
                result["cancelled"] = True
                result["success"] = False
                
                # End the transaction as cancelled
                if transaction_id:
                    await rollback_manager.end_transaction(transaction_id, "cancelled")
        
        return result
    

    async def _display_advanced_plan(
        self,
        plan: AdvancedTaskPlan
    ) -> None:
        """
        Display an advanced task plan to the user.
        
        Args:
            plan: The advanced plan to display
        """
        await display_advanced_plan(plan)
    
    async def _confirm_advanced_plan(
        self, 
        plan: AdvancedTaskPlan, 
        dry_run: bool = False,
        transaction_name: Optional[str] = None
    ) -> bool:
        """
        Get confirmation for executing an advanced plan with comprehensive risk assessment.
        
        This consolidated method provides rich feedback on plan risk and complexity,
        allowing the user to make an informed decision before execution.
        
        Args:
            plan: The advanced plan to execute
            dry_run: Whether this is a dry run (skip confirmation if True)
            transaction_name: Optional name of the transaction (for display purposes)
            
        Returns:
            True if confirmed or dry_run is True, False otherwise
        """
        # Skip confirmation for dry runs
        if dry_run:
            self._logger.debug("Skipping confirmation for dry run")
            return True
        
        # Risk assessment
        high_risk_steps = []
        medium_risk_steps = []
        complex_step_types = set()
        
        # Count of various step types for better risk assessment
        step_type_counts = {}
        
        # Analyze all steps
        for step_id, step in plan.steps.items():
            # Track step types
            step_type = step.type
            step_type_counts[step_type] = step_type_counts.get(step_type, 0) + 1
            
            # Risk categorization
            risk_level = getattr(step, "estimated_risk", 0)
            if risk_level >= 3:
                high_risk_steps.append((step_id, getattr(step, "description", f"Step {step_id}")))
            elif risk_level == 2:
                medium_risk_steps.append((step_id, getattr(step, "description", f"Step {step_id}")))
                
            # Complex step types tracking
            if step_type in [PlanStepType.CODE, PlanStepType.API, PlanStepType.LOOP, PlanStepType.DECISION]:
                complex_step_types.add(step_type)
        
        # Import needed components
        from rich.console import Console
        from rich.panel import Panel
        from rich.table import Table
        from rich.text import Text
        from prompt_toolkit.shortcuts import yes_no_dialog
        
        console = Console()
        
        # Create plan summary
        summary_table = Table(title=f"Advanced Plan Summary: {plan.goal}", expand=False)
        summary_table.add_column("Category", style="cyan")
        summary_table.add_column("Count", style="green")
        
        summary_table.add_row("Total Steps", str(len(plan.steps)))
        for step_type, count in step_type_counts.items():
            summary_table.add_row(f"{step_type.value.capitalize()} Steps", str(count))
            
        # Display plan summary
        console.print("\n")
        console.print(summary_table)
        
        # Display warnings based on risk assessment
        if high_risk_steps:
            warning_text = "[bold red]This plan includes HIGH RISK operations:[/bold red]\n\n"
            for step_id, desc in high_risk_steps[:5]:  # Show at most 5 high-risk steps
                warning_text += f"• Step {step_id}: {desc}\n"
            if len(high_risk_steps) > 5:
                warning_text += f"...and {len(high_risk_steps) - 5} more high-risk steps\n"
            warning_text += "\n[bold red]These steps could make significant changes to your system.[/bold red]"
            
            console.print(Panel(
                warning_text,
                title="⚠️ WARNING: High Risk Operations ⚠️",
                border_style="red",
                expand=False
            ))
        
        # Display cautions for medium-risk steps if there are no high-risk steps
        elif medium_risk_steps:
            caution_text = "[bold yellow]This plan includes MEDIUM RISK operations:[/bold yellow]\n\n"
            for step_id, desc in medium_risk_steps[:5]:  # Show at most 5 medium-risk steps
                caution_text += f"• Step {step_id}: {desc}\n"
            if len(medium_risk_steps) > 5:
                caution_text += f"...and {len(medium_risk_steps) - 5} more medium-risk steps\n"
            
            console.print(Panel(
                caution_text,
                title="⚠️ CAUTION ⚠️",
                border_style="yellow",
                expand=False
            ))
        
        # Show complexity warnings if complex step types are present
        if complex_step_types:
            complex_text = "This plan includes advanced execution steps:\n\n"
            
            if PlanStepType.CODE in complex_step_types:
                complex_text += "• [bold blue]Code Execution[/bold blue]: Will execute custom code\n"
            if PlanStepType.API in complex_step_types:
                complex_text += "• [bold blue]API Calls[/bold blue]: Will make external API requests\n"
            if PlanStepType.LOOP in complex_step_types:
                complex_text += "• [bold blue]Loops[/bold blue]: Will repeat operations for multiple items\n"
            if PlanStepType.DECISION in complex_step_types:
                complex_text += "• [bold blue]Decision Points[/bold blue]: Plan flow depends on conditions\n"
                
            console.print(Panel(
                complex_text,
                title="Advanced Execution Flow",
                border_style="blue",
                expand=False
            ))
        
        # Transaction information if provided
        if transaction_name:
            console.print(Panel(
                f"This plan will execute as transaction: [bold]{transaction_name}[/bold]\n"
                "All operations can be rolled back if needed.",
                title="Transaction Information",
                border_style="green",
                expand=False
            ))
        
        # Get confirmation
        confirmation_text = f"Do you want to execute this {len(plan.steps)}-step advanced plan?"
        if high_risk_steps:
            confirmation_text += " (contains HIGH RISK operations)"
        
        confirmed = yes_no_dialog(
            title="Confirm Advanced Plan Execution",
            text=confirmation_text,
        ).run()
        
        if confirmed:
            self._logger.info("User confirmed advanced plan execution")
        else:
            self._logger.info("User declined advanced plan execution")
            
        return confirmed
    
    def _extract_initial_variables(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """
        Extract variables from context to use as initial variables for plan execution.
        
        Args:
            context: Context information
            
        Returns:
            Dictionary of variables
        """
        initial_vars = {}
        
        # Extract relevant information from context
        if "cwd" in context:
            initial_vars["current_directory"] = context["cwd"]
        
        if "project_root" in context:
            initial_vars["project_root"] = context["project_root"]
        
        if "project_type" in context:
            initial_vars["project_type"] = context["project_type"]
        
        if "enhanced_project" in context:
            # Extract useful project information
            project_info = context["enhanced_project"]
            if "type" in project_info:
                initial_vars["project_type"] = project_info["type"]
            
            if "frameworks" in project_info:
                initial_vars["frameworks"] = project_info["frameworks"]
            
            if "dependencies" in project_info and "top_dependencies" in project_info["dependencies"]:
                initial_vars["dependencies"] = project_info["dependencies"]["top_dependencies"]
        
        if "resolved_files" in context:
            # Extract resolved file references
            files = {}
            for ref in context["resolved_files"]:
                ref_name = ref.get("reference", "").replace(" ", "_")
                if ref_name and "path" in ref:
                    files[ref_name] = ref["path"]
            
            if files:
                initial_vars["files"] = files
        
        # Add session-specific information
        if "session" in context and "entities" in context["session"]:
            entities = {}
            for name, entity in context["session"].get("entities", {}).items():
                if "type" in entity and "value" in entity:
                    entities[name] = {
                        "type": entity["type"],
                        "value": entity["value"]
                    }
            
            if entities:
                initial_vars["entities"] = entities
        
        return initial_vars
    
    def _get_rollback_manager(self):
        """Get the rollback manager from the registry."""
        from angela.api.execution import get_rollback_manager
        return get_rollback_manager()
    
    def _get_error_recovery_manager(self):
        """Get the error recovery manager from the registry."""
        # Use API layer to get error_recovery_manager
        from angela.api.execution import get_error_recovery_manager
        return get_error_recovery_manager()
    
    async def _record_plan_in_transaction(self, plan_id, goal, plan_data, transaction_id):
        """Record a plan in a transaction."""
        rollback_manager = self._get_rollback_manager()
        if rollback_manager:
            await rollback_manager.record_plan_execution(
                plan_id=plan_id,
                goal=goal,
                plan_data=plan_data,
                transaction_id=transaction_id
            )

    
    async def _process_file_content_request(
        self, 
        request: str, 
        context: Dict[str, Any], 
        execute: bool, 
        dry_run: bool
    ) -> Dict[str, Any]:
        """
        Process a file content analysis/manipulation request with transaction support.
        
        Args:
            request: The user request
            context: Context information
            execute: Whether to execute file operations
            dry_run: Whether to simulate execution without making changes
            
        Returns:
            Dictionary with processing results
        """
        self._logger.info(f"Processing file content request: {request}")
        
        # Start a transaction for this operation
        transaction_id = None
        if not dry_run and execute:
            transaction_id = await rollback_manager.start_transaction(f"File content operation: {request[:50]}...")
        
        try:
            # Extract file path from request using file_resolver
            file_path = await self._extract_file_path(request, context)
            
            if not file_path:
                # End the transaction as failed
                if transaction_id:
                    await rollback_manager.end_transaction(transaction_id, "failed")
                    
                return {
                    "request": request,
                    "type": "file_content",
                    "context": context,
                    "error": "Could not determine file path from request",
                    "response": "I couldn't determine which file you're referring to. Please specify the file path.",
                    "success": False
                }
            
            # Determine if this is analysis or manipulation
            operation_type = await self._determine_file_operation_type(request)
            
            result = {
                "request": request,
                "type": "file_content",
                "context": context,
                "file_path": str(file_path),
                "operation_type": operation_type,
                "success": False  # Default to False, will be updated if operation succeeds
            }
            
            if operation_type == "analyze":
                # Analyze file content (no rollback needed)
                try:
                    analysis_result = await content_analyzer.analyze_content(file_path, request)
                    result["analysis"] = analysis_result
                    result["success"] = True
                    
                    # End transaction as completed
                    if transaction_id:
                        await rollback_manager.end_transaction(transaction_id, "completed")
                except Exception as e:
                    self._logger.error(f"Error analyzing file content: {str(e)}")
                    result["error"] = f"Error analyzing file content: {str(e)}"
                    
                    # End transaction as failed
                    if transaction_id:
                        await rollback_manager.end_transaction(transaction_id, "failed")
                
            elif operation_type == "summarize":
                # Summarize file content (no rollback needed)
                try:
                    summary_result = await content_analyzer.summarize_content(file_path)
                    result["summary"] = summary_result
                    result["success"] = True
                    
                    # End transaction as completed
                    if transaction_id:
                        await rollback_manager.end_transaction(transaction_id, "completed")
                except Exception as e:
                    self._logger.error(f"Error summarizing file content: {str(e)}")
                    result["error"] = f"Error summarizing file content: {str(e)}"
                    
                    # End transaction as failed
                    if transaction_id:
                        await rollback_manager.end_transaction(transaction_id, "failed")
                
            elif operation_type == "search":
                # Search file content (no rollback needed)
                try:
                    search_result = await content_analyzer.search_content(file_path, request)
                    result["search_results"] = search_result
                    result["success"] = True
                    
                    # End transaction as completed
                    if transaction_id:
                        await rollback_manager.end_transaction(transaction_id, "completed")
                except Exception as e:
                    self._logger.error(f"Error searching file content: {str(e)}")
                    result["error"] = f"Error searching file content: {str(e)}"
                    
                    # End transaction as failed
                    if transaction_id:
                        await rollback_manager.end_transaction(transaction_id, "failed")
                
            elif operation_type == "manipulate":
                # Manipulate file content
                try:
                    manipulation_result = await content_analyzer.manipulate_content(file_path, request)
                    result["manipulation"] = manipulation_result
                    
                    # Apply changes if requested
                    if execute and not dry_run and manipulation_result.get("has_changes", False):
                        # Get confirmation before applying changes
                        confirmed = await self._confirm_file_changes(
                            file_path, 
                            manipulation_result.get("diff", "No changes")
                        )
                        
                        if confirmed:
                            # Read original content for rollback
                            original_content = manipulation_result.get("original_content", "")
                            modified_content = manipulation_result.get("modified_content", "")
                            
                            # Record the content manipulation for rollback
                            if transaction_id:
                                await rollback_manager.record_content_manipulation(
                                    file_path=file_path,
                                    original_content=original_content,
                                    modified_content=modified_content,
                                    instruction=request,
                                    transaction_id=transaction_id
                                )
                            
                            # Write the changes to the file
                            try:
                                with open(file_path, 'w', encoding='utf-8') as f:
                                    f.write(modified_content)
                                result["changes_applied"] = True
                                result["success"] = True
                                
                                # End transaction as completed
                                if transaction_id:
                                    await rollback_manager.end_transaction(transaction_id, "completed")
                            except Exception as e:
                                self._logger.error(f"Error applying changes to {file_path}: {str(e)}")
                                result["error"] = f"Error applying changes: {str(e)}"
                                result["changes_applied"] = False
                                result["success"] = False
                                
                                # End transaction as failed
                                if transaction_id:
                                    await rollback_manager.end_transaction(transaction_id, "failed")
                        else:
                            result["changes_applied"] = False
                            result["cancelled"] = True
                            
                            # End transaction as cancelled
                            if transaction_id:
                                await rollback_manager.end_transaction(transaction_id, "cancelled")
                    elif dry_run and manipulation_result.get("has_changes", False):
                        result["changes_applied"] = False
                        result["success"] = True
                        result["dry_run"] = True
                        
                        # End transaction as completed for dry run
                        if transaction_id:
                            await rollback_manager.end_transaction(transaction_id, "completed")
                    else:
                        # No changes to apply or not executing
                        result["success"] = True
                        if transaction_id:
                            await rollback_manager.end_transaction(transaction_id, "completed")
                except Exception as e:
                    self._logger.error(f"Error manipulating file content: {str(e)}")
                    result["error"] = f"Error manipulating file content: {str(e)}"
                    
                    # End transaction as failed
                    if transaction_id:
                        await rollback_manager.end_transaction(transaction_id, "failed")
                
            else:
                # Unknown operation type
                result["error"] = f"Unknown file operation type: {operation_type}"
                
                # End transaction as failed
                if transaction_id:
                    await rollback_manager.end_transaction(transaction_id, "failed")
            
            return result
            
        except Exception as e:
            # Handle any exceptions and end the transaction
            if transaction_id:
                await rollback_manager.end_transaction(transaction_id, "failed")
            
            self._logger.exception(f"Error processing file content request: {str(e)}")
            
            return {
                "request": request,
                "type": "file_content",
                "context": context,
                "error": f"Error processing file content request: {str(e)}",
                "success": False
            }
    

    
    async def _handle_execution_errors(
        self,
        plan: TaskPlan,
        execution_results: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """
        Handle errors in multi-step execution with recovery.
        
        Args:
            plan: The task plan
            execution_results: Original execution results
            
        Returns:
            Updated execution results after recovery attempts
        """
        # Get error recovery manager
        error_recovery_manager = self._get_error_recovery_manager()
        
        recovered_results = list(execution_results)  # Copy the original results
        
        # Find failed steps
        for i, result in enumerate(execution_results):
            if not result.get("success", False):
                # Get the corresponding step - safely handle potential index errors
                step = None
                if hasattr(plan, 'steps') and isinstance(plan.steps, list) and i < len(plan.steps):
                    step = plan.steps[i]
                elif hasattr(plan, 'steps') and isinstance(plan.steps, dict):
                    # Try to find the step by id if it's a dictionary
                    step_id = result.get('step_id')
                    if step_id and step_id in plan.steps:
                        step = plan.steps[step_id]
                
                # Only attempt recovery if we have a valid step
                if step:
                    # Attempt recovery
                    recovery_result = await error_recovery_manager.handle_error(
                        step, result, {"plan": plan}
                    )
                    
                    # Update the result
                    if recovery_result.get("recovery_success", False):
                        recovered_results[i] = recovery_result
        
        return recovered_results
    

    async def _process_workflow_definition(
        self, 
        request: str, 
        context: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Process a workflow definition request.
        
        Args:
            request: The user request
            context: Context information
            
        Returns:
            Dictionary with processing results
        """
        self._logger.info(f"Processing workflow definition request: {request}")
        
        # Extract workflow information using AI
        workflow_info = await self._extract_workflow_info(request, context)
        
        if not workflow_info or "name" not in workflow_info:
            return {
                "request": request,
                "type": "workflow_definition",
                "context": context,
                "error": "Could not extract workflow information",
                "response": "I couldn't understand the workflow definition. Please provide a name and description."
            }
        
        # Define workflow
        workflow = await workflow_manager.define_workflow_from_natural_language(
            name=workflow_info["name"],
            description=workflow_info.get("description", ""),
            natural_language=workflow_info.get("steps", request),
            context=context
        )
        
        # Return result
        return {
            "request": request,
            "type": "workflow_definition",
            "context": context,
            "workflow": {
                "name": workflow.name,
                "description": workflow.description,
                "steps": [
                    {
                        "command": step.command,
                        "explanation": step.explanation,
                        "optional": step.optional,
                        "requires_confirmation": step.requires_confirmation
                    }
                    for step in workflow.steps
                ],
                "variables": workflow.variables,
                "step_count": len(workflow.steps)
            },
            "success": True,
            "response": f"Successfully defined workflow '{workflow.name}' with {len(workflow.steps)} steps."
        }
    
    async def _process_workflow_execution(
        self, 
        request: str, 
        context: Dict[str, Any], 
        execute: bool, 
        dry_run: bool
    ) -> Dict[str, Any]:
        """
        Process a workflow execution request.
        
        Args:
            request: The user request
            context: Context information
            execute: Whether to execute the workflow
            dry_run: Whether to simulate execution without making changes
            
        Returns:
            Dictionary with processing results
        """
        self._logger.info(f"Processing workflow execution request: {request}")
        
        # Extract workflow name and variables using AI
        workflow_execution_info = await self._extract_workflow_execution_info(request, context)
        
        if not workflow_execution_info or "name" not in workflow_execution_info:
            return {
                "request": request,
                "type": "workflow_execution",
                "context": context,
                "error": "Could not determine workflow name",
                "response": "I couldn't determine which workflow to run. Please specify the workflow name."
            }
        
        workflow_name = workflow_execution_info["name"]
        variables = workflow_execution_info.get("variables", {})
        
        # Check if workflow exists
        workflow = workflow_manager.get_workflow(workflow_name)
        if not workflow:
            available_workflows = workflow_manager.list_workflows()
            if available_workflows:
                workflow_list = ", ".join([w.name for w in available_workflows])
                return {
                    "request": request,
                    "type": "workflow_execution",
                    "context": context,
                    "error": f"Workflow '{workflow_name}' not found",
                    "response": f"Workflow '{workflow_name}' not found. Available workflows: {workflow_list}"
                }
            else:
                return {
                    "request": request,
                    "type": "workflow_execution",
                    "context": context,
                    "error": "No workflows defined",
                    "response": "No workflows have been defined yet. Use 'define workflow' to create one."
                }
        
        result = {
            "request": request,
            "type": "workflow_execution",
            "context": context,
            "workflow": {
                "name": workflow.name,
                "description": workflow.description,
                "steps": [
                    {
                        "command": step.command,
                        "explanation": step.explanation,
                        "optional": step.optional,
                        "requires_confirmation": step.requires_confirmation
                    }
                    for step in workflow.steps
                ],
                "variables": variables,
                "step_count": len(workflow.steps)
            }
        }
        
        # Execute the workflow if requested
        if execute or dry_run:
            # Display workflow with rich formatting
            await self._display_workflow(workflow, variables)
            
            # Get confirmation
            confirmed = await self._confirm_workflow_execution(workflow, variables, dry_run)
            
            if confirmed or dry_run:
                # Execute workflow
                execution_result = await workflow_manager.execute_workflow(
                    workflow_name=workflow_name,
                    variables=variables,
                    context=context,
                    dry_run=dry_run
                )
                
                result["execution_result"] = execution_result
                result["success"] = execution_result.get("success", False)
            else:
                result["cancelled"] = True
                result["success"] = False
        
        return result
    
    async def _process_clarification_request(
        self, 
        request: str, 
        context: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Process a request for clarification.
        
        Args:
            request: The user request
            context: Context information
            
        Returns:
            Dictionary with processing results
        """
        # Get session context for previous commands
        session = session_manager.get_context()
        recent_commands = session.get("recent_commands", [])
        
        # Build prompt for clarification
        prompt = f"""
You are Angela, an AI terminal assistant. The user is asking for clarification regarding a previous interaction.

Recent commands:
{recent_commands}

User request: {request}

Provide a helpful clarification or explanation about the recent commands or operations. Be concise but thorough.
If the user is asking about how to do something, explain the appropriate command or procedure.
"""
        
        # Call AI service
        api_request = GeminiRequest(prompt=prompt, max_tokens=2000)
        response = await gemini_client.generate_text(api_request)
        
        return {
            "request": request,
            "type": "clarification",
            "context": context,
            "response": response.text
        }
    
    async def _process_unknown_request(
        self, 
        request: str, 
        context: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Process an unknown request type.
        
        Args:
            request: The user request
            context: Context information
            
        Returns:
            Dictionary with processing results
        """
        # Try to get a general response from the AI
        prompt = f"""
You are Angela, an AI terminal assistant. The user has made a request that doesn't clearly match a command, multi-step operation, file manipulation, or workflow.

User request: {request}

Provide a helpful response. If appropriate, suggest what kinds of commands or operations might help with what they're trying to do.
Keep your response concise and focused.
"""
        
        # Call AI service
        api_request = GeminiRequest(prompt=prompt, max_tokens=2000)
        response = await gemini_client.generate_text(api_request)
        
        return {
            "request": request,
            "type": "unknown",
            "context": context,
            "response": response.text
        }
    
    async def _get_ai_suggestion(
        self, 
        request: str, 
        context: Dict[str, Any],
        similar_command: Optional[str] = None,
        intent_result: Optional[Any] = None
    ) -> CommandSuggestion:
        """
        Get a command suggestion from the AI service.
        
        Args:
            request: The user request
            context: Context information about the current environment
            similar_command: Optional similar command from history
            intent_result: Optional intent analysis result
            
        Returns:
            A CommandSuggestion object with the suggested command
        """
        # Build prompt with context, including session context if available
        prompt = build_prompt(request, context, similar_command, intent_result)
        
        # Create a request to the Gemini API
        api_request = GeminiRequest(prompt=prompt)
        
        # Call the Gemini API
        self._logger.info("Sending request to Gemini API")
        try:
            api_response = await gemini_client.generate_text(api_request)
            
            # Parse the response
            suggestion = parse_ai_response(api_response.text)
            
            self._logger.info(f"Received suggestion: {suggestion.command}")
            return suggestion
        except Exception as e:
            self._logger.exception(f"Error getting AI suggestion: {str(e)}")
            # Provide a fallback suggestion
            return CommandSuggestion(
                intent="error",
                command=f"echo 'Error generating suggestion: {str(e)}'",
                explanation="This is a fallback command due to an error in the AI service."
            )
    
    async def _extract_file_path(
        self, 
        request: str, 
        context: Dict[str, Any]
    ) -> Optional[Path]:
        """
        Extract a file path from a request using file_resolver.
        
        Args:
            request: The user request
            context: Context information
            
        Returns:
            Path object if found, None otherwise
        """
        self._logger.debug(f"Extracting file path from: {request}")
        
        try:
            # Try to extract file references
            file_references = await file_resolver.extract_references(request, context)
            
            # If we found any resolved references, return the first one
            for reference, path in file_references:
                if path:
                    # Track as viewed file
                    try:
                        file_activity_tracker.track_file_viewing(path, None, {
                            "request": request,
                            "reference": reference
                        })
                    except Exception as e:
                        self._logger.warning(f"Error tracking file viewing: {str(e)}")
                        
                    return path
            
            # If we found references but couldn't resolve them, use AI extraction as fallback
            if file_references:
                for reference, _ in file_references:
                    # Try to resolve with a broader scope
                    try:
                        path = await file_resolver.resolve_reference(
                            reference, 
                            context,
                            search_scope="project"
                        )
                        if path:
                            # Track as viewed file
                            try:
                                file_activity_tracker.track_file_viewing(path, None, {
                                    "request": request,
                                    "reference": reference
                                })
                            except Exception as e:
                                self._logger.warning(f"Error tracking file viewing: {str(e)}")
                                
                            return path
                    except Exception as e:
                        self._logger.warning(f"Error resolving reference '{reference}': {str(e)}")
            
            # If all else fails, try to extract from the request text directly
            file_patterns = [
                r'file[s]?\s+(?:called|named)\s+"([^"]+)"',
                r'file[s]?\s+(?:called|named)\s+\'([^\']+)\'',
                r'file[s]?\s+([a-zA-Z0-9_\-\.]+\.[a-zA-Z0-9]+)',
                r'(?:in|from|to)\s+(?:the\s+)?file[s]?\s+"([^"]+)"',
                r'(?:in|from|to)\s+(?:the\s+)?file[s]?\s+\'([^\']+)\'',
                r'(?:in|from|to)\s+(?:the\s+)?file[s]?\s+([a-zA-Z0-9_\-\.]+\.[a-zA-Z0-9]+)'
            ]
            
            for pattern in file_patterns:
                match = re.search(pattern, request, re.IGNORECASE)
                if match:
                    file_name = match.group(1)
                    # Check if this file exists in the current directory
                    file_path = Path(context.get('cwd', '.')) / file_name
                    if file_path.exists():
                        return file_path
                    
                    # Check if it exists in the project root
                    if 'project_root' in context:
                        project_path = Path(context['project_root']) / file_name
                        if project_path.exists():
                            return project_path
            
            return None
            
        except Exception as e:
            self._logger.error(f"Error extracting file path: {str(e)}")
            return None
    
    async def _determine_file_operation_type(self, request: str) -> str:
        """
        Determine the type of file operation requested.
        
        Args:
            request: The user request
            
        Returns:
            String indicating the operation type: "analyze", "summarize", "search", or "manipulate"
        """
        # Check for keywords indicating different operation types
        
        # Manipulation keywords
        manipulation_keywords = [
            "change", "modify", "update", "edit", "replace", "rename", "refactor",
            "convert", "transform", "add", "remove", "delete", "fix"
        ]
        
        # Analysis keywords
        analysis_keywords = [
            "analyze", "explain", "understand", "evaluate", "assess", "examine",
            "review", "check", "audit"
        ]
        
        # Summarization keywords
        summarization_keywords = [
            "summarize", "summary", "overview", "brief", "digress", "gist"
        ]
        
        # Search keywords
        search_keywords = [
            "find", "search", "locate", "grep", "look for", "identify", "pinpoint"
        ]
        
        # Normalize request text
        normalized = request.lower()
        
        # Check for each type of operation
        for keyword in manipulation_keywords:
            if keyword in normalized:
                return "manipulate"
        
        for keyword in summarization_keywords:
            if keyword in normalized:
                return "summarize"
        
        for keyword in search_keywords:
            if keyword in normalized:
                return "search"
        
        # Default to analysis
        return "analyze"
    
    async def _extract_workflow_info(
        self, 
        request: str, 
        context: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Extract workflow information from a request.
        
        Args:
            request: The user request
            context: Context information
            
        Returns:
            Dictionary with workflow name, description, and steps
        """
        prompt = f"""
Extract information about a workflow definition from this user request:
"{request}"

Return a JSON object with:
1. name: The workflow name
2. description: A brief description of what the workflow does
3. steps: The sequence of steps or commands to include in the workflow

Format:
{{
  "name": "workflow_name",
  "description": "What this workflow does",
  "steps": "Detailed description of steps"
}}

Include only the JSON object with no additional text.
"""
        
        api_request = GeminiRequest(prompt=prompt, max_tokens=1000)
        response = await gemini_client.generate_text(api_request)
        
        try:
            # Extract JSON from the response
            import json
            import re
            
            # Try to find JSON in the response
            json_match = re.search(r'```(?:json)?\s*(.*?)\s*```', response.text, re.DOTALL)
            if json_match:
                json_str = json_match.group(1)
            else:
                # Assume the entire response is JSON
                json_str = response.text
            
            # Parse JSON
            workflow_info = json.loads(json_str)
            return workflow_info
            
        except Exception as e:
            self._logger.error(f"Error extracting workflow info: {str(e)}")
            return {}
    
    async def _extract_workflow_execution_info(
        self, 
        request: str, 
        context: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Extract workflow execution information from a request.
        
        Args:
            request: The user request
            context: Context information
            
        Returns:
            Dictionary with workflow name and variables
        """
        # Get list of available workflows
        available_workflows = workflow_manager.list_workflows()
        workflow_names = [w.name for w in available_workflows]
        
        prompt = f"""
Extract information about a workflow execution from this user request:
"{request}"

Available workflows: {", ".join(workflow_names) if workflow_names else "None"}

Return a JSON object with:
1. name: The workflow name to execute
2. variables: Any variable values to use (as key-value pairs)

Format:
{{
  "name": "workflow_name",
  "variables": {{
    "var1": "value1",
    "var2": "value2"
  }}
}}

Include only the JSON object with no additional text.
"""
        
        api_request = GeminiRequest(prompt=prompt, max_tokens=1000)
        response = await gemini_client.generate_text(api_request)
        
        try:
            # Extract JSON from the response
            import json
            import re
            
            # Try to find JSON in the response
            json_match = re.search(r'```(?:json)?\s*(.*?)\s*```', response.text, re.DOTALL)
            if json_match:
                json_str = json_match.group(1)
            else:
                # Assume the entire response is JSON
                json_str = response.text
            
            # Parse JSON
            execution_info = json.loads(json_str)
            return execution_info
            
        except Exception as e:
            self._logger.error(f"Error extracting workflow execution info: {str(e)}")
            return {}
    
    async def _display_plan(self, plan: Any) -> None:
        """
        Display a task plan with rich formatting.
        
        Args:
            plan: The task plan to display
        """
        # Use the terminal formatter to display the plan
        from angela.api.shell import get_terminal_formatter
        terminal_formatter = get_terminal_formatter()
        await terminal_formatter.display_task_plan(plan)
    
    async def _confirm_plan_execution(self, plan: Any, dry_run: bool) -> bool:
        """
        Get confirmation to execute a plan.
        
        Args:
            plan: The task plan to execute
            dry_run: Whether this is a dry run
            
        Returns:
            True if confirmed, False otherwise
        """
        if dry_run:
            # No confirmation needed for dry run
            return True
        
        # Check if any steps are high risk
        has_high_risk = any(step.estimated_risk >= 3 for step in plan.steps)
        
        # Import here to avoid circular imports
        from rich.console import Console
        from prompt_toolkit.shortcuts import yes_no_dialog
        
        console = Console()
        
        if has_high_risk:
            # Use a more prominent warning for high-risk plans
            console.print(Panel(
                "⚠️  [bold red]This plan includes HIGH RISK operations[/bold red] ⚠️\n"
                "Some of these steps could make significant changes to your system.",
                border_style="red",
                expand=False
            ))
        
        # Get confirmation
        confirmed = yes_no_dialog(
            title="Confirm Plan Execution",
            text=f"Do you want to execute this {len(plan.steps)}-step plan?",
        ).run()
        
        return confirmed
    
    async def _display_workflow(self, workflow: Any, variables: Dict[str, Any]) -> None:
        """
        Display a workflow with rich formatting.
        
        Args:
            workflow: The workflow to display
            variables: Variables for the workflow
        """
        # Import here to avoid circular imports
        from rich.console import Console
        from rich.table import Table
        from rich.panel import Panel
        from rich.syntax import Syntax
        
        console = Console()
        
        # Create a table for the workflow steps
        table = Table(title=f"Workflow: {workflow.name}")
        table.add_column("#", style="cyan")
        table.add_column("Command", style="green")
        table.add_column("Explanation", style="white")
        table.add_column("Options", style="yellow")
        
        # Add steps to the table
        for i, step in enumerate(workflow.steps):
            # Apply variable substitution
            command = step.command
            for var_name, var_value in variables.items():
                # Remove leading $ if present
                clean_name = var_name[1:] if var_name.startswith('$') else var_name
                
                # Substitute ${VAR} syntax
                command = command.replace(f"${{{clean_name}}}", str(var_value))
                
                # Substitute $VAR syntax
                command = command.replace(f"${clean_name}", str(var_value))
            
            options = []
            if step.optional:
                options.append("Optional")
            if step.requires_confirmation:
                options.append("Requires Confirmation")
            
            table.add_row(
                str(i + 1),
                Syntax(command, "bash", theme="monokai", word_wrap=True).markup,
                step.explanation,
                ", ".join(options) if options else ""
            )
        
        # Display the table
        console.print("\n")
        console.print(Panel(
            workflow.description,
            title=f"Workflow: {workflow.name}",
            border_style="blue"
        ))
        console.print(table)
        
        # Display variables if any
        if variables:
            var_table = Table(title="Variables")
            var_table.add_column("Name", style="cyan")
            var_table.add_column("Value", style="green")
            
            for var_name, var_value in variables.items():
                var_table.add_row(var_name, str(var_value))
            
            console.print(var_table)
    
    async def _confirm_workflow_execution(
        self, 
        workflow: Any, 
        variables: Dict[str, Any], 
        dry_run: bool
    ) -> bool:
        """
        Get confirmation to execute a workflow.
        
        Args:
            workflow: The workflow to execute
            variables: Variables for the workflow
            dry_run: Whether this is a dry run
            
        Returns:
            True if confirmed, False otherwise
        """
        if dry_run:
            # No confirmation needed for dry run
            return True
        
        # Check if any steps require confirmation
        requires_confirmation = any(step.requires_confirmation for step in workflow.steps)
        
        # Import here to avoid circular imports
        from rich.console import Console
        from prompt_toolkit.shortcuts import yes_no_dialog
        
        console = Console()
        
        if requires_confirmation:
            # Use a more prominent warning for confirmation-required workflows
            console.print(Panel(
                "⚠️  [bold yellow]This workflow includes steps that require confirmation[/bold yellow] ⚠️\n"
                "Some of these steps could make significant changes.",
                border_style="yellow",
                expand=False
            ))
        
        # Get confirmation
        confirmed = yes_no_dialog(
            title="Confirm Workflow Execution",
            text=f"Do you want to execute workflow '{workflow.name}' with {len(workflow.steps)} steps?",
        ).run()
        
        return confirmed
    
    async def _confirm_file_changes(self, file_path: Path, diff: str) -> bool:
        """
        Get confirmation for file changes.
        
        Args:
            file_path: Path to the file being changed
            diff: Unified diff of the changes
            
        Returns:
            True if confirmed, False otherwise
        """
        # Import here to avoid circular imports
        from rich.console import Console
        from rich.panel import Panel
        from rich.syntax import Syntax
        from prompt_toolkit.shortcuts import yes_no_dialog
        
        console = Console()
        
        # Display the diff
        console.print("\n")
        console.print(Panel(
            f"Proposed changes to {file_path}:",
            title="File Changes",
            border_style="yellow"
        ))
        console.print(Syntax(diff, "diff", theme="monokai"))
        
        # Get confirmation
        confirmed = yes_no_dialog(
            title="Confirm File Changes",
            text=f"Do you want to apply these changes to {file_path}?",
        ).run()
        
        return confirmed
    
    def _start_background_monitoring(self, command: str, error_analysis: Dict[str, Any]) -> None:
        """
        Start background monitoring for a failed command with proper cleanup.
        
        Args:
            command: The failed command
            error_analysis: Analysis of the error
        """
        # Create and start a background task with timeout
        async def monitored_task():
            try:
                # Set a reasonable timeout for monitoring (e.g., 5 minutes)
                timeout = 300  # seconds
                await asyncio.wait_for(
                    self._monitor_for_suggestions(command, error_analysis),
                    timeout=timeout
                )
            except asyncio.TimeoutError:
                self._logger.info(f"Background monitoring timed out after {timeout} seconds")
            except Exception as e:
                self._logger.error(f"Error in background monitoring: {str(e)}")
        
        # Create the task and add it to our set of background tasks
        task = asyncio.create_task(monitored_task())
        
        # Add the task to our set of background tasks
        self._background_tasks.add(task)
        
        # Define a callback to remove the task when it's done and handle any exceptions
        def task_done_callback(task):
            self._background_tasks.discard(task)
            # Check for exceptions that weren't handled inside the task
            if not task.cancelled():
                exception = task.exception()
                if exception:
                    self._logger.error(f"Unhandled exception in background task: {str(exception)}")
        
        # Add the callback
        task.add_done_callback(task_done_callback)
    
    async def _monitor_for_suggestions(self, command: str, error_analysis: Dict[str, Any]) -> None:
        """
        Monitor and provide suggestions for a failed command.
        
        Args:
            command: The failed command
            error_analysis: Analysis of the error
        """
        # Wait a short time before offering suggestions
        await asyncio.sleep(2)
        
        # Import here to avoid circular imports
        from rich.console import Console
        
        console = Console()
        
        # Generate potential fix suggestions
        suggestions = []
        
        # Add suggestions from error analysis
        if "fix_suggestions" in error_analysis:
            suggestions.extend(error_analysis["fix_suggestions"])
        
        # Add historical fixes if available
        if "historical_fixes" in error_analysis:
            suggestions.extend(error_analysis["historical_fixes"])
        
        # If we have suggestions, offer them
        if suggestions:
            console.print("\n")
            console.print("[bold blue]Suggestion:[/bold blue] Try one of these commands to fix the issue:")
            
            for i, suggestion in enumerate(suggestions[:3], 1):  # Limit to top 3
                console.print(f"  {i}. {suggestion}")
            
            console.print("\nUse 'angela try fix 1' to execute the first suggestion, etc.")
    
    async def process_file_operation(
        self, 
        operation: str, 
        parameters: Dict[str, Any],
        dry_run: bool = False
    ) -> Dict[str, Any]:
        """
        Process a file operation request.
        
        Args:
            operation: The type of file operation (e.g., 'create_file', 'read_file').
            parameters: Parameters for the operation.
            dry_run: Whether to simulate the operation without making changes.
            
        Returns:
            A dictionary with the operation results.
        """
        # Use API layer to access execution_engine
        from angela.api.execution import get_execution_engine
        execution_engine = get_execution_engine()
        
        # Execute the file operation
        return await execution_engine.execute_file_operation(operation, parameters, dry_run=dry_run)



    async def _process_universal_cli_request(
        self, 
        request: str, 
        context: Dict[str, Any], 
        execute: bool, 
        dry_run: bool
    ) -> Dict[str, Any]:
        """
        Process a request using the Universal CLI Translator.
        
        Args:
            request: The user request
            context: Context information
            execute: Whether to execute the command
            dry_run: Whether to simulate execution without making changes
            
        Returns:
            Dictionary with processing results
        """
        self._logger.info(f"Processing universal CLI request: {request}")
        
        # Import here to avoid circular imports - use API layer
        from angela.api.toolchain import get_universal_cli_translator
        universal_cli_translator = get_universal_cli_translator()
        
        # Translate the request into a command
        translation = await universal_cli_translator.translate_request(request, context)
        
        if not translation.get("success"):
            return {
                "request": request,
                "type": "universal_cli",
                "context": context,
                "error": translation.get("error", "Failed to translate request"),
                "response": f"I couldn't translate your request into a command: {translation.get('error', 'Unknown error')}"
            }
        
        # Create result structure
        result = {
            "request": request,
            "type": "universal_cli",
            "context": context,
            "command": translation["command"],
            "tool": translation["tool"],
            "subcommand": translation.get("subcommand", ""),
            "explanation": translation.get("explanation", "")
        }
        
        # Execute the command if requested
        if execute or dry_run:
            execution_result = await self.execute_command(
                command=translation["command"],
                natural_request=request,
                explanation=translation.get("explanation", ""),
                dry_run=dry_run
            )
            
            result["execution"] = execution_result
        
        return result
    
    async def _process_complex_workflow(
        self, 
        request: str, 
        context: Dict[str, Any], 
        execute: bool, 
        dry_run: bool
    ) -> Dict[str, Any]:
        """
        Process a complex workflow request involving multiple tools.
        
        Args:
            request: The user request
            context: Context information
            execute: Whether to execute the workflow
            dry_run: Whether to simulate execution without making changes
            
        Returns:
            Dictionary with processing results
        """
        self._logger.info(f"Processing complex workflow: {request}")
        
        # Use the enhanced task planner for complex workflows - using API layer
        from angela.api.intent import get_enhanced_task_planner
        from angela.api.shell import get_terminal_formatter
        from angela.api.execution import get_rollback_manager
        
        enhanced_task_planner = get_enhanced_task_planner()
        terminal_formatter = get_terminal_formatter()
        rollback_manager = get_rollback_manager()
        
        # Generate a complex plan
        plan = await enhanced_task_planner.plan_advanced_task(request, context, max_steps=30)
        
        # Create result structure
        result = {
            "request": request,
            "type": "complex_workflow",
            "context": context,
            "plan": {
                "id": plan.id,
                "goal": plan.goal,
                "description": plan.description,
                "steps": [
                    {
                        "id": step_id,
                        "type": step.type,
                        "description": step.description,
                        "command": step.command,
                        "dependencies": step.dependencies,
                        "risk": step.estimated_risk
                    }
                    for step_id, step in plan.steps.items()
                ],
                "entry_points": plan.entry_points,
                "step_count": len(plan.steps)
            }
        }
        
        # Execute the plan if requested
        if execute or dry_run:
            # Display the plan with rich formatting
            await display_advanced_plan(plan)
            
            # Get confirmation for plan execution
            confirmed = await self._confirm_advanced_plan_execution(plan, dry_run)
            
            if confirmed or dry_run:
                # Execute the plan with transaction support
                transaction_id = None
                if not dry_run:
                    transaction_id = await rollback_manager.start_transaction(f"Complex workflow: {request[:50]}...")
                
                # Set up variables for cross-step communication if needed
                initial_variables = {
                    "request": request,
                    "workflow_type": "complex",
                    "started_at": datetime.now().isoformat(),
                }
                
                # Add context variables that might be useful
                if context.get("project_root"):
                    initial_variables["project_root"] = str(context["project_root"])
                if context.get("project_type"):
                    initial_variables["project_type"] = context["project_type"]
                
                execution_results = await enhanced_task_planner.execute_advanced_plan(
                    plan, 
                    dry_run=dry_run,
                    transaction_id=transaction_id,
                    initial_variables=initial_variables
                )
                
                result["execution_results"] = execution_results
                result["success"] = execution_results.get("success", False)
                
                # Update transaction status
                if transaction_id:
                    status = "completed" if result["success"] else "failed"
                    await rollback_manager.end_transaction(transaction_id, status)
            else:
                result["cancelled"] = True
                result["success"] = False
        
        return result
    

    async def execute_command(
        self, 
        command: str,
        natural_request: str,
        explanation: Optional[str] = None,
        dry_run: bool = False
    ) -> Dict[str, Any]:
        '''
        Execute a command with adaptive behavior based on user context.
        
        Args:
            command: The command to execute
            natural_request: The original natural language request
            explanation: AI explanation of what the command does
            dry_run: Whether to simulate the command without execution
            
        Returns:
            Dictionary with execution results
        '''
        self._logger.info(f"Preparing to execute command: {command}")
        
        # Get current context for hooks
        context = context_manager.get_context_dict()
        
        # Call pre-execution hook
        await execution_hooks.pre_execute_command(command, context)
        
        # Analyze command risk and impact
        risk_level, risk_reason = classify_command_risk(command)
        impact = analyze_command_impact(command)
        
        # Add to session context
        session_manager.add_command(command)
        
        # Generate command preview if needed
        from angela.api.safety import get_command_preview_generator
        from angela.api.context import get_preferences_manager
        
        command_preview_generator = get_command_preview_generator()
        preferences_manager = get_preferences_manager()
        
        preview = await command_preview_generator.generate_preview(command) if preferences_manager.preferences.ui.show_command_preview else None
        
        # Get adaptive confirmation based on risk level and user history
        confirmed = await get_adaptive_confirmation(
            command=command,
            risk_level=risk_level,
            risk_reason=risk_reason,
            impact=impact,
            preview=preview,
            explanation=explanation,
            natural_request=natural_request,
            dry_run=dry_run
        )
        
        if not confirmed and not dry_run:
            self._logger.info(f"Command execution cancelled by user: {command}")
            return {
                "command": command,
                "success": False,
                "cancelled": True,
                "stdout": "",
                "stderr": "Command execution cancelled by user",
                "return_code": 1,
                "dry_run": dry_run
            }
        
        # Execute the command
        result = await self._execute_with_feedback(command, dry_run)
        
        # Call post-execution hook
        await execution_hooks.post_execute_command(command, result, context)
        
        # Add to history
        history_manager.add_command(
            command=command,
            natural_request=natural_request,
            success=result["success"],
            output=result.get("stdout", ""),
            error=result.get("stderr", ""),
            risk_level=risk_level
        )
        
        # If execution failed, analyze error and suggest fixes
        if not result["success"] and result.get("stderr"):
            result["error_analysis"] = error_analyzer.analyze_error(command, result["stderr"])
            result["fix_suggestions"] = error_analyzer.generate_fix_suggestions(command, result["stderr"])
        
        # Offer to learn from successful executions
        if result["success"] and risk_level > 0:
            from angela.api.safety import get_adaptive_confirmation
            adaptive_confirmation = get_adaptive_confirmation()
            await adaptive_confirmation.offer_command_learning(command)
        
        return result


    async def _handle_monitoring_insight(self, insight_type: str, insight_data: Dict[str, Any]):
        """Handle insights from monitoring systems."""
        self._logger.info(f"Received monitoring insight: {insight_type}")
        
        # Store insight in context for future decision making
        session_manager.add_entity(
            f"monitoring_{insight_type}",
            "monitoring_insight", 
            insight_data
        )
        
        # Potentially trigger actions based on insights
        if insight_type == "critical_resource_warning" and insight_data.get("severity") == "high":
            # Take immediate action
            await self._handle_critical_resource_warning(insight_data)

    async def _process_complex_workflow_request(
        self, 
        request: str, 
        context: Dict[str, Any], 
        execute: bool, 
        dry_run: bool
    ) -> Dict[str, Any]:
        """
        Process a complex workflow request involving multiple tools.
        
        This method handles advanced workflows that span multiple tools and services,
        such as complete CI/CD pipelines, end-to-end deployment processes, etc.
        
        Args:
            request: The user request
            context: Context information
            execute: Whether to execute the workflow
            dry_run: Whether to simulate execution without making changes
            
        Returns:
            Dictionary with processing results
        """
        self._logger.info(f"Processing complex workflow request: {request}")
        
        # Import here to avoid circular imports - use API layer
        from angela.api.intent import get_complex_workflow_planner
        from angela.api.shell import get_terminal_formatter
        from angela.api.execution import get_rollback_manager
        
        complex_workflow_planner = get_complex_workflow_planner()
        terminal_formatter = get_terminal_formatter()
        rollback_manager = get_rollback_manager()
        
        # Start a transaction for this complex workflow
        transaction_id = None
        if not dry_run and execute:
            transaction_id = await rollback_manager.start_transaction(f"Complex workflow: {request[:50]}...")
        
        try:
            # Generate a workflow plan
            workflow_plan = await complex_workflow_planner.plan_complex_workflow(
                request=request,
                context=context,
                max_steps=50  # Allow more steps for complex workflows
            )
            
            # Create result structure
            result = {
                "request": request,
                "type": "complex_workflow",
                "context": context,
                "workflow_plan": {
                    "id": workflow_plan.id,
                    "name": workflow_plan.name,
                    "description": workflow_plan.description,
                    "steps_count": len(workflow_plan.steps),
                    "tools": self._extract_unique_tools(workflow_plan),
                    "estimated_duration": self._estimate_workflow_duration(workflow_plan)
                }
            }
            
            # Execute the workflow if requested
            if execute or dry_run:
                # Display the workflow plan
                await terminal_formatter.display_complex_workflow_plan(workflow_plan)
                
                # Get confirmation from user unless forced
                confirmed = True
                if not context.get("session", {}).get("force_execution", False):
                    confirmed = await self._confirm_complex_workflow_execution(workflow_plan, dry_run)
                
                if confirmed or dry_run:
                    # Execute the workflow with transaction support
                    execution_results = await complex_workflow_planner.execute_complex_workflow(
                        workflow_plan, 
                        dry_run=dry_run,
                        transaction_id=transaction_id
                    )
                    
                    result["execution_results"] = execution_results
                    result["success"] = execution_results.get("success", False)
                    
                    # End transaction based on result
                    if transaction_id:
                        status = "completed" if result["success"] else "failed"
                        await rollback_manager.end_transaction(transaction_id, status)
                else:
                    # User cancelled execution
                    result["cancelled"] = True
                    result["success"] = False
                    
                    # End transaction as cancelled
                    if transaction_id:
                        await rollback_manager.end_transaction(transaction_id, "cancelled")
            
            return result
            
        except Exception as e:
            # Handle any exceptions and end the transaction
            self._logger.exception(f"Error processing complex workflow: {str(e)}")
            
            if transaction_id:
                await rollback_manager.end_transaction(transaction_id, "failed")
            
            return {
                "request": request,
                "type": "complex_workflow",
                "context": context,
                "error": str(e),
                "success": False
            }
    
    def _extract_unique_tools(self, workflow_plan: Any) -> List[str]:
        """
        Extract the unique tools used in a workflow plan.
        
        Args:
            workflow_plan: The workflow plan
            
        Returns:
            List of unique tools
        """
        import shlex  # Make sure shlex is imported
        
        tools = set()
        
        # Safely check if steps exists and is iterable
        if not hasattr(workflow_plan, 'steps') or not workflow_plan.steps:
            return []
        
        # Handle both dictionary and list step structures
        steps = []
        if isinstance(workflow_plan.steps, dict):
            steps = workflow_plan.steps.items()
        elif isinstance(workflow_plan.steps, list):
            steps = [(i, step) for i, step in enumerate(workflow_plan.steps)]
        else:
            # Unknown structure, return empty list
            return []
        
        for step_id, step in steps:
            if hasattr(step, "tool") and step.tool:
                tools.add(step.tool)
            elif hasattr(step, "type") and step.type == "TOOL" and hasattr(step, "tool") and step.tool:
                tools.add(step.tool)
            elif hasattr(step, "command") and step.command:
                # Try to extract tool from command
                try:
                    cmd_parts = shlex.split(step.command)
                    if cmd_parts:
                        tools.add(cmd_parts[0])
                except Exception:
                    # If shlex.split fails, just use the first word as a fallback
                    first_word = step.command.split()[0] if step.command.split() else ""
                    if first_word:
                        tools.add(first_word)
        
        return sorted(list(tools))
    
    def _estimate_workflow_duration(self, workflow_plan: Any) -> int:
        """
        Estimate the duration of a workflow in seconds.
        
        Args:
            workflow_plan: The workflow plan
            
        Returns:
            Estimated duration in seconds
        """
        # Base duration per step type
        step_durations = {
            "COMMAND": 10,  # Simple commands take ~10 seconds
            "TOOL": 30,     # Tool commands might take longer
            "API": 15,      # API calls typically take 15 seconds
            "FILE": 5,      # File operations are usually fast
            "WAIT": 30,     # Wait steps default to 30 seconds
            "DECISION": 2,  # Decisions are quick
            "VALIDATION": 5,  # Validations are usually fast
            "PARALLEL": 40,  # Parallel steps might take longer
            "CUSTOM_CODE": 20,  # Custom code execution
            "NOTIFICATION": 2   # Notifications are instantaneous
        }
        
        total_duration = 0
        
        for step_id, step in workflow_plan.steps.items():
            step_type = getattr(step, "type", "COMMAND")
            
            # Get base duration for this step type
            duration = step_durations.get(step_type, 10)
            
            # Adjust for specific commands or operations
            if hasattr(step, "command") and step.command:
                cmd = step.command.lower()
                
                # Long-running processes
                if any(pattern in cmd for pattern in ["build", "compile", "install", "test", "deploy"]):
                    duration = max(duration, 60)  # At least a minute
                
                # Very long running processes
                if any(pattern in cmd for pattern in ["docker build", "mvn install", "npm build", "deployment"]):
                    duration = max(duration, 300)  # At least 5 minutes
            
            # For wait steps, use the actual timeout if specified
            if step_type == "WAIT" and hasattr(step, "timeout") and step.timeout:
                duration = max(duration, step.timeout)
            
            total_duration += duration
        
        # Adjust for parallel execution
        # This is a simplification - we're not building a full dependency graph
        parallel_reduction = 0
        parallel_steps = sum(1 for step in workflow_plan.steps.values() 
                            if getattr(step, "type", "") == "PARALLEL")
        
        if parallel_steps > 0:
            # Rough estimate - each parallel step reduces total time by ~20%
            parallel_reduction = total_duration * (0.2 * min(parallel_steps, 3))
        
        # Apply the reduction, but ensure we don't go below 10 seconds
        return max(10, int(total_duration - parallel_reduction))
    
    async def _confirm_complex_workflow_execution(self, workflow_plan: Any, dry_run: bool) -> bool:
        """
        Get confirmation for executing a complex workflow.
        
        Args:
            workflow_plan: The workflow plan
            dry_run: Whether this is a dry run
            
        Returns:
            True if confirmed, False otherwise
        """
        if dry_run:
            return True  # No confirmation needed for dry run
        
        # Analyze the workflow risk level
        high_risk_steps = []
        for step_id, step in workflow_plan.steps.items():
            risk_level = getattr(step, "risk_level", 0)
            if risk_level >= 3:  # High risk
                high_risk_steps.append((step_id, getattr(step, "name", f"Step {step_id}")))
        
        # Import here to avoid circular imports
        from rich.console import Console
        from rich.panel import Panel
        from prompt_toolkit.shortcuts import yes_no_dialog
        
        console = Console()
        
        # Display a warning for high-risk steps
        if high_risk_steps:
            warning_text = "This workflow includes HIGH RISK operations:\n\n"
            for step_id, step_name in high_risk_steps:
                warning_text += f"• {step_name} ({step_id})\n"
            warning_text += "\nSome of these steps could make significant changes to your system."
            
            console.print(Panel(
                warning_text,
                title="⚠️ Warning: High Risk Operations ⚠️",
                border_style="red",
                expand=False
            ))
        
        # Display workflow scope and impact
        tools = self._extract_unique_tools(workflow_plan)
        estimated_duration = self._estimate_workflow_duration(workflow_plan)
        
        scope_text = f"This workflow will use {len(tools)} different tools: {', '.join(tools)}\n"
        scope_text += f"Estimated duration: {int(estimated_duration/60)} minutes {estimated_duration%60} seconds\n"
        scope_text += f"Step count: {len(workflow_plan.steps)}"
        
        console.print(Panel(
            scope_text,
            title="Workflow Scope",
            border_style="blue",
            expand=False
        ))
        
        # Get confirmation
        confirmed = yes_no_dialog(
            title="Confirm Complex Workflow Execution",
            text=f"Do you want to execute this complex workflow with {len(workflow_plan.steps)} steps?",
        ).run()
        
        return confirmed

    async def _execute_with_feedback(self, command: str, dry_run: bool = False) -> Dict[str, Any]:
        """
        Execute a command with real-time feedback.
        
        Args:
            command: The command to execute
            dry_run: Whether to simulate execution without making changes
            
        Returns:
            Dictionary with execution results
        """
        self._logger.info(f"{'Dry run of' if dry_run else 'Executing'} command: {command}")
        
        if dry_run:
            return {
                "command": command,
                "success": True,
                "stdout": f"[DRY RUN] Would execute: {command}",
                "stderr": "",
                "return_code": 0,
                "dry_run": True
            }
        
        # Use the execution engine to run the command - using API layer
        from angela.api.execution import get_execution_engine
        execution_engine = get_execution_engine()
        
        stdout, stderr, exit_code = await execution_engine.execute_command(
            command=command,
            check_safety=True
        )
        
        return {
            "command": command,
            "success": exit_code == 0,
            "stdout": stdout,
            "stderr": stderr,
            "return_code": exit_code,
            "dry_run": False
        }


    async def _handle_critical_resource_warning(self, warning_data: Dict[str, Any]) -> None:
        """
        Handle a critical resource warning from the monitoring system.
        
        Args:
            warning_data: Warning data from the monitoring system
        """
        self._logger.warning(f"Handling critical resource warning: {warning_data}")
        
        # Log the warning
        resource_type = warning_data.get("resource_type", "unknown")
        resource_name = warning_data.get("resource_name", "unknown")
        severity = warning_data.get("severity", "unknown")
        
        # Take action based on resource type
        if resource_type == "memory" and severity == "high":
            # Suggest garbage collection or process restart
            from rich.console import Console
            console = Console()
            console.print(f"\n[bold red]Warning:[/bold red] High memory usage detected for {resource_name}")
            console.print("Consider freeing up memory or restarting resource-intensive processes.")
        
        elif resource_type == "disk" and severity == "high":
            # Suggest disk cleanup
            from rich.console import Console
            console = Console()
            console.print(f"\n[bold red]Warning:[/bold red] Low disk space detected for {resource_name}")
            console.print("Consider removing temporary files or unused artifacts.")
        
        elif resource_type == "cpu" and severity == "high":
            # Suggest process throttling
            from rich.console import Console
            console = Console()
            console.print(f"\n[bold red]Warning:[/bold red] High CPU usage detected for {resource_name}")
            console.print("Consider throttling or pausing resource-intensive processes.")
        
        return None  

    # Functions to add to your orchestrator:
    
    async def detect_pipeline_opportunities(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """
        Detect opportunities for CI/CD pipelines in the current context.
        
        Args:
            context: Context information
            
        Returns:
            Dictionary with pipeline opportunities
        """
        self._logger.info("Detecting CI/CD pipeline opportunities")
        
        # Get ci_cd_integration through API layer 
        from angela.api.toolchain import get_ci_cd_integration
        ci_cd_integration = get_ci_cd_integration()
        
        if not ci_cd_integration:
            return {
                "success": False,
                "error": "CI/CD Integration component not available"
            }
        
        project_root = context.get("project_root")
        if not project_root:
            return {
                "success": False,
                "error": "No project root detected in context"
            }
        
        return await ci_cd_integration.detect_project_type(project_root)
    
    async def suggest_complex_workflow(self, request: str, context: Dict[str, Any]) -> Dict[str, Any]:
        """
        Suggest a complex workflow based on a natural language request.
        
        Args:
            request: Natural language request
            context: Context information
            
        Returns:
            Dictionary with suggested workflow information
        """
        self._logger.info(f"Suggesting complex workflow for: {request}")
        
        # Get workflow_engine through API layer
        from angela.api.toolchain import get_cross_tool_workflow_engine
        workflow_engine = get_cross_tool_workflow_engine()
        
        if not workflow_engine:
            return {
                "success": False,
                "error": "Cross Tool Workflow Engine component not available"
            }
        
        # Create a workflow using the engine
        try:
            workflow = await workflow_engine.create_workflow(
                request=request,
                context=context
            )
            
            # Convert to dictionary for return
            workflow_dict = workflow.dict() if hasattr(workflow, "dict") else workflow
            
            return {
                "success": True,
                "workflow": workflow_dict
            }
        except Exception as e:
            self._logger.error(f"Error generating workflow: {str(e)}")
            return {
                "success": False,
                "error": f"Failed to generate workflow: {str(e)}"
            }
    
    async def execute_cross_tool_workflow(
        self,
        request: str,
        context: Dict[str, Any],
        suggested_tools: Optional[List[str]] = None,
        dry_run: bool = False
    ) -> Dict[str, Any]:
        """
        Execute a workflow involving multiple tools.
        
        Args:
            request: Natural language request
            context: Context information
            suggested_tools: Optional list of suggested tools to use
            dry_run: Whether to simulate execution without making changes
            
        Returns:
            Dictionary with execution results
        """
        self._logger.info(f"Executing cross-tool workflow: {request}")
        
        # Get workflow_engine through API layer
        from angela.api.toolchain import get_cross_tool_workflow_engine
        workflow_engine = get_cross_tool_workflow_engine()
        
        if not workflow_engine:
            return {
                "success": False,
                "error": "Cross Tool Workflow Engine component not available"
            }
        
        # Create and execute workflow
        try:
            # Create a workflow
            workflow = await workflow_engine.create_workflow(
                request=request,
                context=context,
                tools=suggested_tools
            )
            
            # Execute the workflow
            return await workflow_engine.execute_workflow(
                workflow=workflow,
                dry_run=dry_run
            )
        except Exception as e:
            self._logger.error(f"Error executing cross-tool workflow: {str(e)}")
            return {
                "success": False,
                "error": f"Failed to execute cross-tool workflow: {str(e)}"
            }
            

# Global orchestrator instance
orchestrator = Orchestrator()

# Synchronous wrapper for backwards compatibility
def process_request(request: str) -> Dict[str, Any]:
    """Synchronous wrapper for processing a request."""
    return asyncio.run(orchestrator.process_request(request))
</file>

</files>
